{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/TatarNLP/tat_monocorpus_v2Lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1526a3",
      "metadata": {
        "id": "3b1526a3"
      },
      "outputs": [],
      "source": [
        "%pip install datasets transformers accelerate bitsandbytes matplotlib sacrebleu huggingface_hub wandb torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49411090",
      "metadata": {
        "id": "49411090"
      },
      "outputs": [],
      "source": [
        "import transformers,  datasets, torch, huggingface_hub, matplotlib, bitsandbytes, accelerate, sacrebleu, wandb\n",
        "print(\"Всё установлено!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d845d1f",
      "metadata": {
        "id": "0d845d1f"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "print('Ok')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee61084",
      "metadata": {
        "id": "5ee61084"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a95d28b5",
      "metadata": {
        "id": "a95d28b5"
      },
      "outputs": [],
      "source": [
        "print(52)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75f26c2",
      "metadata": {
        "id": "c75f26c2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # отключаем GPU\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Загрузка датасета...\")\n",
        "ds = load_dataset(\"IPSAN/tat_monocorpus_v2\", split=\"train\")\n",
        "print(f\"Датасет загружен, размер: {len(ds)}\")\n",
        "\n",
        "# Загружаем токенизатор\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_small(examples):\n",
        "    # Правильно: обрабатываем все тексты в батче\n",
        "    return tokenizer(\n",
        "        examples[\"txt\"],           # список всех текстов в батче\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "print(\"Токенизация 10000 примеров (батчами по 1000)...\")\n",
        "small = ds.select(range(10000)).map(\n",
        "    tokenize_small,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=ds.column_names\n",
        ")\n",
        "print(\"✅ Токенизация 10000 примеров выполнена\")\n",
        "print(f\"Размер токенизированного датасета: {len(small)}\")\n",
        "print(f\"Пример: {small[0]['input_ids'][:10]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efcd49bb",
      "metadata": {
        "id": "efcd49bb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2895ee66",
      "metadata": {
        "id": "2895ee66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6777d57e",
      "metadata": {
        "id": "6777d57e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2977852a",
      "metadata": {
        "id": "2977852a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa90da5",
      "metadata": {
        "id": "2fa90da5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169771ab",
      "metadata": {
        "id": "169771ab",
        "outputId": "816d469a-b11e-46ae-b0b4-6b2b2d59b370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "251\n"
          ]
        }
      ],
      "source": [
        "print(251)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c7d9bd",
      "metadata": {
        "id": "b7c7d9bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697291cc",
      "metadata": {
        "id": "697291cc",
        "outputId": "0afce417-cf67-46be-d4e2-5d87a6a700ae",
        "colab": {
          "referenced_widgets": [
            "9f06f02578844c40a9d42cb2abfd9f0e",
            "05ed7ac295f84c779995b9c8d0695e4a",
            "2ecaab8a0a304fabacea5270e344bd62",
            "46ca17d37d8f41e5ace7ad8013184a80",
            "693a399bd4f8470da5ec643c7579133f",
            "bf2c6ce535b144c2b92b0e0f1cff8be1",
            "45b1f5cdf34d4faeb47f0d885c678c8e",
            "3b22d9dea2d6494b87416e11446d7d31",
            "9921f9126e1146f0a10acbad6c8c9809",
            "269ada14cd7d46339e2297d53c7f7316",
            "da8d2926bc1d4971a845a54307512810",
            "fde73b90e0fe4e358093d8aee5bf2538",
            "7fd77add3d7545779bdb35ac04845dd4",
            "7d4e7007b64f4f48a4ce24a7f4abafac",
            "5e46b86fb4d84a59829d3693a8498d14",
            "906fd1b8130c4e20a1b61912217bddc8",
            "e5df49b42806481e858b3821efe19f34",
            "8e61906238724031ae1486d2b85a306f",
            "41577385b9e94d4599232b061076b25f",
            "ad9209c58fb34ba093a12f1a66d94889",
            "e3ca9f7a119c4a258e131bc013a78c2e",
            "6a41c7acab0945ecae372b2d137fcd63",
            "939ced2d43d14b63b3efaf33e0669068",
            "55253a02d51d43a896e306ee1a259e50",
            "379174604ce4455f9802020e9177f8f5",
            "5c9c8483c1f847efa2d0f38a250d1fea",
            "23afc69a00e047b486fa03ec2d3fe233",
            "0eba14368d394ed89724c43b1d4e3d38",
            "289d4b2322214fa090e50f1a499c1276",
            "6c2fbd84b4664d0a8a040d74052c59b8",
            "a398c184caa2476692d1d1188f6da7d2",
            "d930ba6654e64ff6bcc3f0614a324750",
            "c07f337fea9b4c6793f703f099b56103",
            "f03e44edee9c466aa9aaba2e29914564",
            "1f5858c976214654a214054831c84b6b",
            "791c7d4db1cf453ba7ec354f048679ea",
            "a9b4cb95ca9c48e2b0c5979836c3d2dd",
            "ce1540d4dbe440ab89cc153afe7b8653",
            "18132fe8de774f25b8abe489976fd993",
            "9786753820184f7dbdf37cefd069d836",
            "292cbfcdf6864c728e629b95162cca72",
            "a163efb055274e63aacb47404bf955b4",
            "284c4df585ae4b028032bd63c7150908",
            "b73adf4315f34ddcb6e12adab3121f2b",
            "e06cca5560b549b095efe48f0555ddbb",
            "747dbda362274ee48551042fef4e8f5f",
            "7e4c4fdb948a4e1f815d43eaaf962455",
            "201f1e1f4ba641d5baded69fe4f12de7",
            "4e9d8058290e4251ac68461eae925139",
            "238311a94eba40008283b4d32493eba8",
            "f20e68a2bbb84a14947f9b8eea317a10",
            "1b100aa4d6ef4c909171bdfc4bbf45e1",
            "bcf66864114b4acf9f502be797e6f69d",
            "0234a48331084efb8c50cd846b8c7df8",
            "0e76241dd89f4c02b5b69b9f6f5bb708",
            "74e2d59975a14b909f7660b643ed3f78",
            "9075e2e2f0a74694b4dbb45d18074241",
            "c609fbf136f94416a3262150297fe268",
            "5a3f91e9c4c649b899a8d4f398d620b4",
            "5328f9bd15e54e6db821e709776c6b43",
            "1d29e9382ec24d40ab045b9d7f83f324",
            "237b84def6b546a59df117a6a2abb2f9",
            "b028d24e8713453da7d41a3385ead121",
            "1263c08529d9409e9747810746d7729a",
            "14d1960608a3495f90c00c452887b9af",
            "3a894eec6c7f4e9e87e4c740b0e0372d",
            "62d60531d5344ef6ace41d95d6b3538e",
            "371d6afb23124f67b00c119714def9dc",
            "fda5e90de4fd45d1b606a85e6dc08727",
            "fc19d1cc5767416eacf10e3264e99574",
            "74ed3a2522204507b3c6782ab8542234",
            "abbce5f702e84273be389eb44a76ddb3",
            "be64ba54cb674705be9ba1247f9a83b9",
            "48e803c959294a10b48dafb0c6fb72b0",
            "77e82ee3dfb24db383f7414f5b58f929"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:07:46,523 - INFO - Результаты будут сохранены в: tat_results_final_rf/run_20260223_070746\n",
            "2026-02-23 07:07:46,541 - INFO - Устройство: cuda\n",
            "2026-02-23 07:07:46,542 - INFO - Доступная RAM: svmem(total=67415203840, available=64813182976, percent=3.9, used=2602020864, free=20724514816, active=33844424704, inactive=11157147648, buffers=67960832, cached=44776775680, shared=438272, slab=1400262656)\n",
            "2026-02-23 07:07:46,560 - INFO - GPU: Tesla V100-SXM2-32GB\n",
            "2026-02-23 07:07:46,561 - INFO - Память GPU: 34.079899648 GB\n",
            "2026-02-23 07:07:46,853 - INFO - HTTP Request: GET https://huggingface.co/api/whoami-v2 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:46,857 - INFO - Успешный вход в Hugging Face Hub\n",
            "2026-02-23 07:07:46,857 - INFO - 1. Загрузка датасета IPSAN/tat_monocorpus_v2...\n",
            "2026-02-23 07:07:47,030 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/IPSAN/tat_monocorpus_v2/resolve/main/README.md \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:47,206 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/IPSAN/tat_monocorpus_v2/resolve/798889481b3fc62cd1002c79194fabadd66735fd/tat_monocorpus_v2.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:07:47,713 - INFO - HTTP Request: HEAD https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/IPSAN/tat_monocorpus_v2/IPSAN/tat_monocorpus_v2.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:07:47,889 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/IPSAN/tat_monocorpus_v2/revision/798889481b3fc62cd1002c79194fabadd66735fd \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:48,073 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/IPSAN/tat_monocorpus_v2/resolve/798889481b3fc62cd1002c79194fabadd66735fd/.huggingface.yaml \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:07:48,415 - INFO - HTTP Request: GET https://datasets-server.huggingface.co/info?dataset=IPSAN/tat_monocorpus_v2 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:48,592 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/IPSAN/tat_monocorpus_v2/tree/798889481b3fc62cd1002c79194fabadd66735fd/data?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:48,761 - INFO - HTTP Request: GET https://huggingface.co/api/datasets/IPSAN/tat_monocorpus_v2/tree/798889481b3fc62cd1002c79194fabadd66735fd?recursive=false&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:48,933 - INFO - HTTP Request: HEAD https://huggingface.co/datasets/IPSAN/tat_monocorpus_v2/resolve/798889481b3fc62cd1002c79194fabadd66735fd/dataset_infos.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:07:48,971 - INFO - Всего записей: 229946\n",
            "2026-02-23 07:07:48,974 - INFO - Используется 1000 записей (подвыборка)\n",
            "2026-02-23 07:07:48,975 - INFO - Колонка с текстом: txt\n",
            "2026-02-23 07:07:48,976 - INFO - Обрезка текстов до 2000 символов...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f06f02578844c40a9d42cb2abfd9f0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:07:49,035 - INFO - Разделение на train/val/test...\n",
            "2026-02-23 07:07:49,048 - INFO - Размеры: {'train': 900, 'eval': 50, 'test': 50}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05ed7ac295f84c779995b9c8d0695e4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ecaab8a0a304fabacea5270e344bd62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46ca17d37d8f41e5ace7ad8013184a80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:07:49,136 - INFO - Сырой датасет сохранён.\n",
            "2026-02-23 07:07:49,137 - INFO - Исходные колонки датасета: ['txt', '__key__', '__url__']\n",
            "2026-02-23 07:07:49,604 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:50,153 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:50,322 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilgpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:07:51,322 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilgpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:07:51,491 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilgpt2/tree/main?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:07:51,663 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilgpt2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:07:52,107 - INFO - Токенизация для gpt2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "693a399bd4f8470da5ec643c7579133f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf2c6ce535b144c2b92b0e0f1cff8be1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45b1f5cdf34d4faeb47f0d885c678c8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b22d9dea2d6494b87416e11446d7d31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9921f9126e1146f0a10acbad6c8c9809",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "269ada14cd7d46339e2297d53c7f7316",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:07,152 - INFO - Токенизированный датасет (gpt2) сохранён.\n",
            "2026-02-23 07:08:07,485 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:07,527 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:07,704 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:07,747 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:07,920 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-7B-Instruct/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:08:08,096 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-7B-Instruct/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:08,985 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-7B-Instruct \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:08,987 - INFO - Токенизация для qwen...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da8d2926bc1d4971a845a54307512810",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fde73b90e0fe4e358093d8aee5bf2538",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd77add3d7545779bdb35ac04845dd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d4e7007b64f4f48a4ce24a7f4abafac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e46b86fb4d84a59829d3693a8498d14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "906fd1b8130c4e20a1b61912217bddc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:29,238 - INFO - Токенизированный датасет (qwen) сохранён.\n",
            "2026-02-23 07:08:29,494 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:29,536 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:29,712 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:29,753 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:29,925 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:29,966 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:30,134 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:08:30,340 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:31,270 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:31,310 - INFO - Токенизация для deepseek...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5df49b42806481e858b3821efe19f34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e61906238724031ae1486d2b85a306f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41577385b9e94d4599232b061076b25f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad9209c58fb34ba093a12f1a66d94889",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3ca9f7a119c4a258e131bc013a78c2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a41c7acab0945ecae372b2d137fcd63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:35,934 - INFO - Токенизированный датасет (deepseek) сохранён.\n",
            "2026-02-23 07:08:36,105 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:36,274 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:36,450 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "939ced2d43d14b63b3efaf33e0669068",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:36,738 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:36,908 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:37,083 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer_config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55253a02d51d43a896e306ee1a259e50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:37,290 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:37,357 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:37,534 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:08:37,733 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:38,157 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:38,329 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:38,511 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "379174604ce4455f9802020e9177f8f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:38,890 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/tokenizer.model \"HTTP/1.1 302 Found\"\n",
            "2026-02-23 07:08:39,325 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/xet-read-token/caa1feb0e54d415e2df31207e5f4e273e33509b1 \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c9c8483c1f847efa2d0f38a250d1fea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:41,218 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:08:41,392 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:08:41,562 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:41,736 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/special_tokens_map.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23afc69a00e047b486fa03ec2d3fe233",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:41,931 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 07:08:42,204 - INFO - Токенизация для mistral...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0eba14368d394ed89724c43b1d4e3d38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "289d4b2322214fa090e50f1a499c1276",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c2fbd84b4664d0a8a040d74052c59b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=1):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a398c184caa2476692d1d1188f6da7d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d930ba6654e64ff6bcc3f0614a324750",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c07f337fea9b4c6793f703f099b56103",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:49,164 - INFO - Токенизированный датасет (mistral) сохранён.\n",
            "2026-02-23 07:08:49,167 - INFO - \n",
            "============================================================\n",
            "2026-02-23 07:08:49,168 - INFO - ЗАПУСК ЭКСПЕРИМЕНТОВ\n",
            "2026-02-23 07:08:49,169 - INFO - ============================================================\n",
            "2026-02-23 07:08:49,174 - INFO - \n",
            "Эксперимент 1/45: distilgpt2_baseline (seed=42)\n",
            "2026-02-23 07:08:49,175 - INFO - --- Попытка distilgpt2_baseline (seed=42) с batch_size=8 ---\n",
            "2026-02-23 07:08:49,176 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:08:49,422 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:49,620 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f03e44edee9c466aa9aaba2e29914564",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:08:50,734 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:08:50,904 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.070253</td>\n",
              "      <td>1.660342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.605572</td>\n",
              "      <td>1.516985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.498986</td>\n",
              "      <td>1.446663</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:09:28,909 - INFO - ✅ Успех! Loss=1.3807, PPL=3.98, время=39.7с, пик GPU=1.98GB\n",
            "2026-02-23 07:09:29,161 - INFO - \n",
            "Эксперимент 2/45: distilgpt2_baseline (seed=43)\n",
            "2026-02-23 07:09:29,161 - INFO - --- Попытка distilgpt2_baseline (seed=43) с batch_size=8 ---\n",
            "2026-02-23 07:09:29,162 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:09:29,792 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:09:29,966 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f5858c976214654a214054831c84b6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:09:30,328 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:09:30,504 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.073398</td>\n",
              "      <td>1.669530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.617984</td>\n",
              "      <td>1.506320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.477503</td>\n",
              "      <td>1.439403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:10:04,488 - INFO - ✅ Успех! Loss=1.3763, PPL=3.96, время=35.3с, пик GPU=1.98GB\n",
            "2026-02-23 07:10:04,821 - INFO - \n",
            "Эксперимент 3/45: distilgpt2_baseline (seed=44)\n",
            "2026-02-23 07:10:04,822 - INFO - --- Попытка distilgpt2_baseline (seed=44) с batch_size=8 ---\n",
            "2026-02-23 07:10:04,823 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:10:05,364 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:10:05,545 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "791c7d4db1cf453ba7ec354f048679ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:10:06,644 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:10:06,819 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.062085</td>\n",
              "      <td>1.651844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.587455</td>\n",
              "      <td>1.505263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.494160</td>\n",
              "      <td>1.434469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:10:37,061 - INFO - ✅ Успех! Loss=1.3746, PPL=3.95, время=32.2с, пик GPU=1.98GB\n",
            "2026-02-23 07:10:37,385 - INFO - \n",
            "Эксперимент 4/45: distilgpt2_lora_r8 (seed=42)\n",
            "2026-02-23 07:10:37,385 - INFO - --- Попытка distilgpt2_lora_r8 (seed=42) с batch_size=8 ---\n",
            "2026-02-23 07:10:37,386 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:10:38,004 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:10:38,404 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9b4cb95ca9c48e2b0c5979836c3d2dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:10:41,340 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:10:41,514 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.956595</td>\n",
              "      <td>2.689285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.658339</td>\n",
              "      <td>2.454704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.515966</td>\n",
              "      <td>2.357127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:04,526 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:04,704 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:04,714 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/distilgpt2_lora_r8/seed42/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:04,843 - INFO - ✅ Успех! Loss=2.2936, PPL=9.91, время=27.5с, пик GPU=0.82GB\n",
            "2026-02-23 07:11:05,164 - INFO - \n",
            "Эксперимент 5/45: distilgpt2_lora_r8 (seed=43)\n",
            "2026-02-23 07:11:05,165 - INFO - --- Попытка distilgpt2_lora_r8 (seed=43) с batch_size=8 ---\n",
            "2026-02-23 07:11:05,165 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:11:05,334 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:05,743 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce1540d4dbe440ab89cc153afe7b8653",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:07,854 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:08,040 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.944527</td>\n",
              "      <td>2.678255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.658546</td>\n",
              "      <td>2.452012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.500273</td>\n",
              "      <td>2.353292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:34,257 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:34,427 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:34,435 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/distilgpt2_lora_r8/seed43/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:34,563 - INFO - ✅ Успех! Loss=2.2928, PPL=9.90, время=29.4с, пик GPU=0.82GB\n",
            "2026-02-23 07:11:34,957 - INFO - \n",
            "Эксперимент 6/45: distilgpt2_lora_r8 (seed=44)\n",
            "2026-02-23 07:11:34,958 - INFO - --- Попытка distilgpt2_lora_r8 (seed=44) с batch_size=8 ---\n",
            "2026-02-23 07:11:34,958 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:11:35,128 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:35,296 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18132fe8de774f25b8abe489976fd993",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:36,156 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:38,036 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.941706</td>\n",
              "      <td>2.667173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.634535</td>\n",
              "      <td>2.437067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.507823</td>\n",
              "      <td>2.342896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:57,766 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:57,937 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:57,945 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/distilgpt2_lora_r8/seed44/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:11:58,076 - INFO - ✅ Успех! Loss=2.2809, PPL=9.79, время=23.1с, пик GPU=0.82GB\n",
            "2026-02-23 07:11:58,365 - INFO - \n",
            "Эксперимент 7/45: distilgpt2_lora_r16 (seed=42)\n",
            "2026-02-23 07:11:58,366 - INFO - --- Попытка distilgpt2_lora_r16 (seed=42) с batch_size=8 ---\n",
            "2026-02-23 07:11:58,367 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:11:58,534 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:11:58,707 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9786753820184f7dbdf37cefd069d836",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:02,330 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:02,553 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.872146</td>\n",
              "      <td>2.545855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.512048</td>\n",
              "      <td>2.278636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.355520</td>\n",
              "      <td>2.195698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:27,613 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:27,792 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:27,801 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/distilgpt2_lora_r16/seed42/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:27,968 - INFO - ✅ Успех! Loss=2.1301, PPL=8.42, время=29.6с, пик GPU=0.82GB\n",
            "2026-02-23 07:12:28,265 - INFO - \n",
            "Эксперимент 8/45: distilgpt2_lora_r16 (seed=43)\n",
            "2026-02-23 07:12:28,267 - INFO - --- Попытка distilgpt2_lora_r16 (seed=43) с batch_size=8 ---\n",
            "2026-02-23 07:12:28,267 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:12:28,840 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:29,152 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "292cbfcdf6864c728e629b95162cca72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:29,657 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:29,827 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.868949</td>\n",
              "      <td>2.547534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.523922</td>\n",
              "      <td>2.279603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.339167</td>\n",
              "      <td>2.194460</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:56,254 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:56,433 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:56,448 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/distilgpt2_lora_r16/seed43/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:57,178 - INFO - ✅ Успех! Loss=2.1344, PPL=8.45, время=28.9с, пик GPU=0.82GB\n",
            "2026-02-23 07:12:57,938 - INFO - \n",
            "Эксперимент 9/45: distilgpt2_lora_r16 (seed=44)\n",
            "2026-02-23 07:12:57,939 - INFO - --- Попытка distilgpt2_lora_r16 (seed=44) с batch_size=8 ---\n",
            "2026-02-23 07:12:57,940 - INFO - Загрузка модели: distilgpt2\n",
            "2026-02-23 07:12:58,112 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:58,288 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a163efb055274e63aacb47404bf955b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:12:58,794 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:12:59,333 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.873076</td>\n",
              "      <td>2.548827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.508251</td>\n",
              "      <td>2.280418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.367371</td>\n",
              "      <td>2.197713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:13:23,291 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:13:23,470 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:13:23,481 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/distilgpt2_lora_r16/seed44/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:13:23,752 - INFO - ✅ Успех! Loss=2.1384, PPL=8.49, время=25.8с, пик GPU=0.82GB\n",
            "2026-02-23 07:13:24,041 - INFO - \n",
            "Эксперимент 10/45: gpt2_baseline (seed=42)\n",
            "2026-02-23 07:13:24,042 - INFO - --- Попытка gpt2_baseline (seed=42) с batch_size=8 ---\n",
            "2026-02-23 07:13:24,042 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:13:24,214 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:13:24,391 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "284c4df585ae4b028032bd63c7150908",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:13:25,687 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:13:25,973 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.045627</td>\n",
              "      <td>1.583080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.515752</td>\n",
              "      <td>1.404432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.382310</td>\n",
              "      <td>1.320232</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:14:15,060 - INFO - ✅ Успех! Loss=1.2748, PPL=3.58, время=51.0с, пик GPU=2.69GB\n",
            "2026-02-23 07:14:15,494 - INFO - \n",
            "Эксперимент 11/45: gpt2_baseline (seed=43)\n",
            "2026-02-23 07:14:15,508 - INFO - --- Попытка gpt2_baseline (seed=43) с batch_size=8 ---\n",
            "2026-02-23 07:14:15,509 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:14:15,782 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:14:15,973 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b73adf4315f34ddcb6e12adab3121f2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:14:16,576 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:14:16,745 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.015563</td>\n",
              "      <td>1.593459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.532071</td>\n",
              "      <td>1.401977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.370374</td>\n",
              "      <td>1.317800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:15:07,274 - INFO - ✅ Успех! Loss=1.2720, PPL=3.57, время=51.8с, пик GPU=2.69GB\n",
            "2026-02-23 07:15:08,115 - INFO - \n",
            "Эксперимент 12/45: gpt2_baseline (seed=44)\n",
            "2026-02-23 07:15:08,116 - INFO - --- Попытка gpt2_baseline (seed=44) с batch_size=8 ---\n",
            "2026-02-23 07:15:08,117 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:15:10,243 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:15:10,416 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e06cca5560b549b095efe48f0555ddbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:15:11,604 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:15:11,777 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.007759</td>\n",
              "      <td>1.570406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.503136</td>\n",
              "      <td>1.398363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.384365</td>\n",
              "      <td>1.319490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:16:04,111 - INFO - ✅ Успех! Loss=1.2762, PPL=3.58, время=56.0с, пик GPU=2.69GB\n",
            "2026-02-23 07:16:04,427 - INFO - \n",
            "Эксперимент 13/45: gpt2_lora_r8 (seed=42)\n",
            "2026-02-23 07:16:04,427 - INFO - --- Попытка gpt2_lora_r8 (seed=42) с batch_size=8 ---\n",
            "2026-02-23 07:16:04,428 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:16:04,712 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:16:04,882 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "747dbda362274ee48551042fef4e8f5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:16:05,618 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:16:05,794 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.876397</td>\n",
              "      <td>2.476633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.504965</td>\n",
              "      <td>2.219031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.353448</td>\n",
              "      <td>2.134929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:16:46,799 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:16:46,967 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:16:46,977 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2_lora_r8/seed42/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:16:47,169 - INFO - ✅ Успех! Loss=2.0898, PPL=8.08, время=42.7с, пик GPU=0.92GB\n",
            "2026-02-23 07:16:47,472 - INFO - \n",
            "Эксперимент 14/45: gpt2_lora_r8 (seed=43)\n",
            "2026-02-23 07:16:47,473 - INFO - --- Попытка gpt2_lora_r8 (seed=43) с batch_size=8 ---\n",
            "2026-02-23 07:16:47,474 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:16:47,642 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:16:47,816 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e4c4fdb948a4e1f815d43eaaf962455",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:16:49,752 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:16:49,926 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.868541</td>\n",
              "      <td>2.474039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.519128</td>\n",
              "      <td>2.209412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.325323</td>\n",
              "      <td>2.121203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:17:33,375 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:17:33,557 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:17:33,753 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2_lora_r8/seed43/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:17:34,195 - INFO - ✅ Успех! Loss=2.0833, PPL=8.03, время=46.7с, пик GPU=0.92GB\n",
            "2026-02-23 07:17:35,184 - INFO - \n",
            "Эксперимент 15/45: gpt2_lora_r8 (seed=44)\n",
            "2026-02-23 07:17:35,185 - INFO - --- Попытка gpt2_lora_r8 (seed=44) с batch_size=8 ---\n",
            "2026-02-23 07:17:35,186 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:17:35,440 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:17:35,658 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "201f1e1f4ba641d5baded69fe4f12de7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:17:36,350 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:17:37,152 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.877900</td>\n",
              "      <td>2.483947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.509141</td>\n",
              "      <td>2.223953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.356890</td>\n",
              "      <td>2.135085</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:18:25,258 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:18:25,429 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:18:25,439 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2_lora_r8/seed44/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:18:25,627 - INFO - ✅ Успех! Loss=2.0939, PPL=8.12, время=50.4с, пик GPU=0.92GB\n",
            "2026-02-23 07:18:25,925 - INFO - \n",
            "Эксперимент 16/45: gpt2_lora_r16 (seed=42)\n",
            "2026-02-23 07:18:25,926 - INFO - --- Попытка gpt2_lora_r16 (seed=42) с batch_size=8 ---\n",
            "2026-02-23 07:18:25,927 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:18:26,092 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:18:26,267 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e9d8058290e4251ac68461eae925139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:18:27,137 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:18:27,312 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.773683</td>\n",
              "      <td>2.327027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.354561</td>\n",
              "      <td>2.080414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.211117</td>\n",
              "      <td>2.022348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:19:24,249 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:19:24,421 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:19:24,436 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2_lora_r16/seed42/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:19:24,613 - INFO - ✅ Успех! Loss=1.9733, PPL=7.19, время=58.7с, пик GPU=0.93GB\n",
            "2026-02-23 07:19:24,917 - INFO - \n",
            "Эксперимент 17/45: gpt2_lora_r16 (seed=43)\n",
            "2026-02-23 07:19:24,918 - INFO - --- Попытка gpt2_lora_r16 (seed=43) с batch_size=8 ---\n",
            "2026-02-23 07:19:24,918 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:19:25,089 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:19:25,262 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "238311a94eba40008283b4d32493eba8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:19:25,982 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:19:26,157 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.775255</td>\n",
              "      <td>2.333101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.376790</td>\n",
              "      <td>2.083491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.193483</td>\n",
              "      <td>2.023109</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:20:06,642 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:06,811 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:06,851 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2_lora_r16/seed43/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:20:07,026 - INFO - ✅ Успех! Loss=1.9793, PPL=7.24, время=42.1с, пик GPU=0.93GB\n",
            "2026-02-23 07:20:07,362 - INFO - \n",
            "Эксперимент 18/45: gpt2_lora_r16 (seed=44)\n",
            "2026-02-23 07:20:07,363 - INFO - --- Попытка gpt2_lora_r16 (seed=44) с batch_size=8 ---\n",
            "2026-02-23 07:20:07,363 - INFO - Загрузка модели: gpt2\n",
            "2026-02-23 07:20:07,549 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:07,951 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f20e68a2bbb84a14947f9b8eea317a10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:20:10,494 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:11,468 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 00:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.783044</td>\n",
              "      <td>2.337923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.349852</td>\n",
              "      <td>2.076695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.210742</td>\n",
              "      <td>2.019683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:20:57,132 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:57,305 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:57,318 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2_lora_r16/seed44/bs8/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:20:57,506 - INFO - ✅ Успех! Loss=1.9742, PPL=7.20, время=50.1с, пик GPU=0.93GB\n",
            "2026-02-23 07:20:58,138 - INFO - \n",
            "Эксперимент 19/45: gpt2-medium_baseline (seed=42)\n",
            "2026-02-23 07:20:58,141 - INFO - --- Попытка gpt2-medium_baseline (seed=42) с batch_size=4 ---\n",
            "2026-02-23 07:20:58,141 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:20:58,310 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:20:58,486 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b100aa4d6ef4c909171bdfc4bbf45e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:21:00,939 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:21:01,346 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.904510</td>\n",
              "      <td>1.514856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.467695</td>\n",
              "      <td>1.350339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.281733</td>\n",
              "      <td>1.254979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.163764</td>\n",
              "      <td>1.176355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.072449</td>\n",
              "      <td>1.133620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.002449</td>\n",
              "      <td>1.106224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:23:49,855 - INFO - ✅ Успех! Loss=1.0695, PPL=2.91, время=171.7с, пик GPU=7.12GB\n",
            "2026-02-23 07:23:51,154 - INFO - \n",
            "Эксперимент 20/45: gpt2-medium_baseline (seed=43)\n",
            "2026-02-23 07:23:51,156 - INFO - --- Попытка gpt2-medium_baseline (seed=43) с batch_size=4 ---\n",
            "2026-02-23 07:23:51,156 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:23:51,473 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:23:51,642 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcf66864114b4acf9f502be797e6f69d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:23:52,689 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:23:52,864 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.981211</td>\n",
              "      <td>1.514738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.441012</td>\n",
              "      <td>1.348808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.265082</td>\n",
              "      <td>1.244467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.197932</td>\n",
              "      <td>1.156997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.043818</td>\n",
              "      <td>1.141365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.988378</td>\n",
              "      <td>1.093149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:26:50,798 - INFO - ✅ Успех! Loss=1.0640, PPL=2.90, время=179.6с, пик GPU=7.12GB\n",
            "2026-02-23 07:26:52,035 - INFO - \n",
            "Эксперимент 21/45: gpt2-medium_baseline (seed=44)\n",
            "2026-02-23 07:26:52,036 - INFO - --- Попытка gpt2-medium_baseline (seed=44) с batch_size=4 ---\n",
            "2026-02-23 07:26:52,037 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:26:52,349 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:26:52,522 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0234a48331084efb8c50cd846b8c7df8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:26:53,260 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:26:53,449 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.000650</td>\n",
              "      <td>1.535894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.499584</td>\n",
              "      <td>1.349244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.262379</td>\n",
              "      <td>1.250030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.178232</td>\n",
              "      <td>1.158787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.096989</td>\n",
              "      <td>1.130090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.982160</td>\n",
              "      <td>1.100779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:29:42,752 - INFO - ✅ Успех! Loss=1.0691, PPL=2.91, время=170.7с, пик GPU=7.12GB\n",
            "2026-02-23 07:29:44,154 - INFO - \n",
            "Эксперимент 22/45: gpt2-medium_lora_r8 (seed=42)\n",
            "2026-02-23 07:29:44,155 - INFO - --- Попытка gpt2-medium_lora_r8 (seed=42) с batch_size=4 ---\n",
            "2026-02-23 07:29:44,156 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:29:44,465 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:29:45,152 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e76241dd89f4c02b5b69b9f6f5bb708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:29:49,896 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:29:50,322 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.666259</td>\n",
              "      <td>2.253908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.193259</td>\n",
              "      <td>1.946833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.024101</td>\n",
              "      <td>1.864315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.924863</td>\n",
              "      <td>1.819500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.897366</td>\n",
              "      <td>1.799434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.889910</td>\n",
              "      <td>1.785104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:32:41,465 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:32:41,652 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:32:41,669 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2-medium_lora_r8/seed42/bs4/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:32:42,319 - INFO - ✅ Успех! Loss=1.7359, PPL=5.67, время=178.2с, пик GPU=1.08GB\n",
            "2026-02-23 07:32:42,615 - INFO - \n",
            "Эксперимент 23/45: gpt2-medium_lora_r8 (seed=43)\n",
            "2026-02-23 07:32:42,616 - INFO - --- Попытка gpt2-medium_lora_r8 (seed=43) с batch_size=4 ---\n",
            "2026-02-23 07:32:42,617 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:32:42,799 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:32:42,981 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74e2d59975a14b909f7660b643ed3f78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:32:46,152 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:32:46,332 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.679499</td>\n",
              "      <td>2.262404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.175632</td>\n",
              "      <td>1.941187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.011117</td>\n",
              "      <td>1.859853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.952265</td>\n",
              "      <td>1.820706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.880490</td>\n",
              "      <td>1.797159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.864790</td>\n",
              "      <td>1.777015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:35:22,537 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:35:22,707 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:35:22,734 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2-medium_lora_r8/seed43/bs4/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:35:23,211 - INFO - ✅ Успех! Loss=1.7314, PPL=5.65, время=160.6с, пик GPU=1.08GB\n",
            "2026-02-23 07:35:23,511 - INFO - \n",
            "Эксперимент 24/45: gpt2-medium_lora_r8 (seed=44)\n",
            "2026-02-23 07:35:23,512 - INFO - --- Попытка gpt2-medium_lora_r8 (seed=44) с batch_size=4 ---\n",
            "2026-02-23 07:35:23,514 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:35:24,128 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:35:24,322 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9075e2e2f0a74694b4dbb45d18074241",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:35:27,152 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:35:27,345 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.671854</td>\n",
              "      <td>2.263020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.203639</td>\n",
              "      <td>1.933807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.989680</td>\n",
              "      <td>1.864300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.927874</td>\n",
              "      <td>1.818345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.921157</td>\n",
              "      <td>1.789974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.866463</td>\n",
              "      <td>1.779523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:38:25,104 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:38:25,277 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:38:25,289 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2-medium_lora_r8/seed44/bs4/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:38:25,772 - INFO - ✅ Успех! Loss=1.7341, PPL=5.66, время=182.3с, пик GPU=1.08GB\n",
            "2026-02-23 07:38:26,071 - INFO - \n",
            "Эксперимент 25/45: gpt2-medium_lora_r16 (seed=42)\n",
            "2026-02-23 07:38:26,072 - INFO - --- Попытка gpt2-medium_lora_r16 (seed=42) с batch_size=4 ---\n",
            "2026-02-23 07:38:26,073 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:38:26,245 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:38:26,413 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c609fbf136f94416a3262150297fe268",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:38:28,398 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:38:28,571 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.564128</td>\n",
              "      <td>2.083875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.075653</td>\n",
              "      <td>1.891019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.951268</td>\n",
              "      <td>1.804280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.858910</td>\n",
              "      <td>1.762472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.832846</td>\n",
              "      <td>1.743541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.824783</td>\n",
              "      <td>1.731891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:41:16,920 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:41:17,093 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:41:17,117 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2-medium_lora_r16/seed42/bs4/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:41:17,641 - INFO - ✅ Успех! Loss=1.6729, PPL=5.33, время=171.6с, пик GPU=1.09GB\n",
            "2026-02-23 07:41:17,952 - INFO - \n",
            "Эксперимент 26/45: gpt2-medium_lora_r16 (seed=43)\n",
            "2026-02-23 07:41:17,953 - INFO - --- Попытка gpt2-medium_lora_r16 (seed=43) с batch_size=4 ---\n",
            "2026-02-23 07:41:17,954 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:41:18,172 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:41:18,343 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a3f91e9c4c649b899a8d4f398d620b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:41:19,402 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:41:19,577 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.578010</td>\n",
              "      <td>2.088659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.058773</td>\n",
              "      <td>1.881127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.936691</td>\n",
              "      <td>1.806811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.889660</td>\n",
              "      <td>1.769688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.815217</td>\n",
              "      <td>1.741571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.800178</td>\n",
              "      <td>1.721611</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:44:07,132 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:44:07,306 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:44:07,327 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2-medium_lora_r16/seed43/bs4/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:44:07,799 - INFO - ✅ Успех! Loss=1.6721, PPL=5.32, время=169.8с, пик GPU=1.09GB\n",
            "2026-02-23 07:44:08,100 - INFO - \n",
            "Эксперимент 27/45: gpt2-medium_lora_r16 (seed=44)\n",
            "2026-02-23 07:44:08,101 - INFO - --- Попытка gpt2-medium_lora_r16 (seed=44) с batch_size=4 ---\n",
            "2026-02-23 07:44:08,102 - INFO - Загрузка модели: gpt2-medium\n",
            "2026-02-23 07:44:08,271 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:44:08,445 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5328f9bd15e54e6db821e709776c6b43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:44:09,430 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:44:10,153 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kfu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 02:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.568677</td>\n",
              "      <td>2.085024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.085481</td>\n",
              "      <td>1.864943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.913910</td>\n",
              "      <td>1.804513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.857123</td>\n",
              "      <td>1.768073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.856589</td>\n",
              "      <td>1.732442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.798500</td>\n",
              "      <td>1.723099</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:47:06,605 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:47:06,776 - INFO - HTTP Request: HEAD https://huggingface.co/gpt2-medium/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:47:06,830 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/gpt2-medium_lora_r16/seed44/bs4/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:47:08,323 - INFO - ✅ Успех! Loss=1.6662, PPL=5.29, время=180.2с, пик GPU=1.09GB\n",
            "2026-02-23 07:47:08,636 - INFO - \n",
            "Эксперимент 28/45: qwen2.5_qlora_r8 (seed=42)\n",
            "2026-02-23 07:47:08,637 - INFO - --- Попытка qwen2.5_qlora_r8 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 07:47:08,638 - INFO - Загрузка модели: Qwen/Qwen2.5-7B-Instruct\n",
            "2026-02-23 07:47:08,809 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:47:08,848 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:47:09,022 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:47:09,063 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d29e9382ec24d40ab045b9d7f83f324",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 07:47:17,368 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 07:47:17,413 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 07:47:17,587 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:34, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.595478</td>\n",
              "      <td>2.332445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.279935</td>\n",
              "      <td>2.167556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.106467</td>\n",
              "      <td>2.093951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.051217</td>\n",
              "      <td>2.048829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.025075</td>\n",
              "      <td>1.987057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.981443</td>\n",
              "      <td>1.951864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.904229</td>\n",
              "      <td>1.917731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.901288</td>\n",
              "      <td>1.897548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.894881</td>\n",
              "      <td>1.866571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.769329</td>\n",
              "      <td>1.857084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.735763</td>\n",
              "      <td>1.842312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.671173</td>\n",
              "      <td>1.823397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.674334</td>\n",
              "      <td>1.805148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.676372</td>\n",
              "      <td>1.792717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.667312</td>\n",
              "      <td>1.786286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.578202</td>\n",
              "      <td>1.777317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.728641</td>\n",
              "      <td>1.772364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.673664</td>\n",
              "      <td>1.768988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 08:17:54,782 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:17:54,824 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:17:54,993 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:17:55,035 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:17:55,164 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/qwen2.5_qlora_r8/seed42/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 08:18:06,821 - INFO - ✅ Успех! Loss=1.7675, PPL=5.86, время=1858.2с, пик GPU=13.90GB\n",
            "2026-02-23 08:18:07,240 - INFO - \n",
            "Эксперимент 29/45: qwen2.5_qlora_r8 (seed=43)\n",
            "2026-02-23 08:18:07,255 - INFO - --- Попытка qwen2.5_qlora_r8 (seed=43) с batch_size=1 ---\n",
            "2026-02-23 08:18:07,255 - INFO - Загрузка модели: Qwen/Qwen2.5-7B-Instruct\n",
            "2026-02-23 08:18:07,501 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:18:07,543 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:18:07,729 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:18:08,066 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "237b84def6b546a59df117a6a2abb2f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 08:18:15,607 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:18:15,648 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:18:15,819 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:46, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.645501</td>\n",
              "      <td>2.342557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.280231</td>\n",
              "      <td>2.187869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.103270</td>\n",
              "      <td>2.094699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.048145</td>\n",
              "      <td>2.024252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.998888</td>\n",
              "      <td>1.991329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.966316</td>\n",
              "      <td>1.952031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.823438</td>\n",
              "      <td>1.926641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.974835</td>\n",
              "      <td>1.895229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.895553</td>\n",
              "      <td>1.864474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.685701</td>\n",
              "      <td>1.849244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.696909</td>\n",
              "      <td>1.833652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.750315</td>\n",
              "      <td>1.812864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.736933</td>\n",
              "      <td>1.797457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.692023</td>\n",
              "      <td>1.788804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.669332</td>\n",
              "      <td>1.773440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.697473</td>\n",
              "      <td>1.768856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.618883</td>\n",
              "      <td>1.760894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.633284</td>\n",
              "      <td>1.759574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 08:49:04,886 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:49:04,931 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:49:05,100 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:49:05,144 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:49:05,348 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/qwen2.5_qlora_r8/seed43/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 08:49:17,875 - INFO - ✅ Успех! Loss=1.7624, PPL=5.83, время=1870.6с, пик GPU=16.82GB\n",
            "2026-02-23 08:49:19,136 - INFO - \n",
            "Эксперимент 30/45: qwen2.5_qlora_r8 (seed=44)\n",
            "2026-02-23 08:49:19,137 - INFO - --- Попытка qwen2.5_qlora_r8 (seed=44) с batch_size=1 ---\n",
            "2026-02-23 08:49:19,138 - INFO - Загрузка модели: Qwen/Qwen2.5-7B-Instruct\n",
            "2026-02-23 08:49:20,865 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:49:20,907 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:49:21,081 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:49:21,123 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b028d24e8713453da7d41a3385ead121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 08:49:31,780 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 08:49:31,823 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 08:49:31,999 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:31, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.619048</td>\n",
              "      <td>2.329433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.233665</td>\n",
              "      <td>2.170734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.182039</td>\n",
              "      <td>2.092398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.089593</td>\n",
              "      <td>2.046014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.011916</td>\n",
              "      <td>1.983966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.951290</td>\n",
              "      <td>1.945824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.968241</td>\n",
              "      <td>1.914889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.862620</td>\n",
              "      <td>1.896265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.849590</td>\n",
              "      <td>1.866946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.797468</td>\n",
              "      <td>1.848822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.646603</td>\n",
              "      <td>1.835358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.725771</td>\n",
              "      <td>1.821006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.777185</td>\n",
              "      <td>1.801633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.613229</td>\n",
              "      <td>1.792669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.639446</td>\n",
              "      <td>1.779283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.648401</td>\n",
              "      <td>1.771822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.674215</td>\n",
              "      <td>1.763561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.664021</td>\n",
              "      <td>1.762974</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 09:20:06,398 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:20:06,448 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:20:06,625 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:20:06,669 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:20:06,725 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/qwen2.5_qlora_r8/seed44/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 09:20:16,979 - INFO - ✅ Успех! Loss=1.7624, PPL=5.83, время=1857.8с, пик GPU=19.35GB\n",
            "2026-02-23 09:20:17,418 - INFO - \n",
            "Эксперимент 31/45: qwen2.5_qlora_r16 (seed=42)\n",
            "2026-02-23 09:20:17,419 - INFO - --- Попытка qwen2.5_qlora_r16 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 09:20:17,420 - INFO - Загрузка модели: Qwen/Qwen2.5-7B-Instruct\n",
            "2026-02-23 09:20:18,130 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:20:18,171 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:20:19,130 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:20:19,172 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1263c08529d9409e9747810746d7729a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 09:20:30,621 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:20:30,661 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:20:30,858 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:53, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.520354</td>\n",
              "      <td>2.289086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.231101</td>\n",
              "      <td>2.127902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.054209</td>\n",
              "      <td>2.057036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.007917</td>\n",
              "      <td>2.010324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.968911</td>\n",
              "      <td>1.931445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.919204</td>\n",
              "      <td>1.885950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.850536</td>\n",
              "      <td>1.852365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.835918</td>\n",
              "      <td>1.827397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.826868</td>\n",
              "      <td>1.795022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.671361</td>\n",
              "      <td>1.783595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.634872</td>\n",
              "      <td>1.770273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.568402</td>\n",
              "      <td>1.748879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.567145</td>\n",
              "      <td>1.727599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.563784</td>\n",
              "      <td>1.716965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.561354</td>\n",
              "      <td>1.708689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.473004</td>\n",
              "      <td>1.699854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.621778</td>\n",
              "      <td>1.693283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.555110</td>\n",
              "      <td>1.688898</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 09:51:29,180 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:51:29,221 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:51:29,397 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:51:29,441 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:51:29,590 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/qwen2.5_qlora_r16/seed42/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 09:51:42,673 - INFO - ✅ Успех! Loss=1.6912, PPL=5.43, время=1885.3с, пик GPU=21.43GB\n",
            "2026-02-23 09:51:44,186 - INFO - \n",
            "Эксперимент 32/45: qwen2.5_qlora_r16 (seed=43)\n",
            "2026-02-23 09:51:44,187 - INFO - --- Попытка qwen2.5_qlora_r16 (seed=43) с batch_size=1 ---\n",
            "2026-02-23 09:51:44,188 - INFO - Загрузка модели: Qwen/Qwen2.5-7B-Instruct\n",
            "2026-02-23 09:51:45,370 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:51:45,412 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:51:45,586 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:51:45,627 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d1960608a3495f90c00c452887b9af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 09:51:50,776 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 09:51:50,817 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 09:51:50,988 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 31:17, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.572134</td>\n",
              "      <td>2.292780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.234954</td>\n",
              "      <td>2.145830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.059977</td>\n",
              "      <td>2.056427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.005672</td>\n",
              "      <td>1.975297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.950735</td>\n",
              "      <td>1.938337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.906447</td>\n",
              "      <td>1.892566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.765161</td>\n",
              "      <td>1.865319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.909875</td>\n",
              "      <td>1.830454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.824731</td>\n",
              "      <td>1.793537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.583114</td>\n",
              "      <td>1.782192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.601965</td>\n",
              "      <td>1.758214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.643651</td>\n",
              "      <td>1.733452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.633491</td>\n",
              "      <td>1.719439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.584938</td>\n",
              "      <td>1.710068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.567255</td>\n",
              "      <td>1.696484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.593685</td>\n",
              "      <td>1.690974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.511666</td>\n",
              "      <td>1.683488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.530286</td>\n",
              "      <td>1.682136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 10:23:13,542 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:23:13,736 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:23:14,159 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:23:14,432 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:23:15,205 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/qwen2.5_qlora_r16/seed43/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 10:23:29,168 - INFO - ✅ Успех! Loss=1.6981, PPL=5.46, время=1905.0с, пик GPU=23.58GB\n",
            "2026-02-23 10:23:30,291 - INFO - \n",
            "Эксперимент 33/45: qwen2.5_qlora_r16 (seed=44)\n",
            "2026-02-23 10:23:30,292 - INFO - --- Попытка qwen2.5_qlora_r16 (seed=44) с batch_size=1 ---\n",
            "2026-02-23 10:23:30,293 - INFO - Загрузка модели: Qwen/Qwen2.5-7B-Instruct\n",
            "2026-02-23 10:23:30,611 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:23:30,654 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:23:30,828 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:23:30,872 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a894eec6c7f4e9e87e4c740b0e0372d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 10:23:42,762 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:23:42,806 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:23:42,980 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 31:05, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.549966</td>\n",
              "      <td>2.285206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.183957</td>\n",
              "      <td>2.127887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.139942</td>\n",
              "      <td>2.046000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.039668</td>\n",
              "      <td>1.996733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.961805</td>\n",
              "      <td>1.924611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.894043</td>\n",
              "      <td>1.884407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.903029</td>\n",
              "      <td>1.850071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.796193</td>\n",
              "      <td>1.830571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.781385</td>\n",
              "      <td>1.794755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.692592</td>\n",
              "      <td>1.770849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.538872</td>\n",
              "      <td>1.759150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.616746</td>\n",
              "      <td>1.744906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.670880</td>\n",
              "      <td>1.722343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.503149</td>\n",
              "      <td>1.714546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.532158</td>\n",
              "      <td>1.700247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.543732</td>\n",
              "      <td>1.690867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.558824</td>\n",
              "      <td>1.683642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.554434</td>\n",
              "      <td>1.682507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 10:54:52,757 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:54:52,796 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:54:52,965 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:54:53,004 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:54:53,098 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/qwen2.5_qlora_r16/seed44/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 10:55:06,528 - INFO - ✅ Успех! Loss=1.6894, PPL=5.42, время=1896.2с, пик GPU=25.89GB\n",
            "2026-02-23 10:55:08,260 - INFO - \n",
            "Эксперимент 34/45: deepseek_qlora_r8 (seed=42)\n",
            "2026-02-23 10:55:08,261 - INFO - --- Попытка deepseek_qlora_r8 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 10:55:08,262 - INFO - Загрузка модели: deepseek-ai/deepseek-llm-7b-chat\n",
            "2026-02-23 10:55:09,155 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:55:09,197 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:10,516 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:55:10,557 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:10,727 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 10:55:11,155 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:11,836 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/main \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:12,029 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/discussions?p=0 \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62d60531d5344ef6ace41d95d6b3538e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 10:55:12,215 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:12,390 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:55:12,454 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:12,637 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:55:12,679 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:19,510 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 10:55:19,577 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 10:55:19,752 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:48, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.726775</td>\n",
              "      <td>2.469832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.364158</td>\n",
              "      <td>2.281087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.160533</td>\n",
              "      <td>2.197884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.107640</td>\n",
              "      <td>2.123736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.053992</td>\n",
              "      <td>2.045010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.017577</td>\n",
              "      <td>1.986079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.908826</td>\n",
              "      <td>1.949759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.850640</td>\n",
              "      <td>1.918182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.849818</td>\n",
              "      <td>1.890760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.735696</td>\n",
              "      <td>1.866259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.688786</td>\n",
              "      <td>1.836426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.664117</td>\n",
              "      <td>1.816581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.636056</td>\n",
              "      <td>1.796263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.619749</td>\n",
              "      <td>1.783712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.617379</td>\n",
              "      <td>1.782486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.508526</td>\n",
              "      <td>1.765128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.654608</td>\n",
              "      <td>1.760104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.592326</td>\n",
              "      <td>1.754626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 11:26:10,854 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:10,928 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:11,106 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:11,178 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:11,457 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/deepseek_qlora_r8/seed42/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 11:26:21,658 - INFO - ✅ Успех! Loss=1.7297, PPL=5.64, время=1873.4с, пик GPU=26.71GB\n",
            "2026-02-23 11:26:22,293 - INFO - \n",
            "Эксперимент 35/45: deepseek_qlora_r8 (seed=43)\n",
            "2026-02-23 11:26:22,294 - INFO - --- Попытка deepseek_qlora_r8 (seed=43) с batch_size=1 ---\n",
            "2026-02-23 11:26:22,295 - INFO - Загрузка модели: deepseek-ai/deepseek-llm-7b-chat\n",
            "2026-02-23 11:26:23,254 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:23,307 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:23,491 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:23,547 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:23,731 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 11:26:23,920 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:24,131 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/main \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:24,332 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:24,528 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "371d6afb23124f67b00c119714def9dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 11:26:24,893 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:24,969 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:25,148 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:25,227 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:36,215 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:26:36,295 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:26:36,475 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:58, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.761161</td>\n",
              "      <td>2.478980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.331294</td>\n",
              "      <td>2.293270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.195790</td>\n",
              "      <td>2.182369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.104367</td>\n",
              "      <td>2.099671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.009811</td>\n",
              "      <td>2.042016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.992344</td>\n",
              "      <td>1.996091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.817304</td>\n",
              "      <td>1.958034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.937603</td>\n",
              "      <td>1.923005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.847040</td>\n",
              "      <td>1.882433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.643664</td>\n",
              "      <td>1.861829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.670075</td>\n",
              "      <td>1.839047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.698728</td>\n",
              "      <td>1.817082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.665975</td>\n",
              "      <td>1.803255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.632369</td>\n",
              "      <td>1.785198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.619053</td>\n",
              "      <td>1.775601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.631793</td>\n",
              "      <td>1.764654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.556834</td>\n",
              "      <td>1.760653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.567985</td>\n",
              "      <td>1.757401</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 11:57:38,165 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:57:38,218 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:38,397 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:57:38,445 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:38,741 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/deepseek_qlora_r8/seed43/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 11:57:50,771 - INFO - ✅ Успех! Loss=1.7372, PPL=5.68, время=1888.5с, пик GPU=28.37GB\n",
            "2026-02-23 11:57:51,394 - INFO - \n",
            "Эксперимент 36/45: deepseek_qlora_r8 (seed=44)\n",
            "2026-02-23 11:57:51,395 - INFO - --- Попытка deepseek_qlora_r8 (seed=44) с batch_size=1 ---\n",
            "2026-02-23 11:57:51,397 - INFO - Загрузка модели: deepseek-ai/deepseek-llm-7b-chat\n",
            "2026-02-23 11:57:51,649 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:57:51,695 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:51,879 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:57:51,926 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:52,107 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 11:57:52,291 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:52,485 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/main \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fda5e90de4fd45d1b606a85e6dc08727",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 11:57:52,672 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:52,862 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:53,041 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:57:53,087 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:57:53,339 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:57:53,528 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:58:02,316 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 11:58:02,364 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 11:58:02,590 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:39, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.722729</td>\n",
              "      <td>2.462988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.333540</td>\n",
              "      <td>2.292434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.247469</td>\n",
              "      <td>2.190449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.126012</td>\n",
              "      <td>2.129568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.037937</td>\n",
              "      <td>2.051867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.984354</td>\n",
              "      <td>2.001182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.959675</td>\n",
              "      <td>1.954771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.856435</td>\n",
              "      <td>1.938868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.829285</td>\n",
              "      <td>1.892577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.754407</td>\n",
              "      <td>1.885681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.626094</td>\n",
              "      <td>1.850160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.707610</td>\n",
              "      <td>1.833484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.727843</td>\n",
              "      <td>1.803990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.546573</td>\n",
              "      <td>1.800736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.596181</td>\n",
              "      <td>1.787079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.566519</td>\n",
              "      <td>1.773951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.620209</td>\n",
              "      <td>1.767688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.590540</td>\n",
              "      <td>1.766600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 12:28:44,735 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:28:44,788 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:44,967 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:28:45,016 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:45,282 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/deepseek_qlora_r8/seed44/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 12:28:54,285 - INFO - ✅ Успех! Loss=1.7355, PPL=5.67, время=1862.9с, пик GPU=30.09GB\n",
            "2026-02-23 12:28:54,934 - INFO - \n",
            "Эксперимент 37/45: deepseek_qlora_r16 (seed=42)\n",
            "2026-02-23 12:28:54,936 - INFO - --- Попытка deepseek_qlora_r16 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 12:28:54,937 - INFO - Загрузка модели: deepseek-ai/deepseek-llm-7b-chat\n",
            "2026-02-23 12:28:55,323 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:28:55,372 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:55,550 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:28:55,599 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:55,782 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 12:28:55,968 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc19d1cc5767416eacf10e3264e99574",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 12:28:56,160 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/main \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:56,391 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:56,586 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:56,770 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:28:56,823 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:28:56,998 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:28:57,048 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:29:02,930 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:29:02,992 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:29:03,176 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.648628</td>\n",
              "      <td>2.423782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.325174</td>\n",
              "      <td>2.245763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.120124</td>\n",
              "      <td>2.147932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.060411</td>\n",
              "      <td>2.078772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.001755</td>\n",
              "      <td>1.989611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.962754</td>\n",
              "      <td>1.929735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.853980</td>\n",
              "      <td>1.891456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.789259</td>\n",
              "      <td>1.859996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.792248</td>\n",
              "      <td>1.825648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.650120</td>\n",
              "      <td>1.802030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.603177</td>\n",
              "      <td>1.774773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.575962</td>\n",
              "      <td>1.756058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.547344</td>\n",
              "      <td>1.729772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.527281</td>\n",
              "      <td>1.713396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.520070</td>\n",
              "      <td>1.712568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.419731</td>\n",
              "      <td>1.697504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.562504</td>\n",
              "      <td>1.689996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.489195</td>\n",
              "      <td>1.684063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 12:59:53,852 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:59:53,907 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:59:54,105 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 12:59:54,166 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 12:59:55,164 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/deepseek_qlora_r16/seed42/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:00:08,298 - INFO - ✅ Успех! Loss=1.6699, PPL=5.31, время=1873.4с, пик GPU=31.69GB\n",
            "2026-02-23 13:00:08,729 - INFO - \n",
            "Эксперимент 38/45: deepseek_qlora_r16 (seed=43)\n",
            "2026-02-23 13:00:08,730 - INFO - --- Попытка deepseek_qlora_r16 (seed=43) с batch_size=1 ---\n",
            "2026-02-23 13:00:08,731 - INFO - Загрузка модели: deepseek-ai/deepseek-llm-7b-chat\n",
            "2026-02-23 13:00:09,461 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:00:09,509 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:09,688 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:00:09,737 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:09,919 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 13:00:10,110 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74ed3a2522204507b3c6782ab8542234",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:00:10,465 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/main \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:11,161 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:11,355 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:12,161 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:00:12,212 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:12,393 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:00:12,440 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:21,553 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:00:21,594 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:00:21,769 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 30:30, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.691704</td>\n",
              "      <td>2.431618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.297784</td>\n",
              "      <td>2.258029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.155874</td>\n",
              "      <td>2.143885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.055639</td>\n",
              "      <td>2.058068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.964062</td>\n",
              "      <td>1.990887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.937924</td>\n",
              "      <td>1.944297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.762668</td>\n",
              "      <td>1.902520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.880432</td>\n",
              "      <td>1.862409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.788410</td>\n",
              "      <td>1.816756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.552927</td>\n",
              "      <td>1.790595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.581825</td>\n",
              "      <td>1.765747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.606839</td>\n",
              "      <td>1.747572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.582419</td>\n",
              "      <td>1.731128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.545401</td>\n",
              "      <td>1.712193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.528098</td>\n",
              "      <td>1.701297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.536385</td>\n",
              "      <td>1.686945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.459106</td>\n",
              "      <td>1.682588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.469585</td>\n",
              "      <td>1.679078</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:31:01,229 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:01,276 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:01,462 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:01,510 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:01,671 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/deepseek_qlora_r16/seed43/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:31:10,452 - INFO - ✅ Успех! Loss=1.6704, PPL=5.31, время=1861.7с, пик GPU=33.37GB\n",
            "2026-02-23 13:31:10,870 - INFO - \n",
            "Эксперимент 39/45: deepseek_qlora_r16 (seed=44)\n",
            "2026-02-23 13:31:10,873 - INFO - --- Попытка deepseek_qlora_r16 (seed=44) с batch_size=1 ---\n",
            "2026-02-23 13:31:10,879 - INFO - Загрузка модели: deepseek-ai/deepseek-llm-7b-chat\n",
            "2026-02-23 13:31:11,134 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:11,180 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:11,354 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:11,401 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:11,571 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 13:31:11,756 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abbce5f702e84273be389eb44a76ddb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:31:12,229 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/main \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:13,229 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:14,230 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:14,407 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:14,454 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:14,627 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:14,673 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/3ae070ab36b43054b1449ef99e4a90e6c9e865bb/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:16,313 - ERROR - Не удалось загрузить модель deepseek-ai/deepseek-llm-7b-chat: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 74.38 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 91.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2026-02-23 13:31:16,317 - INFO - \n",
            "Эксперимент 40/45: mistral_qlora_r8 (seed=42)\n",
            "2026-02-23 13:31:16,323 - INFO - --- Попытка mistral_qlora_r8 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 13:31:16,325 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 13:31:17,153 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:17,222 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:17,400 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 13:31:21,165 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:21,238 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:21,441 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 13:31:21,825 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/model.safetensors.index.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 13:31:22,145 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:22,566 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/model.safetensors.index.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be64ba54cb674705be9ba1247f9a83b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:31:22,772 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/revision/main \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48e803c959294a10b48dafb0c6fb72b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77e82ee3dfb24db383f7414f5b58f929",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 13:31:23,001 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/caa1feb0e54d415e2df31207e5f4e273e33509b1/model-00001-of-00003.safetensors \"HTTP/1.1 302 Found\"\n",
            "2026-02-23 13:31:23,169 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/xet-read-token/caa1feb0e54d415e2df31207e5f4e273e33509b1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 13:31:23,173 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/caa1feb0e54d415e2df31207e5f4e273e33509b1/model-00003-of-00003.safetensors \"HTTP/1.1 302 Found\"\n",
            "2026-02-23 13:31:23,187 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/caa1feb0e54d415e2df31207e5f4e273e33509b1/model-00002-of-00003.safetensors \"HTTP/1.1 302 Found\"\n",
            "2026-02-23 13:47:29,997 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/xet-read-token/caa1feb0e54d415e2df31207e5f4e273e33509b1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 14:05:35,145 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/xet-read-token/caa1feb0e54d415e2df31207e5f4e273e33509b1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 14:37:37,101 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/xet-read-token/caa1feb0e54d415e2df31207e5f4e273e33509b1 \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "tat_pipeline_final_rf_fixed_full.py\n",
        "\n",
        "ФИНАЛЬНАЯ ВЕРСИЯ для научной статьи:\n",
        "- Полный датасет Tat Monocorpus (230k)\n",
        "- Модели: GPT family, Qwen, DeepSeek, Mistral (LoRA/QLoRA)\n",
        "- Статистическая значимость (NUM_SEEDS)\n",
        "- Кэширование токенизации, возобновление экспериментов\n",
        "- Логирование в файл, защита от сбоев\n",
        "- АВТОМАТИЧЕСКОЕ СКАЧИВАНИЕ АРХИВА В ЛОКАЛЬНЫЙ КОМПЬЮТЕР\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "import gc\n",
        "import time\n",
        "import sys\n",
        "import logging\n",
        "import zipfile\n",
        "import shutil\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Проверка наличия необходимых библиотек\n",
        "try:\n",
        "    from datasets import load_dataset, DatasetDict, load_from_disk\n",
        "    from transformers import (\n",
        "        AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,\n",
        "        DataCollatorWithPadding, BitsAndBytesConfig, set_seed\n",
        "    )\n",
        "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "    from huggingface_hub import login\n",
        "    from IPython.display import FileLink, display, Javascript\n",
        "except ImportError as e:\n",
        "    print(f\"Ошибка импорта: {e}\")\n",
        "    print(\"Убедитесь, что установлены: transformers datasets peft accelerate bitsandbytes huggingface-hub matplotlib psutil ipython\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ======================= КОНФИГУРАЦИЯ =======================\n",
        "HF_TOKEN = \"\"  # ваш токен\n",
        "DATASET_ID = \"IPSAN/tat_monocorpus_v2\"\n",
        "SUBSET_SIZE = 1000                     # None = все данные (230k)\n",
        "TRUNCATE_CHARS = 2000\n",
        "MAX_LENGTH = 128\n",
        "TOKENIZE_BATCH_SIZE = 500\n",
        "TOKENIZE_NUM_PROC = min(4, max(1, (os.cpu_count() or 4) - 1))\n",
        "SEED = 42\n",
        "NUM_SEEDS = 3                           # для статистической значимости (>1)\n",
        "\n",
        "LEARNING_RATE = 2e-4\n",
        "GRADIENT_ACCUMULATION_STEPS = 2\n",
        "LOGGING_STEPS = 50\n",
        "EPOCHS_BASE = 3\n",
        "EPOCHS_LARGE = 2                        # для 7B моделей\n",
        "RESULTS_BASE = \"./tat_results_final_rf\"\n",
        "\n",
        "# Проверка свободного места (минимум 20 ГБ)\n",
        "free_space = shutil.disk_usage(\".\").free / (1024**3)\n",
        "if free_space < 20:\n",
        "    print(f\"⚠️  Предупреждение: на диске всего {free_space:.1f} ГБ. Рекомендуется минимум 20 ГБ.\")\n",
        "\n",
        "# --- Список экспериментов (LLaMA удалена) ---\n",
        "EXPERIMENTS = [\n",
        "    # DistilGPT2\n",
        "    {\"name\": \"distilgpt2_baseline\", \"model_name\": \"distilgpt2\",\n",
        "     \"use_lora\": False, \"batch_size\": 8, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": False},\n",
        "    {\"name\": \"distilgpt2_lora_r8\", \"model_name\": \"distilgpt2\",\n",
        "     \"use_lora\": True, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"c_attn\"], \"batch_size\": 8, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": True},\n",
        "    {\"name\": \"distilgpt2_lora_r16\", \"model_name\": \"distilgpt2\",\n",
        "     \"use_lora\": True, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"c_attn\"], \"batch_size\": 8, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": True},\n",
        "\n",
        "    # GPT2\n",
        "    {\"name\": \"gpt2_baseline\", \"model_name\": \"gpt2\",\n",
        "     \"use_lora\": False, \"batch_size\": 8, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": False},\n",
        "    {\"name\": \"gpt2_lora_r8\", \"model_name\": \"gpt2\",\n",
        "     \"use_lora\": True, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"c_attn\"], \"batch_size\": 8, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": True},\n",
        "    {\"name\": \"gpt2_lora_r16\", \"model_name\": \"gpt2\",\n",
        "     \"use_lora\": True, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"c_attn\"], \"batch_size\": 8, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": True},\n",
        "\n",
        "    # GPT2-medium\n",
        "    {\"name\": \"gpt2-medium_baseline\", \"model_name\": \"gpt2-medium\",\n",
        "     \"use_lora\": False, \"batch_size\": 4, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": False},\n",
        "    {\"name\": \"gpt2-medium_lora_r8\", \"model_name\": \"gpt2-medium\",\n",
        "     \"use_lora\": True, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"c_attn\"], \"batch_size\": 4, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": True},\n",
        "    {\"name\": \"gpt2-medium_lora_r16\", \"model_name\": \"gpt2-medium\",\n",
        "     \"use_lora\": True, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"c_attn\"], \"batch_size\": 4, \"epochs\": EPOCHS_BASE, \"tokenizer_group\": \"gpt2\", \"fp16\": True},\n",
        "\n",
        "    # Qwen2.5-7B\n",
        "    {\"name\": \"qwen2.5_qlora_r8\", \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "     \"use_lora\": True, \"use_qlora\": True, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "     \"batch_size\": 1, \"epochs\": EPOCHS_LARGE, \"tokenizer_group\": \"qwen\", \"fp16\": True},\n",
        "    {\"name\": \"qwen2.5_qlora_r16\", \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "     \"use_lora\": True, \"use_qlora\": True, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "     \"batch_size\": 1, \"epochs\": EPOCHS_LARGE, \"tokenizer_group\": \"qwen\", \"fp16\": True},\n",
        "\n",
        "    # DeepSeek-7B\n",
        "    {\"name\": \"deepseek_qlora_r8\", \"model_name\": \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "     \"use_lora\": True, \"use_qlora\": True, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "     \"batch_size\": 1, \"epochs\": EPOCHS_LARGE, \"tokenizer_group\": \"deepseek\", \"fp16\": True},\n",
        "    {\"name\": \"deepseek_qlora_r16\", \"model_name\": \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "     \"use_lora\": True, \"use_qlora\": True, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "     \"batch_size\": 1, \"epochs\": EPOCHS_LARGE, \"tokenizer_group\": \"deepseek\", \"fp16\": True},\n",
        "\n",
        "    # Mistral-7B\n",
        "    {\"name\": \"mistral_qlora_r8\", \"model_name\": \"mistralai/Mistral-7B-v0.3\",\n",
        "     \"use_lora\": True, \"use_qlora\": True, \"lora_r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "     \"batch_size\": 1, \"epochs\": EPOCHS_LARGE, \"tokenizer_group\": \"mistral\", \"fp16\": True},\n",
        "    {\"name\": \"mistral_qlora_r16\", \"model_name\": \"mistralai/Mistral-7B-v0.3\",\n",
        "     \"use_lora\": True, \"use_qlora\": True, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n",
        "     \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "     \"batch_size\": 1, \"epochs\": EPOCHS_LARGE, \"tokenizer_group\": \"mistral\", \"fp16\": True},\n",
        "]\n",
        "\n",
        "# ======================= НАСТРОЙКА ЛОГИРОВАНИЯ =======================\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_folder = Path(RESULTS_BASE) / f\"run_{timestamp}\"\n",
        "results_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "log_file = results_folder / \"experiment.log\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file, encoding='utf-8'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(f\"Результаты будут сохранены в: {results_folder}\")\n",
        "\n",
        "# ======================= ПРОВЕРКА УСТРОЙСТВА =======================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Устройство: {device}\")\n",
        "logger.info(f\"Доступная RAM: {psutil.virtual_memory()}\")\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        logger.info(f\"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Не удалось получить информацию о GPU: {e}\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    try:\n",
        "        login(token=HF_TOKEN)\n",
        "        logger.info(\"Успешный вход в Hugging Face Hub\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка входа: {e}\")\n",
        "\n",
        "# ======================= 1. ЗАГРУЗКА ДАННЫХ =======================\n",
        "logger.info(f\"1. Загрузка датасета {DATASET_ID}...\")\n",
        "ds = load_dataset(DATASET_ID, split=\"train\", token=HF_TOKEN if HF_TOKEN else None)\n",
        "logger.info(f\"Всего записей: {len(ds)}\")\n",
        "\n",
        "if SUBSET_SIZE is not None and SUBSET_SIZE < len(ds):\n",
        "    subset = ds.select(range(SUBSET_SIZE))\n",
        "    logger.info(f\"Используется {SUBSET_SIZE} записей (подвыборка)\")\n",
        "else:\n",
        "    subset = ds\n",
        "    logger.info(f\"Используется полный датасет ({len(subset)} записей)\")\n",
        "\n",
        "TEXT_COL = None\n",
        "for col in [\"text\", \"content\", \"sentence\", \"article\", \"utterance\", \"txt\"]:\n",
        "    if col in subset.column_names:\n",
        "        TEXT_COL = col\n",
        "        break\n",
        "if TEXT_COL is None:\n",
        "    TEXT_COL = subset.column_names[0]\n",
        "logger.info(f\"Колонка с текстом: {TEXT_COL}\")\n",
        "\n",
        "def truncate_batch(batch):\n",
        "    return {TEXT_COL: [t[:TRUNCATE_CHARS] if t else \"\" for t in batch[TEXT_COL]]}\n",
        "\n",
        "logger.info(f\"Обрезка текстов до {TRUNCATE_CHARS} символов...\")\n",
        "subset = subset.map(truncate_batch, batched=True, batch_size=1000)\n",
        "\n",
        "logger.info(\"Разделение на train/val/test...\")\n",
        "train_val = subset.train_test_split(test_size=0.10, seed=SEED)\n",
        "val_test = train_val[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_val[\"train\"],\n",
        "    \"eval\": val_test[\"train\"],\n",
        "    \"test\": val_test[\"test\"]\n",
        "})\n",
        "logger.info(f\"Размеры: { {k: len(dataset[k]) for k in dataset} }\")\n",
        "dataset.save_to_disk(str(results_folder / \"raw_subset\"))\n",
        "logger.info(\"Сырой датасет сохранён.\")\n",
        "\n",
        "# ======================= 2. ТОКЕНИЗАЦИЯ С КЭШИРОВАНИЕМ =======================\n",
        "def prepare_tokenizer(model_group):\n",
        "    if model_group == \"gpt2\":\n",
        "        tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "    elif model_group == \"qwen\":\n",
        "        tok = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", trust_remote_code=True)\n",
        "    elif model_group == \"deepseek\":\n",
        "        tok = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\", trust_remote_code=True)\n",
        "    elif model_group == \"mistral\":\n",
        "        tok = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\", trust_remote_code=True)\n",
        "    else:\n",
        "        tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "def create_tokenize_fn(tokenizer, text_column):\n",
        "    \"\"\"Фабрика функций токенизации с защитой от пустых текстов.\"\"\"\n",
        "    def tokenize_fn(examples):\n",
        "        texts = []\n",
        "        for t in examples[text_column]:\n",
        "            if t is None or str(t).strip() == \"\":\n",
        "                texts.append(\" \")   # один пробел даст один токен\n",
        "            else:\n",
        "                texts.append(str(t))\n",
        "        out = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding=False,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        # labels — копия input_ids; делаем списки чтобы избежать вложенных типов\n",
        "        out[\"labels\"] = [list(ids) for ids in out[\"input_ids\"]]\n",
        "        return out\n",
        "    return tokenize_fn\n",
        "\n",
        "tokenizer_map = {}\n",
        "tokenized_map = {}\n",
        "\n",
        "# Конфигурация токенизации для проверки кэша\n",
        "tokenization_config = {\n",
        "    \"MAX_LENGTH\": MAX_LENGTH,\n",
        "    \"TRUNCATE_CHARS\": TRUNCATE_CHARS,\n",
        "    \"TEXT_COL\": TEXT_COL,\n",
        "    \"DATASET_SIZE\": len(dataset[\"train\"]) + len(dataset[\"eval\"]) + len(dataset[\"test\"])\n",
        "}\n",
        "\n",
        "all_columns = dataset[\"train\"].column_names\n",
        "logger.info(f\"Исходные колонки датасета: {all_columns}\")\n",
        "\n",
        "# Определяем группы токенизаторов (LLaMA удалена)\n",
        "groups = [\"gpt2\", \"qwen\", \"deepseek\", \"mistral\"]\n",
        "\n",
        "for group in groups:\n",
        "    tokenizer = prepare_tokenizer(group)\n",
        "    tokenizer_map[group] = tokenizer\n",
        "    tokenized_path = results_folder / f\"tokenized_{group}\"\n",
        "    tokenized_config_path = tokenized_path / \"tokenization_config.json\"\n",
        "\n",
        "    # Проверяем, есть ли уже токенизированный датасет и совпадает ли конфиг\n",
        "    if tokenized_path.exists() and tokenized_config_path.exists():\n",
        "        with open(tokenized_config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            saved_config = json.load(f)\n",
        "        if saved_config == tokenization_config:\n",
        "            logger.info(f\"Загрузка токенизированного датасета {group} из {tokenized_path}\")\n",
        "            tokenized_map[group] = load_from_disk(str(tokenized_path))\n",
        "            continue\n",
        "        else:\n",
        "            logger.info(f\"Конфигурация токенизации для {group} изменилась, перетокенизация...\")\n",
        "    else:\n",
        "        logger.info(f\"Токенизация для {group}...\")\n",
        "\n",
        "    # Токенизируем\n",
        "    tokenized = dataset.map(\n",
        "        create_tokenize_fn(tokenizer, TEXT_COL),\n",
        "        batched=True,\n",
        "        batch_size=TOKENIZE_BATCH_SIZE,\n",
        "        num_proc=1 if group in [\"qwen\", \"deepseek\", \"mistral\"] else TOKENIZE_NUM_PROC,\n",
        "        remove_columns=all_columns,\n",
        "    )\n",
        "    tokenized.save_to_disk(str(tokenized_path))\n",
        "    with open(tokenized_config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(tokenization_config, f, indent=2)\n",
        "    tokenized_map[group] = tokenized\n",
        "    logger.info(f\"Токенизированный датасет ({group}) сохранён.\")\n",
        "\n",
        "# ======================= 3. ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ =======================\n",
        "def cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def compute_ppl(loss):\n",
        "    try:\n",
        "        return math.exp(loss)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_batch_candidates(initial_bs):\n",
        "    candidates = []\n",
        "    bs = initial_bs\n",
        "    while True:\n",
        "        candidates.append(max(1, int(bs)))\n",
        "        if bs == 1:\n",
        "            break\n",
        "        bs = bs // 2\n",
        "    return candidates\n",
        "\n",
        "# ======================= КАСТОМНЫЙ COLLATOR =======================\n",
        "class DataCollatorForCausalLMWithLabelPad:\n",
        "    def __init__(self, tokenizer, label_pad_token_id: int = -100, pad_to_multiple_of: int = None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.base_collator = DataCollatorWithPadding(tokenizer, padding=True, return_tensors=\"pt\",\n",
        "                                                     pad_to_multiple_of=pad_to_multiple_of)\n",
        "\n",
        "    def __call__(self, features):\n",
        "        allowed_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "        filtered_features = [{k: f[k] for k in allowed_keys if k in f} for f in features]\n",
        "        labels = [f.pop(\"labels\") if \"labels\" in f else None for f in filtered_features]\n",
        "        batch = self.base_collator(filtered_features)\n",
        "        if any(l is not None for l in labels):\n",
        "            max_len = batch[\"input_ids\"].shape[1]\n",
        "            padded_labels = []\n",
        "            for l in labels:\n",
        "                if l is None:\n",
        "                    padded = [self.label_pad_token_id] * max_len\n",
        "                else:\n",
        "                    l = list(l)\n",
        "                    if len(l) < max_len:\n",
        "                        padded = l + [self.label_pad_token_id] * (max_len - len(l))\n",
        "                    else:\n",
        "                        padded = l[:max_len]\n",
        "                padded_labels.append(padded)\n",
        "            batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "# ======================= 4. ЗАПУСК ЭКСПЕРИМЕНТА (с возобновлением) =======================\n",
        "def experiment_already_done(exp_name, seed_val, results_dir):\n",
        "    \"\"\"Проверяет, выполнен ли уже эксперимент с данным seed.\"\"\"\n",
        "    marker_file = results_dir / exp_name / f\"seed{seed_val}\" / \"done.txt\"\n",
        "    return marker_file.exists()\n",
        "\n",
        "def run_experiment(exp_cfg, tokenized_data, tokenizer, results_dir, seed_val):\n",
        "    name = exp_cfg[\"name\"]\n",
        "    model_name = exp_cfg[\"model_name\"]\n",
        "    use_lora = exp_cfg.get(\"use_lora\", False)\n",
        "    use_qlora = exp_cfg.get(\"use_qlora\", False)\n",
        "    use_fp16 = exp_cfg.get(\"fp16\", True)\n",
        "    initial_bs = exp_cfg.get(\"batch_size\", 8)\n",
        "    epochs = exp_cfg.get(\"epochs\", 1)\n",
        "\n",
        "    # Если уже выполнен, возвращаем пустой результат (пропускаем)\n",
        "    if experiment_already_done(name, seed_val, results_dir):\n",
        "        logger.info(f\"Эксперимент {name} (seed={seed_val}) уже выполнен, пропускаем.\")\n",
        "        return {\"skipped\": True, \"name\": name, \"seed\": seed_val}\n",
        "\n",
        "    def _fmt(val, fmt=\"{:.4f}\"):\n",
        "        try:\n",
        "            return fmt.format(val) if val is not None else \"None\"\n",
        "        except Exception:\n",
        "            try:\n",
        "                return str(val)\n",
        "            except Exception:\n",
        "                return \"None\"\n",
        "\n",
        "    batch_candidates = get_batch_candidates(initial_bs)\n",
        "    result = {\n",
        "        \"name\": name,\n",
        "        \"model_name\": model_name,\n",
        "        \"seed\": seed_val,\n",
        "        \"attempts\": [],\n",
        "        \"config\": exp_cfg.copy()\n",
        "    }\n",
        "    start_total = time.time()\n",
        "\n",
        "    for bs in batch_candidates:\n",
        "        logger.info(f\"--- Попытка {name} (seed={seed_val}) с batch_size={bs} ---\")\n",
        "        attempt_start = time.time()\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        model = None\n",
        "        trainer = None\n",
        "        try:\n",
        "            load_kwargs = {\n",
        "                \"low_cpu_mem_usage\": True,\n",
        "                \"trust_remote_code\": True,\n",
        "                \"token\": HF_TOKEN\n",
        "            }\n",
        "\n",
        "            if use_qlora:\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float16\n",
        "                )\n",
        "                load_kwargs[\"quantization_config\"] = bnb_config\n",
        "                load_kwargs[\"device_map\"] = \"auto\"\n",
        "            else:\n",
        "                if use_fp16 and torch.cuda.is_available():\n",
        "                    load_kwargs[\"torch_dtype\"] = torch.float16\n",
        "                else:\n",
        "                    load_kwargs[\"torch_dtype\"] = torch.float32\n",
        "\n",
        "            logger.info(f\"Загрузка модели: {model_name}\")\n",
        "            try:\n",
        "                model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Не удалось загрузить модель {model_name}: {e}\")\n",
        "                result[\"attempts\"].append({\"batch_size\": bs, \"status\": \"load_error\", \"error\": str(e)[:200]})\n",
        "                break  # переходим к следующему batch size? но если модель не грузится, все batch size бесполезны\n",
        "\n",
        "            # --- LoRA/PEFT ---\n",
        "            trainable_params = None\n",
        "            if use_lora:\n",
        "                if use_qlora:\n",
        "                    model = prepare_model_for_kbit_training(model)\n",
        "                target_modules = exp_cfg.get(\"target_modules\")\n",
        "                if target_modules is None:\n",
        "                    target_modules = [\"c_attn\"] if \"gpt2\" in model_name.lower() else [\"q_proj\", \"v_proj\"]\n",
        "                lora_config = LoraConfig(\n",
        "                    r=exp_cfg[\"lora_r\"],\n",
        "                    lora_alpha=exp_cfg[\"lora_alpha\"],\n",
        "                    target_modules=target_modules,\n",
        "                    lora_dropout=exp_cfg.get(\"lora_dropout\", 0.0),\n",
        "                    bias=\"none\",\n",
        "                    task_type=TaskType.CAUSAL_LM\n",
        "                )\n",
        "                model = get_peft_model(model, lora_config)\n",
        "                try:\n",
        "                    trainable_params = model.num_parameters(only_trainable=True)\n",
        "                except Exception:\n",
        "                    trainable_params = None\n",
        "            else:\n",
        "                if not hasattr(model, \"hf_device_map\"):\n",
        "                    model = model.to(device)\n",
        "                try:\n",
        "                    model.gradient_checkpointing_enable()\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            output_dir = results_dir / name / f\"seed{seed_val}\" / f\"bs{bs}\"\n",
        "            output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            optim = \"adamw_8bit\" if use_qlora else \"adamw_torch\"\n",
        "\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=str(output_dir),\n",
        "                per_device_train_batch_size=bs,\n",
        "                per_device_eval_batch_size=bs,\n",
        "                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "                learning_rate=LEARNING_RATE,\n",
        "                num_train_epochs=epochs,\n",
        "                logging_steps=LOGGING_STEPS,\n",
        "                eval_strategy=\"steps\",\n",
        "                eval_steps=LOGGING_STEPS,\n",
        "                save_strategy=\"no\",\n",
        "                report_to=\"none\",\n",
        "                fp16=(use_fp16 and torch.cuda.is_available()),\n",
        "                gradient_checkpointing=True,\n",
        "                optim=optim,\n",
        "                seed=seed_val,  # используем переданный seed\n",
        "                remove_unused_columns=True,\n",
        "                dataloader_num_workers=0,\n",
        "                ddp_find_unused_parameters=False if use_lora else None,\n",
        "            )\n",
        "\n",
        "            data_collator = DataCollatorForCausalLMWithLabelPad(\n",
        "                tokenizer=tokenizer,\n",
        "                label_pad_token_id=-100,\n",
        "                pad_to_multiple_of=8\n",
        "            )\n",
        "\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=tokenized_data[\"train\"],\n",
        "                eval_dataset=tokenized_data[\"eval\"],\n",
        "                data_collator=data_collator,\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                trainer.train()\n",
        "            except RuntimeError as e:\n",
        "                msg = str(e).lower()\n",
        "                if \"out of memory\" in msg or (\"cuda\" in msg and \"memory\" in msg):\n",
        "                    logger.warning(f\"OOM при batch_size={bs}, пробуем меньший...\")\n",
        "                    result[\"attempts\"].append({\"batch_size\": bs, \"status\": \"oom\", \"error\": str(e)[:200]})\n",
        "                    try:\n",
        "                        del trainer\n",
        "                    except:\n",
        "                        pass\n",
        "                    try:\n",
        "                        del model\n",
        "                    except:\n",
        "                        pass\n",
        "                    cleanup()\n",
        "                    continue\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            if use_lora:\n",
        "                adapter_save_path = output_dir / \"adapter\"\n",
        "                model.save_pretrained(adapter_save_path)\n",
        "                logger.info(f\"Адаптер сохранён в {adapter_save_path}\")\n",
        "\n",
        "            try:\n",
        "                with open(output_dir / \"log_history.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(getattr(trainer.state, \"log_history\", []), f, indent=2, ensure_ascii=False)\n",
        "            except Exception as e_log:\n",
        "                logger.warning(f\"Не удалось сохранить log_history: {e_log}\")\n",
        "\n",
        "            eval_metrics = {}\n",
        "            try:\n",
        "                eval_metrics = trainer.evaluate(eval_dataset=tokenized_data[\"test\"])\n",
        "            except Exception as e_eval:\n",
        "                logger.warning(f\"Ошибка при evaluate: {e_eval}\")\n",
        "\n",
        "            test_loss = eval_metrics.get(\"eval_loss\") if isinstance(eval_metrics, dict) else None\n",
        "            test_ppl = compute_ppl(test_loss) if test_loss is not None else None\n",
        "\n",
        "            peak_gpu_mem = None\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
        "                except Exception:\n",
        "                    peak_gpu_mem = None\n",
        "\n",
        "            elapsed = time.time() - attempt_start\n",
        "\n",
        "            loss_str = _fmt(test_loss, \"{:.4f}\")\n",
        "            ppl_str = _fmt(test_ppl, \"{:.2f}\")\n",
        "            gpu_str = _fmt(peak_gpu_mem, \"{:.2f}\")\n",
        "\n",
        "            logger.info(f\"✅ Успех! Loss={loss_str}, PPL={ppl_str}, время={elapsed:.1f}с, пик GPU={gpu_str}GB\")\n",
        "\n",
        "            result.update({\n",
        "                \"status\": \"success\",\n",
        "                \"batch_size\": bs,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_perplexity\": test_ppl,\n",
        "                \"trainable_params\": trainable_params,\n",
        "                \"peak_gpu_memory_gb\": peak_gpu_mem,\n",
        "                \"time_seconds\": elapsed,\n",
        "                \"total_time_seconds\": time.time() - start_total\n",
        "            })\n",
        "            result[\"attempts\"].append({\n",
        "                \"batch_size\": bs,\n",
        "                \"status\": \"success\",\n",
        "                \"eval_loss\": test_loss,\n",
        "                \"perplexity\": test_ppl,\n",
        "                \"time_seconds\": elapsed\n",
        "            })\n",
        "\n",
        "            # Помечаем seed-эксперимент как выполненный (независимо от batch_size)\n",
        "            seed_dir = results_dir / name / f\"seed{seed_val}\"\n",
        "            seed_dir.mkdir(parents=True, exist_ok=True)\n",
        "            with open(seed_dir / \"done.txt\", \"w\") as f:\n",
        "                f.write(f\"success with batch_size={bs}\")\n",
        "\n",
        "            try:\n",
        "                del trainer\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                del model\n",
        "            except:\n",
        "                pass\n",
        "            cleanup()\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при batch_size={bs}: {e}\")\n",
        "            result[\"attempts\"].append({\"batch_size\": bs, \"status\": \"error\", \"error\": str(e)[:200]})\n",
        "            try:\n",
        "                del model\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                del trainer\n",
        "            except:\n",
        "                pass\n",
        "            cleanup()\n",
        "            continue\n",
        "\n",
        "    result[\"status\"] = \"failed_all_bs\"\n",
        "    result[\"total_time_seconds\"] = time.time() - start_total\n",
        "    return result\n",
        "\n",
        "# ======================= 5. ЗАПУСК ВСЕХ ЭКСПЕРИМЕНТОВ (с разными seed) =======================\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"ЗАПУСК ЭКСПЕРИМЕНТОВ\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "all_results = []  # будет список словарей, каждый для одного seed-эксперимента\n",
        "with open(results_folder / \"experiments_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(EXPERIMENTS, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "total_experiments = len(EXPERIMENTS) * NUM_SEEDS\n",
        "current_exp = 0\n",
        "\n",
        "for i, exp in enumerate(EXPERIMENTS, 1):\n",
        "    group = exp[\"tokenizer_group\"]\n",
        "    tokenized_data = tokenized_map.get(group)\n",
        "    tokenizer = tokenizer_map.get(group)\n",
        "\n",
        "    if tokenized_data is None or tokenizer is None:\n",
        "        logger.error(f\"Неизвестная группа токенизатора {group}, пропускаем эксперимент {exp['name']}.\")\n",
        "        continue\n",
        "\n",
        "    for seed_offset in range(NUM_SEEDS):\n",
        "        current_exp += 1\n",
        "        current_seed = SEED + seed_offset\n",
        "        set_seed(current_seed)\n",
        "        logger.info(f\"\\nЭксперимент {current_exp}/{total_experiments}: {exp['name']} (seed={current_seed})\")\n",
        "\n",
        "        res = run_experiment(exp, tokenized_data, tokenizer, results_folder, current_seed)\n",
        "        if not res.get(\"skipped\", False):\n",
        "            all_results.append(res)\n",
        "\n",
        "        # Сохраняем промежуточные результаты после каждого seed-эксперимента\n",
        "        with open(results_folder / \"partial_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ======================= 6. АГРЕГАЦИЯ РЕЗУЛЬТАТОВ ПО ЭКСПЕРИМЕНТАМ =======================\n",
        "# Группируем по имени эксперимента (без seed)\n",
        "grouped = {}\n",
        "for r in all_results:\n",
        "    if r.get(\"status\") == \"success\":\n",
        "        name = r[\"name\"]\n",
        "        if name not in grouped:\n",
        "            grouped[name] = []\n",
        "        grouped[name].append(r)\n",
        "\n",
        "# Для каждого имени считаем среднее и стандартное отклонение по PPL и loss\n",
        "aggregated = []\n",
        "for name, runs in grouped.items():\n",
        "    ppls = [r[\"test_perplexity\"] for r in runs if r[\"test_perplexity\"] is not None]\n",
        "    losses = [r[\"test_loss\"] for r in runs if r[\"test_loss\"] is not None]\n",
        "    if not ppls:\n",
        "        continue\n",
        "    agg = {\n",
        "        \"name\": name,\n",
        "        \"model_name\": runs[0][\"model_name\"],\n",
        "        \"num_seeds\": len(runs),\n",
        "        \"test_perplexity_mean\": float(np.mean(ppls)),\n",
        "        \"test_perplexity_std\": float(np.std(ppls)),\n",
        "        \"test_loss_mean\": float(np.mean(losses)),\n",
        "        \"test_loss_std\": float(np.std(losses)),\n",
        "        \"peak_gpu_memory_gb_mean\": float(np.mean([r[\"peak_gpu_memory_gb\"] for r in runs if r[\"peak_gpu_memory_gb\"]])),\n",
        "        \"time_seconds_mean\": float(np.mean([r[\"time_seconds\"] for r in runs])),\n",
        "        \"trainable_params\": runs[0].get(\"trainable_params\"),\n",
        "        \"all_seeds\": runs\n",
        "    }\n",
        "    aggregated.append(agg)\n",
        "\n",
        "# Сохраняем агрегированные результаты\n",
        "with open(results_folder / \"aggregated_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(aggregated, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# CSV с агрегированными данными\n",
        "csv_agg_file = results_folder / \"aggregated_results.csv\"\n",
        "fieldnames_agg = [\"name\", \"model_name\", \"num_seeds\", \"test_perplexity_mean\", \"test_perplexity_std\",\n",
        "                  \"test_loss_mean\", \"test_loss_std\", \"peak_gpu_memory_gb_mean\", \"time_seconds_mean\", \"trainable_params\"]\n",
        "with open(csv_agg_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames_agg)\n",
        "    writer.writeheader()\n",
        "    for a in aggregated:\n",
        "        writer.writerow({\n",
        "            \"name\": a[\"name\"],\n",
        "            \"model_name\": a[\"model_name\"],\n",
        "            \"num_seeds\": a[\"num_seeds\"],\n",
        "            \"test_perplexity_mean\": f\"{a['test_perplexity_mean']:.4f}\",\n",
        "            \"test_perplexity_std\": f\"{a['test_perplexity_std']:.4f}\",\n",
        "            \"test_loss_mean\": f\"{a['test_loss_mean']:.4f}\",\n",
        "            \"test_loss_std\": f\"{a['test_loss_std']:.4f}\",\n",
        "            \"peak_gpu_memory_gb_mean\": f\"{a['peak_gpu_memory_gb_mean']:.2f}\",\n",
        "            \"time_seconds_mean\": f\"{a['time_seconds_mean']:.1f}\",\n",
        "            \"trainable_params\": a[\"trainable_params\"]\n",
        "        })\n",
        "\n",
        "logger.info(\"\\nВсе эксперименты завершены. Результаты сохранены.\")\n",
        "\n",
        "# ======================= 7. ВИЗУАЛИЗАЦИЯ =======================\n",
        "# Определяем, запущено ли в Jupyter\n",
        "in_jupyter = 'ipykernel' in sys.modules\n",
        "\n",
        "if aggregated:\n",
        "    # Сортируем по средней PPL\n",
        "    aggregated.sort(key=lambda x: x[\"test_perplexity_mean\"])\n",
        "    names = [a[\"name\"] for a in aggregated]\n",
        "    ppl_means = [a[\"test_perplexity_mean\"] for a in aggregated]\n",
        "    ppl_stds = [a[\"test_perplexity_std\"] for a in aggregated]\n",
        "\n",
        "    # График 1: Perplexity с error bars\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    y_pos = np.arange(len(names))\n",
        "    plt.barh(y_pos, ppl_means, xerr=ppl_stds, capsize=5, color='steelblue')\n",
        "    plt.yticks(y_pos, names)\n",
        "    plt.xlabel(\"Perplexity (ниже лучше)\")\n",
        "    plt.title(\"Сравнение перплексии на тестовом наборе (среднее ± std)\")\n",
        "    for i, (mean, std) in enumerate(zip(ppl_means, ppl_stds)):\n",
        "        plt.text(mean + std + 0.1, i, f'{mean:.2f} ± {std:.2f}', va='center', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(results_folder / \"perplexity_comparison_with_std.png\", dpi=150)\n",
        "    if in_jupyter:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    # График 2: Boxplot распределения PPL по seed\n",
        "    data_for_box = [grouped[name] for name in names if name in grouped]\n",
        "    if data_for_box:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        box_data = [[r[\"test_perplexity\"] for r in runs] for runs in data_for_box]\n",
        "        plt.boxplot(box_data, labels=names, vert=False)\n",
        "        plt.xlabel(\"Perplexity\")\n",
        "        plt.title(\"Распределение перплексии по seed для каждого эксперимента\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(results_folder / \"perplexity_boxplot.png\", dpi=150)\n",
        "        if in_jupyter:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "    # График 3: Perplexity vs Trainable Params (для LoRA)\n",
        "    lora_agg = [a for a in aggregated if a.get(\"trainable_params\") is not None]\n",
        "    if lora_agg:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        params = [a[\"trainable_params\"] / 1e6 for a in lora_agg]\n",
        "        ppl_lora = [a[\"test_perplexity_mean\"] for a in lora_agg]\n",
        "        err = [a[\"test_perplexity_std\"] for a in lora_agg]\n",
        "        labels = [a[\"name\"] for a in lora_agg]\n",
        "        plt.errorbar(params, ppl_lora, yerr=err, fmt='o', capsize=5)\n",
        "        for i, label in enumerate(labels):\n",
        "            plt.annotate(label, (params[i], ppl_lora[i]), fontsize=8)\n",
        "        plt.xlabel(\"Trainable parameters (millions)\")\n",
        "        plt.ylabel(\"Perplexity\")\n",
        "        plt.title(\"Perplexity vs Number of trainable parameters (LoRA)\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig(results_folder / \"perplexity_vs_params.png\", dpi=150)\n",
        "        if in_jupyter:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close()\n",
        "\n",
        "    # HTML-отчёт\n",
        "    html_report = results_folder / \"report.html\"\n",
        "    with open(html_report, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"<html><head><title>Experiment Report</title>\")\n",
        "        f.write(\"<style>table {border-collapse: collapse;} th, td {border: 1px solid black; padding: 8px;} th {background-color: #f2f2f2;}</style>\")\n",
        "        f.write(\"</head><body>\")\n",
        "        f.write(f\"<h1>Experiment Report - {timestamp}</h1>\")\n",
        "        f.write(f\"<p>Total experiments: {len(all_results)} successful runs across {len(aggregated)} configurations.</p>\")\n",
        "        f.write(\"<h2>Aggregated Results (mean ± std)</h2>\")\n",
        "        f.write(\"<table><tr><th>Name</th><th>Model</th><th>Seeds</th><th>Perplexity</th><th>Loss</th><th>GPU (GB)</th><th>Time (s)</th><th>Trainable Params</th></tr>\")\n",
        "        for a in aggregated:\n",
        "            f.write(f\"<tr><td>{a['name']}</td><td>{a['model_name']}</td><td>{a['num_seeds']}</td>\")\n",
        "            f.write(f\"<td>{a['test_perplexity_mean']:.2f} ± {a['test_perplexity_std']:.2f}</td>\")\n",
        "            f.write(f\"<td>{a['test_loss_mean']:.4f} ± {a['test_loss_std']:.4f}</td>\")\n",
        "            f.write(f\"<td>{a['peak_gpu_memory_gb_mean']:.2f}</td><td>{a['time_seconds_mean']:.1f}</td>\")\n",
        "            f.write(f\"<td>{a['trainable_params']}</td></tr>\")\n",
        "        f.write(\"</table>\")\n",
        "        f.write(\"</body></html>\")\n",
        "    logger.info(f\"HTML-отчёт сохранён: {html_report}\")\n",
        "\n",
        "# ======================= 8. АРХИВАЦИЯ И АВТОМАТИЧЕСКОЕ СКАЧИВАНИЕ =======================\n",
        "zip_path = results_folder.parent / f\"{results_folder.name}.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for root, _, files in os.walk(results_folder):\n",
        "        for file in files:\n",
        "            zf.write(os.path.join(root, file),\n",
        "                     arcname=os.path.relpath(os.path.join(root, file), results_folder))\n",
        "logger.info(f\"Архив с результатами: {zip_path} ({os.path.getsize(zip_path)/1e6:.2f} MB)\")\n",
        "\n",
        "# Автоматическое скачивание в локальный компьютер (только в Jupyter)\n",
        "if in_jupyter:\n",
        "    try:\n",
        "        # Получаем относительный путь от текущей директории для URL\n",
        "        try:\n",
        "            rel_path = zip_path.relative_to(Path.cwd())\n",
        "            url = f\"/files/{rel_path}\"\n",
        "        except ValueError:\n",
        "            # Если архив не в текущей директории, используем просто имя файла (может не работать)\n",
        "            url = f\"/files/{zip_path.name}\"\n",
        "\n",
        "        js_code = f\"\"\"\n",
        "        // Автоматическое скачивание файла\n",
        "        var link = document.createElement('a');\n",
        "        link.href = '{url}';\n",
        "        link.download = '{zip_path.name}';\n",
        "        link.style.display = 'none';\n",
        "        document.body.appendChild(link);\n",
        "        link.click();\n",
        "        document.body.removeChild(link);\n",
        "        console.log('Скачивание {zip_path.name} запущено автоматически.');\n",
        "        \"\"\"\n",
        "        display(Javascript(js_code))\n",
        "        logger.info(f\"Автоматическое скачивание {zip_path.name} запущено в браузере.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Не удалось выполнить автоскачивание: {e}\")\n",
        "        # В качестве запасного варианта показываем ссылку\n",
        "        try:\n",
        "            display(FileLink(zip_path, result_html_prefix=\"Не удалось скачать автоматически. Нажмите для скачивания: \"))\n",
        "        except Exception:\n",
        "            pass\n",
        "else:\n",
        "    logger.info(f\"Архив доступен по пути: {zip_path}\")\n",
        "\n",
        "cleanup()\n",
        "logger.info(\"Пайплайн полностью завершён.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ba9db2",
      "metadata": {
        "id": "d0ba9db2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195c2ee9",
      "metadata": {
        "id": "195c2ee9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa10a35",
      "metadata": {
        "id": "4fa10a35"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.3/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0d64ab",
      "metadata": {
        "id": "2a0d64ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c914527",
      "metadata": {
        "id": "8c914527"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84bc0fd8",
      "metadata": {
        "id": "84bc0fd8",
        "outputId": "e92859ad-3527-4c88-8020-63d891922302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        }
      ],
      "source": [
        "print(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3075e8",
      "metadata": {
        "id": "5e3075e8",
        "outputId": "5e10ef60-0144-4b3f-ecaf-49d117228a86",
        "colab": {
          "referenced_widgets": [
            "38efd4e10d1e43e6b53d0d303a30c33f",
            "ca692f919cea4e7f901b5ca3e629bfc8",
            "ff47e090164346b6af916d8310b9e470",
            "30a4f64ce5a44fd28bb24efdcd48e48c",
            "f428725a6f9d49ddb62548efb5f7da38",
            "47e960e4c1354e12bdb25fd3534baac5"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Используем результаты из: tat_results_final_rf/run_20260223_070746\n",
            "2026-02-23 19:38:10,417 - INFO - Дозапуск экспериментов в: tat_results_final_rf/run_20260223_070746\n",
            "2026-02-23 19:38:10,418 - INFO - Лог файл: tat_results_final_rf/run_20260223_070746/resume_20260223_193810.log\n",
            "2026-02-23 19:38:10,428 - INFO - Устройство: cuda\n",
            "2026-02-23 19:38:10,479 - INFO - GPU: Tesla V100-SXM2-32GB\n",
            "2026-02-23 19:38:10,480 - INFO - Память GPU: 34.08 GB\n",
            "2026-02-23 19:38:10,840 - INFO - HTTP Request: GET https://huggingface.co/api/whoami-v2 \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:10,844 - INFO - Успешный вход в Hugging Face Hub\n",
            "2026-02-23 19:38:10,847 - INFO - \n",
            "============================================================\n",
            "2026-02-23 19:38:10,848 - INFO - ЗАГРУЗКА ТОКЕНИЗИРОВАННЫХ ДАННЫХ\n",
            "2026-02-23 19:38:10,849 - INFO - ============================================================\n",
            "2026-02-23 19:38:10,850 - INFO - Загрузка токенизированного датасета gpt2...\n",
            "2026-02-23 19:38:11,043 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:11,386 - INFO - HTTP Request: HEAD https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:11,589 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilgpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:12,134 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilgpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 19:38:12,314 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilgpt2/tree/main?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:12,503 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilgpt2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:13,667 - INFO - ✓ Токенизатор gpt2 загружен\n",
            "2026-02-23 19:38:13,668 - INFO - Загрузка токенизированного датасета qwen...\n",
            "2026-02-23 19:38:14,134 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:14,186 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:14,364 - INFO - HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:14,411 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2.5-7B-Instruct/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:14,594 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-7B-Instruct/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 19:38:14,772 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-7B-Instruct/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:15,663 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-7B-Instruct \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:15,666 - INFO - ✓ Токенизатор qwen загружен\n",
            "2026-02-23 19:38:15,673 - INFO - Загрузка токенизированного датасета deepseek...\n",
            "2026-02-23 19:38:15,860 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:15,907 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:16,089 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:16,136 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:16,315 - INFO - HTTP Request: HEAD https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:16,362 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/deepseek-ai/deepseek-llm-7b-chat/afbda8b347ec881666061fa67447046fc5164ec8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:16,541 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 19:38:17,134 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:18,648 - INFO - HTTP Request: GET https://huggingface.co/api/models/deepseek-ai/deepseek-llm-7b-chat \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:18,695 - INFO - ✓ Токенизатор deepseek загружен\n",
            "2026-02-23 19:38:18,696 - INFO - Загрузка токенизированного датасета mistral...\n",
            "2026-02-23 19:38:18,890 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:19,186 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:19,372 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:19,421 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:19,830 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:19,879 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:20,058 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 19:38:20,442 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-v0.3/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:20,679 - INFO - ✓ Токенизатор mistral загружен\n",
            "2026-02-23 19:38:20,683 - INFO - \n",
            "============================================================\n",
            "2026-02-23 19:38:20,683 - INFO - ПОИСК НЕЗАВЕРШЁННЫХ ЭКСПЕРИМЕНТОВ\n",
            "2026-02-23 19:38:20,684 - INFO - ============================================================\n",
            "2026-02-23 19:38:20,685 - INFO - 🚫 Исключаем проблемный: deepseek_qlora_r16 (seed=44)\n",
            "2026-02-23 19:38:20,686 - INFO - ⏳ Не запущен: mistral_qlora_r8 (seed=42)\n",
            "2026-02-23 19:38:20,687 - INFO - ⏳ Не запущен: mistral_qlora_r8 (seed=43)\n",
            "2026-02-23 19:38:20,688 - INFO - ⏳ Не запущен: mistral_qlora_r8 (seed=44)\n",
            "2026-02-23 19:38:20,689 - INFO - ⏳ Не запущен: mistral_qlora_r16 (seed=42)\n",
            "2026-02-23 19:38:20,690 - INFO - ⏳ Не запущен: mistral_qlora_r16 (seed=43)\n",
            "2026-02-23 19:38:20,692 - INFO - ⏳ Не запущен: mistral_qlora_r16 (seed=44)\n",
            "2026-02-23 19:38:20,693 - INFO - Исключено проблемных экспериментов: 1\n",
            "2026-02-23 19:38:20,693 - INFO - Найдено 6 незавершённых экспериментов\n",
            "2026-02-23 19:38:20,694 - INFO - \n",
            "📋 Список для дозапуска:\n",
            "2026-02-23 19:38:20,695 - INFO - 1. ⏳ mistral_qlora_r8 (seed=42) - not_started\n",
            "2026-02-23 19:38:20,697 - INFO - 2. ⏳ mistral_qlora_r8 (seed=43) - not_started\n",
            "2026-02-23 19:38:20,697 - INFO - 3. ⏳ mistral_qlora_r8 (seed=44) - not_started\n",
            "2026-02-23 19:38:20,698 - INFO - 4. ⏳ mistral_qlora_r16 (seed=42) - not_started\n",
            "2026-02-23 19:38:20,701 - INFO - 5. ⏳ mistral_qlora_r16 (seed=43) - not_started\n",
            "2026-02-23 19:38:20,703 - INFO - 6. ⏳ mistral_qlora_r16 (seed=44) - not_started\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 19:38:20,704 - INFO - \n",
            "============================================================\n",
            "2026-02-23 19:38:20,705 - INFO - ОПТИМИЗАЦИЯ НАСТРОЕК ДЛЯ 7B МОДЕЛЕЙ\n",
            "2026-02-23 19:38:20,706 - INFO - ============================================================\n",
            "2026-02-23 19:38:20,707 - INFO - ✓ Оптимизирован mistral_qlora_r8: batch_size=1, QLoRA=True\n",
            "2026-02-23 19:38:20,707 - INFO - ✓ Оптимизирован mistral_qlora_r8: batch_size=1, QLoRA=True\n",
            "2026-02-23 19:38:20,708 - INFO - ✓ Оптимизирован mistral_qlora_r8: batch_size=1, QLoRA=True\n",
            "2026-02-23 19:38:20,708 - INFO - ✓ Оптимизирован mistral_qlora_r16: batch_size=1, QLoRA=True\n",
            "2026-02-23 19:38:20,708 - INFO - ✓ Оптимизирован mistral_qlora_r16: batch_size=1, QLoRA=True\n",
            "2026-02-23 19:38:20,709 - INFO - ✓ Оптимизирован mistral_qlora_r16: batch_size=1, QLoRA=True\n",
            "2026-02-23 19:38:20,710 - INFO - \n",
            "============================================================\n",
            "2026-02-23 19:38:20,710 - INFO - ЗАПУСК НЕЗАВЕРШЁННЫХ ЭКСПЕРИМЕНТОВ\n",
            "2026-02-23 19:38:20,712 - INFO - ============================================================\n",
            "2026-02-23 19:38:20,714 - INFO - \n",
            "[1/6] ==================================================\n",
            "2026-02-23 19:38:20,715 - INFO - Запуск: mistral_qlora_r8 (seed=42)\n",
            "2026-02-23 19:38:20,715 - INFO - ==================================================\n",
            "2026-02-23 19:38:20,720 - INFO - Очистка памяти перед запуском...\n",
            "2026-02-23 19:38:20,925 - INFO - GPU память: 0.00GB / 0.00GB\n",
            "2026-02-23 19:38:20,930 - INFO - --- Попытка mistral_qlora_r8 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 19:38:20,930 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 19:38:21,110 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:21,158 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:21,342 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:21,400 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38efd4e10d1e43e6b53d0d303a30c33f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 19:38:30,574 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 19:38:30,615 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 19:38:30,789 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 19:38:32,563 - INFO - 📊 Trainable params: 20,971,520\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 43:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.319622</td>\n",
              "      <td>2.117719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.990593</td>\n",
              "      <td>1.943678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.813255</td>\n",
              "      <td>1.860499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.767526</td>\n",
              "      <td>1.744646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.707189</td>\n",
              "      <td>1.660491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.610729</td>\n",
              "      <td>1.625412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.579155</td>\n",
              "      <td>1.536053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.532296</td>\n",
              "      <td>1.509542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.514023</td>\n",
              "      <td>1.483354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.174462</td>\n",
              "      <td>1.479413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.169152</td>\n",
              "      <td>1.460679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.095877</td>\n",
              "      <td>1.437708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.095480</td>\n",
              "      <td>1.408568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.059064</td>\n",
              "      <td>1.400657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.066337</td>\n",
              "      <td>1.389908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.007649</td>\n",
              "      <td>1.385786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.106416</td>\n",
              "      <td>1.366122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.036292</td>\n",
              "      <td>1.360141</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 20:22:05,395 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 20:22:05,439 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 20:22:06,568 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 20:22:06,613 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 20:22:06,855 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/mistral_qlora_r8/seed42/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 20:22:21,002 - INFO - ✅ Успех! Loss=1.3930, PPL=4.03, время=2640.1с, пик GPU=14.04GB\n",
            "2026-02-23 20:22:21,366 - INFO - GPU память: 0.55GB / 4.15GB\n",
            "2026-02-23 20:22:21,366 - INFO - ✅ Завершён: mistral_qlora_r8 (seed=42)\n",
            "2026-02-23 20:22:21,370 - INFO - 📊 Прогресс: 1/6 (16.7%)\n",
            "2026-02-23 20:22:21,612 - INFO - GPU память: 0.55GB / 4.15GB\n",
            "2026-02-23 20:22:27,127 - INFO - \n",
            "[2/6] ==================================================\n",
            "2026-02-23 20:22:27,129 - INFO - Запуск: mistral_qlora_r8 (seed=43)\n",
            "2026-02-23 20:22:27,130 - INFO - ==================================================\n",
            "2026-02-23 20:22:27,131 - INFO - Очистка памяти перед запуском...\n",
            "2026-02-23 20:22:29,129 - INFO - GPU память: 0.55GB / 4.15GB\n",
            "2026-02-23 20:22:29,133 - INFO - --- Попытка mistral_qlora_r8 (seed=43) с batch_size=1 ---\n",
            "2026-02-23 20:22:29,134 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 20:22:30,299 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 20:22:30,343 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 20:22:30,519 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 20:22:30,565 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca692f919cea4e7f901b5ca3e629bfc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 20:22:36,129 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 20:22:36,175 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 20:22:36,580 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 20:22:37,789 - INFO - 📊 Trainable params: 20,971,520\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 43:36, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.343611</td>\n",
              "      <td>2.099534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.024062</td>\n",
              "      <td>1.946646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.837769</td>\n",
              "      <td>1.839644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.754997</td>\n",
              "      <td>1.743075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.669296</td>\n",
              "      <td>1.666258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.610045</td>\n",
              "      <td>1.605640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.497518</td>\n",
              "      <td>1.565596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.605381</td>\n",
              "      <td>1.506596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.488345</td>\n",
              "      <td>1.465487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.094387</td>\n",
              "      <td>1.464359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.122266</td>\n",
              "      <td>1.448333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.145942</td>\n",
              "      <td>1.440843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.133584</td>\n",
              "      <td>1.406943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.091851</td>\n",
              "      <td>1.396219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.060144</td>\n",
              "      <td>1.386176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.060341</td>\n",
              "      <td>1.377587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.021435</td>\n",
              "      <td>1.369418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.024964</td>\n",
              "      <td>1.362896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 21:06:17,397 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:06:17,441 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:06:17,607 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:06:17,647 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:06:18,198 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/mistral_qlora_r8/seed43/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:15]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 21:06:33,796 - INFO - ✅ Успех! Loss=1.4089, PPL=4.09, время=2644.7с, пик GPU=14.77GB\n",
            "2026-02-23 21:06:34,324 - INFO - GPU память: 1.09GB / 4.15GB\n",
            "2026-02-23 21:06:34,325 - INFO - ✅ Завершён: mistral_qlora_r8 (seed=43)\n",
            "2026-02-23 21:06:34,329 - INFO - 📊 Прогресс: 2/6 (33.3%)\n",
            "2026-02-23 21:06:34,594 - INFO - GPU память: 1.09GB / 4.15GB\n",
            "2026-02-23 21:06:39,600 - INFO - \n",
            "[3/6] ==================================================\n",
            "2026-02-23 21:06:39,601 - INFO - Запуск: mistral_qlora_r8 (seed=44)\n",
            "2026-02-23 21:06:39,602 - INFO - ==================================================\n",
            "2026-02-23 21:06:39,603 - INFO - Очистка памяти перед запуском...\n",
            "2026-02-23 21:06:39,893 - INFO - GPU память: 1.09GB / 4.15GB\n",
            "2026-02-23 21:06:39,901 - INFO - --- Попытка mistral_qlora_r8 (seed=44) с batch_size=1 ---\n",
            "2026-02-23 21:06:39,901 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 21:06:40,147 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:06:40,187 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:06:40,356 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:06:40,397 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff47e090164346b6af916d8310b9e470",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 21:06:47,906 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:06:47,947 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:06:48,118 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 21:06:50,631 - INFO - 📊 Trainable params: 20,971,520\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 44:22, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.272328</td>\n",
              "      <td>2.102236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.974162</td>\n",
              "      <td>1.921032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.915911</td>\n",
              "      <td>1.788537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.783641</td>\n",
              "      <td>1.705284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.685617</td>\n",
              "      <td>1.632681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.600282</td>\n",
              "      <td>1.573993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.587259</td>\n",
              "      <td>1.549856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.493367</td>\n",
              "      <td>1.518706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.482950</td>\n",
              "      <td>1.476625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.171989</td>\n",
              "      <td>1.493946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.067761</td>\n",
              "      <td>1.450931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.124423</td>\n",
              "      <td>1.430187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.179556</td>\n",
              "      <td>1.411018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.034159</td>\n",
              "      <td>1.401933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.057447</td>\n",
              "      <td>1.386686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.042890</td>\n",
              "      <td>1.367930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.056723</td>\n",
              "      <td>1.358531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.054516</td>\n",
              "      <td>1.354692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 21:51:20,738 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:51:20,779 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:51:20,944 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:51:20,984 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:51:21,498 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/mistral_qlora_r8/seed44/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 21:51:36,790 - INFO - ✅ Успех! Loss=1.3875, PPL=4.00, время=2696.9с, пик GPU=15.30GB\n",
            "2026-02-23 21:51:37,222 - INFO - GPU память: 1.63GB / 4.15GB\n",
            "2026-02-23 21:51:37,223 - INFO - ✅ Завершён: mistral_qlora_r8 (seed=44)\n",
            "2026-02-23 21:51:37,227 - INFO - 📊 Прогресс: 3/6 (50.0%)\n",
            "2026-02-23 21:51:37,488 - INFO - GPU память: 1.63GB / 4.15GB\n",
            "2026-02-23 21:51:42,650 - INFO - \n",
            "[4/6] ==================================================\n",
            "2026-02-23 21:51:42,651 - INFO - Запуск: mistral_qlora_r16 (seed=42)\n",
            "2026-02-23 21:51:42,652 - INFO - ==================================================\n",
            "2026-02-23 21:51:42,653 - INFO - Очистка памяти перед запуском...\n",
            "2026-02-23 21:51:42,933 - INFO - GPU память: 1.63GB / 4.15GB\n",
            "2026-02-23 21:51:42,940 - INFO - --- Попытка mistral_qlora_r16 (seed=42) с batch_size=1 ---\n",
            "2026-02-23 21:51:42,941 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 21:51:43,197 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:51:43,241 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:51:43,465 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:51:43,508 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30a4f64ce5a44fd28bb24efdcd48e48c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 21:51:54,778 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 21:51:55,068 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 21:51:55,242 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 21:52:01,331 - INFO - 📊 Trainable params: 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 43:49, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.316013</td>\n",
              "      <td>2.128011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.991828</td>\n",
              "      <td>1.944413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.815672</td>\n",
              "      <td>1.849716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.759529</td>\n",
              "      <td>1.735796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.693200</td>\n",
              "      <td>1.657593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.620865</td>\n",
              "      <td>1.602601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.586967</td>\n",
              "      <td>1.539153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.535670</td>\n",
              "      <td>1.510348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.509992</td>\n",
              "      <td>1.477000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.135051</td>\n",
              "      <td>1.465307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.106322</td>\n",
              "      <td>1.464462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.049583</td>\n",
              "      <td>1.434389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.038120</td>\n",
              "      <td>1.397147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.020362</td>\n",
              "      <td>1.394813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.014052</td>\n",
              "      <td>1.387283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.944028</td>\n",
              "      <td>1.377534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.049083</td>\n",
              "      <td>1.354777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.965897</td>\n",
              "      <td>1.351245</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 22:35:56,487 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 22:35:56,534 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 22:35:56,708 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 22:35:56,751 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 22:35:57,265 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/mistral_qlora_r16/seed42/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:17]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 22:36:15,047 - INFO - ✅ Успех! Loss=1.3783, PPL=3.97, время=2672.1с, пик GPU=15.75GB\n",
            "2026-02-23 22:36:16,051 - INFO - GPU память: 2.16GB / 4.15GB\n",
            "2026-02-23 22:36:16,052 - INFO - ✅ Завершён: mistral_qlora_r16 (seed=42)\n",
            "2026-02-23 22:36:16,061 - INFO - 📊 Прогресс: 4/6 (66.7%)\n",
            "2026-02-23 22:36:16,337 - INFO - GPU память: 2.16GB / 4.15GB\n",
            "2026-02-23 22:36:21,342 - INFO - \n",
            "[5/6] ==================================================\n",
            "2026-02-23 22:36:21,344 - INFO - Запуск: mistral_qlora_r16 (seed=43)\n",
            "2026-02-23 22:36:21,345 - INFO - ==================================================\n",
            "2026-02-23 22:36:21,349 - INFO - Очистка памяти перед запуском...\n",
            "2026-02-23 22:36:21,643 - INFO - GPU память: 2.16GB / 4.15GB\n",
            "2026-02-23 22:36:21,650 - INFO - --- Попытка mistral_qlora_r16 (seed=43) с batch_size=1 ---\n",
            "2026-02-23 22:36:21,651 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 22:36:21,903 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 22:36:21,944 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 22:36:22,150 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 22:36:22,192 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f428725a6f9d49ddb62548efb5f7da38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 22:36:28,357 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 22:36:28,397 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 22:36:28,915 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 22:36:36,451 - INFO - 📊 Trainable params: 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 43:52, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.343172</td>\n",
              "      <td>2.099026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.041057</td>\n",
              "      <td>1.956778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.849766</td>\n",
              "      <td>1.853979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.754476</td>\n",
              "      <td>1.717873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.658997</td>\n",
              "      <td>1.668223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.624013</td>\n",
              "      <td>1.611754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.502897</td>\n",
              "      <td>1.550317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.604491</td>\n",
              "      <td>1.495245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.500741</td>\n",
              "      <td>1.459683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.055760</td>\n",
              "      <td>1.456348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.079792</td>\n",
              "      <td>1.457659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.112320</td>\n",
              "      <td>1.424998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.089424</td>\n",
              "      <td>1.407314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.053443</td>\n",
              "      <td>1.391337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.014730</td>\n",
              "      <td>1.371699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.020620</td>\n",
              "      <td>1.364235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.967996</td>\n",
              "      <td>1.357205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.974330</td>\n",
              "      <td>1.348029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 23:20:33,233 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 23:20:33,277 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 23:20:33,447 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 23:20:33,487 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 23:20:33,982 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/mistral_qlora_r16/seed43/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 23:20:47,896 - INFO - ✅ Успех! Loss=1.3969, PPL=4.04, время=2666.2с, пик GPU=16.38GB\n",
            "2026-02-23 23:20:48,302 - INFO - GPU память: 2.70GB / 6.19GB\n",
            "2026-02-23 23:20:48,303 - INFO - ✅ Завершён: mistral_qlora_r16 (seed=43)\n",
            "2026-02-23 23:20:48,309 - INFO - 📊 Прогресс: 5/6 (83.3%)\n",
            "2026-02-23 23:20:48,575 - INFO - GPU память: 2.70GB / 6.19GB\n",
            "2026-02-23 23:20:53,582 - INFO - \n",
            "[6/6] ==================================================\n",
            "2026-02-23 23:20:53,592 - INFO - Запуск: mistral_qlora_r16 (seed=44)\n",
            "2026-02-23 23:20:53,593 - INFO - ==================================================\n",
            "2026-02-23 23:20:53,594 - INFO - Очистка памяти перед запуском...\n",
            "2026-02-23 23:20:54,790 - INFO - GPU память: 2.70GB / 6.19GB\n",
            "2026-02-23 23:20:54,798 - INFO - --- Попытка mistral_qlora_r16 (seed=44) с batch_size=1 ---\n",
            "2026-02-23 23:20:54,798 - INFO - Загрузка модели: mistralai/Mistral-7B-v0.3\n",
            "2026-02-23 23:20:55,047 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 23:20:55,090 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 23:20:55,261 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 23:20:55,305 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47e960e4c1354e12bdb25fd3534baac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-23 23:21:02,271 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-23 23:21:02,312 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-23 23:21:02,488 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/custom_generate/generate.py \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-23 23:21:10,274 - INFO - 📊 Trainable params: 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 43:25, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.268313</td>\n",
              "      <td>2.092712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.972627</td>\n",
              "      <td>1.916263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.926100</td>\n",
              "      <td>1.791169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.785473</td>\n",
              "      <td>1.685296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.685786</td>\n",
              "      <td>1.611120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.601014</td>\n",
              "      <td>1.575578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.595046</td>\n",
              "      <td>1.544341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.492549</td>\n",
              "      <td>1.514600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.481607</td>\n",
              "      <td>1.496233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.119906</td>\n",
              "      <td>1.494224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.014314</td>\n",
              "      <td>1.465229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.082440</td>\n",
              "      <td>1.445532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.128716</td>\n",
              "      <td>1.396618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.983605</td>\n",
              "      <td>1.397938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.996059</td>\n",
              "      <td>1.385724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.981646</td>\n",
              "      <td>1.367628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.995962</td>\n",
              "      <td>1.357524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.993509</td>\n",
              "      <td>1.350689</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-24 00:04:39,743 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 00:04:39,786 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 00:04:39,954 - INFO - HTTP Request: HEAD https://huggingface.co/mistralai/Mistral-7B-v0.3/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 00:04:39,996 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/mistralai/Mistral-7B-v0.3/caa1feb0e54d415e2df31207e5f4e273e33509b1/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 00:04:40,877 - INFO - Адаптер сохранён в tat_results_final_rf/run_20260223_070746/mistral_qlora_r16/seed44/bs1/adapter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-24 00:04:55,095 - INFO - ✅ Успех! Loss=1.3692, PPL=3.93, время=2640.3с, пик GPU=16.92GB\n",
            "2026-02-24 00:04:56,458 - INFO - GPU память: 3.24GB / 6.19GB\n",
            "2026-02-24 00:04:56,459 - INFO - ✅ Завершён: mistral_qlora_r16 (seed=44)\n",
            "2026-02-24 00:04:56,465 - INFO - 📊 Прогресс: 6/6 (100.0%)\n",
            "2026-02-24 00:04:56,721 - INFO - GPU память: 3.24GB / 6.19GB\n",
            "2026-02-24 00:05:01,727 - INFO - \n",
            "============================================================\n",
            "2026-02-24 00:05:01,729 - INFO - ДОЗАПУСК ЗАВЕРШЁН\n",
            "2026-02-24 00:05:01,732 - INFO - Дополнительно выполнено: 6 экспериментов\n",
            "2026-02-24 00:05:01,733 - INFO - ============================================================\n",
            "2026-02-24 00:05:01,737 - INFO - \n",
            "============================================================\n",
            "2026-02-24 00:05:01,738 - INFO - ОБНОВЛЕНИЕ АГРЕГИРОВАННЫХ РЕЗУЛЬТАТОВ\n",
            "2026-02-24 00:05:01,739 - INFO - ============================================================\n",
            "2026-02-24 00:05:01,758 - INFO - Агрегированные результаты сохранены в tat_results_final_rf/run_20260223_070746/aggregated_results.csv\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-24 00:05:03,556 - INFO - \n",
            "============================================================\n",
            "2026-02-24 00:05:03,557 - INFO - ИТОГОВАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\n",
            "2026-02-24 00:05:03,558 - INFO - ============================================================\n",
            "\n",
            "================================================================================\n",
            "Модель                              Perplexity           Loss                 Seeds     \n",
            "================================================================================\n",
            "gpt2-medium_baseline                2.91 ± 0.01          1.0676 ± 0.0025      3         \n",
            "gpt2_baseline                       3.58 ± 0.01          1.2743 ± 0.0017      3         \n",
            "distilgpt2_baseline                 3.96 ± 0.01          1.3772 ± 0.0026      3         \n",
            "mistral_qlora_r16                   3.98 ± 0.05          1.3815 ± 0.0115      3         \n",
            "mistral_qlora_r8                    4.04 ± 0.04          1.3965 ± 0.0091      3         \n",
            "gpt2-medium_lora_r16                5.31 ± 0.02          1.6704 ± 0.0030      3         \n",
            "deepseek_qlora_r16                  5.32 ± 0.00          1.6707 ± 0.0008      3         \n",
            "qwen2.5_qlora_r16                   5.44 ± 0.02          1.6929 ± 0.0037      3         \n",
            "gpt2-medium_lora_r8                 5.66 ± 0.01          1.7338 ± 0.0019      3         \n",
            "deepseek_qlora_r8                   5.66 ± 0.02          1.7341 ± 0.0032      3         \n",
            "qwen2.5_qlora_r8                    5.84 ± 0.01          1.7641 ± 0.0024      3         \n",
            "gpt2_lora_r16                       7.21 ± 0.02          1.9756 ± 0.0026      3         \n",
            "gpt2_lora_r8                        8.08 ± 0.04          2.0890 ± 0.0044      3         \n",
            "distilgpt2_lora_r16                 8.45 ± 0.03          2.1343 ± 0.0034      3         \n",
            "distilgpt2_lora_r8                  9.87 ± 0.06          2.2891 ± 0.0058      3         \n",
            "================================================================================\n",
            "2026-02-24 00:05:03,562 - INFO - 🎉 ПОЗДРАВЛЯЮ! ВСЕ ЭКСПЕРИМЕНТЫ УСПЕШНО ЗАВЕРШЕНЫ!\n",
            "2026-02-24 00:05:03,563 - INFO - 🚫 Исключён: deepseek_qlora_r16 (seed=44)\n",
            "2026-02-24 00:05:03,566 - INFO - \n",
            "Все результаты сохранены в: tat_results_final_rf/run_20260223_070746\n",
            "2026-02-24 00:05:03,566 - INFO - Лог файл: tat_results_final_rf/run_20260223_070746/resume_20260223_193810.log\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ДОЗАПУСК ОСТАВШИХСЯ ЭКСПЕРИМЕНТОВ (ИСКЛЮЧАЯ DeepSeek R16 seed=44)\n",
        "Запускает только незавершённые эксперименты, пропуская проблемную модель\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "import gc\n",
        "import time\n",
        "import sys\n",
        "import logging\n",
        "import zipfile\n",
        "import shutil\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,\n",
        "    DataCollatorWithPadding, BitsAndBytesConfig, set_seed\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ======================= КОНФИГУРАЦИЯ =======================\n",
        "HF_TOKEN = \"\"\n",
        "RESULTS_BASE = \"./tat_results_final_rf\"\n",
        "\n",
        "# Находим последний запуск\n",
        "runs = sorted([d for d in Path(RESULTS_BASE).iterdir() if d.is_dir() and d.name.startswith(\"run_\")])\n",
        "if not runs:\n",
        "    raise ValueError(\"Не найдено директорий с результатами\")\n",
        "LATEST_RUN = runs[-1]\n",
        "print(f\"Используем результаты из: {LATEST_RUN}\")\n",
        "\n",
        "# Загружаем конфигурацию экспериментов\n",
        "with open(LATEST_RUN / \"experiments_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    EXPERIMENTS = json.load(f)\n",
        "\n",
        "# Загружаем частичные результаты\n",
        "partial_results_path = LATEST_RUN / \"partial_results.json\"\n",
        "if partial_results_path.exists():\n",
        "    with open(partial_results_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        partial_results = json.load(f)\n",
        "else:\n",
        "    partial_results = []\n",
        "\n",
        "# ======================= НАСТРОЙКА ЛОГИРОВАНИЯ =======================\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = LATEST_RUN / f\"resume_{timestamp}.log\"\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file, encoding='utf-8'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(f\"Дозапуск экспериментов в: {LATEST_RUN}\")\n",
        "logger.info(f\"Лог файл: {log_file}\")\n",
        "\n",
        "# ======================= ПРОВЕРКА УСТРОЙСТВА =======================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Устройство: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    logger.info(f\"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    try:\n",
        "        login(token=HF_TOKEN)\n",
        "        logger.info(\"Успешный вход в Hugging Face Hub\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка входа: {e}\")\n",
        "\n",
        "# ======================= ФУНКЦИИ ДЛЯ ПРОВЕРКИ И ЗАПУСКА =======================\n",
        "def experiment_already_done(exp_name, seed_val, results_dir):\n",
        "    \"\"\"Проверяет, выполнен ли уже эксперимент с данным seed.\"\"\"\n",
        "    marker_file = results_dir / exp_name / f\"seed{seed_val}\" / \"done.txt\"\n",
        "    return marker_file.exists()\n",
        "\n",
        "def is_excluded_experiment(exp_name, seed_val):\n",
        "    \"\"\"Проверяет, является ли эксперимент исключённым (DeepSeek R16 seed=44).\"\"\"\n",
        "    if exp_name == \"deepseek_qlora_r16\" and seed_val == 44:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_failed_experiments(experiments, results_dir, num_seeds=3, base_seed=42):\n",
        "    \"\"\"Возвращает список экспериментов, которые не были завершены успешно.\"\"\"\n",
        "    failed = []\n",
        "    excluded_count = 0\n",
        "\n",
        "    for exp in experiments:\n",
        "        for seed_offset in range(num_seeds):\n",
        "            seed = base_seed + seed_offset\n",
        "\n",
        "            if is_excluded_experiment(exp[\"name\"], seed):\n",
        "                logger.info(f\"🚫 Исключаем проблемный: {exp['name']} (seed={seed})\")\n",
        "                excluded_count += 1\n",
        "                continue\n",
        "\n",
        "            if not experiment_already_done(exp[\"name\"], seed, results_dir):\n",
        "                exp_dir = results_dir / exp[\"name\"] / f\"seed{seed}\"\n",
        "                oom_marker = exp_dir / \"oom.txt\"\n",
        "\n",
        "                if oom_marker.exists():\n",
        "                    reason = \"oom\"\n",
        "                    logger.info(f\"⚠️  Предыдущая OOM: {exp['name']} (seed={seed})\")\n",
        "                else:\n",
        "                    reason = \"not_started\"\n",
        "                    logger.info(f\"⏳ Не запущен: {exp['name']} (seed={seed})\")\n",
        "\n",
        "                failed.append({\n",
        "                    \"exp\": exp,\n",
        "                    \"seed\": seed,\n",
        "                    \"reason\": reason\n",
        "                })\n",
        "\n",
        "    logger.info(f\"Исключено проблемных экспериментов: {excluded_count}\")\n",
        "    return failed\n",
        "\n",
        "# ======================= ОЧИСТКА ПАМЯТИ =======================\n",
        "def clear_gpu_memory(aggressive=False):\n",
        "    \"\"\"Очистка GPU памяти\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "        if aggressive:\n",
        "            import ctypes\n",
        "            try:\n",
        "                ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        logger.info(f\"GPU память: {allocated:.2f}GB / {reserved:.2f}GB\")\n",
        "        return allocated, reserved\n",
        "    return 0, 0\n",
        "\n",
        "# ======================= ЗАГРУЗКА ТОКЕНИЗИРОВАННЫХ ДАННЫХ =======================\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"ЗАГРУЗКА ТОКЕНИЗИРОВАННЫХ ДАННЫХ\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "tokenized_map = {}\n",
        "tokenizer_map = {}\n",
        "groups = [\"gpt2\", \"qwen\", \"deepseek\", \"mistral\"]\n",
        "\n",
        "for group in groups:\n",
        "    tokenized_path = LATEST_RUN / f\"tokenized_{group}\"\n",
        "    if tokenized_path.exists():\n",
        "        logger.info(f\"Загрузка токенизированного датасета {group}...\")\n",
        "        tokenized_map[group] = load_from_disk(str(tokenized_path))\n",
        "\n",
        "        try:\n",
        "            if group == \"gpt2\":\n",
        "                tokenizer_map[group] = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "            elif group == \"qwen\":\n",
        "                tokenizer_map[group] = AutoTokenizer.from_pretrained(\n",
        "                    \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "            elif group == \"deepseek\":\n",
        "                tokenizer_map[group] = AutoTokenizer.from_pretrained(\n",
        "                    \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "            elif group == \"mistral\":\n",
        "                tokenizer_map[group] = AutoTokenizer.from_pretrained(\n",
        "                    \"mistralai/Mistral-7B-v0.3\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "\n",
        "            if tokenizer_map[group].pad_token is None:\n",
        "                tokenizer_map[group].pad_token = tokenizer_map[group].eos_token\n",
        "\n",
        "            logger.info(f\"✓ Токенизатор {group} загружен\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"✗ Ошибка загрузки токенизатора {group}: {e}\")\n",
        "    else:\n",
        "        logger.warning(f\"Токенизированный датасет {group} не найден\")\n",
        "\n",
        "# ======================= ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ =======================\n",
        "def compute_ppl(loss):\n",
        "    try:\n",
        "        return math.exp(loss)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_batch_candidates(initial_bs):\n",
        "    candidates = []\n",
        "    bs = initial_bs\n",
        "    while True:\n",
        "        candidates.append(max(1, int(bs)))\n",
        "        if bs == 1:\n",
        "            break\n",
        "        bs = bs // 2\n",
        "    return candidates\n",
        "\n",
        "class DataCollatorForCausalLMWithLabelPad:\n",
        "    def __init__(self, tokenizer, label_pad_token_id: int = -100, pad_to_multiple_of: int = None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "        self.base_collator = DataCollatorWithPadding(\n",
        "            tokenizer,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            pad_to_multiple_of=pad_to_multiple_of\n",
        "        )\n",
        "\n",
        "    def __call__(self, features):\n",
        "        allowed_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "        filtered_features = [{k: f[k] for k in allowed_keys if k in f} for f in features]\n",
        "        labels = [f.pop(\"labels\") if \"labels\" in f else None for f in filtered_features]\n",
        "        batch = self.base_collator(filtered_features)\n",
        "        if any(l is not None for l in labels):\n",
        "            max_len = batch[\"input_ids\"].shape[1]\n",
        "            padded_labels = []\n",
        "            for l in labels:\n",
        "                if l is None:\n",
        "                    padded = [self.label_pad_token_id] * max_len\n",
        "                else:\n",
        "                    l = list(l)\n",
        "                    if len(l) < max_len:\n",
        "                        padded = l + [self.label_pad_token_id] * (max_len - len(l))\n",
        "                    else:\n",
        "                        padded = l[:max_len]\n",
        "                padded_labels.append(padded)\n",
        "            batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "# ======================= ОСНОВНАЯ ФУНКЦИЯ ЗАПУСКА =======================\n",
        "def run_experiment(exp_cfg, tokenized_data, tokenizer, results_dir, seed_val):\n",
        "    name = exp_cfg[\"name\"]\n",
        "    model_name = exp_cfg[\"model_name\"]\n",
        "    use_lora = exp_cfg.get(\"use_lora\", False)\n",
        "    use_qlora = exp_cfg.get(\"use_qlora\", False)\n",
        "    use_fp16 = exp_cfg.get(\"fp16\", True)\n",
        "    initial_bs = exp_cfg.get(\"batch_size\", 8)\n",
        "    epochs = exp_cfg.get(\"epochs\", 1)\n",
        "\n",
        "    # !!! ВАЖНО: Отключаем упрощение архитектуры для Mistral !!!\n",
        "    simplify_architecture = False  # Было: \"mistral\" in model_name.lower() and \"7b\" in model_name.lower()\n",
        "\n",
        "    # Проверка на уже выполненный\n",
        "    if experiment_already_done(name, seed_val, results_dir):\n",
        "        logger.info(f\"Эксперимент {name} (seed={seed_val}) уже выполнен, пропускаем.\")\n",
        "        return {\"skipped\": True, \"name\": name, \"seed\": seed_val}\n",
        "\n",
        "    def _fmt(val, fmt=\"{:.4f}\"):\n",
        "        try:\n",
        "            return fmt.format(val) if val is not None else \"None\"\n",
        "        except Exception:\n",
        "            try:\n",
        "                return str(val)\n",
        "            except Exception:\n",
        "                return \"None\"\n",
        "\n",
        "    batch_candidates = get_batch_candidates(initial_bs)\n",
        "    result = {\n",
        "        \"name\": name,\n",
        "        \"model_name\": model_name,\n",
        "        \"seed\": seed_val,\n",
        "        \"attempts\": [],\n",
        "        \"config\": exp_cfg.copy()\n",
        "    }\n",
        "    start_total = time.time()\n",
        "\n",
        "    seed_dir = results_dir / name / f\"seed{seed_val}\"\n",
        "    seed_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with open(seed_dir / \"in_progress.txt\", \"w\") as f:\n",
        "        f.write(f\"started at {datetime.now()}\")\n",
        "\n",
        "    for bs in batch_candidates:\n",
        "        logger.info(f\"--- Попытка {name} (seed={seed_val}) с batch_size={bs} ---\")\n",
        "        attempt_start = time.time()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        model = None\n",
        "        trainer = None\n",
        "\n",
        "        try:\n",
        "            load_kwargs = {\n",
        "                \"low_cpu_mem_usage\": True,\n",
        "                \"trust_remote_code\": True,\n",
        "                \"token\": HF_TOKEN\n",
        "            }\n",
        "\n",
        "            # Настройка квантизации для QLoRA\n",
        "            if use_qlora:\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float16\n",
        "                )\n",
        "                load_kwargs[\"quantization_config\"] = bnb_config\n",
        "                load_kwargs[\"device_map\"] = \"auto\"\n",
        "            else:\n",
        "                if use_fp16 and torch.cuda.is_available():\n",
        "                    load_kwargs[\"torch_dtype\"] = torch.float16\n",
        "                else:\n",
        "                    load_kwargs[\"torch_dtype\"] = torch.float32\n",
        "\n",
        "            logger.info(f\"Загрузка модели: {model_name}\")\n",
        "\n",
        "            try:\n",
        "                model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Не удалось загрузить модель {model_name}: {e}\")\n",
        "                result[\"attempts\"].append({\n",
        "                    \"batch_size\": bs,\n",
        "                    \"status\": \"load_error\",\n",
        "                    \"error\": str(e)[:200]\n",
        "                })\n",
        "                if \"out of memory\" in str(e).lower():\n",
        "                    with open(seed_dir / \"oom.txt\", \"w\") as f:\n",
        "                        f.write(f\"OOM at {datetime.now()}\\n{str(e)}\")\n",
        "                break\n",
        "\n",
        "            # LoRA настройка\n",
        "            trainable_params = None\n",
        "            if use_lora:\n",
        "                if use_qlora:\n",
        "                    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "                target_modules = exp_cfg.get(\"target_modules\")\n",
        "                if target_modules is None:\n",
        "                    target_modules = [\"c_attn\"] if \"gpt2\" in model_name.lower() else [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "                lora_config = LoraConfig(\n",
        "                    r=exp_cfg[\"lora_r\"],\n",
        "                    lora_alpha=exp_cfg[\"lora_alpha\"],\n",
        "                    target_modules=target_modules,\n",
        "                    lora_dropout=exp_cfg.get(\"lora_dropout\", 0.0),\n",
        "                    bias=\"none\",\n",
        "                    task_type=TaskType.CAUSAL_LM\n",
        "                )\n",
        "                model = get_peft_model(model, lora_config)\n",
        "                try:\n",
        "                    trainable_params = model.num_parameters(only_trainable=True)\n",
        "                    logger.info(f\"📊 Trainable params: {trainable_params:,}\")\n",
        "                except Exception:\n",
        "                    trainable_params = None\n",
        "            else:\n",
        "                if not hasattr(model, \"hf_device_map\"):\n",
        "                    model = model.to(device)\n",
        "                try:\n",
        "                    model.gradient_checkpointing_enable()\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            output_dir = seed_dir / f\"bs{bs}\"\n",
        "            output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            optim = \"adamw_8bit\" if use_qlora else \"adamw_torch\"\n",
        "\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=str(output_dir),\n",
        "                per_device_train_batch_size=bs,\n",
        "                per_device_eval_batch_size=bs,\n",
        "                gradient_accumulation_steps=2,\n",
        "                learning_rate=2e-4,\n",
        "                num_train_epochs=epochs,\n",
        "                logging_steps=50,\n",
        "                eval_strategy=\"steps\",\n",
        "                eval_steps=50,\n",
        "                save_strategy=\"no\",\n",
        "                report_to=\"none\",\n",
        "                fp16=(use_fp16 and torch.cuda.is_available()),\n",
        "                gradient_checkpointing=True,\n",
        "                optim=optim,\n",
        "                seed=seed_val,\n",
        "                remove_unused_columns=True,\n",
        "                dataloader_num_workers=0,\n",
        "                ddp_find_unused_parameters=False if use_lora else None,\n",
        "            )\n",
        "\n",
        "            data_collator = DataCollatorForCausalLMWithLabelPad(\n",
        "                tokenizer=tokenizer,\n",
        "                label_pad_token_id=-100,\n",
        "                pad_to_multiple_of=8\n",
        "            )\n",
        "\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=tokenized_data[\"train\"],\n",
        "                eval_dataset=tokenized_data[\"eval\"],\n",
        "                data_collator=data_collator,\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                trainer.train()\n",
        "            except RuntimeError as e:\n",
        "                msg = str(e).lower()\n",
        "                if \"out of memory\" in msg or (\"cuda\" in msg and \"memory\" in msg):\n",
        "                    logger.warning(f\"OOM при batch_size={bs}\")\n",
        "                    result[\"attempts\"].append({\n",
        "                        \"batch_size\": bs,\n",
        "                        \"status\": \"oom\",\n",
        "                        \"error\": str(e)[:200]\n",
        "                    })\n",
        "                    with open(seed_dir / \"oom.txt\", \"w\") as f:\n",
        "                        f.write(f\"OOM at {datetime.now()}\\n{str(e)}\")\n",
        "                    try:\n",
        "                        del trainer\n",
        "                    except:\n",
        "                        pass\n",
        "                    try:\n",
        "                        del model\n",
        "                    except:\n",
        "                        pass\n",
        "                    clear_gpu_memory(aggressive=True)\n",
        "                    continue\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            if use_lora:\n",
        "                adapter_save_path = output_dir / \"adapter\"\n",
        "                model.save_pretrained(adapter_save_path)\n",
        "                logger.info(f\"Адаптер сохранён в {adapter_save_path}\")\n",
        "\n",
        "            try:\n",
        "                with open(output_dir / \"log_history.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(getattr(trainer.state, \"log_history\", []), f, indent=2, ensure_ascii=False)\n",
        "            except Exception as e_log:\n",
        "                logger.warning(f\"Не удалось сохранить log_history: {e_log}\")\n",
        "\n",
        "            eval_metrics = {}\n",
        "            try:\n",
        "                eval_metrics = trainer.evaluate(eval_dataset=tokenized_data[\"test\"])\n",
        "            except Exception as e_eval:\n",
        "                logger.warning(f\"Ошибка при evaluate: {e_eval}\")\n",
        "\n",
        "            test_loss = eval_metrics.get(\"eval_loss\") if isinstance(eval_metrics, dict) else None\n",
        "            test_ppl = compute_ppl(test_loss) if test_loss is not None else None\n",
        "\n",
        "            peak_gpu_mem = None\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
        "                except Exception:\n",
        "                    peak_gpu_mem = None\n",
        "\n",
        "            elapsed = time.time() - attempt_start\n",
        "\n",
        "            loss_str = _fmt(test_loss, \"{:.4f}\")\n",
        "            ppl_str = _fmt(test_ppl, \"{:.2f}\")\n",
        "            gpu_str = _fmt(peak_gpu_mem, \"{:.2f}\")\n",
        "\n",
        "            logger.info(f\"✅ Успех! Loss={loss_str}, PPL={ppl_str}, время={elapsed:.1f}с, пик GPU={gpu_str}GB\")\n",
        "\n",
        "            result.update({\n",
        "                \"status\": \"success\",\n",
        "                \"batch_size\": bs,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_perplexity\": test_ppl,\n",
        "                \"trainable_params\": trainable_params,\n",
        "                \"peak_gpu_memory_gb\": peak_gpu_mem,\n",
        "                \"time_seconds\": elapsed,\n",
        "                \"total_time_seconds\": time.time() - start_total\n",
        "            })\n",
        "            result[\"attempts\"].append({\n",
        "                \"batch_size\": bs,\n",
        "                \"status\": \"success\",\n",
        "                \"eval_loss\": test_loss,\n",
        "                \"perplexity\": test_ppl,\n",
        "                \"time_seconds\": elapsed\n",
        "            })\n",
        "\n",
        "            if (seed_dir / \"in_progress.txt\").exists():\n",
        "                (seed_dir / \"in_progress.txt\").unlink()\n",
        "            with open(seed_dir / \"done.txt\", \"w\") as f:\n",
        "                f.write(f\"success with batch_size={bs} at {datetime.now()}\")\n",
        "\n",
        "            try:\n",
        "                del trainer\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                del model\n",
        "            except:\n",
        "                pass\n",
        "            clear_gpu_memory()\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ошибка при batch_size={bs}: {e}\")\n",
        "            result[\"attempts\"].append({\"batch_size\": bs, \"status\": \"error\", \"error\": str(e)[:200]})\n",
        "            try:\n",
        "                del model\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                del trainer\n",
        "            except:\n",
        "                pass\n",
        "            clear_gpu_memory(aggressive=True)\n",
        "            continue\n",
        "\n",
        "    result[\"status\"] = \"failed_all_bs\"\n",
        "    result[\"total_time_seconds\"] = time.time() - start_total\n",
        "\n",
        "    if (seed_dir / \"in_progress.txt\").exists():\n",
        "        (seed_dir / \"in_progress.txt\").unlink()\n",
        "\n",
        "    return result\n",
        "\n",
        "# ======================= ПОИСК НЕЗАВЕРШЁННЫХ ЭКСПЕРИМЕНТОВ =======================\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"ПОИСК НЕЗАВЕРШЁННЫХ ЭКСПЕРИМЕНТОВ\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "failed_experiments = get_failed_experiments(EXPERIMENTS, LATEST_RUN, num_seeds=3, base_seed=42)\n",
        "logger.info(f\"Найдено {len(failed_experiments)} незавершённых экспериментов\")\n",
        "\n",
        "if not failed_experiments:\n",
        "    logger.info(\"🎉 Все эксперименты уже завершены!\")\n",
        "else:\n",
        "    logger.info(\"\\n📋 Список для дозапуска:\")\n",
        "    for i, fe in enumerate(failed_experiments, 1):\n",
        "        status_symbol = \"⚠️\" if fe['reason'] == 'oom' else \"⏳\"\n",
        "        logger.info(f\"{i}. {status_symbol} {fe['exp']['name']} (seed={fe['seed']}) - {fe['reason']}\")\n",
        "\n",
        "# ======================= ОПТИМИЗАЦИЯ ДЛЯ 7B МОДЕЛЕЙ =======================\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"ОПТИМИЗАЦИЯ НАСТРОЕК ДЛЯ 7B МОДЕЛЕЙ\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "for fe in failed_experiments:\n",
        "    exp = fe['exp']\n",
        "    if \"7b\" in exp['model_name'].lower() or \"7B\" in exp['model_name']:\n",
        "        exp['batch_size'] = 1\n",
        "        exp['use_qlora'] = True\n",
        "        exp['fp16'] = True\n",
        "        exp['gradient_checkpointing'] = True\n",
        "        logger.info(f\"✓ Оптимизирован {exp['name']}: batch_size=1, QLoRA=True\")\n",
        "\n",
        "# ======================= ЗАПУСК ЭКСПЕРИМЕНТОВ =======================\n",
        "if failed_experiments:\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"ЗАПУСК НЕЗАВЕРШЁННЫХ ЭКСПЕРИМЕНТОВ\")\n",
        "    logger.info(\"=\"*60)\n",
        "\n",
        "    resumed_results = []\n",
        "    total = len(failed_experiments)\n",
        "\n",
        "    for i, fe in enumerate(failed_experiments, 1):\n",
        "        exp = fe['exp']\n",
        "        seed = fe['seed']\n",
        "        group = exp['tokenizer_group']\n",
        "\n",
        "        logger.info(f\"\\n[{i}/{total}] {'='*50}\")\n",
        "        logger.info(f\"Запуск: {exp['name']} (seed={seed})\")\n",
        "        logger.info(f\"{'='*50}\")\n",
        "\n",
        "        tokenized_data = tokenized_map.get(group)\n",
        "        tokenizer = tokenizer_map.get(group)\n",
        "\n",
        "        if tokenized_data is None or tokenizer is None:\n",
        "            logger.error(f\"❌ Нет токенизированных данных для группы {group}, пропускаем\")\n",
        "            continue\n",
        "\n",
        "        logger.info(\"Очистка памяти перед запуском...\")\n",
        "        clear_gpu_memory(aggressive=True)\n",
        "\n",
        "        set_seed(seed)\n",
        "\n",
        "        try:\n",
        "            res = run_experiment(exp, tokenized_data, tokenizer, LATEST_RUN, seed)\n",
        "\n",
        "            if not res.get(\"skipped\", False):\n",
        "                resumed_results.append(res)\n",
        "                logger.info(f\"✅ Завершён: {exp['name']} (seed={seed})\")\n",
        "            else:\n",
        "                logger.info(f\"⏭️ Пропущен: {exp['name']} (seed={seed})\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"⏸️ Дозапуск прерван пользователем\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Критическая ошибка при запуске {exp['name']} (seed={seed}): {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        all_results = partial_results + resumed_results\n",
        "        with open(LATEST_RUN / \"partial_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        with open(LATEST_RUN / \"resumed_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(resumed_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"📊 Прогресс: {i}/{total} ({i/total*100:.1f}%)\")\n",
        "\n",
        "        clear_gpu_memory(aggressive=True)\n",
        "        time.sleep(5)\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"ДОЗАПУСК ЗАВЕРШЁН\")\n",
        "    logger.info(f\"Дополнительно выполнено: {len(resumed_results)} экспериментов\")\n",
        "    logger.info(\"=\"*60)\n",
        "\n",
        "# ======================= ОБНОВЛЕНИЕ АГРЕГИРОВАННЫХ РЕЗУЛЬТАТОВ =======================\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"ОБНОВЛЕНИЕ АГРЕГИРОВАННЫХ РЕЗУЛЬТАТОВ\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "with open(LATEST_RUN / \"partial_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    all_results = json.load(f)\n",
        "\n",
        "grouped = {}\n",
        "for r in all_results:\n",
        "    if r.get(\"status\") == \"success\":\n",
        "        name = r[\"name\"]\n",
        "        if name not in grouped:\n",
        "            grouped[name] = []\n",
        "        grouped[name].append(r)\n",
        "\n",
        "aggregated = []\n",
        "for name, runs in grouped.items():\n",
        "    ppls = [r[\"test_perplexity\"] for r in runs if r[\"test_perplexity\"] is not None]\n",
        "    losses = [r[\"test_loss\"] for r in runs if r[\"test_loss\"] is not None]\n",
        "    if not ppls:\n",
        "        continue\n",
        "\n",
        "    agg = {\n",
        "        \"name\": name,\n",
        "        \"model_name\": runs[0][\"model_name\"],\n",
        "        \"num_seeds\": len(runs),\n",
        "        \"test_perplexity_mean\": float(np.mean(ppls)),\n",
        "        \"test_perplexity_std\": float(np.std(ppls)),\n",
        "        \"test_loss_mean\": float(np.mean(losses)),\n",
        "        \"test_loss_std\": float(np.std(losses)),\n",
        "        \"peak_gpu_memory_gb_mean\": float(np.mean([r[\"peak_gpu_memory_gb\"] for r in runs if r[\"peak_gpu_memory_gb\"]])),\n",
        "        \"time_seconds_mean\": float(np.mean([r[\"time_seconds\"] for r in runs])),\n",
        "        \"trainable_params\": runs[0].get(\"trainable_params\"),\n",
        "        \"all_seeds\": runs\n",
        "    }\n",
        "    aggregated.append(agg)\n",
        "\n",
        "with open(LATEST_RUN / \"aggregated_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(aggregated, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "csv_agg_file = LATEST_RUN / \"aggregated_results.csv\"\n",
        "fieldnames_agg = [\"name\", \"model_name\", \"num_seeds\", \"test_perplexity_mean\", \"test_perplexity_std\",\n",
        "                  \"test_loss_mean\", \"test_loss_std\", \"peak_gpu_memory_gb_mean\", \"time_seconds_mean\", \"trainable_params\"]\n",
        "\n",
        "with open(csv_agg_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames_agg)\n",
        "    writer.writeheader()\n",
        "    for a in aggregated:\n",
        "        writer.writerow({\n",
        "            \"name\": a[\"name\"],\n",
        "            \"model_name\": a[\"model_name\"],\n",
        "            \"num_seeds\": a[\"num_seeds\"],\n",
        "            \"test_perplexity_mean\": f\"{a['test_perplexity_mean']:.4f}\",\n",
        "            \"test_perplexity_std\": f\"{a['test_perplexity_std']:.4f}\",\n",
        "            \"test_loss_mean\": f\"{a['test_loss_mean']:.4f}\",\n",
        "            \"test_loss_std\": f\"{a['test_loss_std']:.4f}\",\n",
        "            \"peak_gpu_memory_gb_mean\": f\"{a['peak_gpu_memory_gb_mean']:.2f}\",\n",
        "            \"time_seconds_mean\": f\"{a['time_seconds_mean']:.1f}\",\n",
        "            \"trainable_params\": a[\"trainable_params\"]\n",
        "        })\n",
        "\n",
        "logger.info(f\"Агрегированные результаты сохранены в {csv_agg_file}\")\n",
        "\n",
        "# ======================= ВИЗУАЛИЗАЦИЯ =======================\n",
        "if aggregated:\n",
        "    aggregated.sort(key=lambda x: x[\"test_perplexity_mean\"])\n",
        "    names = [a[\"name\"] for a in aggregated]\n",
        "    ppl_means = [a[\"test_perplexity_mean\"] for a in aggregated]\n",
        "    ppl_stds = [a[\"test_perplexity_std\"] for a in aggregated]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    y_pos = np.arange(len(names))\n",
        "    plt.barh(y_pos, ppl_means, xerr=ppl_stds, capsize=5, color='steelblue')\n",
        "    plt.yticks(y_pos, names)\n",
        "    plt.xlabel(\"Perplexity (ниже лучше)\")\n",
        "    plt.title(\"Сравнение перплексии на тестовом наборе\")\n",
        "\n",
        "    for i, (mean, std) in enumerate(zip(ppl_means, ppl_stds)):\n",
        "        plt.text(mean + std + 0.1, i, f'{mean:.2f} ± {std:.2f}', va='center', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(LATEST_RUN / \"perplexity_comparison_updated.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# ======================= ФИНАЛЬНЫЙ ОТЧЁТ =======================\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"ИТОГОВАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"{'Модель':<35} {'Perplexity':<20} {'Loss':<20} {'Seeds':<10}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for a in sorted(aggregated, key=lambda x: x[\"test_perplexity_mean\"]):\n",
        "    name = a['name'][:35]\n",
        "    ppl = f\"{a['test_perplexity_mean']:.2f} ± {a['test_perplexity_std']:.2f}\"\n",
        "    loss = f\"{a['test_loss_mean']:.4f} ± {a['test_loss_std']:.4f}\"\n",
        "    seeds = str(a['num_seeds'])\n",
        "    print(f\"{name:<35} {ppl:<20} {loss:<20} {seeds:<10}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Проверяем завершение\n",
        "all_completed = True\n",
        "excluded_found = False\n",
        "\n",
        "for exp in EXPERIMENTS:\n",
        "    for seed_offset in range(3):\n",
        "        seed = 42 + seed_offset\n",
        "\n",
        "        if is_excluded_experiment(exp[\"name\"], seed):\n",
        "            excluded_found = True\n",
        "            continue\n",
        "\n",
        "        if not experiment_already_done(exp[\"name\"], seed, LATEST_RUN):\n",
        "            all_completed = False\n",
        "            logger.warning(f\"⚠️  Не завершён: {exp['name']} (seed={seed})\")\n",
        "\n",
        "if all_completed:\n",
        "    logger.info(\"🎉 ПОЗДРАВЛЯЮ! ВСЕ ЭКСПЕРИМЕНТЫ УСПЕШНО ЗАВЕРШЕНЫ!\")\n",
        "    if excluded_found:\n",
        "        logger.info(\"🚫 Исключён: deepseek_qlora_r16 (seed=44)\")\n",
        "else:\n",
        "    logger.info(\"⏳ Некоторые эксперименты ещё не завершены.\")\n",
        "\n",
        "logger.info(f\"\\nВсе результаты сохранены в: {LATEST_RUN}\")\n",
        "logger.info(f\"Лог файл: {log_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ffffe7",
      "metadata": {
        "id": "68ffffe7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354303c2",
      "metadata": {
        "id": "354303c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711f61f6",
      "metadata": {
        "id": "711f61f6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b537ed",
      "metadata": {
        "id": "17b537ed",
        "outputId": "25987bf1-34c8-4aae-f21f-08d4fa7b182e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c1b428",
      "metadata": {
        "id": "52c1b428",
        "outputId": "2276deb8-ef70-4a08-cb2e-e2e071f74893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Архив создан: tat_results_complete.zip\n",
            "Размер: 1118.42 MB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<a href='tat_results_complete.zip' target='_blank'>tat_results_complete.zip</a><br>"
            ],
            "text/plain": [
              "/home/kfu/tat_results_complete.zip"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "from IPython.display import FileLink, display\n",
        "\n",
        "# Создаем архив с полными результатами\n",
        "results_path = Path(\"./tat_results_final_rf/run_20260223_070746\")\n",
        "zip_name = \"tat_results_complete.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(results_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, results_path)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"Архив создан: {zip_name}\")\n",
        "print(f\"Размер: {os.path.getsize(zip_name)/1024/1024:.2f} MB\")\n",
        "display(FileLink(zip_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780a2063",
      "metadata": {
        "id": "780a2063"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66324f36",
      "metadata": {
        "id": "66324f36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae8b09c",
      "metadata": {
        "id": "5ae8b09c",
        "outputId": "5964d8d4-9607-4813-bcfd-f15f4da36616",
        "colab": {
          "referenced_widgets": [
            "94ee6496d8c943479fbdbde0144a9158",
            "8abef9c6aa6349eea3805f3648d58006",
            "d238fb1be28048b1ac1ad10493e85282",
            "1c482cee8c514bb792e059f1575d5c3d",
            "3bf58d565bbc4b1680de56335a35435a"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🧪 ТЕСТИРОВАНИЕ ОБУЧЕННЫХ МОДЕЛЕЙ\n",
            "Результаты из: tat_results_final_rf/run_20260223_070746\n",
            "======================================================================\n",
            "\n",
            "📋 Найденные эксперименты:\n",
            "  1. gpt2_lora_r8\n",
            "  2. distilgpt2_lora_r16\n",
            "  3. gpt2-medium_baseline\n",
            "  4. mistral_qlora_r8\n",
            "  5. distilgpt2_baseline\n",
            "  6. qwen2.5_qlora_r8\n",
            "  7. gpt2-medium_lora_r16\n",
            "  8. deepseek_qlora_r8\n",
            "  9. mistral_qlora_r16\n",
            "  10. qwen2.5_qlora_r16\n",
            "  11. gpt2-medium_lora_r8\n",
            "  12. gpt2_baseline\n",
            "  13. gpt2_lora_r16\n",
            "  14. distilgpt2_lora_r8\n",
            "  15. deepseek_qlora_r16\n",
            "\n",
            "🎮 Выберите режим:\n",
            "  1. Интерактивный режим для одной модели\n",
            "  2. Сравнить несколько моделей\n",
            "  3. Вычислить perplexity для всех моделей\n",
            "  0. Выход\n",
            "\n",
            "Ваш выбор (0-3): 2\n",
            "\n",
            "Выберите модели для сравнения (введите индексы через пробел):\n",
            "  1. gpt2_lora_r8\n",
            "  2. distilgpt2_lora_r16\n",
            "  3. gpt2-medium_baseline\n",
            "  4. mistral_qlora_r8\n",
            "  5. distilgpt2_baseline\n",
            "  6. qwen2.5_qlora_r8\n",
            "  7. gpt2-medium_lora_r16\n",
            "  8. deepseek_qlora_r8\n",
            "  9. mistral_qlora_r16\n",
            "  10. qwen2.5_qlora_r16\n",
            "  11. gpt2-medium_lora_r8\n",
            "  12. gpt2_baseline\n",
            "  13. gpt2_lora_r16\n",
            "  14. distilgpt2_lora_r8\n",
            "  15. deepseek_qlora_r16\n",
            "Индексы: 3 4 9 12 15\n",
            "Сид для gpt2-medium_baseline (42, 43, 44): 42\n",
            "Batch size для gpt2-medium_baseline (обычно 1 или 4): 4\n",
            "\n",
            "🔧 Загрузка модели: gpt2-medium_baseline\n",
            "   Базовая модель: gpt2-medium\n",
            "   Путь к адаптеру: tat_results_final_rf/run_20260223_070746/gpt2-medium_baseline/seed42/bs4\n",
            "✓ Токенизатор загружен\n",
            "⏳ Загрузка базовой модели...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94ee6496d8c943479fbdbde0144a9158",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Базовая модель загружена\n",
            "⚠️ LoRA адаптер не найден, использую базовую модель\n",
            "Сид для mistral_qlora_r8 (42, 43, 44): 42\n",
            "Batch size для mistral_qlora_r8 (обычно 1 или 4): 1\n",
            "\n",
            "🔧 Загрузка модели: mistral_qlora_r8\n",
            "   Базовая модель: mistralai/Mistral-7B-v0.3\n",
            "   Путь к адаптеру: tat_results_final_rf/run_20260223_070746/mistral_qlora_r8/seed42/bs1\n",
            "✓ Токенизатор загружен\n",
            "⏳ Загрузка базовой модели...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8abef9c6aa6349eea3805f3648d58006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Базовая модель загружена\n",
            "⏳ Загрузка LoRA адаптера из tat_results_final_rf/run_20260223_070746/mistral_qlora_r8/seed42/bs1/adapter...\n",
            "✓ LoRA адаптер загружен\n",
            "Сид для mistral_qlora_r16 (42, 43, 44): 42\n",
            "Batch size для mistral_qlora_r16 (обычно 1 или 4): 1\n",
            "\n",
            "🔧 Загрузка модели: mistral_qlora_r16\n",
            "   Базовая модель: mistralai/Mistral-7B-v0.3\n",
            "   Путь к адаптеру: tat_results_final_rf/run_20260223_070746/mistral_qlora_r16/seed42/bs1\n",
            "✓ Токенизатор загружен\n",
            "⏳ Загрузка базовой модели...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d238fb1be28048b1ac1ad10493e85282",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Базовая модель загружена\n",
            "⏳ Загрузка LoRA адаптера из tat_results_final_rf/run_20260223_070746/mistral_qlora_r16/seed42/bs1/adapter...\n",
            "✓ LoRA адаптер загружен\n",
            "Сид для gpt2_baseline (42, 43, 44): 42\n",
            "Batch size для gpt2_baseline (обычно 1 или 4): 8\n",
            "\n",
            "🔧 Загрузка модели: gpt2_baseline\n",
            "   Базовая модель: gpt2\n",
            "   Путь к адаптеру: tat_results_final_rf/run_20260223_070746/gpt2_baseline/seed42/bs8\n",
            "✓ Токенизатор загружен\n",
            "⏳ Загрузка базовой модели...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c482cee8c514bb792e059f1575d5c3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Базовая модель загружена\n",
            "⚠️ LoRA адаптер не найден, использую базовую модель\n",
            "Сид для deepseek_qlora_r16 (42, 43, 44): 42\n",
            "Batch size для deepseek_qlora_r16 (обычно 1 или 4): 1\n",
            "\n",
            "🔧 Загрузка модели: deepseek_qlora_r16\n",
            "   Базовая модель: deepseek-ai/deepseek-llm-7b-chat\n",
            "   Путь к адаптеру: tat_results_final_rf/run_20260223_070746/deepseek_qlora_r16/seed42/bs1\n",
            "✓ Токенизатор загружен\n",
            "⏳ Загрузка базовой модели...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bf58d565bbc4b1680de56335a35435a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Базовая модель загружена\n",
            "⏳ Загрузка LoRA адаптера из tat_results_final_rf/run_20260223_070746/deepseek_qlora_r16/seed42/bs1/adapter...\n",
            "✓ LoRA адаптер загружен\n",
            "\n",
            "======================================================================\n",
            "🔬 СРАВНЕНИЕ МОДЕЛЕЙ\n",
            "======================================================================\n",
            "\n",
            "📌 Промпт: 'Минем исемем'\n",
            "--------------------------------------------------\n",
            "\n",
            "[gpt2-medium_baseline_s42]:\n",
            "  Минем исемемоматья.\n",
            "\"The first thing you should do when dealing with the problem of mass suicide is to stop shooting people,\" said Pavel Semenov, a Russian psychologist who has studied some 300 suicides in 20 years and discovered that\n",
            "\n",
            "[mistral_qlora_r8_s42]:\n",
            "  Минем исемемнең 10 яшьлек аны белән сөйләделәр\n",
            "\n",
            "(Казан, 24 гыйнвар, «Татар-информ»). Бу к\n",
            "\n",
            "[mistral_qlora_r16_s42]:\n",
            "  Минем исемем Илсур Метшин, башкаладагы даими мэрының тикшерелегенә үз бәяләрен ачты\n",
            "\n",
            "(Казан, 20\n",
            "\n",
            "[gpt2_baseline_s42]:\n",
            "  Минем исемемальторовых укв, p. 2-5) (n). I am aware of the fact that a person may have committed an offence against this law but does not necessarily know how it is\n",
            "\n",
            "[deepseek_qlora_r16_s42]:\n",
            "  Минем исемем өлкәсендә катнаштыручыларга турыгын уртая\n",
            "\n",
            "(Казан, 27 октябрь, \"Татар\n",
            "--------------------------------------------------\n",
            "\n",
            "📌 Промпт: 'Бүгенге көндә'\n",
            "--------------------------------------------------\n",
            "\n",
            "[gpt2-medium_baseline_s42]:\n",
            "  Бүгенге көндәєу раскимлоть. (It's a joke about me being in the USA.)\n",
            "\n",
            "Dana: That is great, I love it! How many more times do we have to say that\n",
            "\n",
            "[mistral_qlora_r8_s42]:\n",
            "  Бүгенге көндә Казанда янгынча төркемдә коронавирус белән авыручылар 139е тапкырды\n",
            "\n",
            "(Казан, 2\n",
            "\n",
            "[mistral_qlora_r16_s42]:\n",
            "  Бүгенге көндә ТР Дәүләт Советының VIII сессиясе уздырылачак\n",
            "\n",
            "(Казан, 27 май, “Татар-информ”, Миләүш\n",
            "\n",
            "[gpt2_baseline_s42]:\n",
            "  Бүгенге көндәмых стакиров,\n",
            ".\n",
            "\n",
            " (8) [B]The person who is under the influence of narcotics for a period to be determined as an actual act or omission in this particular case: The\n",
            "\n",
            "[deepseek_qlora_r16_s42]:\n",
            "  Бүгенге көндә буенча якшырга Әлмәт укучыларын алдыз\n",
            "\n",
            "(Казан, 21 июль, \"Татар-информ\",\n",
            "--------------------------------------------------\n",
            "\n",
            "📌 Промпт: 'Татарстан -'\n",
            "--------------------------------------------------\n",
            "\n",
            "[gpt2-medium_baseline_s42]:\n",
            "  Татарстан - Мимечкой удо́вля, 18/12/2008\n",
            "\n",
            "St. Petersburg (Russia)\n",
            "\n",
            " a_babu@miamiherald-news2.com /\n",
            "\n",
            "[mistral_qlora_r8_s42]:\n",
            "  Татарстан - Россия халкы үзенчәлекчелеге булдыру мөмкинлеге бәйрәме\n",
            "\n",
            "(Казан, 14 июнь, “Татар-\n",
            "\n",
            "[mistral_qlora_r16_s42]:\n",
            "  Татарстан - \"Ватанлылар бердәмлеге\" кандидатлары белән 19 майда уздырыла торган Президенты вазифасына гариза и\n",
            "\n",
            "[gpt2_baseline_s42]:\n",
            "  Татарстан - F.I.K., L'Institut, 1997) and Praktov's work on the history of Russian political philosophy from antiquity to contemporary times (Stiftung und Römische Geschichte der Erfurt-\n",
            "\n",
            "[deepseek_qlora_r16_s42]:\n",
            "  Татарстан - Кинебләр һава тамаша кичерлыгының алдын эшлере тикшергән\n",
            "\n",
            "(Казан, 1\n",
            "--------------------------------------------------\n",
            "\n",
            "📌 Промпт: 'Мәктәптә без'\n",
            "--------------------------------------------------\n",
            "\n",
            "[gpt2-medium_baseline_s42]:\n",
            "  Мәктәптә безулова ширдмы - 1,700 , 2,100 – 6.99\n",
            "The list of top-rated songs from 2015 is very long and contains many great hits including: 'Porcelain\n",
            "\n",
            "[mistral_qlora_r8_s42]:\n",
            "  Мәктәптә безнең 30 еллыгына какчак чаралар күбрәке - «Яңа гасыр»\n",
            "\n",
            "(Казан, 23 гыйнвар, «Та\n",
            "\n",
            "[mistral_qlora_r16_s42]:\n",
            "  Мәктәптә безнең “экстра-класс”ы үз ишекләрен күрсәтеләчәк\n",
            "\n",
            "(Казан, 24 август, “Татар-информ\n",
            "\n",
            "[gpt2_baseline_s42]:\n",
            "  Мәктәптә безансичо́лов, удрывья.\n",
            "The third column is an attempt to show the difference between a political party and its supporters (the \"Voters\") who support it or\n",
            "\n",
            "[deepseek_qlora_r16_s42]:\n",
            "  Мәктәптә безнең җырлы “без” багын булган иҗатнарларны көтелә\n",
            "\n",
            "(Казан, 15 сентябр\n",
            "--------------------------------------------------\n",
            "\n",
            "📌 Промпт: 'Кыш көне'\n",
            "--------------------------------------------------\n",
            "\n",
            "[gpt2-medium_baseline_s42]:\n",
            "  Кыш көнериться умалков:\n",
            "\n",
            "\"The fact that the original Russian text of this book is preserved in English shows how important it was for Russia to preserve its national identity and language, especially when considering\n",
            "\n",
            "[mistral_qlora_r8_s42]:\n",
            "  Кыш көне белән Казанда “Студентлар үзәге” ачык фестиваль барлыкка тәкъдим итте\n",
            "\n",
            "(Казан, 2\n",
            "\n",
            "[mistral_qlora_r16_s42]:\n",
            "  Кыш көне, беренче тапкыр һәм ике атнада түбән берничә ялып узган «Тантана» фестивале башкаручы\n",
            "\n",
            "[gpt2_baseline_s42]:\n",
            "  Кыш көнеки судатро́мой, hýrücken.\n",
            "A number of people came to this website and told me that I should learn Russian first before coming here because they believe in their\n",
            "\n",
            "[deepseek_qlora_r16_s42]:\n",
            "  Кыш көненең 70 еллыгын катнашучылар әзерләп\n",
            "\n",
            "(Казан, 13 октябрь, \"Татар-инфор\n",
            "--------------------------------------------------\n",
            "\n",
            "👋 До свидания!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "test_trained_models.py\n",
        "\n",
        "Скрипт для тестирования обученных моделей из эксперимента.\n",
        "Поддерживает:\n",
        "- Загрузку сохраненных моделей (полных и LoRA адаптеров)\n",
        "- Вычисление perplexity на тестовом наборе\n",
        "- Интерактивную генерацию текста\n",
        "- Пакетное сравнение моделей на заданных промптах\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import gc\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# ======================= КОНФИГУРАЦИЯ =======================\n",
        "# Путь к результатам эксперимента (укажите ваш последний запуск)\n",
        "RESULTS_PATH = Path(\"./tat_results_final_rf/run_20260223_070746\")\n",
        "\n",
        "# Максимальная длина для генерации\n",
        "MAX_NEW_TOKENS = 100\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.9\n",
        "TOP_K = 50\n",
        "\n",
        "# ======================= ЗАГРУЗКА ДАННЫХ =======================\n",
        "def load_test_dataset(results_path: Path, group: str):\n",
        "    \"\"\"Загружает тестовую выборку для указанной группы токенизатора.\"\"\"\n",
        "    tokenized_path = results_path / f\"tokenized_{group}\"\n",
        "    if not tokenized_path.exists():\n",
        "        print(f\"❌ Токенизированный датасет для {group} не найден: {tokenized_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        dataset = load_from_disk(str(tokenized_path))\n",
        "        if \"test\" in dataset:\n",
        "            print(f\"✓ Загружен тестовый набор для {group}: {len(dataset['test'])} примеров\")\n",
        "            return dataset[\"test\"]\n",
        "        else:\n",
        "            print(f\"⚠️ В датасете {group} нет тестовой выборки\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка загрузки датасета {group}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ======================= ЗАГРУЗКА МОДЕЛИ =======================\n",
        "def load_model(\n",
        "    experiment_path: Path,\n",
        "    seed: int,\n",
        "    batch_size: int,\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Загружает сохраненную модель.\n",
        "\n",
        "    Args:\n",
        "        experiment_path: Путь к эксперименту (например, tat_results/.../mistral_qlora_r16)\n",
        "        seed: Сид (42, 43, 44)\n",
        "        batch_size: Размер батча, с которым модель была сохранена (bs1, bs2, ...)\n",
        "\n",
        "    Returns:\n",
        "        (model, tokenizer, config)\n",
        "    \"\"\"\n",
        "    seed_path = experiment_path / f\"seed{seed}\" / f\"bs{batch_size}\"\n",
        "\n",
        "    if not seed_path.exists():\n",
        "        print(f\"❌ Путь не найден: {seed_path}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Загружаем конфигурацию эксперимента\n",
        "    config_path = RESULTS_PATH / \"experiments_config.json\"\n",
        "    exp_name = experiment_path.name\n",
        "\n",
        "    if config_path.exists():\n",
        "        with open(config_path, \"r\") as f:\n",
        "            all_configs = json.load(f)\n",
        "        exp_config = next((c for c in all_configs if c[\"name\"] == exp_name), None)\n",
        "    else:\n",
        "        exp_config = None\n",
        "\n",
        "    if exp_config is None:\n",
        "        print(f\"⚠️ Конфигурация для {exp_name} не найдена, используем стандартные параметры\")\n",
        "        base_model_name = \"mistralai/Mistral-7B-v0.3\"  # По умолчанию\n",
        "        use_qlora = True\n",
        "        use_lora = True\n",
        "    else:\n",
        "        base_model_name = exp_config[\"model_name\"]\n",
        "        use_qlora = exp_config.get(\"use_qlora\", False)\n",
        "        use_lora = exp_config.get(\"use_lora\", False)\n",
        "\n",
        "    print(f\"\\n🔧 Загрузка модели: {exp_name}\")\n",
        "    print(f\"   Базовая модель: {base_model_name}\")\n",
        "    print(f\"   Путь к адаптеру: {seed_path}\")\n",
        "\n",
        "    # Загружаем токенизатор\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"✓ Токенизатор загружен\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка загрузки токенизатора: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Настройки загрузки базовой модели\n",
        "    load_kwargs = {\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "        \"trust_remote_code\": True,\n",
        "        \"device_map\": \"auto\" if use_qlora else device,\n",
        "    }\n",
        "\n",
        "    if use_qlora:\n",
        "        # Для QLoRA моделей загружаем с той же квантизацией\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        load_kwargs[\"quantization_config\"] = bnb_config\n",
        "        load_kwargs[\"torch_dtype\"] = torch.float16\n",
        "    else:\n",
        "        load_kwargs[\"torch_dtype\"] = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    # Загружаем базовую модель\n",
        "    try:\n",
        "        print(\"⏳ Загрузка базовой модели...\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, **load_kwargs)\n",
        "        print(\"✓ Базовая модель загружена\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка загрузки базовой модели: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Загружаем LoRA адаптер, если он есть\n",
        "    adapter_path = seed_path / \"adapter\"\n",
        "    if adapter_path.exists() and use_lora:\n",
        "        try:\n",
        "            print(f\"⏳ Загрузка LoRA адаптера из {adapter_path}...\")\n",
        "            model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "            print(\"✓ LoRA адаптер загружен\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка загрузки LoRA адаптера: {e}\")\n",
        "            model = base_model\n",
        "    else:\n",
        "        print(\"⚠️ LoRA адаптер не найден, использую базовую модель\")\n",
        "        model = base_model\n",
        "\n",
        "    model.eval()\n",
        "    return model, tokenizer, exp_config\n",
        "\n",
        "# ======================= ВЫЧИСЛЕНИЕ PERPLEXITY =======================\n",
        "@torch.no_grad()\n",
        "def compute_perplexity(model, tokenizer, test_dataset, max_samples=100):\n",
        "    \"\"\"\n",
        "    Вычисляет среднюю perplexity на тестовой выборке.\n",
        "\n",
        "    Args:\n",
        "        model: Загруженная модель\n",
        "        tokenizer: Токенизатор\n",
        "        test_dataset: Тестовый датасет (токенизированный)\n",
        "        max_samples: Максимальное количество примеров для оценки\n",
        "\n",
        "    Returns:\n",
        "        Средняя perplexity\n",
        "    \"\"\"\n",
        "    if test_dataset is None:\n",
        "        return None\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    n_samples = min(len(test_dataset), max_samples)\n",
        "\n",
        "    print(f\"\\n📊 Вычисление perplexity на {n_samples} примерах...\")\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        if i % 20 == 0:\n",
        "            print(f\"   Прогресс: {i}/{n_samples}\")\n",
        "\n",
        "        # Берем пример из датасета\n",
        "        item = test_dataset[i]\n",
        "        input_ids = torch.tensor(item[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "        attention_mask = torch.tensor(item[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "\n",
        "        # Вычисляем loss\n",
        "        try:\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            loss = outputs.loss.item()\n",
        "            losses.append(loss)\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Ошибка на примере {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not losses:\n",
        "        return None\n",
        "\n",
        "    avg_loss = sum(losses) / len(losses)\n",
        "    try:\n",
        "        ppl = math.exp(avg_loss)\n",
        "    except OverflowError:\n",
        "        ppl = float('inf')\n",
        "\n",
        "    print(f\"\\n✅ Результаты на {len(losses)} примерах:\")\n",
        "    print(f\"   Средний loss: {avg_loss:.4f}\")\n",
        "    print(f\"   Perplexity: {ppl:.2f}\")\n",
        "\n",
        "    return ppl\n",
        "\n",
        "# ======================= ГЕНЕРАЦИЯ ТЕКСТА =======================\n",
        "def generate_text(model, tokenizer, prompt: str, max_new_tokens=100,\n",
        "                  temperature=0.7, top_p=0.9, top_k=50, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Генерирует текст на основе промпта.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# ======================= ИНТЕРАКТИВНЫЙ РЕЖИМ =======================\n",
        "def interactive_mode(model, tokenizer, model_name):\n",
        "    \"\"\"Интерактивный режим для одной модели.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🤖 Интерактивный режим для {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"Команды:\")\n",
        "    print(\"  /quit - выход\")\n",
        "    print(\"  /temp <value> - изменить температуру (0.1-2.0)\")\n",
        "    print(\"  /clear - очистить экран\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    temperature = TEMPERATURE\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\n📝 Введите промпт (или команду): \").strip()\n",
        "\n",
        "            if prompt.lower() == \"/quit\":\n",
        "                break\n",
        "            elif prompt.lower().startswith(\"/temp\"):\n",
        "                try:\n",
        "                    new_temp = float(prompt.split()[1])\n",
        "                    temperature = max(0.1, min(2.0, new_temp))\n",
        "                    print(f\"✓ Температура изменена на {temperature}\")\n",
        "                except:\n",
        "                    print(\"⚠️ Используйте: /temp <значение>\")\n",
        "                continue\n",
        "            elif prompt.lower() == \"/clear\":\n",
        "                os.system('clear' if os.name == 'posix' else 'cls')\n",
        "                continue\n",
        "            elif not prompt:\n",
        "                continue\n",
        "\n",
        "            print(\"\\n⏳ Генерация...\")\n",
        "            generated = generate_text(\n",
        "                model, tokenizer, prompt,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                temperature=temperature,\n",
        "                top_p=TOP_P,\n",
        "                top_k=TOP_K\n",
        "            )\n",
        "\n",
        "            print(\"\\n\" + \"-\"*40)\n",
        "            print(\"✨ Результат:\")\n",
        "            print(generated[0])\n",
        "            print(\"-\"*40)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n⏸️ Прервано пользователем\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка: {e}\")\n",
        "\n",
        "# ======================= СРАВНЕНИЕ МОДЕЛЕЙ =======================\n",
        "def compare_models(models_dict: Dict[str, Tuple], prompts: List[str]):\n",
        "    \"\"\"\n",
        "    Сравнивает несколько моделей на одних и тех же промптах.\n",
        "\n",
        "    Args:\n",
        "        models_dict: {имя_модели: (model, tokenizer)}\n",
        "        prompts: Список промптов для тестирования\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"🔬 СРАВНЕНИЕ МОДЕЛЕЙ\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for prompt in prompts:\n",
        "        print(f\"\\n📌 Промпт: '{prompt}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for model_name, (model, tokenizer) in models_dict.items():\n",
        "            try:\n",
        "                generated = generate_text(\n",
        "                    model, tokenizer, prompt,\n",
        "                    max_new_tokens=50,\n",
        "                    temperature=0.8,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "                print(f\"\\n[{model_name}]:\")\n",
        "                print(f\"  {generated[0]}\")\n",
        "                results.setdefault(model_name, []).append(generated[0])\n",
        "            except Exception as e:\n",
        "                print(f\"\\n[{model_name}]: ❌ Ошибка - {e}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ======================= ОСНОВНАЯ ФУНКЦИЯ =======================\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"🧪 ТЕСТИРОВАНИЕ ОБУЧЕННЫХ МОДЕЛЕЙ\")\n",
        "    print(f\"Результаты из: {RESULTS_PATH}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Проверяем наличие результатов\n",
        "    if not RESULTS_PATH.exists():\n",
        "        print(f\"❌ Путь не найден: {RESULTS_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Получаем список всех экспериментов\n",
        "    experiments = [d for d in RESULTS_PATH.iterdir()\n",
        "                   if d.is_dir() and not d.name.startswith(('tokenized_', 'raw_subset'))]\n",
        "\n",
        "    print(f\"\\n📋 Найденные эксперименты:\")\n",
        "    for i, exp in enumerate(experiments, 1):\n",
        "        print(f\"  {i}. {exp.name}\")\n",
        "\n",
        "    # Выбор режима\n",
        "    print(\"\\n🎮 Выберите режим:\")\n",
        "    print(\"  1. Интерактивный режим для одной модели\")\n",
        "    print(\"  2. Сравнить несколько моделей\")\n",
        "    print(\"  3. Вычислить perplexity для всех моделей\")\n",
        "    print(\"  0. Выход\")\n",
        "\n",
        "    choice = input(\"\\nВаш выбор (0-3): \").strip()\n",
        "\n",
        "    if choice == \"0\":\n",
        "        return\n",
        "\n",
        "    elif choice == \"1\":\n",
        "        # Интерактивный режим\n",
        "        exp_idx = int(input(f\"Выберите эксперимент (1-{len(experiments)}): \")) - 1\n",
        "        exp_path = experiments[exp_idx]\n",
        "\n",
        "        seed = int(input(\"Сид (42, 43, 44): \"))\n",
        "        batch_size = int(input(\"Batch size (обычно 1 или 4): \"))\n",
        "\n",
        "        model, tokenizer, config = load_model(exp_path, seed, batch_size)\n",
        "\n",
        "        if model is not None:\n",
        "            group = config.get(\"tokenizer_group\", \"unknown\") if config else \"unknown\"\n",
        "            test_dataset = load_test_dataset(RESULTS_PATH, group)\n",
        "\n",
        "            if test_dataset is not None:\n",
        "                compute_perplexity(model, tokenizer, test_dataset, max_samples=50)\n",
        "\n",
        "            interactive_mode(model, tokenizer, f\"{exp_path.name} (seed={seed})\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # Сравнение моделей\n",
        "        models_to_compare = {}\n",
        "\n",
        "        print(\"\\nВыберите модели для сравнения (введите индексы через пробел):\")\n",
        "        for i, exp in enumerate(experiments, 1):\n",
        "            print(f\"  {i}. {exp.name}\")\n",
        "\n",
        "        indices = input(\"Индексы: \").strip().split()\n",
        "\n",
        "        for idx_str in indices:\n",
        "            try:\n",
        "                idx = int(idx_str) - 1\n",
        "                exp_path = experiments[idx]\n",
        "\n",
        "                seed = int(input(f\"Сид для {exp_path.name} (42, 43, 44): \"))\n",
        "                batch_size = int(input(f\"Batch size для {exp_path.name} (обычно 1 или 4): \"))\n",
        "\n",
        "                model, tokenizer, config = load_model(exp_path, seed, batch_size)\n",
        "\n",
        "                if model is not None:\n",
        "                    models_to_compare[f\"{exp_path.name}_s{seed}\"] = (model, tokenizer)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Ошибка загрузки модели {idx_str}: {e}\")\n",
        "\n",
        "        if models_to_compare:\n",
        "            # Промпты для сравнения\n",
        "            test_prompts = [\n",
        "                \"Минем исемем\",\n",
        "                \"Бүгенге көндә\",\n",
        "                \"Татарстан -\",\n",
        "                \"Мәктәптә без\",\n",
        "                \"Кыш көне\"\n",
        "            ]\n",
        "\n",
        "            compare_models(models_to_compare, test_prompts)\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # Пакетное вычисление perplexity\n",
        "        print(\"\\n📊 Пакетное вычисление perplexity\")\n",
        "\n",
        "        results = []\n",
        "        for exp_path in experiments:\n",
        "            for seed in [42, 43, 44]:\n",
        "                # Пробуем разные batch_size\n",
        "                for bs in [1, 4, 8]:\n",
        "                    model, tokenizer, config = load_model(exp_path, seed, bs)\n",
        "                    if model is not None:\n",
        "                        group = config.get(\"tokenizer_group\", \"unknown\") if config else \"unknown\"\n",
        "                        test_dataset = load_test_dataset(RESULTS_PATH, group)\n",
        "\n",
        "                        if test_dataset is not None:\n",
        "                            ppl = compute_perplexity(model, tokenizer, test_dataset, max_samples=100)\n",
        "                            results.append({\n",
        "                                \"experiment\": exp_path.name,\n",
        "                                \"seed\": seed,\n",
        "                                \"batch_size\": bs,\n",
        "                                \"perplexity\": ppl\n",
        "                            })\n",
        "                        break  # Нашли рабочую конфигурацию, выходим из цикла по bs\n",
        "                    else:\n",
        "                        continue\n",
        "                    break\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        if results:\n",
        "            output_file = RESULTS_PATH / \"test_results.json\"\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"\\n✅ Результаты сохранены в {output_file}\")\n",
        "\n",
        "    print(\"\\n👋 До свидания!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9cd4ef",
      "metadata": {
        "id": "0e9cd4ef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed780978",
      "metadata": {
        "id": "ed780978",
        "outputId": "caab93f2-d9ad-4163-f816-1ad2a508ae19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 163920\r\n",
            "drwxrwxr-x 2 kfu kfu      4096 Feb 23 22:35 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
            "drwxrwxr-x 3 kfu kfu      4096 Feb 23 22:35 \u001b[01;34m..\u001b[0m/\r\n",
            "-rw-rw-r-- 1 kfu kfu      1056 Feb 23 22:35 adapter_config.json\r\n",
            "-rw-rw-r-- 1 kfu kfu 167832240 Feb 23 22:35 adapter_model.safetensors\r\n",
            "-rw-rw-r-- 1 kfu kfu      5204 Feb 23 22:35 README.md\r\n"
          ]
        }
      ],
      "source": [
        "ls -la tat_results_final_rf/run_20260223_070746/mistral_qlora_r16/seed42/bs1/adapter/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3207e2ae",
      "metadata": {
        "id": "3207e2ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eef27a5",
      "metadata": {
        "id": "2eef27a5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}