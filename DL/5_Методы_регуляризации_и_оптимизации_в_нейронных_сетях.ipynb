{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAPyAqiIBrLMtvcXiO4J7P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/DL/5_%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D0%B8_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D0%B2_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D1%85_%D1%81%D0%B5%D1%82%D1%8F%D1%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Методы регуляризации и оптимизации в нейронных сетях"
      ],
      "metadata": {
        "id": "EjRIMJhjlN3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. **Дропаут (Dropout)**  \n",
        "Дропаут — это метод регуляризации, используемый в нейронных сетях для предотвращения переобучения. Идея метода заключается в случайном \"отключении\" (занулении) части нейронов во время обучения с определённой вероятностью $p$. Это позволяет снизить коадаптацию нейронов, то есть их чрезмерную зависимость друг от друга, что способствует повышению обобщающей способности модели.  \n",
        "\n",
        "а) **Прямой и обратный проход через линейный слой с Dropout**  \n",
        "Рассмотрим линейный слой с функцией активации $f$. Пусть на вход подаётся наблюдение $h_1$ размерности $[1 \\times d]$. Тогда выход слоя вычисляется по формуле:  \n",
        "$$\n",
        "h_2 = f(h_1 \\cdot W + b),\n",
        "$$  \n",
        "где $W$ — матрица весов размерности $[d \\times k]$, $b$ — вектор смещений размерности $[1 \\times k]$, а $h_2$ — выходной вектор размерности $[1 \\times k]$.  \n",
        "\n",
        "Для применения дропаута каждый нейрон отключается с вероятностью $p$. Это реализуется путём поэлементного умножения выходного вектора $h_2$ на вектор $d$ размерности $[1 \\times k]$, где каждый элемент $d_j$ имеет распределение Бернулли:  \n",
        "$$\n",
        "d_j =\n",
        "\\begin{cases}\n",
        "0, & \\text{с вероятностью } p, \\\\\n",
        "1, & \\text{с вероятностью } 1 - p.\n",
        "\\end{cases}\n",
        "$$  \n",
        "Обозначив поэлементное умножение через $\\odot$, получим:  \n",
        "$$\n",
        "h_2 = d \\odot f(h_1 \\cdot W + b).\n",
        "$$  \n",
        "Если на вход подаётся матрица $H_1$ размерности $[n \\times d]$, то выход слоя с дропаутом вычисляется как:  \n",
        "$$\n",
        "H_2 = D \\odot f(H_1 \\cdot W + b),\n",
        "$$  \n",
        "где $D$ — матрица размерности $[n \\times k]$, состоящая из нулей и единиц, сгенерированных по распределению Бернулли.  \n",
        "\n",
        "Без дропаута шаг обратного распространения ошибки (backpropagation) выглядит следующим образом:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial H_1} = \\frac{\\partial L}{\\partial H_2} \\cdot W^T \\odot f'(H_1 \\cdot W + b),\n",
        "$$  \n",
        "где $\\frac{\\partial L}{\\partial H_1}$ — градиент по входу слоя, $\\frac{\\partial L}{\\partial H_2}$ — градиент по выходу слоя, а $f'$ — производная функции активации.  \n",
        "\n",
        "Для слоя с дропаутом шаг обратного распространения ошибки модифицируется следующим образом:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial H_1} = \\frac{\\partial L}{\\partial H_2} \\cdot W^T \\odot f'(H_1 \\cdot W + b) \\odot D.\n",
        "$$  \n",
        "Таким образом, градиенты по отключённым нейронам зануляются, и сеть игнорирует их при обновлении параметров.  \n",
        "\n",
        "Производная по весам слоя вычисляется как:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = H_1^T \\cdot \\frac{\\partial L}{\\partial H_1},\n",
        "$$  \n",
        "и именно на эту величину выполняется шаг градиентного спуска.  \n",
        "\n",
        "б) **Роль Dropout в предотвращении переобучения**  \n",
        "Дропаут помогает бороться с переобучением за счёт предотвращения коадаптации нейронов. При обучении случайное отключение части нейронов приводит к тому, что оставшиеся нейроны вынуждены учиться более независимо, что снижает риск переобучения на конкретных признаках.  \n",
        "\n",
        "Кроме того, дропаут можно интерпретировать как усреднение множества различных моделей, получаемых путём случайного отключения нейронов. Это эквивалентно обучению ансамбля из $2^N$ моделей, где $N$ — количество нейронов, которые могут быть отключены.  \n",
        "\n",
        "в) **Применение Dropout на этапе тестирования**  \n",
        "На этапе тестирования дропаут не применяется, так как это может привести к нестабильности предсказаний. Вместо этого используется масштабирование выходов нейронов на коэффициент $1 - p$, чтобы сохранить ожидаемое значение выхода.  \n",
        "\n",
        "Рассмотрим пример с двумя нейронами, каждый из которых отключается с вероятностью $0.4$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v5izlbfJtHPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "eisYp3VQtmzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Пусть выходы нейронов равны $x_1$ и $x_2$, а их веса — $w_1$ и $w_2$. Тогда ожидаемое значение выхода $h$ вычисляется как:  \n",
        "$$\n",
        "E(h) = 0.6 \\cdot 0.6 \\cdot (w_1 \\cdot x_1 + w_2 \\cdot x_2) + 0.6 \\cdot 0.4 \\cdot (w_1 \\cdot x_1) + 0.4 \\cdot 0.6 \\cdot (w_2 \\cdot x_2) + 0.4 \\cdot 0.4 \\cdot 0 = 0.6 \\cdot (w_1 \\cdot x_1 + w_2 \\cdot x_2).\n",
        "$$  \n",
        "Таким образом, для получения корректного результата на этапе тестирования достаточно умножить выходы нейронов на $0.6$.  \n",
        "\n",
        "г) **Обратный дропаут (Inverted Dropout)**  \n",
        "Обратный дропаут — это модификация метода, при которой выходы нейронов на этапе обучения масштабируются на коэффициент $\\frac{1}{1 - p}$. Это позволяет избежать необходимости масштабирования на этапе тестирования. Прямой проход при обратном дропауте выглядит следующим образом:  \n",
        "$$\n",
        "H_2 = \\frac{1}{1 - p} \\cdot D \\odot f(H_1 \\cdot W + b).\n",
        "$$  \n",
        "При этом на этапе тестирования выходы нейронов используются без дополнительного масштабирования:  \n",
        "$$\n",
        "H_2 = f(H_1 \\cdot W + b).\n",
        "$$  \n",
        "Такой подход упрощает реализацию и снижает вероятность ошибок при переходе между этапами обучения и тестирования.\n",
        "\n",
        "\n",
        "д) **Шаг градиентного спуска для линейного слоя с обратным дропаутом и без него**  \n",
        "Для линейного слоя без дропаута производная по весам $W$ вычисляется следующим образом:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = H_1^T \\cdot \\frac{\\partial L}{\\partial H_1},\n",
        "$$  \n",
        "где $H_1$ — входные данные, $\\frac{\\partial L}{\\partial H_1}$ — градиент по выходу слоя. Шаг градиентного спуска для обновления весов выглядит как:  \n",
        "$$\n",
        "W_t = W_{t-1} - \\eta_t \\cdot \\frac{\\partial L}{\\partial W}(W_{t-1}),\n",
        "$$  \n",
        "где $\\eta_t$ — скорость обучения на шаге $t$.  \n",
        "\n",
        "При использовании обратного дропаута формула для шага градиентного спуска модифицируется:  \n",
        "$$\n",
        "W_t = W_{t-1} - \\eta_t \\cdot \\frac{1}{1 - p} \\cdot \\frac{\\partial L}{\\partial W}(W_{t-1}).\n",
        "$$  \n",
        "Таким образом, скорость обучения дополнительно масштабируется на коэффициент $\\frac{1}{1 - p} \\geq 1$. Это приводит к увеличению величины шага градиентного спуска, что может ускорить процесс обучения, но также требует осторожности в выборе скорости обучения $\\eta_t$, чтобы избежать нестабильности.  \n",
        "\n",
        "е) **Вероятность отключения всего слоя и параметры дропаута**  \n",
        "Пусть на слое имеется $k$ нейронов, каждый из которых отключается с вероятностью $p$. Вероятность того, что весь слой будет отключён, равна $p^k$.  \n",
        "\n",
        "Рассмотрим случайную величину $N$, которая обозначает количество включённых нейронов. Каждый нейрон включается с вероятностью $1 - p$, поэтому математическое ожидание $N$ вычисляется как:  \n",
        "$$\n",
        "E(N) = E(Y_1 + \\dots + Y_k) = (1 - p) \\cdot k,\n",
        "$$  \n",
        "где $Y_i$ — индикаторная случайная величина, равная 1, если $i$-й нейрон включён, и 0 в противном случае.  \n",
        "\n",
        "Дисперсия $N$ для биномиального распределения равна:  \n",
        "$$\n",
        "\\text{Var}(N) = k \\cdot p \\cdot (1 - p).\n",
        "$$  \n",
        "\n",
        "Если требуется оставить четверть работающих нейронов, то $E(N) = \\frac{k}{4}$. Подставляя в формулу математического ожидания, получаем:  \n",
        "$$\n",
        "(1 - p) \\cdot k = \\frac{k}{4} \\implies p = 0.75.\n",
        "$$  \n",
        "\n",
        "ё) **Вероятность отключения хотя бы одного слоя в сети**  \n",
        "Пусть нейронная сеть состоит из трёх слоёв с количеством нейронов $k_1$, $k_2$ и $k_3$ соответственно. Вероятность отключения каждого слоя равна $p^{k_1}$, $p^{k_2}$ и $p^{k_3}$.  \n",
        "\n",
        "Вероятность того, что ни один из слоёв не отключится, вычисляется как:  \n",
        "$$\n",
        "q = (1 - p^{k_1}) \\cdot (1 - p^{k_2}) \\cdot (1 - p^{k_3}).\n",
        "$$  \n",
        "Тогда вероятность того, что хотя бы один слой отключится, равна:  \n",
        "$$\n",
        "1 - q.\n",
        "$$  \n",
        "\n",
        "ж) **Влияние дропаута на нормализацию данных и метод Alpha Dropout**  \n",
        "Функция активации SELU (Scaled Exponential Linear Unit) обладает свойством самонормализации: при правильной инициализации весов выход линейного слоя с SELU имеет нулевое среднее и единичную дисперсию. Однако добавление дропаута нарушает это свойство.  \n",
        "\n",
        "При использовании обратного дропаута выход слоя масштабируется следующим образом:  \n",
        "$$\n",
        "h = \\frac{1}{1 - p} \\cdot d \\cdot x,\n",
        "$$  \n",
        "где $d$ — случайная величина, принимающая значение 1 с вероятностью $1 - p$ и 0 с вероятностью $p$, а $x$ — выход нейрона.  \n",
        "\n",
        "Математическое ожидание $h$ остаётся неизменным:  \n",
        "$$\n",
        "E(h) = E\\left(\\frac{1}{1 - p} \\cdot d \\cdot x\\right) = \\frac{1}{1 - p} \\cdot E(d) \\cdot E(x) = E(x).\n",
        "$$  \n",
        "\n",
        "Однако дисперсия изменяется:  \n",
        "$$\n",
        "\\text{Var}(h) = \\text{Var}\\left(\\frac{1}{1 - p} \\cdot d \\cdot x\\right) = \\frac{1}{(1 - p)^2} \\cdot \\text{Var}(d) \\cdot \\text{Var}(x) = \\frac{p}{1 - p} \\cdot \\text{Var}(x).\n",
        "$$  \n",
        "\n",
        "Для сохранения нормализации данных предлагается метод Alpha Dropout. В этом методе выход нейрона заменяется на $\\alpha'$ (константа, зависящая от параметров SELU) с вероятностью $p$. Формула для Alpha Dropout выглядит следующим образом:  \n",
        "$$\n",
        "d(t) =\n",
        "\\begin{cases}\n",
        "h, & \\text{с вероятностью } 1 - p, \\\\\n",
        "\\alpha', & \\text{с вероятностью } p.\n",
        "\\end{cases}\n",
        "$$  \n",
        "\n",
        "Для сохранения нулевого математического ожидания и единичной дисперсии выход масштабируется и сдвигается:  \n",
        "$$\n",
        "d(t) \\cdot a + b,\n",
        "$$  \n",
        "где константы $a$ и $b$ выбираются так, чтобы выполнялись условия:  \n",
        "$$\n",
        "E(a \\cdot (d \\cdot x + (1 - d) \\cdot \\alpha') + b) = 0, \\\\\n",
        "\\text{Var}(a \\cdot (d \\cdot x + (1 - d) \\cdot \\alpha') + b) = 1.\n",
        "$$  \n",
        "\n",
        "Решая систему уравнений, получаем:  \n",
        "$$\n",
        "a = \\left(p + (\\alpha')^2 \\cdot p \\cdot (1 - p)\\right)^{-0.5}, \\\\\n",
        "b = -\\left(p + (\\alpha')^2 \\cdot p \\cdot (1 - p)\\right)^{-0.5} \\cdot (1 - p) \\cdot \\alpha'.\n",
        "$$  \n",
        "\n",
        "Это преобразование сохраняет эффект разреженности от дропаута и поддерживает нормализацию данных, что особенно важно при использовании функции активации SELU.\n"
      ],
      "metadata": {
        "id": "IRNG7HKrtm96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Регуляризация\n",
        "\n",
        "Рассмотрим задачу регрессии, в которой целевая переменная $y$ зависит от признака $x$. Предположим, что зависимость между $y$ и $x$ может быть описана линейной моделью без свободного члена:  \n",
        "$$y_i = w \\cdot x_i,$$  \n",
        "где $w$ — параметр модели, который необходимо оценить. Если значение $x = 0$, то модель предсказывает $y = 0$, что соответствует условию задачи.\n",
        "\n",
        "Для оценки параметра $w$ используется целевая функция с L2-регуляризацией:  \n",
        "$$Q(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - w x_i)^2 + \\lambda w^2 \\to \\min_w,$$  \n",
        "где $\\lambda$ — параметр регуляризации, контролирующий степень штрафа за увеличение значения $w$.\n",
        "\n",
        "### а) Нахождение оптимального $w$ при произвольном $\\lambda$\n",
        "\n",
        "Для нахождения оптимального значения $w$ продифференцируем целевую функцию $Q(w)$ по $w$ и приравняем производную к нулю:  \n",
        "$$\n",
        "\\frac{dQ(w)}{dw} = \\frac{1}{n} \\sum_{i=1}^n -2 \\cdot (y_i - w x_i) \\cdot x_i + 2 \\lambda w = 0.\n",
        "$$  \n",
        "Упростим выражение:  \n",
        "$$\n",
        "\\frac{1}{n} \\sum_{i=1}^n y_i x_i - w \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i^2 - \\lambda w = 0.\n",
        "$$  \n",
        "Перенесем слагаемые, содержащие $w$, в одну сторону:  \n",
        "$$\n",
        "\\frac{1}{n} \\sum_{i=1}^n y_i x_i = w \\left( \\frac{1}{n} \\sum_{i=1}^n x_i^2 + \\lambda \\right).\n",
        "$$  \n",
        "Отсюда оптимальное значение $w$ выражается как:  \n",
        "$$\n",
        "\\hat{w} = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2 + n \\lambda}.\n",
        "$$  \n",
        "\n",
        "**Интерпретация:**  \n",
        "С увеличением параметра регуляризации $\\lambda$ значение $\\hat{w}$ стремится к нулю. Это предотвращает переобучение модели, ограничивая рост коэффициентов. Регуляризация особенно полезна, когда модель слишком сильно подстраивается под данные, что приводит к большим значениям коэффициентов.\n",
        "\n",
        "### б) Подбор оптимального $\\lambda$ с помощью кросс-валидации leave-one-out (LOO)\n",
        "\n",
        "Кросс-валидация leave-one-out (LOO) заключается в следующем:  \n",
        "1. На каждом шаге $i$ ($i = 1, 2, \\dots, n$) модель обучается на всех наблюдениях, кроме $i$-го.  \n",
        "2. Затем модель тестируется на $i$-м наблюдении, и вычисляется ошибка предсказания.  \n",
        "3. После проведения $n$ таких шагов вычисляется средняя ошибка по всем тестовым наблюдениям.  \n",
        "\n",
        "Оптимальное значение $\\lambda$ выбирается как то, которое минимизирует среднюю ошибку:  \n",
        "$$\n",
        "\\lambda_{\\text{CV}} = \\arg\\min_\\lambda \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i^{(-i)})^2,\n",
        "$$  \n",
        "где $\\hat{y}_i^{(-i)}$ — предсказание модели, обученной на всех наблюдениях, кроме $i$-го.\n",
        "\n",
        "### в) Нахождение оптимального $w$ при $\\lambda_{\\text{CV}}$\n",
        "\n",
        "После нахождения $\\lambda_{\\text{CV}}$ оптимальное значение $w$ вычисляется по формуле, полученной в пункте (а):  \n",
        "$$\n",
        "\\hat{w} = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2 + n \\lambda_{\\text{CV}}}.\n",
        "$$  \n",
        "\n",
        "### г) Ускорение LOO-кросс-валидации для Ridge-регрессии\n",
        "\n",
        "Для Ridge-регрессии можно избежать необходимости обучать $n$ моделей, используя матричные вычисления. Оптимальные коэффициенты для Ridge-регрессии могут быть выражены в виде:  \n",
        "$$\n",
        "\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y,\n",
        "$$  \n",
        "где $X$ — матрица признаков, $y$ — вектор целевых значений, $I$ — единичная матрица.  \n",
        "\n",
        "С помощью метода шляпных матриц (hat matrix) и свойств линейной алгебры можно вычислить предсказания для каждого наблюдения без повторного обучения модели. Это позволяет значительно ускорить процесс кросс-валидации.\n",
        "\n",
        "### д) Регуляризация на выходе нейрона в нейронных сетях\n",
        "\n",
        "Наложение регуляризации на выход нейрона в нейронных сетях используется для следующих целей:  \n",
        "1. **Контроль переобучения:** Регуляризация ограничивает рост весов, предотвращая излишнюю сложность модели.  \n",
        "2. **Улучшение обобщающей способности:** Ограничение весов помогает модели лучше работать на новых данных.  \n",
        "3. **Стабилизация обучения:** Регуляризация может предотвратить взрыв градиентов, что особенно важно в глубоких сетях.  \n",
        "4. **Интерпретируемость:** Меньшие значения весов могут упростить интерпретацию модели.  \n",
        "\n",
        "Таким образом, регуляризация на выходе нейрона является важным инструментом для улучшения качества и устойчивости нейронных сетей."
      ],
      "metadata": {
        "id": "Hp2J-WZ3qNpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Weight Decay  \n",
        "В случае $L_2$-регуляризации к базовой функции потерь добавляется дополнительное слагаемое, и вместо функционала  \n",
        "$$\n",
        "L(w) = \\frac{1}{n} \\cdot \\sum\\limits_{i=1}^{n} \\nabla_w L(y_i, x_i, w)\n",
        "$$  \n",
        "оптимизируется функционал  \n",
        "$$\n",
        "Q_\\lambda(w) = L(w) + \\frac{1}{2}\\lambda \\cdot ||w||^2_2,\n",
        "$$  \n",
        "где $\\lambda$ — параметр регуляризации. Будем считать, что регуляризатор применяется ко всем весам нейронной сети. Обычно регуляризатор добавляется к функции потерь для предотвращения переобучения. Градиентный спуск может быть переписан с учётом регуляризатора в несколько иной форме, которая называется **weight decay**. В современных библиотеках для обучения нейронных сетей у оптимизаторов часто присутствует соответствующий параметр. Рассмотрим переписывание градиентного спуска с учётом регуляризатора для нескольких методов оптимизации.\n",
        "\n",
        "### а) Momentum-SGD с $L_2$-регуляризацией  \n",
        "Шаг momentum-SGD с учётом $L_2$-регуляризации может быть выражен в виде:  \n",
        "$$\n",
        "w_t = g(\\lambda) \\cdot w_{t-1} - \\eta_t \\cdot h(\\nabla_w L(w_{t-1})),\n",
        "$$  \n",
        "где $\\eta_t$ — learning rate на шаге $t$, а $g(\\lambda)$ и $h(\\cdot)$ — функции, зависящие от параметра регуляризации и градиента соответственно.  \n",
        "\n",
        "Один шаг momentum-SGD с $L_2$-регуляризацией описывается следующей системой уравнений:  \n",
        "$$\n",
        "\\begin{cases}\n",
        "g_t = \\nabla_w Q(w_{t-1}) + \\lambda \\cdot w_{t-1}, \\\\\n",
        "m_t = \\mu \\cdot m_{t-1} + g_t, \\\\\n",
        "w_t = w_{t-1} - \\eta_t \\cdot m_t,\n",
        "\\end{cases}\n",
        "$$  \n",
        "где $\\mu$ — коэффициент инерции (momentum), а $m_t$ — накопленный градиент.  \n",
        "\n",
        "Подставив первую строку во вторую, а вторую в третью, получим:  \n",
        "$$\n",
        "w_t = w_{t-1} - \\eta_t \\cdot \\left( \\mu \\cdot m_{t-1} + \\nabla_w L(w_{t-1}) + \\lambda \\cdot w_{t-1} \\right) =\n",
        "$$  \n",
        "$$\n",
        "= \\underbrace{(1 - \\eta_t \\cdot \\lambda)}_{<1} \\cdot w_{t-1} - \\eta_t \\cdot \\left( \\mu \\cdot m_{t-1} + \\nabla_w L(w_{t-1}) \\right).\n",
        "$$  \n",
        "\n",
        "Таким образом, при добавлении $L_2$-регуляризации каждый шаг градиентного спуска выполняется с учётом сдвига весов на константу, что эквивалентно применению weight decay. Этот параметр часто используется в оптимизаторах для нейронных сетей вместо явной регуляризации.\n",
        "\n",
        "### б) Adam с $L_2$-регуляризацией  \n",
        "Шаг Adam с учётом $L_2$-регуляризации может быть выражен в виде:  \n",
        "$$\n",
        "w_t = g(\\lambda) \\cdot w_{t-1} - \\eta_t \\cdot h(\\nabla_w L(w_{t-1})),\n",
        "$$  \n",
        "где $g(\\lambda)$ и $h(\\cdot)$ — функции, зависящие от параметра регуляризации и градиента.  \n",
        "\n",
        "Один шаг Adam с $L_2$-регуляризацией описывается следующей системой уравнений:  \n",
        "$$\n",
        "\\begin{cases}\n",
        "g_t = \\nabla_w Q(w_{t-1}) + \\lambda \\cdot w_{t-1}, \\\\\n",
        "m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t, \\\\\n",
        "v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2, \\\\\n",
        "\\hat{m}_t = \\frac{1}{1-\\beta_1^t} \\cdot m_t, \\\\\n",
        "\\hat{v}_t = \\frac{1}{1-\\beta_2^t} \\cdot v_t, \\\\\n",
        "w_t = w_{t-1} - \\eta_t \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon},\n",
        "\\end{cases}\n",
        "$$  \n",
        "где $\\beta_1$ и $\\beta_2$ — гиперпараметры, контролирующие экспоненциальное затухание для градиента и его квадрата соответственно, а $\\varepsilon$ — малая константа для численной стабильности.  \n",
        "\n",
        "Выполнив подстановки, получим:  \n",
        "$$\n",
        "w_t = w_{t-1} - \\eta_t \\cdot \\frac{m_t}{1-\\beta_1^t} \\cdot \\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon} =\n",
        "$$  \n",
        "$$\n",
        "= w_{t-1} - \\eta_t \\cdot \\frac{\\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t}{1-\\beta_1^t} \\cdot \\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon} =\n",
        "$$  \n",
        "$$\n",
        "= w_{t-1} - \\eta_t \\cdot \\frac{\\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot (\\nabla_w Q(w_{t-1}) + \\lambda \\cdot w_{t-1})}{1-\\beta_1^t} \\cdot \\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon} =\n",
        "$$  \n",
        "$$\n",
        "= w_{t-1} \\cdot \\left( \\underbrace{1}_{\\text{вектор единиц}} - \\frac{\\eta_t \\cdot \\lambda \\cdot (1-\\beta_1)}{1-\\beta_1^t} \\cdot \\underbrace{\\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon}}_{(*)} \\right) - \\dots\n",
        "$$  \n",
        "\n",
        "Здесь $(*)$ указывает на то, что регуляризация работает по-разному для различных весов, что может приводить к их неравномерному затуханию. В случае Adam, из-за высокой эффективности оптимизации, существует риск переобучения.  \n",
        "\n",
        "### в) AdamW  \n",
        "Для решения проблемы переобучения в Adam была предложена модификация AdamW, в которой weight decay учитывается отдельно от градиента. Уравнения для AdamW имеют вид:  \n",
        "$$\n",
        "\\begin{cases}\n",
        "g_t = \\nabla_w Q(w_{t-1}), \\\\\n",
        "m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t, \\\\\n",
        "v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2, \\\\\n",
        "\\hat{m}_t = \\frac{1}{1-\\beta_1^t} \\cdot m_t, \\\\\n",
        "\\hat{v}_t = \\frac{1}{1-\\beta_2^t} \\cdot v_t, \\\\\n",
        "w_t = (1 - \\eta_t \\cdot \\lambda) \\cdot w_{t-1} - \\eta_t \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}.\n",
        "\\end{cases}\n",
        "$$  \n",
        "В AdamW weight decay применяется непосредственно к весам, что позволяет более эффективно контролировать переобучение."
      ],
      "metadata": {
        "id": "ZDKPaqWeqayg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Нормализация по батчам (Batch Normalization)\n",
        "\n",
        "Нормализация по батчам (Batch Normalization, BN) — это мощная техника, которая была предложена для ускорения обучения глубоких нейронных сетей и борьбы с внутренним ковариационным сдвигом (internal covariate shift). Внутренний ковариационный сдвиг возникает, когда распределение входных данных для каждого слоя сети меняется в процессе обучения, что затрудняет настройку параметров и замедляет сходимость. Batch Normalization решает эту проблему путем нормализации активаций каждого слоя по мини-батчам данных.\n",
        "\n",
        "\n",
        "\n",
        "## Математическая основа Batch Normalization\n",
        "\n",
        "### 1. Нормализация активаций\n",
        "\n",
        "Пусть у нас есть мини-батч данных $B = \\{x_1, x_2, \\dots, x_m\\}$, где $m$ — размер батча. Для каждого элемента батча $x_i$ вычисляется его нормализованное значение. Процесс нормализации состоит из следующих шагов:\n",
        "\n",
        "#### a) Вычисление среднего и дисперсии по батчу\n",
        "\n",
        "Сначала вычисляется среднее значение $\\mu_B$ и дисперсия $\\sigma_B^2$ по батчу:\n",
        "\n",
        "$$\n",
        "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\mu_B$ — среднее значение активаций по батчу,\n",
        "- $\\sigma_B^2$ — дисперсия активаций по батчу.\n",
        "\n",
        "#### b) Нормализация\n",
        "\n",
        "Затем каждый элемент батча нормализуется с использованием среднего и дисперсии:\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\hat{x}_i$ — нормализованное значение,\n",
        "- $\\varepsilon$ — малая константа (например, $10^{-5}$), добавленная для численной стабильности (чтобы избежать деления на ноль).\n",
        "\n",
        "#### c) Масштабирование и сдвиг\n",
        "\n",
        "После нормализации применяется масштабирование и сдвиг с помощью обучаемых параметров $\\gamma$ и $\\beta$:\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\cdot \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\gamma$ — параметр масштабирования,\n",
        "- $\\beta$ — параметр сдвига.\n",
        "\n",
        "Эти параметры позволяют сети адаптировать нормализованные значения под конкретные задачи и восстанавливать выразительную способность сети, если нормализация нежелательна.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Обучение и инференс\n",
        "\n",
        "#### a) Обучение\n",
        "\n",
        "Во время обучения среднее $\\mu_B$ и дисперсия $\\sigma_B^2$ вычисляются для каждого батча. Кроме того, поддерживаются скользящие средние $\\mu_{\\text{running}}$ и $\\sigma^2_{\\text{running}}$, которые обновляются после каждого батча:\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{running}} = \\alpha \\cdot \\mu_{\\text{running}} + (1 - \\alpha) \\cdot \\mu_B\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2_{\\text{running}} = \\alpha \\cdot \\sigma^2_{\\text{running}} + (1 - \\alpha) \\cdot \\sigma_B^2\n",
        "$$\n",
        "\n",
        "Здесь $\\alpha$ — коэффициент затухания (обычно близкий к 1, например, 0.9).\n",
        "\n",
        "#### b) Инференс\n",
        "\n",
        "Во время инференса (тестирования) используются скользящие средние $\\mu_{\\text{running}}$ и $\\sigma^2_{\\text{running}}$, а не статистики текущего батча:\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_{\\text{running}}}{\\sqrt{\\sigma^2_{\\text{running}} + \\varepsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\cdot \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Пример использования в нейронной сети\n",
        "\n",
        "Рассмотрим пример использования Batch Normalization в полносвязной нейронной сети. Пусть у нас есть сеть с двумя скрытыми слоями:\n"
      ],
      "metadata": {
        "id": "hff3ndglrdD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eNkQRAeDtLUO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Здесь:\n",
        "- `fc1`, `fc2`, `fc3` — полносвязные слои,\n",
        "- `bn1`, `bn2` — слои Batch Normalization, применяемые после каждого полносвязного слоя перед активационной функцией ReLU.\n",
        "\n",
        "\n",
        "## Преимущества Batch Normalization\n",
        "\n",
        "1. **Ускорение обучения:** BN позволяет использовать более высокие learning rates, что ускоряет сходимость.\n",
        "2. **Устойчивость к инициализации весов:** BN делает сеть менее чувствительной к начальной инициализации весов.\n",
        "3. **Регуляризация:** BN действует как слабый регуляризатор, что может уменьшить необходимость в других методах регуляризации, таких как dropout.\n",
        "\n",
        "\n",
        "## Недостатки и ограничения\n",
        "\n",
        "1. **Зависимость от размера батча:** BN требует достаточно большого размера батча для точной оценки среднего и дисперсии. На маленьких батчах это может привести к нестабильности.\n",
        "2. **Сложность в рекуррентных сетях:** Применение BN в рекуррентных сетях (RNN, LSTM) требует специальных модификаций.\n",
        "3. **Проблемы с распределенным обучением:** В распределенных системах синхронизация статистик между устройствами может быть сложной.\n",
        "\n",
        "\n",
        "## Альтернативы Batch Normalization\n",
        "\n",
        "В случаях, когда Batch Normalization не подходит, могут использоваться альтернативные методы:\n",
        "\n",
        "1. **Layer Normalization (LN):** Нормализация по слоям, которая нормализует активации по нейронам, а не по батчам.\n",
        "2. **Instance Normalization (IN):** Нормализация по каждому отдельному примеру, часто используется в задачах генерации изображений.\n",
        "3. **Group Normalization (GN):** Нормализация по группам нейронов, что может быть полезно при маленьких батчах.\n",
        "\n",
        "### Пример Layer Normalization\n",
        "\n"
      ],
      "metadata": {
        "id": "fPkr_3SltOvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.ln1 = nn.LayerNorm(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.ln2 = nn.LayerNorm(128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JZN7mylFtWuO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, Batch Normalization — это мощный инструмент для улучшения обучения глубоких нейронных сетей. Она позволяет ускорить сходимость, уменьшить зависимость от инициализации весов и действует как слабый регуляризатор. Однако в некоторых случаях, таких как маленькие батчи или рекуррентные сети, могут потребоваться альтернативные методы нормализации. Понимание математической основы BN помогает эффективно применять эту технику и адаптировать её под конкретные задачи."
      ],
      "metadata": {
        "id": "uFxoT8jKtZq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Нормализация VS дропаут  \n",
        "\n",
        "В обучении нейронных сетей часто возникают проблемы, связанные с внутренним ковариационным сдвигом (internal covariate shift) и переобучением. Для решения этих проблем используются два основных метода: **нормализация** и **дропаут**. Оба метода имеют свои особенности, математические обоснования и области применения. Рассмотрим их подробно.\n",
        "\n",
        "\n",
        "\n",
        "## 5.1 Нормализация  \n",
        "\n",
        "Нормализация — это метод, который позволяет стабилизировать распределение входных данных для каждого слоя нейронной сети. Это помогает ускорить обучение и улучшить обобщающую способность модели. Наиболее популярным методом нормализации является **Batch Normalization (BN)**.\n",
        "\n",
        "### 5.1.1 Batch Normalization  \n",
        "\n",
        "Batch Normalization работает следующим образом:  \n",
        "1. Для каждого мини-батча вычисляются среднее значение и дисперсия по каждому признаку.  \n",
        "2. Данные нормализуются с использованием этих статистик.  \n",
        "3. Вводятся обучаемые параметры масштаба и сдвига для восстановления выразительности модели.  \n",
        "\n",
        "Математически это выражается следующим образом:  \n",
        "\n",
        "1. Вычисление среднего и дисперсии для мини-батча:  \n",
        "$$\n",
        "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i, \\quad \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2,\n",
        "$$  \n",
        "   где $m$ — размер мини-батча, $x_i$ — входные данные.  \n",
        "\n",
        "2. Нормализация данных:  \n",
        "$$\n",
        "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}},\n",
        "$$  \n",
        "   где $\\varepsilon$ — малая константа для численной стабильности.  \n",
        "\n",
        "3. Масштабирование и сдвиг:  \n",
        "$$\n",
        "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta,\n",
        "$$  \n",
        "   где $\\gamma$ и $\\beta$ — обучаемые параметры.  \n",
        "\n",
        "#### Преимущества Batch Normalization:  \n",
        "- Ускорение сходимости за счёт стабилизации градиентов.  \n",
        "- Снижение чувствительности к начальной инициализации весов.  \n",
        "- Позволяет использовать более высокие learning rates.  \n",
        "\n",
        "#### Пример:  \n",
        "Предположим, у нас есть мини-батч из 4 элементов: $x = [1, 2, 3, 4]$.  \n",
        "1. Вычисляем среднее и дисперсию:  \n",
        "$$\n",
        "   \\mu_B = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\sigma_B^2 = \\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4} = 1.25.\n",
        "$$  \n",
        "2. Нормализуем данные:  \n",
        "$$\n",
        "   \\hat{x}_i = \\frac{x_i - 2.5}{\\sqrt{1.25 + \\varepsilon}}.\n",
        "$$  \n",
        "3. Применяем масштабирование и сдвиг:  \n",
        "$$\n",
        "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta.\n",
        "$$  \n",
        "\n",
        "\n",
        "\n",
        "## 5.2 Дропаут  \n",
        "\n",
        "Дропаут (Dropout) — это метод регуляризации, который случайным образом \"отключает\" (обнуляет) часть нейронов во время обучения. Это предотвращает переобучение, заставляя сеть учиться более устойчивым признакам.  \n",
        "\n",
        "### 5.2.1 Математическое описание  \n",
        "\n",
        "Пусть $p$ — вероятность того, что нейрон останется активным (обычно $p = 0.5$). Тогда для каждого нейрона $x_i$ на этапе обучения:  \n",
        "$$\n",
        "x_i' =\n",
        "\\begin{cases}\n",
        "\\frac{x_i}{p}, & \\text{с вероятностью } p, \\\\\n",
        "0, & \\text{с вероятностью } 1-p.\n",
        "\\end{cases}\n",
        "$$  \n",
        "На этапе тестирования все нейроны остаются активными, но их выходы умножаются на $p$.  \n",
        "\n",
        "#### Пример:  \n",
        "Пусть у нас есть слой из 4 нейронов: $x = [1, 2, 3, 4]$, и $p = 0.5$.  \n",
        "1. Генерируем маску: $m = [1, 0, 1, 0]$ (случайно отключаем нейроны).  \n",
        "2. Применяем дропаут:  \n",
        "$$\n",
        "   x' = \\left[ \\frac{1}{0.5}, 0, \\frac{3}{0.5}, 0 \\right] = [2, 0, 6, 0].\n",
        "$$  \n",
        "\n",
        "#### Преимущества дропаута:  \n",
        "- Эффективно предотвращает переобучение.  \n",
        "- Прост в реализации и не требует дополнительных параметров.  \n",
        "- Может использоваться совместно с другими методами регуляризации.  \n",
        "\n",
        "\n",
        "\n",
        "## 5.3 Сравнение нормализации и дропаута  \n",
        "\n",
        "| Характеристика          | Нормализация (BN)                     | Дропаут                          |\n",
        "|-||-|\n",
        "| **Цель**                | Стабилизация распределения данных     | Предотвращение переобучения      |\n",
        "| **Принцип работы**      | Нормализация по мини-батчу           | Случайное отключение нейронов    |\n",
        "| **Параметры**           | $\\gamma$, $\\beta$             | Вероятность $p$              |\n",
        "| **Влияние на обучение** | Ускорение сходимости                  | Снижение переобучения            |\n",
        "| **Применение**          | На каждом слое                       | На скрытых слоях                 |\n",
        "\n",
        "\n",
        "\n",
        "## 5.4 Совместное использование  \n",
        "\n",
        "Нормализация и дропаут могут использоваться вместе. Например, в современных архитектурах нейронных сетей (например, ResNet) часто применяется Batch Normalization после каждого свёрточного слоя, а дропаут — в полносвязных слоях.  \n",
        "\n",
        "### Пример совместного использования:  \n",
        "1. Применяем Batch Normalization:  \n",
        "$$\n",
        "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta.\n",
        "$$  \n",
        "2. Применяем дропаут:  \n",
        "$$\n",
        "   y_i' =\n",
        "   \\begin{cases}\n",
        "   \\frac{y_i}{p}, & \\text{с вероятностью } p, \\\\\n",
        "   0, & \\text{с вероятностью } 1-p.\n",
        "   \\end{cases}\n",
        "$$  \n",
        "\n",
        "\n",
        "\n",
        "Таким образом, нормализация и дропаут — это два мощных инструмента, которые решают разные задачи в обучении нейронных сетей. Нормализация стабилизирует распределение данных и ускоряет обучение, а дропаут предотвращает переобучение, добавляя случайность в процесс обучения. Их совместное использование позволяет создавать более устойчивые и эффективные модели."
      ],
      "metadata": {
        "id": "xFaUNeR5td0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем Batch Normalization и Dropout на Python с использованием библиотеки NumPy. Это поможет лучше понять, как эти методы работают на практике.\n",
        "\n",
        "Реализация Batch Normalization"
      ],
      "metadata": {
        "id": "tB6ZViQwuqjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BatchNormalization:\n",
        "    def __init__(self, epsilon=1e-5, momentum=0.9):\n",
        "        self.epsilon = epsilon  # Малая константа для численной стабильности\n",
        "        self.momentum = momentum  # Параметр для скользящего среднего\n",
        "        self.running_mean = None  # Скользящее среднее для среднего значения\n",
        "        self.running_var = None  # Скользящее среднее для дисперсии\n",
        "        self.gamma = None  # Параметр масштаба\n",
        "        self.beta = None  # Параметр сдвига\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            # Вычисляем среднее и дисперсию по мини-батчу\n",
        "            mean = np.mean(x, axis=0)\n",
        "            var = np.var(x, axis=0)\n",
        "\n",
        "            # Обновляем скользящее среднее\n",
        "            if self.running_mean is None:\n",
        "                self.running_mean = mean\n",
        "                self.running_var = var\n",
        "            else:\n",
        "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "\n",
        "            # Нормализация\n",
        "            x_normalized = (x - mean) / np.sqrt(var + self.epsilon)\n",
        "\n",
        "            # Инициализация gamma и beta, если они не были инициализированы\n",
        "            if self.gamma is None:\n",
        "                self.gamma = np.ones(x.shape[1])\n",
        "                self.beta = np.zeros(x.shape[1])\n",
        "\n",
        "            # Масштабирование и сдвиг\n",
        "            out = self.gamma * x_normalized + self.beta\n",
        "        else:\n",
        "            # На этапе тестирования используем скользящее среднее\n",
        "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "            out = self.gamma * x_normalized + self.beta\n",
        "\n",
        "        return out\n",
        "\n",
        "# Пример использования\n",
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float32)\n",
        "bn = BatchNormalization()\n",
        "print(\"Batch Normalization (Training):\")\n",
        "print(bn.forward(x, training=True))\n",
        "print(\"Batch Normalization (Testing):\")\n",
        "print(bn.forward(x, training=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7iIIy_Aul16",
        "outputId": "ef08d536-a1a8-4931-a673-f24793ef34d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Normalization (Training):\n",
            "[[-1.34163547 -1.34163547]\n",
            " [-0.44721183 -0.44721183]\n",
            " [ 0.44721183  0.44721183]\n",
            " [ 1.34163547  1.34163547]]\n",
            "Batch Normalization (Testing):\n",
            "[[-1.34163547 -1.34163547]\n",
            " [-0.44721183 -0.44721183]\n",
            " [ 0.44721183  0.44721183]\n",
            " [ 1.34163547  1.34163547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация Dropout"
      ],
      "metadata": {
        "id": "F-OlQafkus_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p  # Вероятность того, что нейрон останется активным\n",
        "        self.mask = None  # Маска для отключения нейронов\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            # Генерируем маску с вероятностью p\n",
        "            self.mask = (np.random.rand(*x.shape) < self.p) / self.p\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            # На этапе тестирования все нейроны активны\n",
        "            return x\n",
        "\n",
        "# Пример использования\n",
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float32)\n",
        "dropout = Dropout(p=0.5)\n",
        "print(\"Dropout (Training):\")\n",
        "print(dropout.forward(x, training=True))\n",
        "print(\"Dropout (Testing):\")\n",
        "print(dropout.forward(x, training=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8vQY-enuvhm",
        "outputId": "75797b8d-175f-4076-db73-e37293ad2aa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout (Training):\n",
            "[[2. 0.]\n",
            " [0. 0.]\n",
            " [6. 0.]\n",
            " [0. 0.]]\n",
            "Dropout (Testing):\n",
            "[[1. 2.]\n",
            " [2. 3.]\n",
            " [3. 4.]\n",
            " [4. 5.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Совместное использование Batch Normalization и Dropout\n",
        "\n",
        "Теперь объединим оба метода в одной модели:"
      ],
      "metadata": {
        "id": "l8YJFCw5uxUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.bn = BatchNormalization()\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        # Применяем Batch Normalization\n",
        "        x = self.bn.forward(x, training=training)\n",
        "        # Применяем Dropout\n",
        "        x = self.dropout.forward(x, training=training)\n",
        "        return x\n",
        "\n",
        "# Пример использования\n",
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float32)\n",
        "model = NeuralNetwork()\n",
        "print(\"Neural Network (Training):\")\n",
        "print(model.forward(x, training=True))\n",
        "print(\"Neural Network (Testing):\")\n",
        "print(model.forward(x, training=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ymzs-7lu4rX",
        "outputId": "f991cbfb-e911-4417-c522-6a4e906e60af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network (Training):\n",
            "[[-0.         -0.        ]\n",
            " [-0.89442366 -0.89442366]\n",
            " [ 0.          0.89442366]\n",
            " [ 2.68327093  0.        ]]\n",
            "Neural Network (Testing):\n",
            "[[-1.34163547 -1.34163547]\n",
            " [-0.44721183 -0.44721183]\n",
            " [ 0.44721183  0.44721183]\n",
            " [ 1.34163547  1.34163547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Инициализация весов\n",
        "\n",
        "**Важное замечание:**  \n",
        "Необходимо доработать задачу, связанную с использованием функции активации ReLU. В данном случае рассматривается симметричная функция активации. Для инициализации весов предлагается использовать равномерное распределение:  \n",
        "$$w_i \\sim U\\left[-\\frac{1}{\\sqrt{n_{in}}}, \\frac{1}{\\sqrt{n_{in}}}\\right],$$  \n",
        "где $n_{in}$ — количество входных нейронов слоя.\n",
        "\n",
        "\n",
        "\n",
        "### а) Покажите, что это будет приводить к затуханию дисперсии при переходе от одного слоя к другому.\n",
        "\n",
        "**Решение:**  \n",
        "Найдём дисперсию веса $w_i$. Для равномерного распределения $U[a, b]$ дисперсия вычисляется по формуле:  \n",
        "$$\\text{Var}(w_i) = \\frac{(b - a)^2}{12}.$$  \n",
        "Подставляя параметры распределения $a = -\\frac{1}{\\sqrt{n_{in}}}$ и $b = \\frac{1}{\\sqrt{n_{in}}}$, получаем:  \n",
        "$$\\text{Var}(w_i) = \\frac{\\left(\\frac{1}{\\sqrt{n_{in}}} - \\left(-\\frac{1}{\\sqrt{n_{in}}}\\right)\\right)^2}{12} = \\frac{\\left(\\frac{2}{\\sqrt{n_{in}}}\\right)^2}{12} = \\frac{4}{12 \\cdot n_{in}} = \\frac{1}{3 \\cdot n_{in}}.$$\n",
        "\n",
        "Рассмотрим линейный слой с симметричной функцией активации. Предполагается, что веса инициализируются независимо друг от друга и от входных данных, а входные данные также независимы. Тогда дисперсия выхода слоя $h_i$ вычисляется следующим образом:  \n",
        "$$\\text{Var}(h_i) = \\text{Var}\\left(\\sum_{i=1}^{n_{in}} w_i x_i\\right) = \\sum_{i=1}^{n_{in}} \\text{Var}(w_i \\cdot x_i).$$  \n",
        "Используя формулу для дисперсии произведения независимых случайных величин, получаем:  \n",
        "$$\\text{Var}(w_i \\cdot x_i) = \\mathbb{E}^2(x_i) \\cdot \\text{Var}(w_i) + \\text{Var}(x_i) \\cdot \\mathbb{E}^2(w_i) + \\text{Var}(x_i) \\cdot \\text{Var}(w_i).$$  \n",
        "Поскольку функция активации симметрична, математическое ожидание входных данных равно нулю: $\\mathbb{E}(x_i) = 0$. Также предполагается, что веса инициализируются с нулевым средним: $\\mathbb{E}(w_i) = 0$. Тогда формула упрощается:  \n",
        "$$\\text{Var}(w_i \\cdot x_i) = \\text{Var}(x_i) \\cdot \\text{Var}(w_i).$$  \n",
        "Следовательно, дисперсия выхода слоя равна:  \n",
        "$$\\text{Var}(h_i) = \\sum_{i=1}^{n_{in}} \\text{Var}(x_i) \\cdot \\text{Var}(w_i) = n_{in} \\cdot \\text{Var}(x_i) \\cdot \\text{Var}(w_i).$$  \n",
        "Подставляя значение $\\text{Var}(w_i) = \\frac{1}{3 \\cdot n_{in}}$, получаем:  \n",
        "$$\\text{Var}(h_i) = n_{in} \\cdot \\text{Var}(x_i) \\cdot \\frac{1}{3 \\cdot n_{in}} = \\frac{1}{3} \\cdot \\text{Var}(x_i).$$  \n",
        "Таким образом, дисперсия уменьшается в три раза при переходе от выхода предыдущего слоя к выходу нового слоя.\n",
        "\n",
        "\n",
        "\n",
        "### б) Какими нужно взять параметры равномерного распределения, чтобы дисперсия не затухала?\n",
        "\n",
        "**Решение:**  \n",
        "Для того чтобы дисперсия не затухала, необходимо, чтобы $\\text{Var}(h_i) = \\text{Var}(x_i)$. Из предыдущего пункта следует, что для этого требуется:  \n",
        "$$n_{in} \\cdot \\text{Var}(x_i) \\cdot \\text{Var}(w_i) = \\text{Var}(x_i).$$  \n",
        "Отсюда получаем условие на дисперсию весов:  \n",
        "$$\\text{Var}(w_i) = \\frac{1}{n_{in}}.$$  \n",
        "Для равномерного распределения $U[a, b]$ дисперсия равна $\\frac{(b - a)^2}{12}$. Подставляя $\\text{Var}(w_i) = \\frac{1}{n_{in}}$, получаем:  \n",
        "$$\\frac{(b - a)^2}{12} = \\frac{1}{n_{in}}.$$  \n",
        "Если выбрать $a = -\\sqrt{\\frac{3}{n_{in}}}$ и $b = \\sqrt{\\frac{3}{n_{in}}}$, то:  \n",
        "$$\\frac{\\left(\\sqrt{\\frac{3}{n_{in}}} - \\left(-\\sqrt{\\frac{3}{n_{in}}}\\right)\\right)^2}{12} = \\frac{\\left(2 \\sqrt{\\frac{3}{n_{in}}}\\right)^2}{12} = \\frac{12}{12 \\cdot n_{in}} = \\frac{1}{n_{in}}.$$  \n",
        "Таким образом, подходящее равномерное распределение:  \n",
        "$$w_i \\sim U\\left[-\\sqrt{\\frac{3}{n_{in}}}, \\sqrt{\\frac{3}{n_{in}}}\\right].$$\n",
        "\n",
        "\n",
        "\n",
        "### в) Инициализация весов из нормального распределения\n",
        "\n",
        "**Решение:**  \n",
        "Для нормального распределения $N(\\mu, \\sigma^2)$ дисперсия равна $\\sigma^2$. Чтобы дисперсия не затухала, необходимо, чтобы:  \n",
        "$$\\text{Var}(w_i) = \\frac{1}{n_{in}}.$$  \n",
        "Следовательно, параметры нормального распределения должны быть:  \n",
        "$$w_i \\sim N\\left(0, \\frac{1}{n_{in}}\\right).$$\n",
        "\n",
        "\n",
        "\n",
        "### г) Компромисс между прямым и обратным распространением ошибки\n",
        "\n",
        "**Решение:**  \n",
        "При прямом распространении ошибки на вход нейрона поступает $n_{in}$ слагаемых, а при обратном распространении — $n_{out}$ градиентов. Количество весов между слоями может значительно варьироваться, что приводит к невозможности одновременно поддерживать неизменными дисперсии при прямом и обратном распространении.  \n",
        "\n",
        "Для компромисса предлагается инициализировать веса из распределения с дисперсией, учитывающей как количество входных, так и выходных нейронов:  \n",
        "$$\\text{Var}(w_i) = \\frac{2}{n_{in} + n_{out}}.$$  \n",
        "Такая инициализация называется **инициализацией Ксавие (Xavier)** или **инициализацией Глорота (Glorot)**. Она позволяет балансировать дисперсию при прямом и обратном распространении ошибки.  \n",
        "\n",
        "Для нормального распределения:  \n",
        "$$w_i \\sim N\\left(0, \\frac{2}{n_{in} + n_{out}}\\right).$$  \n",
        "Для равномерного распределения:  \n",
        "$$w_i \\sim U\\left[-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right].$$  \n",
        "\n",
        "\n",
        "\n",
        "[1] Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics* (pp. 249–256)."
      ],
      "metadata": {
        "id": "E6DGztlbu5dJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация инициализации весов на Python с использованием библиотеки `numpy` может быть выполнена следующим образом. Мы рассмотрим инициализацию весов для равномерного и нормального распределений, а также инициализацию Ксавие (Xavier/Glorot).\n",
        "\n",
        "\n",
        "### 1. Инициализация весов из равномерного распределения\n",
        "\n",
        "Для равномерного распределения $U[-a, a]$, где $a = \\frac{1}{\\sqrt{n_{in}}}$:\n",
        "\n"
      ],
      "metadata": {
        "id": "7RNdy8t9wosK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def uniform_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация весов из равномерного распределения.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    a = 1 / np.sqrt(n_in)  # Параметр для равномерного распределения\n",
        "    weights = np.random.uniform(-a, a, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "MEUjKqQfwcef"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Инициализация весов из нормального распределения\n",
        "\n",
        "Для нормального распределения $N(0, \\frac{1}{n_{in}})$:\n"
      ],
      "metadata": {
        "id": "56OqT2qewsbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация весов из нормального распределения.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    std_dev = 1 / np.sqrt(n_in)  # Стандартное отклонение\n",
        "    weights = np.random.normal(0, std_dev, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "PPIMd_x0wwdz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Инициализация Ксавие (Xavier/Glorot)\n",
        "\n",
        "Для инициализации Ксавие используется компромисс между количеством входных и выходных нейронов. Дисперсия весов выбирается как $\\frac{2}{n_{in} + n_{out}}$.\n",
        "\n",
        "#### а) Для нормального распределения:\n"
      ],
      "metadata": {
        "id": "Q08HCkZ3wzgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_normal_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация Ксавие (Xavier) с нормальным распределением.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    std_dev = np.sqrt(2 / (n_in + n_out))  # Стандартное отклонение\n",
        "    weights = np.random.normal(0, std_dev, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "kvkoCSmGw3K0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### б) Для равномерного распределения:\n"
      ],
      "metadata": {
        "id": "GdGt6XsOw5cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_uniform_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация Ксавие (Xavier) с равномерным распределением.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    limit = np.sqrt(6 / (n_in + n_out))  # Границы распределения\n",
        "    weights = np.random.uniform(-limit, limit, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "CPIZttmCw8as"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Пример использования\n"
      ],
      "metadata": {
        "id": "q_x3Bks7w_2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры слоя\n",
        "n_in = 100  # Количество входных нейронов\n",
        "n_out = 50  # Количество выходных нейронов\n",
        "\n",
        "# Инициализация весов\n",
        "weights_uniform = uniform_initialization(n_in, n_out)\n",
        "weights_normal = normal_initialization(n_in, n_out)\n",
        "weights_xavier_normal = xavier_normal_initialization(n_in, n_out)\n",
        "weights_xavier_uniform = xavier_uniform_initialization(n_in, n_out)\n",
        "\n",
        "# Проверка формы матрицы весов\n",
        "print(\"Uniform Initialization Shape:\", weights_uniform.shape)\n",
        "print(\"Normal Initialization Shape:\", weights_normal.shape)\n",
        "print(\"Xavier Normal Initialization Shape:\", weights_xavier_normal.shape)\n",
        "print(\"Xavier Uniform Initialization Shape:\", weights_xavier_uniform.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1VsTFHHxELx",
        "outputId": "1235a093-67f7-4a08-b456-585330e4d1f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform Initialization Shape: (100, 50)\n",
            "Normal Initialization Shape: (100, 50)\n",
            "Xavier Normal Initialization Shape: (100, 50)\n",
            "Xavier Uniform Initialization Shape: (100, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Проверка дисперсии\n",
        "\n",
        "Для проверки дисперсии весов можно использовать следующий код:\n"
      ],
      "metadata": {
        "id": "WnuL_gIExGFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_variance(weights):\n",
        "    \"\"\"\n",
        "    Проверка дисперсии весов.\n",
        "\n",
        "    Параметры:\n",
        "    weights (np.ndarray): Матрица весов.\n",
        "\n",
        "    Возвращает:\n",
        "    variance (float): Дисперсия весов.\n",
        "    \"\"\"\n",
        "    return np.var(weights)\n",
        "\n",
        "# Проверка дисперсии для каждого метода инициализации\n",
        "print(\"Uniform Initialization Variance:\", check_variance(weights_uniform))\n",
        "print(\"Normal Initialization Variance:\", check_variance(weights_normal))\n",
        "print(\"Xavier Normal Initialization Variance:\", check_variance(weights_xavier_normal))\n",
        "print(\"Xavier Uniform Initialization Variance:\", check_variance(weights_xavier_uniform))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zzwMBAexLMm",
        "outputId": "50af2ea5-5d0b-4d99-a246-e385b3210777"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform Initialization Variance: 0.003266599757793463\n",
            "Normal Initialization Variance: 0.010020046718884046\n",
            "Xavier Normal Initialization Variance: 0.012923777250064506\n",
            "Xavier Uniform Initialization Variance: 0.013435334909320308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Результаты\n",
        "\n",
        "- Для равномерной инициализации дисперсия будет близка к $\\frac{1}{3 \\cdot n_{in}}$.\n",
        "- Для нормальной инициализации дисперсия будет близка к $\\frac{1}{n_{in}}$.\n",
        "- Для инициализации Ксавие дисперсия будет близка к $\\frac{2}{n_{in} + n_{out}}$.\n",
        "\n",
        "Эти методы инициализации помогают избежать проблем с затуханием или взрывом градиентов при обучении нейронных сетей."
      ],
      "metadata": {
        "id": "47t5uoRjxNtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Жесткая и бесполезная задача про инициализацию  \n",
        "Рассмотрим линейный слой без функции активации $H = XW + b$, где $X$ — матрица входных данных, $W$ — матрица весов, $b$ — вектор смещений. Предположим, что требуется, чтобы выходы этого слоя имели нормальное распределение, то есть $h_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. При этом предполагается, что наблюдения и признаки независимы друг от друга.\n",
        "\n",
        "### а) Пусть $x_{ij} \\sim \\mathcal{N}(0, 1)$. Какое распределение можно использовать для инициализации $W$ и $b$?  \n",
        "**Решение**  \n",
        "Рассмотрим один из выходов линейного слоя:  \n",
        "$$\n",
        "h_{11} = x_{11}w_{11} + x_{12}w_{21} + \\dots + x_{1k}w_{k1} + b_1.\n",
        "$$  \n",
        "Предположим, что веса инициализируются независимо от данных, то есть случайные величины $x_{ij}$ и $w_{jk}$ независимы. Тогда математическое ожидание $h_{11}$ можно выразить следующим образом:  \n",
        "$$\n",
        "\\mathbb{E}(h_{11}) = \\mathbb{E}(x_{11}w_{11} + x_{12}w_{21} + \\dots + x_{1k}w_{k1} + b_1) = \\mathbb{E}(x_{11}w_{11}) + \\mathbb{E}(x_{12}w_{21}) + \\dots + \\mathbb{E}(x_{1k}w_{k1}) + \\mathbb{E}(b_1).\n",
        "$$  \n",
        "Поскольку $\\mathbb{E}(x_{ij}) = 0$, то:  \n",
        "$$\n",
        "\\mathbb{E}(h_{11}) = \\mathbb{E}(x_{11}) \\cdot \\mathbb{E}(w_{11}) + \\mathbb{E}(x_{12}) \\cdot \\mathbb{E}(w_{21}) + \\dots + \\mathbb{E}(x_{1k}) \\cdot \\mathbb{E}(w_{k1}) + \\mathbb{E}(b_1) = 0.\n",
        "$$  \n",
        "Таким образом, математическое ожидание весов может быть любым, но $\\mathbb{E}(b_1)$ должно быть нулевым.  \n",
        "\n",
        "Теперь рассмотрим дисперсию:  \n",
        "$$\n",
        "\\text{Var}(h_{11}) = \\text{Var}(x_{11}w_{11} + x_{12}w_{21} + \\dots + x_{1k}w_{k1} + b_1) = \\text{Var}(x_{11}w_{11}) + \\text{Var}(x_{12}w_{21}) + \\dots + \\text{Var}(x_{1k}w_{k1}) + \\text{Var}(b_1).\n",
        "$$  \n",
        "Поскольку $\\text{Var}(x_{ij}) = 1$, то:  \n",
        "$$\n",
        "\\text{Var}(h_{11}) = \\text{Var}(x_{11}) \\cdot \\text{Var}(w_{11}) + \\text{Var}(x_{12}) \\cdot \\text{Var}(w_{21}) + \\dots + \\text{Var}(x_{1k}) \\cdot \\text{Var}(w_{k1}) + \\text{Var}(b_1) = s \\cdot k + s_b = \\sigma^2,\n",
        "$$  \n",
        "где $s = \\text{Var}(w_{ij})$, $s_b = \\text{Var}(b_1)$.  \n",
        "\n",
        "Для того чтобы итоговое распределение $h_{11}$ было нормальным, необходимо, чтобы каждое слагаемое $x_{ij}w_{jk}$ также было нормально распределено. Это можно достичь, если инициализировать веса $w_{jk}$ из распределения Бернулли, $w_{jk} \\sim \\text{Bern}(p)$. В этом случае:  \n",
        "$$\n",
        "\\text{Var}(w_{jk}) = p(1 - p) = \\frac{\\sigma^2}{k + 1}.\n",
        "$$  \n",
        "Отсюда получаем квадратное уравнение для $p$:  \n",
        "$$\n",
        "p^2 - p + \\frac{\\sigma^2}{k + 1} = 0.\n",
        "$$  \n",
        "Решения этого уравнения:  \n",
        "$$\n",
        "p_{1,2} = 0.5 \\cdot \\left(1 \\pm \\sqrt{1 - \\frac{4\\sigma^2}{k + 1}}\\right).\n",
        "$$  \n",
        "Однако нет гарантии, что $p$ будет находиться в интервале \\([0, 1]\\).  \n",
        "\n",
        "Альтернативный подход заключается в том, чтобы инициализировать веса $w_{jk}$ так, чтобы они принимали значения $1$ и $-1$ с равными вероятностями $0.5$. В этом случае:  \n",
        "$$\n",
        "\\mathbb{E}(w_{jk}) = 0, \\quad \\text{Var}(w_{jk}) = 1.\n",
        "$$  \n",
        "Тогда дисперсия $h_{11}$ будет:  \n",
        "$$\n",
        "\\text{Var}(h_{11}) = \\frac{k}{4} + s_b = \\sigma^2.\n",
        "$$  \n",
        "Следовательно, параметр $b$ следует инициализировать как:  \n",
        "$$\n",
        "b \\sim \\mathcal{N}\\left(0, \\sigma^2 - \\frac{k}{4}\\right).\n",
        "$$  \n",
        "\n",
        "### б) Пусть $x_{ij} \\sim \\mathcal{U}[0; a]$. Какое распределение можно использовать для инициализации $W$ и $b$?  \n",
        "**Решение**  \n",
        "Для равномерного распределения $x_{ij} \\sim \\mathcal{U}[0; a]$ математическое ожидание и дисперсия равны:  \n",
        "$$\n",
        "\\mathbb{E}(x_{ij}) = \\frac{a}{2}, \\quad \\text{Var}(x_{ij}) = \\frac{a^2}{12}.\n",
        "$$  \n",
        "Рассмотрим математическое ожидание и дисперсию выхода $h_i$:  \n",
        "$$\n",
        "\\mathbb{E}(h_i) = \\frac{a}{2} \\cdot \\mathbb{E}(w_{11}) + \\frac{a}{2} \\cdot \\mathbb{E}(w_{21}) + \\dots + \\frac{a}{2} \\cdot \\mathbb{E}(w_{k1}) + \\mathbb{E}(b_1) = 0.\n",
        "$$  \n",
        "Отсюда следует, что $\\mathbb{E}(w_{jk}) = 0$ и $\\mathbb{E}(b_k) = 0$.  \n",
        "\n",
        "Дисперсия $h_i$ выражается как:  \n",
        "$$\n",
        "\\text{Var}(h_i) = \\frac{a^2}{12} \\cdot \\text{Var}(w_{11}) + \\frac{a^2}{12} \\cdot \\text{Var}(w_{21}) + \\dots + \\frac{a^2}{12} \\cdot \\text{Var}(w_{k1}) + \\text{Var}(b_1) = \\sigma^2.\n",
        "$$  \n",
        "Таким образом:  \n",
        "$$\n",
        "\\frac{a^2}{12} \\cdot k \\cdot s_w + s_b = \\sigma^2,\n",
        "$$  \n",
        "где $s_w = \\text{Var}(w_{jk})$, $s_b = \\text{Var}(b_1)$.  \n",
        "\n",
        "Для того чтобы произведение $x_{ij} \\cdot w_{jk}$ было нормально распределено, необходимо выбрать распределение весов $w_{jk}$ таким образом, чтобы плотность распределения произведения соответствовала нормальному распределению. Плотность распределения произведения двух независимых случайных величин $Z$ и $Y$ может быть найдена по формуле:  \n",
        "$$\n",
        "f_X(x) = \\int_0^{+\\infty} z \\cdot f_Z(zx) \\cdot f_Y(z) \\, dz - \\int_{-\\infty}^0 z \\cdot f_Z(zx) \\cdot f_Y(z) \\, dz.\n",
        "$$  \n",
        "Для равномерного распределения $f_Y(y) = \\frac{1}{a}$ при $y \\in [0; a]$, иначе $f_Y(y) = 0$. Тогда:  \n",
        "$$\n",
        "f_X(x) = \\frac{1}{a \\sqrt{2\\pi}} \\int_0^a z \\cdot e^{-\\frac{z^2 x^2}{2}} \\, dz = \\frac{1}{a \\sqrt{2\\pi}} \\cdot \\frac{1}{x^2} \\left(1 - e^{-\\frac{(a x)^2}{2}}\\right).\n",
        "$$  \n",
        "Таким образом, плотность распределения весов $w_{jk}$ должна иметь вид:  \n",
        "$$\n",
        "f_w(x) = \\frac{1}{a \\sqrt{2\\pi}} \\cdot \\frac{1}{x^2} \\left(1 - e^{-\\frac{(a x)^2}{2}}\\right), \\quad x \\in (-\\infty, +\\infty).\n",
        "$$  \n",
        "В этом случае $x_{ij} \\cdot w_{jk} \\sim \\mathcal{N}(0, 1)$, а вектор смещений $b$ можно инициализировать как:  \n",
        "$$\n",
        "b \\sim \\mathcal{N}(0, \\sigma^2 - k).\n",
        "$$  \n",
        "\n",
        "---  \n",
        "[1] Вывод этой формулы можно найти в учебнике по теории вероятностей Гнеденко на странице 143."
      ],
      "metadata": {
        "id": "bpgWhiKYxS_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем на Python решение задачи инициализации весов и смещений для линейного слоя, чтобы выходы имели нормальное распределение. Рассмотрим оба случая: когда входные данные $x_{ij}$ распределены нормально и когда они распределены равномерно.\n",
        "\n",
        "### Реализация на Python\n"
      ],
      "metadata": {
        "id": "MXTAh8rfx6kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_weights_normal_input(k, sigma):\n",
        "    \"\"\"\n",
        "    Инициализация весов и смещений для случая, когда x_{ij} ~ N(0, 1).\n",
        "\n",
        "    Параметры:\n",
        "    k (int): Количество входных признаков.\n",
        "    sigma (float): Желаемое стандартное отклонение выходов.\n",
        "\n",
        "    Возвращает:\n",
        "    W (np.array): Матрица весов формы (k, 1).\n",
        "    b (np.array): Вектор смещений формы (1,).\n",
        "    \"\"\"\n",
        "    # Инициализация весов из распределения Бернулли\n",
        "    p = 0.5  # Вероятность для распределения Бернулли\n",
        "    W = np.random.choice([-1, 1], size=(k, 1), p=[p, p])\n",
        "\n",
        "    # Инициализация смещений\n",
        "    s_b = sigma**2 - k / 4  # Дисперсия для смещений\n",
        "    b = np.random.normal(0, np.sqrt(s_b), size=(1,))\n",
        "\n",
        "    return W, b\n",
        "\n",
        "def initialize_weights_uniform_input(k, sigma, a):\n",
        "    \"\"\"\n",
        "    Инициализация весов и смещений для случая, когда x_{ij} ~ U[0, a].\n",
        "\n",
        "    Параметры:\n",
        "    k (int): Количество входных признаков.\n",
        "    sigma (float): Желаемое стандартное отклонение выходов.\n",
        "    a (float): Верхняя граница равномерного распределения.\n",
        "\n",
        "    Возвращает:\n",
        "    W (np.array): Матрица весов формы (k, 1).\n",
        "    b (np.array): Вектор смещений формы (1,).\n",
        "    \"\"\"\n",
        "    # Инициализация весов с использованием специального распределения\n",
        "    # Для простоты используем нормальное распределение с подходящей дисперсией\n",
        "    s_w = (12 * sigma**2) / (a**2 * k)  # Дисперсия для весов\n",
        "    W = np.random.normal(0, np.sqrt(s_w), size=(k, 1))\n",
        "\n",
        "    # Инициализация смещений\n",
        "    s_b = sigma**2 - (a**2 * k * s_w) / 12  # Дисперсия для смещений\n",
        "    b = np.random.normal(0, np.sqrt(s_b), size=(1,))\n",
        "\n",
        "    return W, b\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Параметры\n",
        "    k = 10  # Количество входных признаков\n",
        "    sigma = 1.0  # Желаемое стандартное отклонение выходов\n",
        "    a = 2.0  # Верхняя граница равномерного распределения\n",
        "\n",
        "    # Случай 1: x_{ij} ~ N(0, 1)\n",
        "    W_normal, b_normal = initialize_weights_normal_input(k, sigma)\n",
        "    print(\"Случай 1: x_{ij} ~ N(0, 1)\")\n",
        "    print(\"Веса W:\\n\", W_normal)\n",
        "    print(\"Смещения b:\\n\", b_normal)\n",
        "\n",
        "    # Случай 2: x_{ij} ~ U[0, a]\n",
        "    W_uniform, b_uniform = initialize_weights_uniform_input(k, sigma, a)\n",
        "    print(\"\\nСлучай 2: x_{ij} ~ U[0, a]\")\n",
        "    print(\"Веса W:\\n\", W_uniform)\n",
        "    print(\"Смещения b:\\n\", b_uniform)"
      ],
      "metadata": {
        "id": "PAC9L1ctyOrV",
        "outputId": "06b08478-925a-4ba0-c8fe-5fcc068f146e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Случай 1: x_{ij} ~ N(0, 1)\n",
            "Веса W:\n",
            " [[ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]]\n",
            "Смещения b:\n",
            " [nan]\n",
            "\n",
            "Случай 2: x_{ij} ~ U[0, a]\n",
            "Веса W:\n",
            " [[ 0.749215  ]\n",
            " [ 0.69730708]\n",
            " [-0.35155464]\n",
            " [-0.71352368]\n",
            " [ 0.70246156]\n",
            " [ 0.00315189]\n",
            " [-0.26502586]\n",
            " [ 0.15085651]\n",
            " [ 0.81139341]\n",
            " [ 0.47928628]]\n",
            "Смещения b:\n",
            " [0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-4cc7da1b8bdc>:21: RuntimeWarning: invalid value encountered in sqrt\n",
            "  b = np.random.normal(0, np.sqrt(s_b), size=(1,))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Пояснение к коду:\n",
        "1. **Случай 1: $x_{ij} \\sim \\mathcal{N}(0, 1)$**:\n",
        "   - Веса $W$ инициализируются из распределения Бернулли, принимая значения $-1$ и $1$ с вероятностью $0.5$.\n",
        "   - Смещения $b$ инициализируются из нормального распределения с дисперсией $\\sigma^2 - \\frac{k}{4}$.\n",
        "\n",
        "2. **Случай 2: $x_{ij} \\sim \\mathcal{U}[0, a]$**:\n",
        "   - Веса $W$ инициализируются из нормального распределения с дисперсией $s_w = \\frac{12 \\sigma^2}{a^2 k}$.\n",
        "   - Смещения $b$ инициализируются из нормального распределения с дисперсией $s_b = \\sigma^2 - \\frac{a^2 k s_w}{12}$.\n"
      ],
      "metadata": {
        "id": "-VWi7hC9yWha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Анализ архитектур нейронных сетей для задач регрессии и классификации изображений\n",
        "\n",
        "Рассмотрим несколько примеров архитектур нейронных сетей, используемых для решения задач машинного обучения, и проанализируем допущенные ошибки.\n",
        "\n",
        "а) Задача регрессии: прогнозирование цен на недвижимость\n",
        "\n",
        "Архитектура сети:\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(InputLayer([39]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, kernel_initializer=keras.initializers.zeros()))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "```\n",
        "\n",
        "Решение:\n",
        "1. Инициализация весов нулями (kernel_initializer=keras.initializers.zeros()) является некорректной, так как приводит к симметричности градиентов и невозможности обучения сети. Рекомендуется использовать инициализацию Хе (He initialization) или Ксавье (Xavier initialization):\n",
        "$$\n",
        "   W_{ij} \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)\n",
        "$$\n",
        "   где $n_{in}$ и $n_{out}$ - количество входных и выходных нейронов соответственно.\n",
        "\n",
        "2. Отсутствие функции активации между слоями делает сеть эквивалентной однослойному перцептрону. Рекомендуется добавить нелинейность, например, ReLU:\n",
        "$$\n",
        "   \\text{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "3. Использование BatchNormalization в качестве первого слоя не имеет смысла, так как входные данные уже должны быть нормализованы.\n",
        "\n",
        "4. Оптимизатор SGD без настройки скорости обучения может привести к медленной сходимости. Рекомендуется использовать Adam с параметрами по умолчанию.\n",
        "\n",
        "5. Отсутствие метрик для оценки качества модели. Рекомендуется добавить метрику Mean Absolute Error (MAE):\n",
        "$$\n",
        "   \\text{MAE} = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "б) Задача классификации изображений 28×28 на 10 классов\n",
        "\n",
        "Архитектура сети:\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(InputLayer([28, 28, 1]))\n",
        "model.add(Conv2D(filters=512, kernel_size=(10, 10)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('softmax'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.add(Dropout(0.1))\n",
        "model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "```\n",
        "\n",
        "Решение:\n",
        "1. Количество фильтров (512) в первом сверточном слое избыточно. Рекомендуется начинать с меньшего числа фильтров (32-64) и увеличивать их количество в глубину сети:\n",
        "$$\n",
        "   \\text{filters} = [32, 64, 128, 256]\n",
        "$$\n",
        "\n",
        "2. Размер ядра свертки (10×10) слишком велик. Оптимальный размер для начальных слоев - 3×3 или 5×5:\n",
        "$$\n",
        "   \\text{kernel\\_size} = (3, 3)\n",
        "$$\n",
        "\n",
        "3. Использование softmax в промежуточных слоях некорректно. Функция softmax должна применяться только на выходном слое:\n",
        "$$\n",
        "   \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
        "$$\n",
        "\n",
        "4. Применение Dropout после softmax разрушает вероятностные свойства выходов. Dropout следует применять только к скрытым слоям:\n",
        "$$\n",
        "   \\text{Dropout}(x) = \\begin{cases}\n",
        "   \\frac{x}{1-p} & \\text{с вероятностью } 1-p \\\\\n",
        "   0 & \\text{с вероятностью } p\n",
        "   \\end{cases}\n",
        "$$\n",
        "\n",
        "5. Функция потерь mean_squared_error не подходит для задач классификации. Рекомендуется использовать categorical_crossentropy:\n",
        "$$\n",
        "   \\text{CCE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "в) Задача классификации изображений 100×100 на 10 классов\n",
        "\n",
        "Архитектура сети:\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(InputLayer([100, 100, 3]))\n",
        "for filters in [32, 64, 128, 256]:\n",
        "    model.add(Conv2D(filters, kernel_size=(5, 5)))\n",
        "    model.add(Conv2D(filters, kernel_size=(1, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_accuracy')\n",
        "```\n",
        "\n",
        "Решение:\n",
        "1. Отсутствие padding='same' в сверточных слоях приводит к уменьшению размерности изображения. Рекомендуется добавить padding:\n",
        "$$\n",
        "   \\text{output\\_size} = \\left\\lfloor\\frac{\\text{input\\_size} + 2p - k}{s}\\right\\rfloor + 1\n",
        "$$\n",
        "   где $p$ - padding, $k$ - kernel size, $s$ - stride.\n",
        "\n",
        "2. Размер пулинга (3×3) слишком велик. Рекомендуется использовать (2×2):\n",
        "$$\n",
        "   \\text{pool\\_size} = (2, 2)\n",
        "$$\n",
        "\n",
        "3. Функция потерь sparse_categorical_accuracy не является функцией потерь. Рекомендуется использовать sparse_categorical_crossentropy:\n",
        "$$\n",
        "   \\text{SCCE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "г) Задача классификации RGB-изображений 100×100 на 10 классов\n",
        "\n",
        "Архитектура сети:\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(InputLayer([100, 100, 3]))\n",
        "    \n",
        "model.add(Conv2D(filters=512, kernel_size=(3, 3), kernel_initializer=\"glorot_uniform\"))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    \n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), kernel_initializer=\"glorot_uniform\"))\n",
        "model.add(Activation('relu'))\n",
        "    \n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), kernel_initializer=\"glorot_uniform\"))\n",
        "model.add(Conv2D(filters=32, kernel_size=(1, 1), kernel_initializer=\"glorot_uniform\"))\n",
        "model.add(MaxPool2D(pool_size=(10, 10)))\n",
        "    \n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Dropout(rate=1))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dropout(rate=0.5))\n",
        "```\n",
        "\n",
        "Решение:\n",
        "1. Количество фильтров в первом слое (512) избыточно. Рекомендуется начать с 32-64 фильтров.\n",
        "\n",
        "2. Уменьшение количества фильтров в глубину сети снижает её ёмкость. Рекомендуется увеличивать количество фильтров:\n",
        "$$\n",
        "   \\text{filters} = [64, 128, 256, 512]\n",
        "$$\n",
        "\n",
        "3. Размер пулинга (10×10) слишком велик. Рекомендуется использовать (2×2).\n",
        "\n",
        "4. Инициализация Glorot Uniform подходит для tanh, но для ReLU рекомендуется использовать He initialization:\n",
        "$$\n",
        "   W_{ij} \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)\n",
        "$$\n",
        "\n",
        "5. Свёртка 1×1 без предшествующей нелинейности теряет смысл. Рекомендуется добавить ReLU перед ней.\n",
        "\n",
        "6. Dropout с rate=1 полностью отключает слой. Рекомендуется использовать значение 0.2-0.5.\n",
        "\n",
        "7. Sigmoid на выходном слое не подходит для многоклассовой классификации. Рекомендуется использовать softmax.\n",
        "\n",
        "8. Отсутствие активации между Dense слоями снижает выразительную способность сети. Рекомендуется добавить ReLU.\n",
        "\n",
        "9. Перестановка ReLU после MaxPooling может улучшить производительность без изменения функциональности.\n",
        "\n",
        "10. Для задач классификации с несколькими классами на выходном слое должен использоваться softmax:\n",
        "$$\n",
        "    \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
        "$$\n",
        "\n",
        "[1] Задача составлена по мотивам семинара из Школы анализа данных (ШАД)."
      ],
      "metadata": {
        "id": "yQu5wsjMyXGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Модные ячейки: ResNet, DenseNet и другие архитектуры\n",
        "\n",
        "В современных нейронных сетях используются различные архитектурные блоки (ячейки), которые позволяют улучшить качество модели, ускорить обучение и избежать проблем, таких как исчезающие градиенты или переобучение. В этой лекции мы рассмотрим несколько популярных архитектурных решений, таких как ResNet, DenseNet, и их математические основы.\n",
        "\n",
        "\n",
        "\n",
        "## 1. ResNet (Residual Networks)\n",
        "\n",
        "ResNet (остаточные сети) — одна из самых популярных архитектур, предложенная в 2015 году. Основная идея ResNet заключается в использовании **остаточных связей** (skip connections), которые позволяют градиентам проходить напрямую через сеть, избегая проблем исчезающих градиентов.\n",
        "\n",
        "### Математическая формулировка\n",
        "\n",
        "Пусть $H(x)$ — это целевое отображение, которое мы хотим аппроксимировать. В ResNet вместо прямого обучения $H(x)$, сеть обучается аппроксимировать **остаточную функцию** $F(x) = H(x) - x$. Таким образом, итоговое отображение выражается как:\n",
        "$$\n",
        "H(x) = F(x) + x,\n",
        "$$\n",
        "где $F(x)$ — это остаточная функция, а $x$ — входные данные, которые передаются через skip connection.\n",
        "\n",
        "#### Уравнение forward pass для ResNet:\n",
        "Для слоя $l$:\n",
        "$$\n",
        "x_{l+1} = F(x_l, W_l) + x_l,\n",
        "$$\n",
        "где:\n",
        "- $x_l$ — вход на слое $l$,\n",
        "- $W_l$ — веса слоя $l$,\n",
        "- $F(x_l, W_l)$ — остаточная функция, которая может состоять из нескольких слоёв (например, свёртки, BatchNorm, ReLU).\n",
        "\n",
        "#### Обратное распространение (backpropagation):\n",
        "Градиент потери $L$ по весам $W_l$ вычисляется как:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_l} = \\frac{\\partial L}{\\partial x_{l+1}} \\cdot \\frac{\\partial x_{l+1}}{\\partial W_l}.\n",
        "$$\n",
        "Благодаря skip connection, градиент распространяется как через остаточную функцию, так и напрямую:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_l} = \\frac{\\partial L}{\\partial x_{l+1}} \\cdot \\left( \\frac{\\partial F(x_l, W_l)}{\\partial x_l} + 1 \\right).\n",
        "$$\n",
        "Это позволяет градиентам \"протекать\" через сеть, даже если $\\frac{\\partial F(x_l, W_l)}{\\partial x_l}$ становится малой.\n",
        "\n",
        "\n",
        "## 2. DenseNet (Densely Connected Networks)\n",
        "\n",
        "DenseNet — это архитектура, в которой каждый слой соединён со всеми последующими слоями. Это позволяет улучшить поток градиентов и повторно использовать признаки на разных уровнях.\n",
        "\n",
        "### Математическая формулировка\n",
        "\n",
        "Пусть $x_l$ — это выход слоя $l$. В DenseNet каждый слой получает на вход конкатенацию всех предыдущих выходов:\n",
        "$$\n",
        "x_l = H_l([x_0, x_1, \\dots, x_{l-1}]),\n",
        "$$\n",
        "где $H_l$ — это функция, которая может включать свёртки, BatchNorm и активации, а $[x_0, x_1, \\dots, x_{l-1}]$ — конкатенация всех предыдущих выходов.\n",
        "\n",
        "#### Уравнение forward pass для DenseNet:\n",
        "Для слоя $l$:\n",
        "$$\n",
        "x_l = H_l([x_0, x_1, \\dots, x_{l-1}]),\n",
        "$$\n",
        "где $[x_0, x_1, \\dots, x_{l-1}]$ — это конкатенация всех предыдущих выходов.\n",
        "\n",
        "#### Обратное распространение (backpropagation):\n",
        "Градиент потери $L$ по весам $W_l$ вычисляется как:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_l} = \\sum_{k=l}^{L} \\frac{\\partial L}{\\partial x_k} \\cdot \\frac{\\partial x_k}{\\partial W_l},\n",
        "$$\n",
        "где $L$ — общее количество слоёв. Это связано с тем, что каждый слой влияет на все последующие слои.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Inception (GoogLeNet)\n",
        "\n",
        "Inception — это архитектура, которая использует параллельные свёртки с разными размерами ядер для извлечения признаков на разных масштабах. Основная идея заключается в том, чтобы комбинировать признаки, полученные с помощью разных фильтров.\n",
        "\n",
        "### Математическая формулировка\n",
        "\n",
        "Пусть $x$ — входной тензор. В Inception применяются несколько параллельных операций свёртки:\n",
        "$$\n",
        "y = [Conv_{1x1}(x), Conv_{3x3}(x), Conv_{5x5}(x), MaxPool(x)],\n",
        "$$\n",
        "где:\n",
        "- $Conv_{1x1}(x)$ — свёртка с ядром 1x1,\n",
        "- $Conv_{3x3}(x)$ — свёртка с ядром 3x3,\n",
        "- $Conv_{5x5}(x)$ — свёртка с ядром 5x5,\n",
        "- $MaxPool(x)$ — операция max pooling.\n",
        "\n",
        "Затем результаты этих операций конкатенируются:\n",
        "$$\n",
        "y = Concat(y_1, y_2, y_3, y_4).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4. Squeeze-and-Excitation Networks (SENet)\n",
        "\n",
        "SENet — это архитектура, которая добавляет механизм внимания к каналам (channel-wise attention). Основная идея заключается в том, чтобы динамически перевзвешивать каналы в зависимости от их важности.\n",
        "\n",
        "### Математическая формулировка\n",
        "\n",
        "Пусть $x$ — входной тензор размерности $C \\times H \\times W$, где $C$ — количество каналов, $H$ и $W$ — высота и ширина. SENet добавляет два шага:\n",
        "1. **Squeeze**: Глобальное усреднение по пространственным измерениям:\n",
        "   $$\n",
        "   z_c = \\frac{1}{H \\cdot W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_c(i, j).\n",
        "   $$\n",
        "2. **Excitation**: Применение полносвязного слоя для получения весов каналов:\n",
        "   $$\n",
        "   s = \\sigma(W_2 \\cdot ReLU(W_1 \\cdot z)),\n",
        "   $$\n",
        "   где $W_1$ и $W_2$ — веса полносвязных слоёв, а $\\sigma$ — сигмоида.\n",
        "\n",
        "Затем входной тензор умножается на полученные веса:\n",
        "$$\n",
        "\\hat{x}_c = s_c \\cdot x_c.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Упражнения\n",
        "\n",
        "1. **ResNet**:\n",
        "   - Напишите уравнения forward pass и backpropagation для ResNet с двумя остаточными блоками.\n",
        "   - Объясните, как skip connection помогает избежать исчезающих градиентов.\n"
      ],
      "metadata": {
        "id": "s5hSwtimzDNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Остаточный блок\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Skip connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)  # Skip connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Простая ResNet с двумя остаточными блоками\n",
        "class SimpleResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(64, 64, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, stride=2)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, stride):\n",
        "        return nn.Sequential(\n",
        "            ResidualBlock(in_channels, out_channels, stride),\n",
        "            ResidualBlock(out_channels, out_channels, stride=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Пример использования\n",
        "model = SimpleResNet()\n",
        "input_tensor = torch.randn(1, 3, 32, 32)  # Пример входного тензора\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Проверка формы выхода"
      ],
      "metadata": {
        "id": "fr3scTACz8Zv",
        "outputId": "5bf10235-55de-48c5-a9b1-530f6cc670d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **DenseNet**:\n",
        "   - Реализуйте forward pass для DenseNet с тремя слоями.\n",
        "   - Вычислите градиент потери по весам для одного слоя в DenseNet.\n"
      ],
      "metadata": {
        "id": "VoOBqOSIz8mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(F.relu(self.bn(x)))\n",
        "        return torch.cat([x, out], 1)  # Конкатенация\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=32):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = DenseLayer(64, growth_rate)\n",
        "        self.layer2 = DenseLayer(64 + growth_rate, growth_rate)\n",
        "        self.layer3 = DenseLayer(64 + 2 * growth_rate, growth_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "# Пример использования\n",
        "model = DenseNet()\n",
        "input_tensor = torch.randn(1, 3, 32, 32)  # Пример входного тензора\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Проверка формы выхода\n",
        "\n",
        "# Вычисление градиента\n",
        "output.sum().backward()  # Пример вычисления градиента\n",
        "print(model.layer1.conv.weight.grad)  # Градиент по весам первого слоя"
      ],
      "metadata": {
        "id": "RmNvWFaez98A",
        "outputId": "78dbb615-4b91-4fd2-f5b2-04ddb9aa2fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 160, 32, 32])\n",
            "tensor([[[[364.2170, 395.3300, 384.6144],\n",
            "          [393.7863, 405.4398, 398.8358],\n",
            "          [372.8969, 393.8392, 383.8637]],\n",
            "\n",
            "         [[382.5582, 393.5932, 383.2644],\n",
            "          [391.1706, 408.4508, 393.8076],\n",
            "          [371.9312, 398.5501, 390.0039]],\n",
            "\n",
            "         [[382.4754, 403.6316, 384.1597],\n",
            "          [397.9782, 417.4872, 402.6215],\n",
            "          [384.1032, 401.5205, 396.1471]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[377.0802, 395.3242, 384.8465],\n",
            "          [383.8300, 398.2366, 395.9992],\n",
            "          [369.8520, 394.0409, 392.1115]],\n",
            "\n",
            "         [[372.1234, 394.3519, 380.8982],\n",
            "          [391.9119, 400.7574, 399.9700],\n",
            "          [376.2524, 393.3100, 386.9035]],\n",
            "\n",
            "         [[376.6210, 388.6219, 385.1083],\n",
            "          [391.0900, 402.9782, 399.9229],\n",
            "          [377.5531, 396.0152, 384.2265]]],\n",
            "\n",
            "\n",
            "        [[[359.4077, 391.0461, 366.7545],\n",
            "          [398.1277, 404.0374, 382.2026],\n",
            "          [392.0501, 426.2551, 401.3162]],\n",
            "\n",
            "         [[406.0994, 394.1514, 376.4550],\n",
            "          [414.5896, 411.2976, 393.5685],\n",
            "          [394.9413, 406.9641, 377.4913]],\n",
            "\n",
            "         [[394.3047, 402.4160, 372.8033],\n",
            "          [404.7945, 416.6225, 380.0488],\n",
            "          [405.4561, 425.0988, 400.7772]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[382.5080, 394.6651, 372.2210],\n",
            "          [399.4278, 404.1973, 409.4286],\n",
            "          [393.3673, 421.5286, 392.0826]],\n",
            "\n",
            "         [[374.1912, 393.1404, 365.5818],\n",
            "          [404.2378, 413.1570, 387.3763],\n",
            "          [412.2047, 435.8177, 384.8945]],\n",
            "\n",
            "         [[399.0628, 369.3508, 371.9271],\n",
            "          [395.9294, 402.2033, 403.2852],\n",
            "          [407.7102, 394.7719, 397.0438]]],\n",
            "\n",
            "\n",
            "        [[[351.3500, 408.7938, 403.6160],\n",
            "          [412.6804, 384.3120, 396.8928],\n",
            "          [399.4736, 389.0846, 404.1535]],\n",
            "\n",
            "         [[313.3144, 418.5858, 403.5909],\n",
            "          [400.2908, 369.9471, 390.3365],\n",
            "          [367.6327, 413.2431, 431.6082]],\n",
            "\n",
            "         [[398.2672, 378.2749, 397.9925],\n",
            "          [384.0795, 419.3643, 399.2902],\n",
            "          [345.9421, 387.9069, 410.2175]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[366.1430, 371.8095, 376.8029],\n",
            "          [378.2124, 384.4998, 399.8548],\n",
            "          [387.7407, 347.6362, 368.1901]],\n",
            "\n",
            "         [[412.1524, 369.4916, 415.6919],\n",
            "          [423.9017, 372.4972, 393.8335],\n",
            "          [380.0261, 381.7758, 360.8330]],\n",
            "\n",
            "         [[373.6221, 405.7836, 335.6949],\n",
            "          [390.2265, 430.9172, 375.9305],\n",
            "          [382.7139, 390.1983, 368.9698]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[384.7311, 386.5357, 392.3493],\n",
            "          [382.3078, 405.5550, 387.3024],\n",
            "          [365.5757, 394.3173, 378.9773]],\n",
            "\n",
            "         [[362.7870, 402.7860, 384.1083],\n",
            "          [380.5131, 401.9726, 382.7000],\n",
            "          [386.2748, 395.7672, 395.5748]],\n",
            "\n",
            "         [[377.0776, 396.3652, 398.0633],\n",
            "          [380.4377, 406.3819, 379.8188],\n",
            "          [380.3430, 403.3832, 387.7969]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[378.3181, 391.6526, 362.8217],\n",
            "          [395.3423, 382.2839, 392.8126],\n",
            "          [374.4832, 358.0548, 372.5185]],\n",
            "\n",
            "         [[381.8559, 405.7244, 371.5416],\n",
            "          [377.8100, 407.8133, 373.5602],\n",
            "          [353.8554, 399.2004, 369.9111]],\n",
            "\n",
            "         [[365.8034, 393.4647, 359.8414],\n",
            "          [390.2643, 424.5581, 394.7180],\n",
            "          [380.4318, 387.8371, 372.1940]]],\n",
            "\n",
            "\n",
            "        [[[368.0459, 398.7549, 382.1380],\n",
            "          [385.7260, 410.6171, 397.4666],\n",
            "          [379.4577, 400.9355, 401.0220]],\n",
            "\n",
            "         [[382.9796, 406.0440, 395.4075],\n",
            "          [385.5244, 418.0919, 407.9386],\n",
            "          [385.7658, 405.1550, 396.6902]],\n",
            "\n",
            "         [[383.6385, 410.7546, 405.7708],\n",
            "          [395.3707, 412.1246, 410.0565],\n",
            "          [384.1392, 406.0651, 397.6671]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[377.1857, 389.7121, 398.1456],\n",
            "          [381.5021, 400.9299, 401.5459],\n",
            "          [373.1199, 397.0383, 386.5353]],\n",
            "\n",
            "         [[371.9754, 395.7642, 393.0327],\n",
            "          [386.1510, 410.4203, 400.3138],\n",
            "          [378.2671, 389.2950, 381.7786]],\n",
            "\n",
            "         [[375.8664, 386.1366, 388.8446],\n",
            "          [393.3745, 412.8516, 402.2666],\n",
            "          [373.3632, 393.0817, 386.7911]]],\n",
            "\n",
            "\n",
            "        [[[381.7425, 401.0544, 383.5527],\n",
            "          [390.9335, 410.1668, 387.5367],\n",
            "          [370.3356, 379.9589, 380.7712]],\n",
            "\n",
            "         [[386.2238, 400.2527, 393.7691],\n",
            "          [382.0901, 405.5350, 376.0034],\n",
            "          [376.5474, 381.3123, 374.1167]],\n",
            "\n",
            "         [[389.5938, 409.2482, 390.2001],\n",
            "          [396.1552, 401.9763, 398.2839],\n",
            "          [384.9386, 391.7424, 376.2361]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[373.6833, 399.3262, 379.7726],\n",
            "          [389.5373, 405.0124, 399.4977],\n",
            "          [371.7697, 373.7588, 382.2650]],\n",
            "\n",
            "         [[385.0815, 396.8825, 378.9721],\n",
            "          [394.6768, 391.5736, 407.9952],\n",
            "          [372.0816, 375.4705, 383.0807]],\n",
            "\n",
            "         [[390.6175, 382.0508, 387.9321],\n",
            "          [389.4125, 407.1494, 387.2843],\n",
            "          [377.5912, 396.2747, 365.5610]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Inception**:\n",
        "   - Напишите уравнения для Inception модуля с ядрами 1x1, 3x3 и 5x5.\n",
        "   - Объясните, почему Inception эффективен для извлечения признаков на разных масштабах.\n",
        "\n"
      ],
      "metadata": {
        "id": "fLNbIclpyRlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 96, kernel_size=1),\n",
        "            nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        branch_pool = self.branch_pool(x)\n",
        "        return torch.cat([branch1x1, branch3x3, branch5x5, branch_pool], 1)\n",
        "\n",
        "# Пример использования\n",
        "model = InceptionModule(3)\n",
        "input_tensor = torch.randn(1, 3, 32, 32)  # Пример входного тензора\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Проверка формы выхода"
      ],
      "metadata": {
        "id": "FLhEZLRRz_nl",
        "outputId": "09b183b9-c72a-40c5-b628-18a56255fd0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **SENet**:\n",
        "   - Реализуйте Squeeze-and-Excitation блок на Python.\n",
        "   - Объясните, как механизм внимания в SENet улучшает качество модели.\n",
        "\n"
      ],
      "metadata": {
        "id": "bItKnhb7z_6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.squeeze(x).view(b, c)\n",
        "        y = self.excitation(y).view(b, c, 1, 1)\n",
        "        return x * y  # Применение весов каналов\n",
        "\n",
        "# Пример использования\n",
        "model = SEBlock(64)\n",
        "input_tensor = torch.randn(1, 64, 32, 32)  # Пример входного тензора\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Проверка формы выхода"
      ],
      "metadata": {
        "id": "2fVFB6dK0BBK",
        "outputId": "f6359425-0510-4211-e57b-a1fc4aef1552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Эти архитектуры являются основой современных глубоких нейронных сетей. Понимание их математической основы позволяет не только использовать их, но и создавать новые модификации для решения конкретных задач."
      ],
      "metadata": {
        "id": "OPu9juiZ0BL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fZQ-EKv00R9V"
      }
    }
  ]
}