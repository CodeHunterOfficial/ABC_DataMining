{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFfBKptiKv39Ng10DkNKZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/DL/5_%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D0%B8_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D0%B2_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D1%85_%D1%81%D0%B5%D1%82%D1%8F%D1%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Методы регуляризации и оптимизации в нейронных сетях"
      ],
      "metadata": {
        "id": "EjRIMJhjlN3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. **Дропаут (Dropout)**  \n",
        "Дропаут — это метод регуляризации, используемый в нейронных сетях для предотвращения переобучения. Идея метода заключается в случайном \"отключении\" (занулении) части нейронов во время обучения с определённой вероятностью $p$. Это позволяет снизить коадаптацию нейронов, то есть их чрезмерную зависимость друг от друга, что способствует повышению обобщающей способности модели.  \n",
        "\n",
        "а) **Прямой и обратный проход через линейный слой с Dropout**  \n",
        "Рассмотрим линейный слой с функцией активации $f$. Пусть на вход подаётся наблюдение $h_1$ размерности $[1 \\times d]$. Тогда выход слоя вычисляется по формуле:  \n",
        "$$\n",
        "h_2 = f(h_1 \\cdot W + b),\n",
        "$$  \n",
        "где $W$ — матрица весов размерности $[d \\times k]$, $b$ — вектор смещений размерности $[1 \\times k]$, а $h_2$ — выходной вектор размерности $[1 \\times k]$.  \n",
        "\n",
        "Для применения дропаута каждый нейрон отключается с вероятностью $p$. Это реализуется путём поэлементного умножения выходного вектора $h_2$ на вектор $d$ размерности $[1 \\times k]$, где каждый элемент $d_j$ имеет распределение Бернулли:  \n",
        "$$\n",
        "d_j =\n",
        "\\begin{cases}\n",
        "0, & \\text{с вероятностью } p, \\\\\n",
        "1, & \\text{с вероятностью } 1 - p.\n",
        "\\end{cases}\n",
        "$$  \n",
        "Обозначив поэлементное умножение через $\\odot$, получим:  \n",
        "$$\n",
        "h_2 = d \\odot f(h_1 \\cdot W + b).\n",
        "$$  \n",
        "Если на вход подаётся матрица $H_1$ размерности $[n \\times d]$, то выход слоя с дропаутом вычисляется как:  \n",
        "$$\n",
        "H_2 = D \\odot f(H_1 \\cdot W + b),\n",
        "$$  \n",
        "где $D$ — матрица размерности $[n \\times k]$, состоящая из нулей и единиц, сгенерированных по распределению Бернулли.  \n",
        "\n",
        "Без дропаута шаг обратного распространения ошибки (backpropagation) выглядит следующим образом:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial H_1} = \\frac{\\partial L}{\\partial H_2} \\cdot W^T \\odot f'(H_1 \\cdot W + b),\n",
        "$$  \n",
        "где $\\frac{\\partial L}{\\partial H_1}$ — градиент по входу слоя, $\\frac{\\partial L}{\\partial H_2}$ — градиент по выходу слоя, а $f'$ — производная функции активации.  \n",
        "\n",
        "Для слоя с дропаутом шаг обратного распространения ошибки модифицируется следующим образом:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial H_1} = \\frac{\\partial L}{\\partial H_2} \\cdot W^T \\odot f'(H_1 \\cdot W + b) \\odot D.\n",
        "$$  \n",
        "Таким образом, градиенты по отключённым нейронам зануляются, и сеть игнорирует их при обновлении параметров.  \n",
        "\n",
        "Производная по весам слоя вычисляется как:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = H_1^T \\cdot \\frac{\\partial L}{\\partial H_1},\n",
        "$$  \n",
        "и именно на эту величину выполняется шаг градиентного спуска.  \n",
        "\n",
        "б) **Роль Dropout в предотвращении переобучения**  \n",
        "Дропаут помогает бороться с переобучением за счёт предотвращения коадаптации нейронов. При обучении случайное отключение части нейронов приводит к тому, что оставшиеся нейроны вынуждены учиться более независимо, что снижает риск переобучения на конкретных признаках.  \n",
        "\n",
        "Кроме того, дропаут можно интерпретировать как усреднение множества различных моделей, получаемых путём случайного отключения нейронов. Это эквивалентно обучению ансамбля из $2^N$ моделей, где $N$ — количество нейронов, которые могут быть отключены.  \n",
        "\n",
        "в) **Применение Dropout на этапе тестирования**  \n",
        "На этапе тестирования дропаут не применяется, так как это может привести к нестабильности предсказаний. Вместо этого используется масштабирование выходов нейронов на коэффициент $1 - p$, чтобы сохранить ожидаемое значение выхода.  \n",
        "\n",
        "Рассмотрим пример с двумя нейронами, каждый из которых отключается с вероятностью $0.4$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v5izlbfJtHPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "eisYp3VQtmzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Пусть выходы нейронов равны $x_1$ и $x_2$, а их веса — $w_1$ и $w_2$. Тогда ожидаемое значение выхода $h$ вычисляется как:  \n",
        "$$\n",
        "E(h) = 0.6 \\cdot 0.6 \\cdot (w_1 \\cdot x_1 + w_2 \\cdot x_2) + 0.6 \\cdot 0.4 \\cdot (w_1 \\cdot x_1) + 0.4 \\cdot 0.6 \\cdot (w_2 \\cdot x_2) + 0.4 \\cdot 0.4 \\cdot 0 = 0.6 \\cdot (w_1 \\cdot x_1 + w_2 \\cdot x_2).\n",
        "$$  \n",
        "Таким образом, для получения корректного результата на этапе тестирования достаточно умножить выходы нейронов на $0.6$.  \n",
        "\n",
        "г) **Обратный дропаут (Inverted Dropout)**  \n",
        "Обратный дропаут — это модификация метода, при которой выходы нейронов на этапе обучения масштабируются на коэффициент $\\frac{1}{1 - p}$. Это позволяет избежать необходимости масштабирования на этапе тестирования. Прямой проход при обратном дропауте выглядит следующим образом:  \n",
        "$$\n",
        "H_2 = \\frac{1}{1 - p} \\cdot D \\odot f(H_1 \\cdot W + b).\n",
        "$$  \n",
        "При этом на этапе тестирования выходы нейронов используются без дополнительного масштабирования:  \n",
        "$$\n",
        "H_2 = f(H_1 \\cdot W + b).\n",
        "$$  \n",
        "Такой подход упрощает реализацию и снижает вероятность ошибок при переходе между этапами обучения и тестирования.\n",
        "\n",
        "\n",
        "д) **Шаг градиентного спуска для линейного слоя с обратным дропаутом и без него**  \n",
        "Для линейного слоя без дропаута производная по весам $W$ вычисляется следующим образом:  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = H_1^T \\cdot \\frac{\\partial L}{\\partial H_1},\n",
        "$$  \n",
        "где $H_1$ — входные данные, $\\frac{\\partial L}{\\partial H_1}$ — градиент по выходу слоя. Шаг градиентного спуска для обновления весов выглядит как:  \n",
        "$$\n",
        "W_t = W_{t-1} - \\eta_t \\cdot \\frac{\\partial L}{\\partial W}(W_{t-1}),\n",
        "$$  \n",
        "где $\\eta_t$ — скорость обучения на шаге $t$.  \n",
        "\n",
        "При использовании обратного дропаута формула для шага градиентного спуска модифицируется:  \n",
        "$$\n",
        "W_t = W_{t-1} - \\eta_t \\cdot \\frac{1}{1 - p} \\cdot \\frac{\\partial L}{\\partial W}(W_{t-1}).\n",
        "$$  \n",
        "Таким образом, скорость обучения дополнительно масштабируется на коэффициент $\\frac{1}{1 - p} \\geq 1$. Это приводит к увеличению величины шага градиентного спуска, что может ускорить процесс обучения, но также требует осторожности в выборе скорости обучения $\\eta_t$, чтобы избежать нестабильности.  \n",
        "\n",
        "е) **Вероятность отключения всего слоя и параметры дропаута**  \n",
        "Пусть на слое имеется $k$ нейронов, каждый из которых отключается с вероятностью $p$. Вероятность того, что весь слой будет отключён, равна $p^k$.  \n",
        "\n",
        "Рассмотрим случайную величину $N$, которая обозначает количество включённых нейронов. Каждый нейрон включается с вероятностью $1 - p$, поэтому математическое ожидание $N$ вычисляется как:  \n",
        "$$\n",
        "E(N) = E(Y_1 + \\dots + Y_k) = (1 - p) \\cdot k,\n",
        "$$  \n",
        "где $Y_i$ — индикаторная случайная величина, равная 1, если $i$-й нейрон включён, и 0 в противном случае.  \n",
        "\n",
        "Дисперсия $N$ для биномиального распределения равна:  \n",
        "$$\n",
        "\\text{Var}(N) = k \\cdot p \\cdot (1 - p).\n",
        "$$  \n",
        "\n",
        "Если требуется оставить четверть работающих нейронов, то $E(N) = \\frac{k}{4}$. Подставляя в формулу математического ожидания, получаем:  \n",
        "$$\n",
        "(1 - p) \\cdot k = \\frac{k}{4} \\implies p = 0.75.\n",
        "$$  \n",
        "\n",
        "ё) **Вероятность отключения хотя бы одного слоя в сети**  \n",
        "Пусть нейронная сеть состоит из трёх слоёв с количеством нейронов $k_1$, $k_2$ и $k_3$ соответственно. Вероятность отключения каждого слоя равна $p^{k_1}$, $p^{k_2}$ и $p^{k_3}$.  \n",
        "\n",
        "Вероятность того, что ни один из слоёв не отключится, вычисляется как:  \n",
        "$$\n",
        "q = (1 - p^{k_1}) \\cdot (1 - p^{k_2}) \\cdot (1 - p^{k_3}).\n",
        "$$  \n",
        "Тогда вероятность того, что хотя бы один слой отключится, равна:  \n",
        "$$\n",
        "1 - q.\n",
        "$$  \n",
        "\n",
        "ж) **Влияние дропаута на нормализацию данных и метод Alpha Dropout**  \n",
        "Функция активации SELU (Scaled Exponential Linear Unit) обладает свойством самонормализации: при правильной инициализации весов выход линейного слоя с SELU имеет нулевое среднее и единичную дисперсию. Однако добавление дропаута нарушает это свойство.  \n",
        "\n",
        "При использовании обратного дропаута выход слоя масштабируется следующим образом:  \n",
        "$$\n",
        "h = \\frac{1}{1 - p} \\cdot d \\cdot x,\n",
        "$$  \n",
        "где $d$ — случайная величина, принимающая значение 1 с вероятностью $1 - p$ и 0 с вероятностью $p$, а $x$ — выход нейрона.  \n",
        "\n",
        "Математическое ожидание $h$ остаётся неизменным:  \n",
        "$$\n",
        "E(h) = E\\left(\\frac{1}{1 - p} \\cdot d \\cdot x\\right) = \\frac{1}{1 - p} \\cdot E(d) \\cdot E(x) = E(x).\n",
        "$$  \n",
        "\n",
        "Однако дисперсия изменяется:  \n",
        "$$\n",
        "\\text{Var}(h) = \\text{Var}\\left(\\frac{1}{1 - p} \\cdot d \\cdot x\\right) = \\frac{1}{(1 - p)^2} \\cdot \\text{Var}(d) \\cdot \\text{Var}(x) = \\frac{p}{1 - p} \\cdot \\text{Var}(x).\n",
        "$$  \n",
        "\n",
        "Для сохранения нормализации данных предлагается метод Alpha Dropout. В этом методе выход нейрона заменяется на $\\alpha'$ (константа, зависящая от параметров SELU) с вероятностью $p$. Формула для Alpha Dropout выглядит следующим образом:  \n",
        "$$\n",
        "d(t) =\n",
        "\\begin{cases}\n",
        "h, & \\text{с вероятностью } 1 - p, \\\\\n",
        "\\alpha', & \\text{с вероятностью } p.\n",
        "\\end{cases}\n",
        "$$  \n",
        "\n",
        "Для сохранения нулевого математического ожидания и единичной дисперсии выход масштабируется и сдвигается:  \n",
        "$$\n",
        "d(t) \\cdot a + b,\n",
        "$$  \n",
        "где константы $a$ и $b$ выбираются так, чтобы выполнялись условия:  \n",
        "$$\n",
        "E(a \\cdot (d \\cdot x + (1 - d) \\cdot \\alpha') + b) = 0, \\\\\n",
        "\\text{Var}(a \\cdot (d \\cdot x + (1 - d) \\cdot \\alpha') + b) = 1.\n",
        "$$  \n",
        "\n",
        "Решая систему уравнений, получаем:  \n",
        "$$\n",
        "a = \\left(p + (\\alpha')^2 \\cdot p \\cdot (1 - p)\\right)^{-0.5}, \\\\\n",
        "b = -\\left(p + (\\alpha')^2 \\cdot p \\cdot (1 - p)\\right)^{-0.5} \\cdot (1 - p) \\cdot \\alpha'.\n",
        "$$  \n",
        "\n",
        "Это преобразование сохраняет эффект разреженности от дропаута и поддерживает нормализацию данных, что особенно важно при использовании функции активации SELU.\n"
      ],
      "metadata": {
        "id": "IRNG7HKrtm96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Регуляризация\n",
        "\n",
        "Рассмотрим задачу регрессии, в которой целевая переменная $y$ зависит от признака $x$. Предположим, что зависимость между $y$ и $x$ может быть описана линейной моделью без свободного члена:  \n",
        "$$y_i = w \\cdot x_i,$$  \n",
        "где $w$ — параметр модели, который необходимо оценить. Если значение $x = 0$, то модель предсказывает $y = 0$, что соответствует условию задачи.\n",
        "\n",
        "Для оценки параметра $w$ используется целевая функция с L2-регуляризацией:  \n",
        "$$Q(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - w x_i)^2 + \\lambda w^2 \\to \\min_w,$$  \n",
        "где $\\lambda$ — параметр регуляризации, контролирующий степень штрафа за увеличение значения $w$.\n",
        "\n",
        "### а) Нахождение оптимального $w$ при произвольном $\\lambda$\n",
        "\n",
        "Для нахождения оптимального значения $w$ продифференцируем целевую функцию $Q(w)$ по $w$ и приравняем производную к нулю:  \n",
        "$$\n",
        "\\frac{dQ(w)}{dw} = \\frac{1}{n} \\sum_{i=1}^n -2 \\cdot (y_i - w x_i) \\cdot x_i + 2 \\lambda w = 0.\n",
        "$$  \n",
        "Упростим выражение:  \n",
        "$$\n",
        "\\frac{1}{n} \\sum_{i=1}^n y_i x_i - w \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i^2 - \\lambda w = 0.\n",
        "$$  \n",
        "Перенесем слагаемые, содержащие $w$, в одну сторону:  \n",
        "$$\n",
        "\\frac{1}{n} \\sum_{i=1}^n y_i x_i = w \\left( \\frac{1}{n} \\sum_{i=1}^n x_i^2 + \\lambda \\right).\n",
        "$$  \n",
        "Отсюда оптимальное значение $w$ выражается как:  \n",
        "$$\n",
        "\\hat{w} = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2 + n \\lambda}.\n",
        "$$  \n",
        "\n",
        "**Интерпретация:**  \n",
        "С увеличением параметра регуляризации $\\lambda$ значение $\\hat{w}$ стремится к нулю. Это предотвращает переобучение модели, ограничивая рост коэффициентов. Регуляризация особенно полезна, когда модель слишком сильно подстраивается под данные, что приводит к большим значениям коэффициентов.\n",
        "\n",
        "### б) Подбор оптимального $\\lambda$ с помощью кросс-валидации leave-one-out (LOO)\n",
        "\n",
        "Кросс-валидация leave-one-out (LOO) заключается в следующем:  \n",
        "1. На каждом шаге $i$ ($i = 1, 2, \\dots, n$) модель обучается на всех наблюдениях, кроме $i$-го.  \n",
        "2. Затем модель тестируется на $i$-м наблюдении, и вычисляется ошибка предсказания.  \n",
        "3. После проведения $n$ таких шагов вычисляется средняя ошибка по всем тестовым наблюдениям.  \n",
        "\n",
        "Оптимальное значение $\\lambda$ выбирается как то, которое минимизирует среднюю ошибку:  \n",
        "$$\n",
        "\\lambda_{\\text{CV}} = \\arg\\min_\\lambda \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i^{(-i)})^2,\n",
        "$$  \n",
        "где $\\hat{y}_i^{(-i)}$ — предсказание модели, обученной на всех наблюдениях, кроме $i$-го.\n",
        "\n",
        "### в) Нахождение оптимального $w$ при $\\lambda_{\\text{CV}}$\n",
        "\n",
        "После нахождения $\\lambda_{\\text{CV}}$ оптимальное значение $w$ вычисляется по формуле, полученной в пункте (а):  \n",
        "$$\n",
        "\\hat{w} = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2 + n \\lambda_{\\text{CV}}}.\n",
        "$$  \n",
        "\n",
        "### г) Ускорение LOO-кросс-валидации для Ridge-регрессии\n",
        "\n",
        "Для Ridge-регрессии можно избежать необходимости обучать $n$ моделей, используя матричные вычисления. Оптимальные коэффициенты для Ridge-регрессии могут быть выражены в виде:  \n",
        "$$\n",
        "\\hat{w} = (X^T X + \\lambda I)^{-1} X^T y,\n",
        "$$  \n",
        "где $X$ — матрица признаков, $y$ — вектор целевых значений, $I$ — единичная матрица.  \n",
        "\n",
        "С помощью метода шляпных матриц (hat matrix) и свойств линейной алгебры можно вычислить предсказания для каждого наблюдения без повторного обучения модели. Это позволяет значительно ускорить процесс кросс-валидации.\n",
        "\n",
        "### д) Регуляризация на выходе нейрона в нейронных сетях\n",
        "\n",
        "Наложение регуляризации на выход нейрона в нейронных сетях используется для следующих целей:  \n",
        "1. **Контроль переобучения:** Регуляризация ограничивает рост весов, предотвращая излишнюю сложность модели.  \n",
        "2. **Улучшение обобщающей способности:** Ограничение весов помогает модели лучше работать на новых данных.  \n",
        "3. **Стабилизация обучения:** Регуляризация может предотвратить взрыв градиентов, что особенно важно в глубоких сетях.  \n",
        "4. **Интерпретируемость:** Меньшие значения весов могут упростить интерпретацию модели.  \n",
        "\n",
        "Таким образом, регуляризация на выходе нейрона является важным инструментом для улучшения качества и устойчивости нейронных сетей."
      ],
      "metadata": {
        "id": "Hp2J-WZ3qNpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Weight Decay  \n",
        "В случае $L_2$-регуляризации к базовой функции потерь добавляется дополнительное слагаемое, и вместо функционала  \n",
        "$$\n",
        "L(w) = \\frac{1}{n} \\cdot \\sum\\limits_{i=1}^{n} \\nabla_w L(y_i, x_i, w)\n",
        "$$  \n",
        "оптимизируется функционал  \n",
        "$$\n",
        "Q_\\lambda(w) = L(w) + \\frac{1}{2}\\lambda \\cdot ||w||^2_2,\n",
        "$$  \n",
        "где $\\lambda$ — параметр регуляризации. Будем считать, что регуляризатор применяется ко всем весам нейронной сети. Обычно регуляризатор добавляется к функции потерь для предотвращения переобучения. Градиентный спуск может быть переписан с учётом регуляризатора в несколько иной форме, которая называется **weight decay**. В современных библиотеках для обучения нейронных сетей у оптимизаторов часто присутствует соответствующий параметр. Рассмотрим переписывание градиентного спуска с учётом регуляризатора для нескольких методов оптимизации.\n",
        "\n",
        "### а) Momentum-SGD с $L_2$-регуляризацией  \n",
        "Шаг momentum-SGD с учётом $L_2$-регуляризации может быть выражен в виде:  \n",
        "$$\n",
        "w_t = g(\\lambda) \\cdot w_{t-1} - \\eta_t \\cdot h(\\nabla_w L(w_{t-1})),\n",
        "$$  \n",
        "где $\\eta_t$ — learning rate на шаге $t$, а $g(\\lambda)$ и $h(\\cdot)$ — функции, зависящие от параметра регуляризации и градиента соответственно.  \n",
        "\n",
        "Один шаг momentum-SGD с $L_2$-регуляризацией описывается следующей системой уравнений:  \n",
        "$$\n",
        "\\begin{cases}\n",
        "g_t = \\nabla_w Q(w_{t-1}) + \\lambda \\cdot w_{t-1}, \\\\\n",
        "m_t = \\mu \\cdot m_{t-1} + g_t, \\\\\n",
        "w_t = w_{t-1} - \\eta_t \\cdot m_t,\n",
        "\\end{cases}\n",
        "$$  \n",
        "где $\\mu$ — коэффициент инерции (momentum), а $m_t$ — накопленный градиент.  \n",
        "\n",
        "Подставив первую строку во вторую, а вторую в третью, получим:  \n",
        "$$\n",
        "w_t = w_{t-1} - \\eta_t \\cdot \\left( \\mu \\cdot m_{t-1} + \\nabla_w L(w_{t-1}) + \\lambda \\cdot w_{t-1} \\right) =\n",
        "$$  \n",
        "$$\n",
        "= \\underbrace{(1 - \\eta_t \\cdot \\lambda)}_{<1} \\cdot w_{t-1} - \\eta_t \\cdot \\left( \\mu \\cdot m_{t-1} + \\nabla_w L(w_{t-1}) \\right).\n",
        "$$  \n",
        "\n",
        "Таким образом, при добавлении $L_2$-регуляризации каждый шаг градиентного спуска выполняется с учётом сдвига весов на константу, что эквивалентно применению weight decay. Этот параметр часто используется в оптимизаторах для нейронных сетей вместо явной регуляризации.\n",
        "\n",
        "### б) Adam с $L_2$-регуляризацией  \n",
        "Шаг Adam с учётом $L_2$-регуляризации может быть выражен в виде:  \n",
        "$$\n",
        "w_t = g(\\lambda) \\cdot w_{t-1} - \\eta_t \\cdot h(\\nabla_w L(w_{t-1})),\n",
        "$$  \n",
        "где $g(\\lambda)$ и $h(\\cdot)$ — функции, зависящие от параметра регуляризации и градиента.  \n",
        "\n",
        "Один шаг Adam с $L_2$-регуляризацией описывается следующей системой уравнений:  \n",
        "$$\n",
        "\\begin{cases}\n",
        "g_t = \\nabla_w Q(w_{t-1}) + \\lambda \\cdot w_{t-1}, \\\\\n",
        "m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t, \\\\\n",
        "v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2, \\\\\n",
        "\\hat{m}_t = \\frac{1}{1-\\beta_1^t} \\cdot m_t, \\\\\n",
        "\\hat{v}_t = \\frac{1}{1-\\beta_2^t} \\cdot v_t, \\\\\n",
        "w_t = w_{t-1} - \\eta_t \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon},\n",
        "\\end{cases}\n",
        "$$  \n",
        "где $\\beta_1$ и $\\beta_2$ — гиперпараметры, контролирующие экспоненциальное затухание для градиента и его квадрата соответственно, а $\\varepsilon$ — малая константа для численной стабильности.  \n",
        "\n",
        "Выполнив подстановки, получим:  \n",
        "$$\n",
        "w_t = w_{t-1} - \\eta_t \\cdot \\frac{m_t}{1-\\beta_1^t} \\cdot \\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon} =\n",
        "$$  \n",
        "$$\n",
        "= w_{t-1} - \\eta_t \\cdot \\frac{\\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t}{1-\\beta_1^t} \\cdot \\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon} =\n",
        "$$  \n",
        "$$\n",
        "= w_{t-1} - \\eta_t \\cdot \\frac{\\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot (\\nabla_w Q(w_{t-1}) + \\lambda \\cdot w_{t-1})}{1-\\beta_1^t} \\cdot \\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon} =\n",
        "$$  \n",
        "$$\n",
        "= w_{t-1} \\cdot \\left( \\underbrace{1}_{\\text{вектор единиц}} - \\frac{\\eta_t \\cdot \\lambda \\cdot (1-\\beta_1)}{1-\\beta_1^t} \\cdot \\underbrace{\\frac{1}{\\sqrt{\\hat{v}_t} + \\varepsilon}}_{(*)} \\right) - \\dots\n",
        "$$  \n",
        "\n",
        "Здесь $(*)$ указывает на то, что регуляризация работает по-разному для различных весов, что может приводить к их неравномерному затуханию. В случае Adam, из-за высокой эффективности оптимизации, существует риск переобучения.  \n",
        "\n",
        "### в) AdamW  \n",
        "Для решения проблемы переобучения в Adam была предложена модификация AdamW, в которой weight decay учитывается отдельно от градиента. Уравнения для AdamW имеют вид:  \n",
        "$$\n",
        "\\begin{cases}\n",
        "g_t = \\nabla_w Q(w_{t-1}), \\\\\n",
        "m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t, \\\\\n",
        "v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2, \\\\\n",
        "\\hat{m}_t = \\frac{1}{1-\\beta_1^t} \\cdot m_t, \\\\\n",
        "\\hat{v}_t = \\frac{1}{1-\\beta_2^t} \\cdot v_t, \\\\\n",
        "w_t = (1 - \\eta_t \\cdot \\lambda) \\cdot w_{t-1} - \\eta_t \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}.\n",
        "\\end{cases}\n",
        "$$  \n",
        "В AdamW weight decay применяется непосредственно к весам, что позволяет более эффективно контролировать переобучение."
      ],
      "metadata": {
        "id": "ZDKPaqWeqayg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Нормализация по батчам (Batch Normalization)\n",
        "\n",
        "Нормализация по батчам (Batch Normalization, BN) — это мощная техника, которая была предложена для ускорения обучения глубоких нейронных сетей и борьбы с внутренним ковариационным сдвигом (internal covariate shift). Внутренний ковариационный сдвиг возникает, когда распределение входных данных для каждого слоя сети меняется в процессе обучения, что затрудняет настройку параметров и замедляет сходимость. Batch Normalization решает эту проблему путем нормализации активаций каждого слоя по мини-батчам данных.\n",
        "\n",
        "\n",
        "\n",
        "## Математическая основа Batch Normalization\n",
        "\n",
        "### 1. Нормализация активаций\n",
        "\n",
        "Пусть у нас есть мини-батч данных $B = \\{x_1, x_2, \\dots, x_m\\}$, где $m$ — размер батча. Для каждого элемента батча $x_i$ вычисляется его нормализованное значение. Процесс нормализации состоит из следующих шагов:\n",
        "\n",
        "#### a) Вычисление среднего и дисперсии по батчу\n",
        "\n",
        "Сначала вычисляется среднее значение $\\mu_B$ и дисперсия $\\sigma_B^2$ по батчу:\n",
        "\n",
        "$$\n",
        "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\mu_B$ — среднее значение активаций по батчу,\n",
        "- $\\sigma_B^2$ — дисперсия активаций по батчу.\n",
        "\n",
        "#### b) Нормализация\n",
        "\n",
        "Затем каждый элемент батча нормализуется с использованием среднего и дисперсии:\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\hat{x}_i$ — нормализованное значение,\n",
        "- $\\varepsilon$ — малая константа (например, $10^{-5}$), добавленная для численной стабильности (чтобы избежать деления на ноль).\n",
        "\n",
        "#### c) Масштабирование и сдвиг\n",
        "\n",
        "После нормализации применяется масштабирование и сдвиг с помощью обучаемых параметров $\\gamma$ и $\\beta$:\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\cdot \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\gamma$ — параметр масштабирования,\n",
        "- $\\beta$ — параметр сдвига.\n",
        "\n",
        "Эти параметры позволяют сети адаптировать нормализованные значения под конкретные задачи и восстанавливать выразительную способность сети, если нормализация нежелательна.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Обучение и инференс\n",
        "\n",
        "#### a) Обучение\n",
        "\n",
        "Во время обучения среднее $\\mu_B$ и дисперсия $\\sigma_B^2$ вычисляются для каждого батча. Кроме того, поддерживаются скользящие средние $\\mu_{\\text{running}}$ и $\\sigma^2_{\\text{running}}$, которые обновляются после каждого батча:\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{running}} = \\alpha \\cdot \\mu_{\\text{running}} + (1 - \\alpha) \\cdot \\mu_B\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2_{\\text{running}} = \\alpha \\cdot \\sigma^2_{\\text{running}} + (1 - \\alpha) \\cdot \\sigma_B^2\n",
        "$$\n",
        "\n",
        "Здесь $\\alpha$ — коэффициент затухания (обычно близкий к 1, например, 0.9).\n",
        "\n",
        "#### b) Инференс\n",
        "\n",
        "Во время инференса (тестирования) используются скользящие средние $\\mu_{\\text{running}}$ и $\\sigma^2_{\\text{running}}$, а не статистики текущего батча:\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_{\\text{running}}}{\\sqrt{\\sigma^2_{\\text{running}} + \\varepsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\cdot \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Пример использования в нейронной сети\n",
        "\n",
        "Рассмотрим пример использования Batch Normalization в полносвязной нейронной сети. Пусть у нас есть сеть с двумя скрытыми слоями:\n"
      ],
      "metadata": {
        "id": "hff3ndglrdD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eNkQRAeDtLUO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Здесь:\n",
        "- `fc1`, `fc2`, `fc3` — полносвязные слои,\n",
        "- `bn1`, `bn2` — слои Batch Normalization, применяемые после каждого полносвязного слоя перед активационной функцией ReLU.\n",
        "\n",
        "\n",
        "## Преимущества Batch Normalization\n",
        "\n",
        "1. **Ускорение обучения:** BN позволяет использовать более высокие learning rates, что ускоряет сходимость.\n",
        "2. **Устойчивость к инициализации весов:** BN делает сеть менее чувствительной к начальной инициализации весов.\n",
        "3. **Регуляризация:** BN действует как слабый регуляризатор, что может уменьшить необходимость в других методах регуляризации, таких как dropout.\n",
        "\n",
        "\n",
        "## Недостатки и ограничения\n",
        "\n",
        "1. **Зависимость от размера батча:** BN требует достаточно большого размера батча для точной оценки среднего и дисперсии. На маленьких батчах это может привести к нестабильности.\n",
        "2. **Сложность в рекуррентных сетях:** Применение BN в рекуррентных сетях (RNN, LSTM) требует специальных модификаций.\n",
        "3. **Проблемы с распределенным обучением:** В распределенных системах синхронизация статистик между устройствами может быть сложной.\n",
        "\n",
        "\n",
        "## Альтернативы Batch Normalization\n",
        "\n",
        "В случаях, когда Batch Normalization не подходит, могут использоваться альтернативные методы:\n",
        "\n",
        "1. **Layer Normalization (LN):** Нормализация по слоям, которая нормализует активации по нейронам, а не по батчам.\n",
        "2. **Instance Normalization (IN):** Нормализация по каждому отдельному примеру, часто используется в задачах генерации изображений.\n",
        "3. **Group Normalization (GN):** Нормализация по группам нейронов, что может быть полезно при маленьких батчах.\n",
        "\n",
        "### Пример Layer Normalization\n",
        "\n"
      ],
      "metadata": {
        "id": "fPkr_3SltOvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.ln1 = nn.LayerNorm(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.ln2 = nn.LayerNorm(128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JZN7mylFtWuO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, Batch Normalization — это мощный инструмент для улучшения обучения глубоких нейронных сетей. Она позволяет ускорить сходимость, уменьшить зависимость от инициализации весов и действует как слабый регуляризатор. Однако в некоторых случаях, таких как маленькие батчи или рекуррентные сети, могут потребоваться альтернативные методы нормализации. Понимание математической основы BN помогает эффективно применять эту технику и адаптировать её под конкретные задачи."
      ],
      "metadata": {
        "id": "uFxoT8jKtZq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Нормализация VS дропаут  \n",
        "\n",
        "В обучении нейронных сетей часто возникают проблемы, связанные с внутренним ковариационным сдвигом (internal covariate shift) и переобучением. Для решения этих проблем используются два основных метода: **нормализация** и **дропаут**. Оба метода имеют свои особенности, математические обоснования и области применения. Рассмотрим их подробно.\n",
        "\n",
        "\n",
        "\n",
        "## 5.1 Нормализация  \n",
        "\n",
        "Нормализация — это метод, который позволяет стабилизировать распределение входных данных для каждого слоя нейронной сети. Это помогает ускорить обучение и улучшить обобщающую способность модели. Наиболее популярным методом нормализации является **Batch Normalization (BN)**.\n",
        "\n",
        "### 5.1.1 Batch Normalization  \n",
        "\n",
        "Batch Normalization работает следующим образом:  \n",
        "1. Для каждого мини-батча вычисляются среднее значение и дисперсия по каждому признаку.  \n",
        "2. Данные нормализуются с использованием этих статистик.  \n",
        "3. Вводятся обучаемые параметры масштаба и сдвига для восстановления выразительности модели.  \n",
        "\n",
        "Математически это выражается следующим образом:  \n",
        "\n",
        "1. Вычисление среднего и дисперсии для мини-батча:  \n",
        "$$\n",
        "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i, \\quad \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2,\n",
        "$$  \n",
        "   где $m$ — размер мини-батча, $x_i$ — входные данные.  \n",
        "\n",
        "2. Нормализация данных:  \n",
        "$$\n",
        "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}},\n",
        "$$  \n",
        "   где $\\varepsilon$ — малая константа для численной стабильности.  \n",
        "\n",
        "3. Масштабирование и сдвиг:  \n",
        "$$\n",
        "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta,\n",
        "$$  \n",
        "   где $\\gamma$ и $\\beta$ — обучаемые параметры.  \n",
        "\n",
        "#### Преимущества Batch Normalization:  \n",
        "- Ускорение сходимости за счёт стабилизации градиентов.  \n",
        "- Снижение чувствительности к начальной инициализации весов.  \n",
        "- Позволяет использовать более высокие learning rates.  \n",
        "\n",
        "#### Пример:  \n",
        "Предположим, у нас есть мини-батч из 4 элементов: $x = [1, 2, 3, 4]$.  \n",
        "1. Вычисляем среднее и дисперсию:  \n",
        "$$\n",
        "   \\mu_B = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\sigma_B^2 = \\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4} = 1.25.\n",
        "$$  \n",
        "2. Нормализуем данные:  \n",
        "$$\n",
        "   \\hat{x}_i = \\frac{x_i - 2.5}{\\sqrt{1.25 + \\varepsilon}}.\n",
        "$$  \n",
        "3. Применяем масштабирование и сдвиг:  \n",
        "$$\n",
        "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta.\n",
        "$$  \n",
        "\n",
        "\n",
        "\n",
        "## 5.2 Дропаут  \n",
        "\n",
        "Дропаут (Dropout) — это метод регуляризации, который случайным образом \"отключает\" (обнуляет) часть нейронов во время обучения. Это предотвращает переобучение, заставляя сеть учиться более устойчивым признакам.  \n",
        "\n",
        "### 5.2.1 Математическое описание  \n",
        "\n",
        "Пусть $p$ — вероятность того, что нейрон останется активным (обычно $p = 0.5$). Тогда для каждого нейрона $x_i$ на этапе обучения:  \n",
        "$$\n",
        "x_i' =\n",
        "\\begin{cases}\n",
        "\\frac{x_i}{p}, & \\text{с вероятностью } p, \\\\\n",
        "0, & \\text{с вероятностью } 1-p.\n",
        "\\end{cases}\n",
        "$$  \n",
        "На этапе тестирования все нейроны остаются активными, но их выходы умножаются на $p$.  \n",
        "\n",
        "#### Пример:  \n",
        "Пусть у нас есть слой из 4 нейронов: $x = [1, 2, 3, 4]$, и $p = 0.5$.  \n",
        "1. Генерируем маску: $m = [1, 0, 1, 0]$ (случайно отключаем нейроны).  \n",
        "2. Применяем дропаут:  \n",
        "$$\n",
        "   x' = \\left[ \\frac{1}{0.5}, 0, \\frac{3}{0.5}, 0 \\right] = [2, 0, 6, 0].\n",
        "$$  \n",
        "\n",
        "#### Преимущества дропаута:  \n",
        "- Эффективно предотвращает переобучение.  \n",
        "- Прост в реализации и не требует дополнительных параметров.  \n",
        "- Может использоваться совместно с другими методами регуляризации.  \n",
        "\n",
        "\n",
        "\n",
        "## 5.3 Сравнение нормализации и дропаута  \n",
        "\n",
        "| Характеристика          | Нормализация (BN)                     | Дропаут                          |\n",
        "|-||-|\n",
        "| **Цель**                | Стабилизация распределения данных     | Предотвращение переобучения      |\n",
        "| **Принцип работы**      | Нормализация по мини-батчу           | Случайное отключение нейронов    |\n",
        "| **Параметры**           | $\\gamma$, $\\beta$             | Вероятность $p$              |\n",
        "| **Влияние на обучение** | Ускорение сходимости                  | Снижение переобучения            |\n",
        "| **Применение**          | На каждом слое                       | На скрытых слоях                 |\n",
        "\n",
        "\n",
        "\n",
        "## 5.4 Совместное использование  \n",
        "\n",
        "Нормализация и дропаут могут использоваться вместе. Например, в современных архитектурах нейронных сетей (например, ResNet) часто применяется Batch Normalization после каждого свёрточного слоя, а дропаут — в полносвязных слоях.  \n",
        "\n",
        "### Пример совместного использования:  \n",
        "1. Применяем Batch Normalization:  \n",
        "$$\n",
        "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta.\n",
        "$$  \n",
        "2. Применяем дропаут:  \n",
        "$$\n",
        "   y_i' =\n",
        "   \\begin{cases}\n",
        "   \\frac{y_i}{p}, & \\text{с вероятностью } p, \\\\\n",
        "   0, & \\text{с вероятностью } 1-p.\n",
        "   \\end{cases}\n",
        "$$  \n",
        "\n",
        "\n",
        "\n",
        "Таким образом, нормализация и дропаут — это два мощных инструмента, которые решают разные задачи в обучении нейронных сетей. Нормализация стабилизирует распределение данных и ускоряет обучение, а дропаут предотвращает переобучение, добавляя случайность в процесс обучения. Их совместное использование позволяет создавать более устойчивые и эффективные модели."
      ],
      "metadata": {
        "id": "xFaUNeR5td0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем Batch Normalization и Dropout на Python с использованием библиотеки NumPy. Это поможет лучше понять, как эти методы работают на практике.\n",
        "\n",
        "Реализация Batch Normalization"
      ],
      "metadata": {
        "id": "tB6ZViQwuqjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BatchNormalization:\n",
        "    def __init__(self, epsilon=1e-5, momentum=0.9):\n",
        "        self.epsilon = epsilon  # Малая константа для численной стабильности\n",
        "        self.momentum = momentum  # Параметр для скользящего среднего\n",
        "        self.running_mean = None  # Скользящее среднее для среднего значения\n",
        "        self.running_var = None  # Скользящее среднее для дисперсии\n",
        "        self.gamma = None  # Параметр масштаба\n",
        "        self.beta = None  # Параметр сдвига\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            # Вычисляем среднее и дисперсию по мини-батчу\n",
        "            mean = np.mean(x, axis=0)\n",
        "            var = np.var(x, axis=0)\n",
        "\n",
        "            # Обновляем скользящее среднее\n",
        "            if self.running_mean is None:\n",
        "                self.running_mean = mean\n",
        "                self.running_var = var\n",
        "            else:\n",
        "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "\n",
        "            # Нормализация\n",
        "            x_normalized = (x - mean) / np.sqrt(var + self.epsilon)\n",
        "\n",
        "            # Инициализация gamma и beta, если они не были инициализированы\n",
        "            if self.gamma is None:\n",
        "                self.gamma = np.ones(x.shape[1])\n",
        "                self.beta = np.zeros(x.shape[1])\n",
        "\n",
        "            # Масштабирование и сдвиг\n",
        "            out = self.gamma * x_normalized + self.beta\n",
        "        else:\n",
        "            # На этапе тестирования используем скользящее среднее\n",
        "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "            out = self.gamma * x_normalized + self.beta\n",
        "\n",
        "        return out\n",
        "\n",
        "# Пример использования\n",
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float32)\n",
        "bn = BatchNormalization()\n",
        "print(\"Batch Normalization (Training):\")\n",
        "print(bn.forward(x, training=True))\n",
        "print(\"Batch Normalization (Testing):\")\n",
        "print(bn.forward(x, training=False))"
      ],
      "metadata": {
        "id": "z7iIIy_Aul16",
        "outputId": "ef08d536-a1a8-4931-a673-f24793ef34d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Normalization (Training):\n",
            "[[-1.34163547 -1.34163547]\n",
            " [-0.44721183 -0.44721183]\n",
            " [ 0.44721183  0.44721183]\n",
            " [ 1.34163547  1.34163547]]\n",
            "Batch Normalization (Testing):\n",
            "[[-1.34163547 -1.34163547]\n",
            " [-0.44721183 -0.44721183]\n",
            " [ 0.44721183  0.44721183]\n",
            " [ 1.34163547  1.34163547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация Dropout"
      ],
      "metadata": {
        "id": "F-OlQafkus_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p  # Вероятность того, что нейрон останется активным\n",
        "        self.mask = None  # Маска для отключения нейронов\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            # Генерируем маску с вероятностью p\n",
        "            self.mask = (np.random.rand(*x.shape) < self.p) / self.p\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            # На этапе тестирования все нейроны активны\n",
        "            return x\n",
        "\n",
        "# Пример использования\n",
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float32)\n",
        "dropout = Dropout(p=0.5)\n",
        "print(\"Dropout (Training):\")\n",
        "print(dropout.forward(x, training=True))\n",
        "print(\"Dropout (Testing):\")\n",
        "print(dropout.forward(x, training=False))"
      ],
      "metadata": {
        "id": "t8vQY-enuvhm",
        "outputId": "75797b8d-175f-4076-db73-e37293ad2aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout (Training):\n",
            "[[2. 0.]\n",
            " [0. 0.]\n",
            " [6. 0.]\n",
            " [0. 0.]]\n",
            "Dropout (Testing):\n",
            "[[1. 2.]\n",
            " [2. 3.]\n",
            " [3. 4.]\n",
            " [4. 5.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Совместное использование Batch Normalization и Dropout\n",
        "\n",
        "Теперь объединим оба метода в одной модели:"
      ],
      "metadata": {
        "id": "l8YJFCw5uxUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.bn = BatchNormalization()\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        # Применяем Batch Normalization\n",
        "        x = self.bn.forward(x, training=training)\n",
        "        # Применяем Dropout\n",
        "        x = self.dropout.forward(x, training=training)\n",
        "        return x\n",
        "\n",
        "# Пример использования\n",
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5]], dtype=np.float32)\n",
        "model = NeuralNetwork()\n",
        "print(\"Neural Network (Training):\")\n",
        "print(model.forward(x, training=True))\n",
        "print(\"Neural Network (Testing):\")\n",
        "print(model.forward(x, training=False))"
      ],
      "metadata": {
        "id": "8ymzs-7lu4rX",
        "outputId": "f991cbfb-e911-4417-c522-6a4e906e60af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network (Training):\n",
            "[[-0.         -0.        ]\n",
            " [-0.89442366 -0.89442366]\n",
            " [ 0.          0.89442366]\n",
            " [ 2.68327093  0.        ]]\n",
            "Neural Network (Testing):\n",
            "[[-1.34163547 -1.34163547]\n",
            " [-0.44721183 -0.44721183]\n",
            " [ 0.44721183  0.44721183]\n",
            " [ 1.34163547  1.34163547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Инициализация весов\n",
        "\n",
        "**Важное замечание:**  \n",
        "Необходимо доработать задачу, связанную с использованием функции активации ReLU. В данном случае рассматривается симметричная функция активации. Для инициализации весов предлагается использовать равномерное распределение:  \n",
        "$$w_i \\sim U\\left[-\\frac{1}{\\sqrt{n_{in}}}, \\frac{1}{\\sqrt{n_{in}}}\\right],$$  \n",
        "где $n_{in}$ — количество входных нейронов слоя.\n",
        "\n",
        "\n",
        "\n",
        "### а) Покажите, что это будет приводить к затуханию дисперсии при переходе от одного слоя к другому.\n",
        "\n",
        "**Решение:**  \n",
        "Найдём дисперсию веса $w_i$. Для равномерного распределения $U[a, b]$ дисперсия вычисляется по формуле:  \n",
        "$$\\text{Var}(w_i) = \\frac{(b - a)^2}{12}.$$  \n",
        "Подставляя параметры распределения $a = -\\frac{1}{\\sqrt{n_{in}}}$ и $b = \\frac{1}{\\sqrt{n_{in}}}$, получаем:  \n",
        "$$\\text{Var}(w_i) = \\frac{\\left(\\frac{1}{\\sqrt{n_{in}}} - \\left(-\\frac{1}{\\sqrt{n_{in}}}\\right)\\right)^2}{12} = \\frac{\\left(\\frac{2}{\\sqrt{n_{in}}}\\right)^2}{12} = \\frac{4}{12 \\cdot n_{in}} = \\frac{1}{3 \\cdot n_{in}}.$$\n",
        "\n",
        "Рассмотрим линейный слой с симметричной функцией активации. Предполагается, что веса инициализируются независимо друг от друга и от входных данных, а входные данные также независимы. Тогда дисперсия выхода слоя $h_i$ вычисляется следующим образом:  \n",
        "$$\\text{Var}(h_i) = \\text{Var}\\left(\\sum_{i=1}^{n_{in}} w_i x_i\\right) = \\sum_{i=1}^{n_{in}} \\text{Var}(w_i \\cdot x_i).$$  \n",
        "Используя формулу для дисперсии произведения независимых случайных величин, получаем:  \n",
        "$$\\text{Var}(w_i \\cdot x_i) = \\mathbb{E}^2(x_i) \\cdot \\text{Var}(w_i) + \\text{Var}(x_i) \\cdot \\mathbb{E}^2(w_i) + \\text{Var}(x_i) \\cdot \\text{Var}(w_i).$$  \n",
        "Поскольку функция активации симметрична, математическое ожидание входных данных равно нулю: $\\mathbb{E}(x_i) = 0$. Также предполагается, что веса инициализируются с нулевым средним: $\\mathbb{E}(w_i) = 0$. Тогда формула упрощается:  \n",
        "$$\\text{Var}(w_i \\cdot x_i) = \\text{Var}(x_i) \\cdot \\text{Var}(w_i).$$  \n",
        "Следовательно, дисперсия выхода слоя равна:  \n",
        "$$\\text{Var}(h_i) = \\sum_{i=1}^{n_{in}} \\text{Var}(x_i) \\cdot \\text{Var}(w_i) = n_{in} \\cdot \\text{Var}(x_i) \\cdot \\text{Var}(w_i).$$  \n",
        "Подставляя значение $\\text{Var}(w_i) = \\frac{1}{3 \\cdot n_{in}}$, получаем:  \n",
        "$$\\text{Var}(h_i) = n_{in} \\cdot \\text{Var}(x_i) \\cdot \\frac{1}{3 \\cdot n_{in}} = \\frac{1}{3} \\cdot \\text{Var}(x_i).$$  \n",
        "Таким образом, дисперсия уменьшается в три раза при переходе от выхода предыдущего слоя к выходу нового слоя.\n",
        "\n",
        "\n",
        "\n",
        "### б) Какими нужно взять параметры равномерного распределения, чтобы дисперсия не затухала?\n",
        "\n",
        "**Решение:**  \n",
        "Для того чтобы дисперсия не затухала, необходимо, чтобы $\\text{Var}(h_i) = \\text{Var}(x_i)$. Из предыдущего пункта следует, что для этого требуется:  \n",
        "$$n_{in} \\cdot \\text{Var}(x_i) \\cdot \\text{Var}(w_i) = \\text{Var}(x_i).$$  \n",
        "Отсюда получаем условие на дисперсию весов:  \n",
        "$$\\text{Var}(w_i) = \\frac{1}{n_{in}}.$$  \n",
        "Для равномерного распределения $U[a, b]$ дисперсия равна $\\frac{(b - a)^2}{12}$. Подставляя $\\text{Var}(w_i) = \\frac{1}{n_{in}}$, получаем:  \n",
        "$$\\frac{(b - a)^2}{12} = \\frac{1}{n_{in}}.$$  \n",
        "Если выбрать $a = -\\sqrt{\\frac{3}{n_{in}}}$ и $b = \\sqrt{\\frac{3}{n_{in}}}$, то:  \n",
        "$$\\frac{\\left(\\sqrt{\\frac{3}{n_{in}}} - \\left(-\\sqrt{\\frac{3}{n_{in}}}\\right)\\right)^2}{12} = \\frac{\\left(2 \\sqrt{\\frac{3}{n_{in}}}\\right)^2}{12} = \\frac{12}{12 \\cdot n_{in}} = \\frac{1}{n_{in}}.$$  \n",
        "Таким образом, подходящее равномерное распределение:  \n",
        "$$w_i \\sim U\\left[-\\sqrt{\\frac{3}{n_{in}}}, \\sqrt{\\frac{3}{n_{in}}}\\right].$$\n",
        "\n",
        "\n",
        "\n",
        "### в) Инициализация весов из нормального распределения\n",
        "\n",
        "**Решение:**  \n",
        "Для нормального распределения $N(\\mu, \\sigma^2)$ дисперсия равна $\\sigma^2$. Чтобы дисперсия не затухала, необходимо, чтобы:  \n",
        "$$\\text{Var}(w_i) = \\frac{1}{n_{in}}.$$  \n",
        "Следовательно, параметры нормального распределения должны быть:  \n",
        "$$w_i \\sim N\\left(0, \\frac{1}{n_{in}}\\right).$$\n",
        "\n",
        "\n",
        "\n",
        "### г) Компромисс между прямым и обратным распространением ошибки\n",
        "\n",
        "**Решение:**  \n",
        "При прямом распространении ошибки на вход нейрона поступает $n_{in}$ слагаемых, а при обратном распространении — $n_{out}$ градиентов. Количество весов между слоями может значительно варьироваться, что приводит к невозможности одновременно поддерживать неизменными дисперсии при прямом и обратном распространении.  \n",
        "\n",
        "Для компромисса предлагается инициализировать веса из распределения с дисперсией, учитывающей как количество входных, так и выходных нейронов:  \n",
        "$$\\text{Var}(w_i) = \\frac{2}{n_{in} + n_{out}}.$$  \n",
        "Такая инициализация называется **инициализацией Ксавие (Xavier)** или **инициализацией Глорота (Glorot)**. Она позволяет балансировать дисперсию при прямом и обратном распространении ошибки.  \n",
        "\n",
        "Для нормального распределения:  \n",
        "$$w_i \\sim N\\left(0, \\frac{2}{n_{in} + n_{out}}\\right).$$  \n",
        "Для равномерного распределения:  \n",
        "$$w_i \\sim U\\left[-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right].$$  \n",
        "\n",
        "\n",
        "\n",
        "[1] Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics* (pp. 249–256)."
      ],
      "metadata": {
        "id": "E6DGztlbu5dJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация инициализации весов на Python с использованием библиотеки `numpy` может быть выполнена следующим образом. Мы рассмотрим инициализацию весов для равномерного и нормального распределений, а также инициализацию Ксавие (Xavier/Glorot).\n",
        "\n",
        "\n",
        "### 1. Инициализация весов из равномерного распределения\n",
        "\n",
        "Для равномерного распределения $U[-a, a]$, где $a = \\frac{1}{\\sqrt{n_{in}}}$:\n",
        "\n"
      ],
      "metadata": {
        "id": "7RNdy8t9wosK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def uniform_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация весов из равномерного распределения.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    a = 1 / np.sqrt(n_in)  # Параметр для равномерного распределения\n",
        "    weights = np.random.uniform(-a, a, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "MEUjKqQfwcef"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Инициализация весов из нормального распределения\n",
        "\n",
        "Для нормального распределения $N(0, \\frac{1}{n_{in}})$:\n"
      ],
      "metadata": {
        "id": "56OqT2qewsbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация весов из нормального распределения.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    std_dev = 1 / np.sqrt(n_in)  # Стандартное отклонение\n",
        "    weights = np.random.normal(0, std_dev, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "PPIMd_x0wwdz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Инициализация Ксавие (Xavier/Glorot)\n",
        "\n",
        "Для инициализации Ксавие используется компромисс между количеством входных и выходных нейронов. Дисперсия весов выбирается как $\\frac{2}{n_{in} + n_{out}}$.\n",
        "\n",
        "#### а) Для нормального распределения:\n"
      ],
      "metadata": {
        "id": "Q08HCkZ3wzgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_normal_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация Ксавие (Xavier) с нормальным распределением.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    std_dev = np.sqrt(2 / (n_in + n_out))  # Стандартное отклонение\n",
        "    weights = np.random.normal(0, std_dev, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "kvkoCSmGw3K0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### б) Для равномерного распределения:\n"
      ],
      "metadata": {
        "id": "GdGt6XsOw5cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_uniform_initialization(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Инициализация Ксавие (Xavier) с равномерным распределением.\n",
        "\n",
        "    Параметры:\n",
        "    n_in (int): Количество входных нейронов.\n",
        "    n_out (int): Количество выходных нейронов.\n",
        "\n",
        "    Возвращает:\n",
        "    weights (np.ndarray): Матрица весов формы (n_in, n_out).\n",
        "    \"\"\"\n",
        "    limit = np.sqrt(6 / (n_in + n_out))  # Границы распределения\n",
        "    weights = np.random.uniform(-limit, limit, size=(n_in, n_out))\n",
        "    return weights"
      ],
      "metadata": {
        "id": "CPIZttmCw8as"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Пример использования\n"
      ],
      "metadata": {
        "id": "q_x3Bks7w_2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры слоя\n",
        "n_in = 100  # Количество входных нейронов\n",
        "n_out = 50  # Количество выходных нейронов\n",
        "\n",
        "# Инициализация весов\n",
        "weights_uniform = uniform_initialization(n_in, n_out)\n",
        "weights_normal = normal_initialization(n_in, n_out)\n",
        "weights_xavier_normal = xavier_normal_initialization(n_in, n_out)\n",
        "weights_xavier_uniform = xavier_uniform_initialization(n_in, n_out)\n",
        "\n",
        "# Проверка формы матрицы весов\n",
        "print(\"Uniform Initialization Shape:\", weights_uniform.shape)\n",
        "print(\"Normal Initialization Shape:\", weights_normal.shape)\n",
        "print(\"Xavier Normal Initialization Shape:\", weights_xavier_normal.shape)\n",
        "print(\"Xavier Uniform Initialization Shape:\", weights_xavier_uniform.shape)"
      ],
      "metadata": {
        "id": "G1VsTFHHxELx",
        "outputId": "1235a093-67f7-4a08-b456-585330e4d1f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform Initialization Shape: (100, 50)\n",
            "Normal Initialization Shape: (100, 50)\n",
            "Xavier Normal Initialization Shape: (100, 50)\n",
            "Xavier Uniform Initialization Shape: (100, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Проверка дисперсии\n",
        "\n",
        "Для проверки дисперсии весов можно использовать следующий код:\n"
      ],
      "metadata": {
        "id": "WnuL_gIExGFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_variance(weights):\n",
        "    \"\"\"\n",
        "    Проверка дисперсии весов.\n",
        "\n",
        "    Параметры:\n",
        "    weights (np.ndarray): Матрица весов.\n",
        "\n",
        "    Возвращает:\n",
        "    variance (float): Дисперсия весов.\n",
        "    \"\"\"\n",
        "    return np.var(weights)\n",
        "\n",
        "# Проверка дисперсии для каждого метода инициализации\n",
        "print(\"Uniform Initialization Variance:\", check_variance(weights_uniform))\n",
        "print(\"Normal Initialization Variance:\", check_variance(weights_normal))\n",
        "print(\"Xavier Normal Initialization Variance:\", check_variance(weights_xavier_normal))\n",
        "print(\"Xavier Uniform Initialization Variance:\", check_variance(weights_xavier_uniform))"
      ],
      "metadata": {
        "id": "3zzwMBAexLMm",
        "outputId": "50af2ea5-5d0b-4d99-a246-e385b3210777",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform Initialization Variance: 0.003266599757793463\n",
            "Normal Initialization Variance: 0.010020046718884046\n",
            "Xavier Normal Initialization Variance: 0.012923777250064506\n",
            "Xavier Uniform Initialization Variance: 0.013435334909320308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Результаты\n",
        "\n",
        "- Для равномерной инициализации дисперсия будет близка к $\\frac{1}{3 \\cdot n_{in}}$.\n",
        "- Для нормальной инициализации дисперсия будет близка к $\\frac{1}{n_{in}}$.\n",
        "- Для инициализации Ксавие дисперсия будет близка к $\\frac{2}{n_{in} + n_{out}}$.\n",
        "\n",
        "Эти методы инициализации помогают избежать проблем с затуханием или взрывом градиентов при обучении нейронных сетей."
      ],
      "metadata": {
        "id": "47t5uoRjxNtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bpgWhiKYxS_l"
      }
    }
  ]
}