{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqF344gcWAbJ9foajjQOh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/DL/8.%20Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Seq2Seq\n",
        "\n",
        "##Введение в Seq2Seq и Encoder\n",
        "\n",
        "Модель **Seq2Seq** (Sequence-to-Sequence) представляет собой архитектуру нейронных сетей, которая предназначена для обработки последовательностей данных. Она состоит из двух основных компонентов: **Encoder** и **Decoder**. В этой статье мы сосредоточимся на первом компоненте — **Encoder**, который отвечает за преобразование входной последовательности в фиксированный контекстный вектор.\n",
        "\n",
        "#### Основная цель Encoder'а\n",
        "Основная задача **Encoder'а** заключается в том, чтобы преобразовать входную последовательность (например, предложение или последовательность символов) в некоторое внутреннее представление, которое можно использовать для дальнейшей обработки. Это внутреннее представление часто называют **контекстным вектором** или **скрытым состоянием**.\n",
        "\n",
        "### Архитектура Encoder'а\n",
        "\n",
        "Типичный **Encoder** основан на рекуррентной нейронной сети (RNN), либо её модификациях, таких как LSTM (Long Short-Term Memory) или GRU (Gated Recurrent Unit). Эти модели специально разработаны для работы с последовательностями данных, где текущий элемент зависит от предыдущих.\n",
        "\n",
        "#### 1. Входные данные\n",
        "Пусть у нас есть входная последовательность $\\mathbf{x} = [x_1, x_2, \\dots, x_T]$, где каждый элемент $x_t$ является словом или символом, представленным в виде вектора признаков (например, эмбеддинга). Длина последовательности равна $T$.\n",
        "\n",
        "#### 2. Преобразование входных данных\n",
        "Каждый элемент $x_t$ подается на вход RNN/LSTM/GRU по очереди. На каждом временном шаге $t$, модель вычисляет новое скрытое состояние $h_t$, которое зависит как от текущего входного элемента $x_t$, так и от предыдущего скрытого состояния $h_{t-1}$.\n",
        "\n",
        "Формально, процесс вычисления скрытого состояния на шаге $t$ можно записать следующим образом:\n",
        "\n",
        "$$\n",
        "h_t = f(h_{t-1}, x_t)\n",
        "$$\n",
        "\n",
        "где $f$ — это функция перехода, определяемая архитектурой RNN/LSTM/GRU.\n",
        "\n",
        "##### Пример для простой RNN:\n",
        "Для простой рекуррентной нейронной сети функция перехода имеет вид:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)\n",
        "$$\n",
        "\n",
        "где $W_h$ и $W_x$ — матрицы весов, $b_h$ — смещение (bias), а $\\tanh$ — нелинейная функция активации.\n",
        "\n",
        "##### Пример для LSTM:\n",
        "Для LSTM функция перехода более сложная и включает дополнительные параметры, такие как \"входные ворота\" ($i_t$), \"забывающие ворота\" ($f_t$) и \"выходные ворота\" ($o_t$):\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
        "$$\n",
        "$$\n",
        "f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
        "$$\n",
        "$$\n",
        "o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
        "$$\n",
        "$$\n",
        "\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)\n",
        "$$\n",
        "$$\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "$$\n",
        "h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "Здесь $\\sigma$ — сигмоидная функция активации, $\\odot$ — поэлементное произведение, а $c_t$ — ячейка памяти LSTM.\n",
        "\n",
        "### Контекстный вектор\n",
        "\n",
        "После того как все элементы входной последовательности были обработаны, последнее скрытое состояние $h_T$ (или ячейка памяти $c_T$ для LSTM) используется как **контекстный вектор**. Этот вектор содержит информацию о всей входной последовательности и передается дальше в **Decoder**, который использует его для генерации выходной последовательности.\n",
        "\n",
        "В некоторых случаях вместо использования только последнего скрытого состояния $h_T$ применяется метод усреднения всех скрытых состояний $h_1, h_2, \\dots, h_T$:\n",
        "\n",
        "$$\n",
        "\\mathbf{c} = \\frac{1}{T} \\sum_{t=1}^T h_t\n",
        "$$\n",
        "\n",
        "Этот подход позволяет получить более репрезентативное представление всей последовательности.\n",
        "\n",
        "### Математическое объяснение процесса кодирования\n",
        "\n",
        "Рассмотрим подробно, как работает кодирование для простой RNN:\n",
        "\n",
        "1. **Инициализация**: Скрытые состояния инициализируются начальным значением $h_0$, обычно это нулевой вектор.\n",
        "   \n",
        "$$\n",
        "   h_0 = \\mathbf{0}\n",
        "$$\n",
        "\n",
        "2. **Вычисление скрытых состояний**:\n",
        "   Для каждого временного шага $t$ выполняется следующее:\n",
        "\n",
        "$$\n",
        "   h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)\n",
        "$$\n",
        "\n",
        "   Здесь $W_h$ и $W_x$ — матрицы весов, которые обучаемы, $b_h$ — смещение. Функция $\\tanh$ обеспечивает нелинейность, что позволяет модели запоминать более сложные зависимости между элементами последовательности.\n",
        "\n",
        "3. **Контекстный вектор**:\n",
        "   После завершения обработки всех элементов последовательности, последнее скрытое состояние $h_T$ используется как контекстный вектор:\n",
        "\n",
        "$$\n",
        "   \\mathbf{c} = h_T\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, **Encoder** в модели **Seq2Seq** играет ключевую роль в преобразовании входной последовательности в фиксированное представление, которое затем используется **Decoder'ом** для генерации выходной последовательности. Основной принцип работы Encoder'а заключается в использовании рекуррентных структур (RNN, LSTM, GRU) для обработки последовательности и вычисления скрытых состояний, которые содержат информацию о всей последовательности.\n",
        "\n",
        "Эта информация затем передается в **Decoder**, который будет рассмотрен в следующей части.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Введение в Decoder Seq2Seq\n",
        "\n",
        "**Decoder** — это второй компонент модели **Seq2Seq**, который отвечает за генерацию выходной последовательности на основе контекстного вектора, полученного от **Encoder'а**. Как и Encoder, Decoder также обычно основан на рекуррентных нейронных сетях (RNN), LSTM или GRU.\n",
        "\n",
        "#### Основная цель Decoder'а\n",
        "Основная задача **Decoder'а** заключается в том, чтобы преобразовать контекстный вектор (или скрытое состояние) из **Encoder'а** в выходную последовательность. Эта последовательность может быть, например, переводом исходного предложения, ответом на вопрос или продолжением текста.\n",
        "\n",
        "### Архитектура Decoder'а\n",
        "\n",
        "Для простоты объяснения будем рассматривать Decoder на основе RNN, но все описанные принципы применимы и к более сложным архитектурам, таким как LSTM и GRU.\n",
        "\n",
        "#### 1. Входные данные\n",
        "Пусть у нас есть входная последовательность $\\mathbf{x} = [x_1, x_2, \\dots, x_T]$, которая была обработана **Encoder'ом** и преобразована в контекстный вектор $\\mathbf{c}$. Теперь наша задача — сгенерировать выходную последовательность $\\mathbf{y} = [y_1, y_2, \\dots, y_{T'}]$, где $T'$ — длина выходной последовательности.\n",
        "\n",
        "#### 2. Инициализация Decoder'а\n",
        "Перед началом работы **Decoder'а**, его начальное скрытое состояние $s_0$ инициализируется контекстным вектором $\\mathbf{c}$:\n",
        "\n",
        "$$\n",
        "s_0 = \\mathbf{c}\n",
        "$$\n",
        "\n",
        "В некоторых моделях используется комбинация контекстного вектора и начального скрытого состояния:\n",
        "\n",
        "$$\n",
        "s_0 = \\tanh(W_s [\\mathbf{c}, h_0])\n",
        "$$\n",
        "\n",
        "где $W_s$ — обучаемая матрица весов, а $h_0$ — начальное скрытое состояние, которое может быть нулевым вектором.\n",
        "\n",
        "#### 3. Генерация выходной последовательности\n",
        "На каждом временном шаге $t$, **Decoder** принимает текущее скрытое состояние $s_{t-1}$ и предыдущий выходной символ $y_{t-1}$ (на первом шаге это может быть специальный токен `<start>`), и вычисляет новое скрытое состояние $s_t$ и вероятности для всех возможных следующих символов.\n",
        "\n",
        "Формально, процесс можно записать следующим образом:\n",
        "\n",
        "$$\n",
        "s_t = f(s_{t-1}, y_{t-1})\n",
        "$$\n",
        "\n",
        "где $f$ — функция перехода, определяемая архитектурой RNN/LSTM/GRU.\n",
        "\n",
        "##### Пример для простой RNN:\n",
        "Для простой рекуррентной нейронной сети функция перехода имеет вид:\n",
        "\n",
        "$$\n",
        "s_t = \\tanh(W_s s_{t-1} + W_y y_{t-1} + b_s)\n",
        "$$\n",
        "\n",
        "где $W_s$ и $W_y$ — матрицы весов, $b_s$ — смещение (bias).\n",
        "\n",
        "##### Пример для LSTM:\n",
        "Для LSTM функция перехода более сложная и включает дополнительные параметры, такие как \"входные ворота\" ($i_t$), \"забывающие ворота\" ($f_t$) и \"выходные ворота\" ($o_t$):\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_i [s_{t-1}, y_{t-1}] + b_i)\n",
        "$$\n",
        "$$\n",
        "f_t = \\sigma(W_f [s_{t-1}, y_{t-1}] + b_f)\n",
        "$$\n",
        "$$\n",
        "o_t = \\sigma(W_o [s_{t-1}, y_{t-1}] + b_o)\n",
        "$$\n",
        "$$\n",
        "\\tilde{c}_t = \\tanh(W_c [s_{t-1}, y_{t-1}] + b_c)\n",
        "$$\n",
        "$$\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "$$\n",
        "s_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "Здесь $\\sigma$ — сигмоидная функция активации, $\\odot$ — поэлементное произведение, а $c_t$ — ячейка памяти LSTM.\n",
        "\n",
        "#### 4. Вычисление вероятностей выходных символов\n",
        "После того как новое скрытое состояние $s_t$ было вычислено, **Decoder** использует его для вычисления вероятностей всех возможных следующих символов. Обычно это делается с помощью линейного слоя и softmax функции:\n",
        "\n",
        "$$\n",
        "\\hat{y}_t = \\text{softmax}(W_o s_t + b_o)\n",
        "$$\n",
        "\n",
        "где $W_o$ — матрица весов, $b_o$ — смещение, а $\\hat{y}_t$ — вектор вероятностей для всех возможных символов.\n",
        "\n",
        "#### 5. Выбор следующего символа\n",
        "На каждом временном шаге выбирается следующий символ $y_t$ либо как наиболее вероятный символ (greedy search):\n",
        "\n",
        "$$\n",
        "y_t = \\arg\\max(\\hat{y}_t)\n",
        "$$\n",
        "\n",
        "либо с использованием других стратегий выбора, таких как beam search или sampling.\n",
        "\n",
        "### Улучшенные методы: Attention Mechanism\n",
        "\n",
        "Одним из важных улучшений модели **Seq2Seq** является механизм внимания (**Attention Mechanism**). Он позволяет Decoder'у не только использовать фиксированный контекстный вектор, но и обращаться к различным частям входной последовательности на каждом временном шаге.\n",
        "\n",
        "#### Механизм внимания\n",
        "Механизм внимания добавляет дополнительный шаг к работе **Decoder'а**, позволяющий ему \"смотреть\" на различные части входной последовательности при генерации каждого нового символа.\n",
        "\n",
        "1. **Вычисление весов внимания**: Для каждого временного шага $t$ **Decoder** вычисляет веса внимания $\\alpha_{t,i}$ для каждой позиции $i$ входной последовательности:\n",
        "\n",
        "$$\n",
        "   e_{t,i} = v^T \\tanh(W_h h_i + W_s s_{t-1} + b_a)\n",
        "$$\n",
        "   \n",
        "$$\n",
        "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^T \\exp(e_{t,j})}\n",
        "$$\n",
        "\n",
        "   Здесь $h_i$ — скрытое состояние **Encoder'а** на позиции $i$, $s_{t-1}$ — скрытое состояние **Decoder'а** на предыдущем шаге, а $v$, $W_h$, $W_s$ и $b_a$ — обучаемые параметры.\n",
        "\n",
        "2. **Вычисление контекстного вектора**: На основе весов внимания вычисляется новый контекстный вектор $\\mathbf{c}_t$:\n",
        "\n",
        "$$\n",
        "   \\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} h_i\n",
        "$$\n",
        "\n",
        "3. **Объединение контекстного вектора с текущим скрытым состоянием**: Контекстный вектор $\\mathbf{c}_t$ объединяется с текущим скрытым состоянием $s_{t-1}$ для вычисления нового скрытого состояния $s_t$:\n",
        "\n",
        "$$\n",
        "   s_t = f(s_{t-1}, y_{t-1}, \\mathbf{c}_t)\n",
        "$$\n",
        "\n",
        "4. **Вычисление вероятностей выходных символов**: Аналогично базовой модели, вычисляются вероятности для всех возможных символов:\n",
        "\n",
        "$$\n",
        "   \\hat{y}_t = \\text{softmax}(W_o [s_t, \\mathbf{c}_t] + b_o)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, **Decoder** в модели **Seq2Seq** играет ключевую роль в генерации выходной последовательности на основе контекстного вектора, полученного от **Encoder'а**. Основной принцип работы Decoder'а заключается в использовании рекуррентных структур (RNN, LSTM, GRU) для пошаговой генерации символов и вычисления вероятностей для всех возможных следующих символов.\n",
        "\n",
        "Использование механизма внимания значительно улучшает качество модели, позволяя Decoder'у \"смотреть\" на различные части входной последовательности при генерации каждого нового символа. Это особенно полезно для длинных и сложных последовательностей, где фиксированный контекстный вектор может не содержать всей необходимой информации.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Конкретный числовой пример для Seq2Seq с Encoder и Decoder\n",
        "\n",
        "Для простоты будем рассматривать модель на основе простых рекуррентных нейронных сетей (RNN) без механизма внимания. Предположим, что у нас есть входная последовательность $\\mathbf{x} = [x_1, x_2]$ и мы хотим сгенерировать выходную последовательность $\\mathbf{y} = [y_1, y_2]$.\n",
        "\n",
        "#### Входные данные\n",
        "Пусть наша входная последовательность состоит из двух элементов: $x_1 = 1$ и $x_2 = 2$. Эти элементы представлены в виде эмбеддингов:\n",
        "\n",
        "$$\n",
        "x_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}, \\quad x_2 = \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "##### Инициализация скрытого состояния\n",
        "Начальное скрытое состояние $h_0$ равно нулевому вектору:\n",
        "\n",
        "$$\n",
        "h_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление скрытых состояний\n",
        "Для простой RNN функция перехода имеет вид:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)\n",
        "$$\n",
        "\n",
        "Предположим, что матрицы весов и смещения имеют следующие значения:\n",
        "\n",
        "$$\n",
        "W_h = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix}, \\quad W_x = \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix}, \\quad b_h = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 1 (t=1):**\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh \\left( W_h h_0 + W_x x_1 + b_h \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh \\left( \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.3 \\cdot 0.1 + 0.4 \\cdot 0.2 \\\\ 0.1 \\cdot 0.1 + 0.2 \\cdot 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.11 \\\\ 0.05 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh \\left( \\begin{bmatrix} 0.11 \\\\ 0.05 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.21 \\\\ 0.15 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "h_1 = \\begin{bmatrix} \\tanh(0.21) \\\\ \\tanh(0.15) \\end{bmatrix} \\approx \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 2 (t=2):**\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh \\left( W_h h_1 + W_x x_2 + b_h \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh \\left( \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix} \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} + \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix} \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} = \\begin{bmatrix} 0.5 \\cdot 0.207 + 0.1 \\cdot 0.149 \\\\ 0.2 \\cdot 0.207 + 0.6 \\cdot 0.149 \\end{bmatrix} = \\begin{bmatrix} 0.1184 \\\\ 0.1314 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix} = \\begin{bmatrix} 0.3 \\cdot 0.3 + 0.4 \\cdot 0.4 \\\\ 0.1 \\cdot 0.3 + 0.2 \\cdot 0.4 \\end{bmatrix} = \\begin{bmatrix} 0.25 \\\\ 0.11 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh \\left( \\begin{bmatrix} 0.1184 \\\\ 0.1314 \\end{bmatrix} + \\begin{bmatrix} 0.25 \\\\ 0.11 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.4684 \\\\ 0.3414 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "h_2 = \\begin{bmatrix} \\tanh(0.4684) \\\\ \\tanh(0.3414) \\end{bmatrix} \\approx \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Контекстный вектор $\\mathbf{c}$ будет равен последнему скрытому состоянию $h_2$:\n",
        "\n",
        "$$\n",
        "\\mathbf{c} = \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "##### Инициализация скрытого состояния\n",
        "Начальное скрытое состояние $s_0$ инициализируется контекстным вектором $\\mathbf{c}$:\n",
        "\n",
        "$$\n",
        "s_0 = \\mathbf{c} = \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление скрытых состояний и вероятностей\n",
        "Для простоты предположим, что у нас есть два возможных символа $y_1$ и $y_2$, которые могут быть сгенерированы на каждом шаге.\n",
        "\n",
        "Матрицы весов и смещения для Decoder'а:\n",
        "\n",
        "$$\n",
        "W_s = \\begin{bmatrix} 0.4 & 0.3 \\\\ 0.2 & 0.5 \\end{bmatrix}, \\quad W_y = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix}, \\quad b_s = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Матрица весов для вычисления вероятностей:\n",
        "\n",
        "$$\n",
        "W_o = \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix}, \\quad b_o = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 1 (t=1):**\n",
        "\n",
        "На первом шаге используется специальный токен `<start>` с эмбеддингом $y_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$:\n",
        "\n",
        "$$\n",
        "s_1 = \\tanh \\left( W_s s_0 + W_y y_0 + b_s \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "s_1 = \\tanh \\left( \\begin{bmatrix} 0.4 & 0.3 \\\\ 0.2 & 0.5 \\end{bmatrix} \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix} + \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.4 & 0.3 \\\\ 0.2 & 0.5 \\end{bmatrix} \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix} = \\begin{bmatrix} 0.4 \\cdot 0.438 + 0.3 \\cdot 0.331 \\\\ 0.2 \\cdot 0.438 + 0.5 \\cdot 0.331 \\end{bmatrix} = \\begin{bmatrix} 0.2829 \\\\ 0.2481 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "s_1 = \\tanh \\left( \\begin{bmatrix} 0.2829 \\\\ 0.2481 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.3829 \\\\ 0.3481 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "s_1 = \\begin{bmatrix} \\tanh(0.3829) \\\\ \\tanh(0.3481) \\end{bmatrix} \\approx \\begin{bmatrix} 0.365 \\\\ 0.335 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем вероятности для всех возможных символов:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\text{softmax} \\left( W_o s_1 + b_o \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\text{softmax} \\left( \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.365 \\\\ 0.335 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.365 \\\\ 0.335 \\end{bmatrix} = \\begin{bmatrix} 0.5 \\cdot 0.365 + 0.6 \\cdot 0.335 \\\\ 0.7 \\cdot 0.365 + 0.8 \\cdot 0.335 \\end{bmatrix} = \\begin{bmatrix} 0.3845 \\\\ 0.5015 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\text{softmax} \\left( \\begin{bmatrix} 0.3845 \\\\ 0.5015 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\text{softmax} \\left( \\begin{bmatrix} 0.4845 \\\\ 0.6015 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию softmax:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\begin{bmatrix} \\frac{\\exp(0.4845)}{\\exp(0.4845) + \\exp(0.6015)} \\\\ \\frac{\\exp(0.6015)}{\\exp(0.4845) + \\exp(0.6015)} \\end{bmatrix} \\approx \\begin{bmatrix} 0.467 \\\\ 0.533 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выбираем наиболее вероятный символ $y_1$:\n",
        "\n",
        "$$\n",
        "y_1 = \\arg\\max(\\hat{y}_1) = 1\n",
        "$$\n",
        "\n",
        "**Шаг 2 (t=2):**\n",
        "\n",
        "Теперь используем символ $y_1$ с эмбеддингом $y_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$:\n",
        "\n",
        "$$\n",
        "s_2 = \\tanh \\left( W_s s_1 + W_y y_1 + b_s \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "s_2 = \\tanh \\left( \\begin{bmatrix} 0.4 & 0.3 \\\\ 0.2 & 0.5 \\end{bmatrix} \\begin{bmatrix} 0.365 \\\\ 0.335 \\end{bmatrix} + \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix} \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.4 & 0.3 \\\\ 0.2 & 0.5 \\end{bmatrix} \\begin{bmatrix} 0.365 \\\\ 0.335 \\end{bmatrix} = \\begin{bmatrix} 0.2829 \\\\ 0.2481 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix} \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.6 \\cdot 0.1 + 0.7 \\cdot 0.2 \\\\ 0.8 \\cdot 0.1 + 0.9 \\cdot 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.26 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "s_2 = \\tanh \\left( \\begin{bmatrix} 0.2829 \\\\ 0.2481 \\end{bmatrix} + \\begin{bmatrix} 0.2 \\\\ 0.26 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.5829 \\\\ 0.6081 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "s_2 = \\begin{bmatrix} \\tanh(0.5829) \\\\ \\tanh(0.6081) \\end{bmatrix} \\approx \\begin{bmatrix} 0.524 \\\\ 0.542 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем вероятности для всех возможных символов:\n",
        "\n",
        "$$\n",
        "\\hat{y}_2 = \\text{softmax} \\left( W_o s_2 + b_o \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "\\hat{y}_2 = \\text{softmax} \\left( \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.524 \\\\ 0.542 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.524 \\\\ 0.542 \\end{bmatrix} = \\begin{bmatrix} 0.5 \\cdot 0.524 + 0.6 \\cdot 0.542 \\\\ 0.7 \\cdot 0.524 + 0.8 \\cdot 0.542 \\end{bmatrix} = \\begin{bmatrix} 0.5732 \\\\ 0.8044 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "\\hat{y}_2 = \\text{softmax} \\left( \\begin{bmatrix} 0.5732 \\\\ 0.8044 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\text{softmax} \\left( \\begin{bmatrix} 0.6732 \\\\ 0.9044 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию softmax:\n",
        "\n",
        "$$\n",
        "\\hat{y}_2 = \\begin{bmatrix} \\frac{\\exp(0.6732)}{\\exp(0.6732) + \\exp(0.9044)} \\\\ \\frac{\\exp(0.9044)}{\\exp(0.6732) + \\exp(0.9044)} \\end{bmatrix} \\approx \\begin{bmatrix} 0.356 \\\\ 0.644 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выбираем наиболее вероятный символ $y_2$:\n",
        "\n",
        "$$\n",
        "y_2 = \\arg\\max(\\hat{y}_2) = 1\n",
        "$$\n",
        "\n",
        "Таким образом, в данном числовом примере мы показали, как работает **Encoder** и **Decoder** в модели **Seq2Seq**. Мы рассмотрели процесс кодирования входной последовательности в контекстный вектор и использование этого вектора для генерации выходной последовательности. Хотя этот пример основан на простых RNN, принципы остаются такими же для более сложных архитектур, таких как LSTM и GRU.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Механизм внимания в модели Seq2Seq\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Механизм внимания (Attention Mechanism) является одной из ключевых концепций в современных моделях глубокого обучения, особенно в задачах обработки естественного языка (NLP), машинного перевода и других задачах, связанных с последовательностями. Он позволяет модели \"смотреть\" на различные части входной последовательности при генерации каждого нового символа, что значительно улучшает качество модели.\n",
        "\n",
        "В этой лекции мы подробно рассмотрим основные принципы работы механизма внимания, его математическое описание и практические аспекты использования в модели **Seq2Seq**.\n",
        "\n",
        "### 1. Основные принципы механизма внимания\n",
        "\n",
        "#### Что такое внимание?\n",
        "\n",
        "Механизм внимания можно рассматривать как способ \"фокусировки\" модели на определённых частях входной последовательности при генерации выходной последовательности. В отличие от базовой модели **Seq2Seq**, где используется только фиксированный контекстный вектор, механизм внимания позволяет Decoder'у обращаться к различным частям входной последовательности на каждом временном шаге.\n",
        "\n",
        "#### Пример из жизни\n",
        "Для лучшего понимания представьте себе процесс перевода текста. Когда человек переводит предложение, он не просто запоминает весь исходный текст и затем пытается воспроизвести его на другом языке. Вместо этого, он последовательно читает каждое слово или фразу, анализирует её контекст и переводит её по мере продвижения. Это похоже на то, как работает механизм внимания в нейронных сетях.\n",
        "\n",
        "\n",
        "### 1.1. Проблема классических моделей Seq2Seq\n",
        "В классических моделях Seq2Seq Encoder преобразует входную последовательность в фиксированный контекстный вектор $c$, который передается Decoder. Однако у этого подхода есть два основных недостатка:\n",
        "1. **Потеря информации**: Фиксированный контекстный вектор может не сохранить всю информацию из длинных входных последовательностей.\n",
        "2. **Неспособность фокусироваться на важных частях**: Decoder не может динамически выбирать, какие части входной последовательности наиболее важны для генерации текущего элемента выходной последовательности.\n",
        "\n",
        "\n",
        "### 2. Математическое описание механизма внимания\n",
        "\n",
        "Рассмотрим подробно, как работает механизм внимания в модели **Seq2Seq**.\n",
        "\n",
        "#### Шаги механизма внимания\n",
        "\n",
        "1. **Вычисление весов внимания**: Для каждого временного шага $t$ **Decoder** вычисляет веса внимания $\\alpha_{t,i}$ для каждой позиции $i$ входной последовательности:\n",
        "\n",
        "$$\n",
        "   e_{t,i} = v^T \\tanh(W_h h_i + W_s s_{t-1} + b_a)\n",
        "$$\n",
        "\n",
        "   Здесь:\n",
        "   - $h_i$ — скрытое состояние **Encoder'а** на позиции $i$.\n",
        "   - $s_{t-1}$ — скрытое состояние **Decoder'а** на предыдущем шаге.\n",
        "   - $v$, $W_h$, $W_s$ и $b_a$ — обучаемые параметры.\n",
        "\n",
        "   После вычисления всех $e_{t,i}$, веса внимания $\\alpha_{t,i}$ нормализуются с помощью функции softmax:\n",
        "\n",
        "$$\n",
        "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^T \\exp(e_{t,j})}\n",
        "$$\n",
        "\n",
        "   Эти веса показывают, насколько важна каждая позиция входной последовательности для текущего шага декодирования.\n",
        "\n",
        "2. **Вычисление контекстного вектора**: На основе весов внимания вычисляется новый контекстный вектор $\\mathbf{c}_t$:\n",
        "\n",
        "$$\n",
        "   \\mathbf{c}_t = \\sum_{i=1}^T \\alpha_{t,i} h_i\n",
        "$$\n",
        "\n",
        "   Этот вектор представляет собой взвешенную сумму всех скрытых состояний **Encoder'а**, где веса определяются механизмом внимания.\n",
        "\n",
        "3. **Объединение контекстного вектора с текущим скрытым состоянием**: Контекстный вектор $\\mathbf{c}_t$ объединяется с текущим скрытым состоянием $s_{t-1}$ для вычисления нового скрытого состояния $s_t$. Обычно это делается с использованием рекуррентной функции перехода, такой как RNN, LSTM или GRU:\n",
        "\n",
        "$$\n",
        "   s_t = f(s_{t-1}, y_{t-1}, \\mathbf{c}_t)\n",
        "$$\n",
        "\n",
        "   Здесь $f$ — функция перехода, которая может быть определена следующим образом для простой RNN:\n",
        "\n",
        "$$\n",
        "   s_t = \\tanh(W_s [s_{t-1}, y_{t-1}] + W_c \\mathbf{c}_t + b_s)\n",
        "$$\n",
        "\n",
        "   Для более сложных архитектур, таких как LSTM или GRU, формулы будут несколько сложнее, но идея остается той же: контекстный вектор используется для модификации скрытого состояния.\n",
        "\n",
        "4. **Вычисление вероятностей выходных символов**: Аналогично базовой модели, вычисляются вероятности для всех возможных символов:\n",
        "\n",
        "$$\n",
        "   \\hat{y}_t = \\text{softmax}(W_o [s_t, \\mathbf{c}_t] + b_o)\n",
        "$$\n",
        "\n",
        "   Здесь $W_o$ и $b_o$ — обучаемые параметры, которые используются для преобразования скрытого состояния и контекстного вектора в вероятности для всех возможных выходных символов.\n",
        "\n",
        "### 3. Типы механизмов внимания\n",
        "\n",
        "Существует несколько различных типов механизмов внимания, каждый из которых имеет свои особенности и области применения.\n",
        "\n",
        "#### a. **Additive Attention (Bahdanau Attention)**\n",
        "\n",
        "Этот тип внимания был предложен в статье Bahdanau et al. (2014) и использует аддитивную комбинацию скрытых состояний **Encoder'а** и **Decoder'а** для вычисления весов внимания:\n",
        "\n",
        "$$\n",
        "e_{t,i} = v^T \\tanh(W_h h_i + W_s s_{t-1} + b_a)\n",
        "$$\n",
        "\n",
        "Здесь $v$, $W_h$, $W_s$ и $b_a$ — обучаемые параметры. Преимущество этого подхода заключается в том, что он позволяет модели учиться комбинировать информацию из разных источников (скрытых состояний **Encoder'а** и **Decoder'а**) для вычисления весов внимания.\n",
        "\n",
        "#### b. **Dot-Product Attention (Luong Attention)**\n",
        "\n",
        "Этот тип внимания был предложен в статье Luong et al. (2015) и использует скалярное произведение между скрытыми состояниями **Encoder'а** и **Decoder'а** для вычисления весов внимания:\n",
        "\n",
        "$$\n",
        "e_{t,i} = s_{t-1}^T h_i\n",
        "$$\n",
        "\n",
        "Этот подход проще и быстрее, чем аддитивное внимание, поскольку не требует дополнительных параметров. Однако он может быть менее эффективным для длинных последовательностей, поскольку скалярное произведение может привести к большим значениям, что затрудняет стабильное обучение модели.\n",
        "\n",
        "#### c. **Scaled Dot-Product Attention**\n",
        "\n",
        "Этот тип внимания был предложен в статье Vaswani et al. (2017) и является основой для модели Transformer. Он использует скалярное произведение между скрытыми состояниями **Encoder'а** и **Decoder'а**, но масштабирует его на обратный квадратный корень от размерности скрытого состояния:\n",
        "\n",
        "$$\n",
        "e_{t,i} = \\frac{s_{t-1}^T h_i}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "где $d_k$ — размерность скрытого состояния. Это масштабирование помогает стабилизировать обучение модели, особенно для длинных последовательностей.\n",
        "\n",
        "### 4. Расширенные механизмы внимания\n",
        "\n",
        "Помимо базовых типов внимания, существует множество расширенных механизмов внимания, которые позволяют модели учитывать дополнительные факторы и улучшать качество генерации.\n",
        "\n",
        "#### a. **Multi-Head Attention**\n",
        "\n",
        "Этот механизм был предложен в статье Vaswani et al. (2017) и используется в модели Transformer. Он позволяет модели вычислять несколько независимых наборов весов внимания, каждый из которых фокусируется на разных аспектах входной последовательности:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O\n",
        "$$\n",
        "\n",
        "где $Q$, $K$ и $V$ — матрицы запросов, ключей и значений соответственно, а $W_O$ — обучаемая матрица весов. Каждый \"голова\" (head) вычисляет свой собственный набор весов внимания и контекстных векторов, что позволяет модели учитывать больше информации при генерации выходной последовательности.\n",
        "\n",
        "#### b. **Self-Attention**\n",
        "\n",
        "Этот механизм позволяет модели вычислять веса внимания между элементами одной и той же последовательности, что полезно для задач, таких как машинный перевод или генерация текста. В этом случае матрицы запросов, ключей и значений строятся из одной и той же последовательности:\n",
        "\n",
        "$$\n",
        "\\text{SelfAttention}(X) = \\text{Attention}(X, X, X)\n",
        "$$\n",
        "\n",
        "Этот механизм позволяет модели учитывать зависимости между всеми элементами последовательности, что значительно улучшает качество генерации.\n",
        "\n",
        "### 5. Практическое применение механизма внимания\n",
        "\n",
        "Механизм внимания широко используется в различных задачах обработки естественного языка, таких как машинный перевод, суммаризация текста, ответы на вопросы и многие другие. Он позволяет моделям учитывать контекст всей входной последовательности при генерации каждого нового символа, что значительно улучшает качество генерации.\n",
        "\n",
        "#### a. **Машинный перевод**\n",
        "\n",
        "В задаче машинного перевода механизм внимания позволяет модели учитывать зависимости между словами в исходном и целевом языках, что значительно улучшает качество перевода. Например, модель может использовать внимание для выравнивания слов в исходном и целевом языках, что позволяет ей лучше учитывать контекст и грамматические особенности обоих языков.\n",
        "\n",
        "#### b. **Текстовая суммаризация**\n",
        "\n",
        "В задаче текстовой суммаризации механизм внимания позволяет модели учитывать важность различных частей исходного текста при генерации резюме. Например, модель может использовать внимание для выделения наиболее важных предложений и фраз, которые должны быть включены в резюме.\n",
        "\n",
        "#### c. **Ответы на вопросы**\n",
        "\n",
        "В задаче ответов на вопросы механизм внимания позволяет модели учитывать контекст вопроса и документа, который содержит ответ. Например, модель может использовать внимание для выделения наиболее релевантных частей документа, которые содержат ответ на вопрос.\n",
        "\n",
        "Таким образом, механизм внимания является одной из ключевых концепций в современных моделях глубокого обучения, особенно в задачах обработки естественного языка. Он позволяет моделям учитывать контекст всей входной последовательности при генерации каждого нового символа, что значительно улучшает качество генерации. В этой лекции мы рассмотрели основные принципы работы механизма внимания, его математическое описание и практические аспекты использования в модели **Seq2Seq**. Надеюсь, эта информация поможет вам лучше понять, как работает механизм внимания и как его можно использовать в своих проектах.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Конкретный числовой пример для механизма внимания в модели Seq2Seq\n",
        "\n",
        "Для лучшего понимания, давайте рассмотрим конкретный числовой пример работы механизма внимания в модели **Seq2Seq**. Мы будем использовать простую RNN с аддитивным механизмом внимания (Bahdanau Attention).\n",
        "\n",
        "#### Входные данные\n",
        "Пусть наша входная последовательность состоит из двух элементов: $x_1 = 1$ и $x_2 = 2$. Эти элементы представлены в виде эмбеддингов:\n",
        "\n",
        "$$\n",
        "x_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}, \\quad x_2 = \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "##### Инициализация скрытого состояния\n",
        "Начальное скрытое состояние $h_0$ равно нулевому вектору:\n",
        "\n",
        "$$\n",
        "h_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление скрытых состояний\n",
        "Для простой RNN функция перехода имеет вид:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)\n",
        "$$\n",
        "\n",
        "Предположим, что матрицы весов и смещения имеют следующие значения:\n",
        "\n",
        "$$\n",
        "W_h = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix}, \\quad W_x = \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix}, \\quad b_h = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 1 (t=1):**\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh \\left( W_h h_0 + W_x x_1 + b_h \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh \\left( \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.11 \\\\ 0.05 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh \\left( \\begin{bmatrix} 0.11 \\\\ 0.05 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.21 \\\\ 0.15 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "h_1 = \\begin{bmatrix} \\tanh(0.21) \\\\ \\tanh(0.15) \\end{bmatrix} \\approx \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 2 (t=2):**\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh \\left( W_h h_1 + W_x x_2 + b_h \\right)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh \\left( \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix} \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} + \\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.5 & 0.1 \\\\ 0.2 & 0.6 \\end{bmatrix} \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} = \\begin{bmatrix} 0.1184 \\\\ 0.1314 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.3 & 0.4 \\\\ 0.1 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ 0.4 \\end{bmatrix} = \\begin{bmatrix} 0.25 \\\\ 0.11 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh \\left( \\begin{bmatrix} 0.1184 \\\\ 0.1314 \\end{bmatrix} + \\begin{bmatrix} 0.25 \\\\ 0.11 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.4684 \\\\ 0.3414 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "h_2 = \\begin{bmatrix} \\tanh(0.4684) \\\\ \\tanh(0.3414) \\end{bmatrix} \\approx \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, мы получили скрытые состояния **Encoder'а**:\n",
        "\n",
        "$$\n",
        "h_1 = \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix}, \\quad h_2 = \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "##### Инициализация скрытого состояния\n",
        "Начальное скрытое состояние $s_0$ инициализируется нулевым вектором:\n",
        "\n",
        "$$\n",
        "s_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление весов внимания\n",
        "\n",
        "Предположим, что матрицы весов и смещения для механизма внимания имеют следующие значения:\n",
        "\n",
        "$$\n",
        "W_h = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix}, \\quad W_s = \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix}, \\quad v = \\begin{bmatrix} 0.9 \\\\ 1.0 \\end{bmatrix}, \\quad b_a = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 1 (t=1):**\n",
        "\n",
        "На первом шаге используется специальный токен `<start>` с эмбеддингом $y_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$:\n",
        "\n",
        "$$\n",
        "e_{1,1} = v^T \\tanh(W_h h_1 + W_s s_0 + b_a)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "e_{1,1} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\tanh \\left( \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} + \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} = \\begin{bmatrix} 0.0505 \\\\ 0.1135 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "e_{1,1} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\tanh \\left( \\begin{bmatrix} 0.0505 \\\\ 0.1135 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\tanh \\left( \\begin{bmatrix} 0.1505 \\\\ 0.2135 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "\\tanh \\left( \\begin{bmatrix} 0.1505 \\\\ 0.2135 \\end{bmatrix} \\right) \\approx \\begin{bmatrix} 0.149 \\\\ 0.209 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "e_{1,1} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\begin{bmatrix} 0.149 \\\\ 0.209 \\end{bmatrix} = 0.9 \\cdot 0.149 + 1.0 \\cdot 0.209 = 0.3471\n",
        "$$\n",
        "\n",
        "Аналогично вычисляем $e_{1,2}$:\n",
        "\n",
        "$$\n",
        "e_{1,2} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\tanh \\left( \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix} + \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix} = \\begin{bmatrix} 0.110 \\\\ 0.256 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "e_{1,2} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\tanh \\left( \\begin{bmatrix} 0.110 \\\\ 0.256 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\tanh \\left( \\begin{bmatrix} 0.210 \\\\ 0.356 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "\\tanh \\left( \\begin{bmatrix} 0.210 \\\\ 0.356 \\end{bmatrix} \\right) \\approx \\begin{bmatrix} 0.206 \\\\ 0.342 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "e_{1,2} = \\begin{bmatrix} 0.9 & 1.0 \\end{bmatrix} \\begin{bmatrix} 0.206 \\\\ 0.342 \\end{bmatrix} = 0.9 \\cdot 0.206 + 1.0 \\cdot 0.342 = 0.5474\n",
        "$$\n",
        "\n",
        "Вычисляем веса внимания:\n",
        "\n",
        "$$\n",
        "\\alpha_{1,1} = \\frac{\\exp(0.3471)}{\\exp(0.3471) + \\exp(0.5474)} \\approx 0.438\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\alpha_{1,2} = \\frac{\\exp(0.5474)}{\\exp(0.3471) + \\exp(0.5474)} \\approx 0.562\n",
        "$$\n",
        "\n",
        "##### Вычисление контекстного вектора\n",
        "\n",
        "$$\n",
        "\\mathbf{c}_1 = \\alpha_{1,1} h_1 + \\alpha_{1,2} h_2 = 0.438 \\begin{bmatrix} 0.207 \\\\ 0.149 \\end{bmatrix} + 0.562 \\begin{bmatrix} 0.438 \\\\ 0.331 \\end{bmatrix} = \\begin{bmatrix} 0.314 \\\\ 0.241 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Объединение контекстного вектора с текущим скрытым состоянием\n",
        "\n",
        "$$\n",
        "s_1 = \\tanh(W_s [s_0, y_0] + W_c \\mathbf{c}_1 + b_s)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "s_1 = \\tanh \\left( \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.314 \\\\ 0.241 \\end{bmatrix} \\right) = \\tanh \\left( \\begin{bmatrix} 0.314 \\\\ 0.241 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию активации $\\tanh$:\n",
        "\n",
        "$$\n",
        "s_1 = \\begin{bmatrix} \\tanh(0.314) \\\\ \\tanh(0.241) \\end{bmatrix} \\approx \\begin{bmatrix} 0.302 \\\\ 0.236 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление вероятностей выходных символов\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\text{softmax}(W_o [s_1, \\mathbf{c}_1] + b_o)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\text{softmax} \\left( \\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.302 \\\\ 0.236 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем произведение матриц:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0.5 & 0.6 \\\\ 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.302 \\\\ 0.236 \\end{bmatrix} = \\begin{bmatrix} 0.283 \\\\ 0.393 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь добавляем смещение:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\text{softmax} \\left( \\begin{bmatrix} 0.283 \\\\ 0.393 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix} \\right) = \\text{softmax} \\left( \\begin{bmatrix} 0.383 \\\\ 0.493 \\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "Применяем функцию softmax:\n",
        "\n",
        "$$\n",
        "\\hat{y}_1 = \\begin{bmatrix} \\frac{\\exp(0.383)}{\\exp(0.383) + \\exp(0.493)} \\\\ \\frac{\\exp(0.493)}{\\exp(0.383) + \\exp(0.493)} \\end{bmatrix} \\approx \\begin{bmatrix} 0.467 \\\\ 0.533 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выбираем наиболее вероятный символ $y_1$:\n",
        "\n",
        "$$\n",
        "y_1 = \\arg\\max(\\hat{y}_1) = 1\n",
        "$$\n",
        "\n",
        "### Заключение\n",
        "\n",
        "В данном числовом примере мы показали, как работает механизм внимания в модели **Seq2Seq**. Мы подробно рассмотрели процесс кодирования входной последовательности в скрытые состояния **Encoder'а**, вычисление весов внимания, контекстного вектора и нового скрытого состояния **Decoder'а**, а также вычисление вероятностей для всех возможных символов. Этот пример демонстрирует, как механизм внимания позволяет модели \"смотреть\" на различные части входной последовательности при генерации каждого нового символа, что значительно улучшает качество модели.\n",
        "\n"
      ],
      "metadata": {
        "id": "3SCBN088jkF0"
      }
    }
  ]
}