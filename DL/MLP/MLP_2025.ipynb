{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNIOepxb618Rh7KHqOPSCHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/DL/MLP/MLP_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Многослойный персептрон с алгоритмом обратного распространения ошибки (Backpropagation)\n",
        "\n",
        "\n",
        "\n",
        "## 1. Архитектура нейронной сети\n",
        "\n",
        "Рассмотрим простую архитектуру многослойного персептрона, состоящую из трёх слоёв:\n",
        "\n",
        "- **Входной слой**: содержит один нейрон, принимающий скалярный вход $ x $.\n",
        "- **Скрытый слой**: состоит из одного нейрона с функцией активации $ \\sigma_h $.\n",
        "- **Выходной слой**: содержит один нейрон с функцией активации $ \\sigma_o $.\n",
        "\n",
        "**Параметры модели:**\n",
        "- $ w_1 $ — вес связи между входным и скрытым слоями,\n",
        "- $ b_1 $ — смещение нейрона скрытого слоя,\n",
        "- $ w_2 $ — вес связи между скрытым и выходным слоями,\n",
        "- $ b_2 $ — смещение нейрона выходного слоя.\n",
        "\n",
        "\n",
        "## 2. Прямой проход (Forward Pass)\n",
        "\n",
        "Прямой проход представляет собой последовательное вычисление выхода сети на основе входных данных и текущих параметров.\n",
        "\n",
        "### Последовательные вычисления:\n",
        "\n",
        "#### 1. Вычисление на скрытом слое:\n",
        "$$\n",
        "z_1 = w_1 \\cdot x + b_1\n",
        "$$\n",
        "$$\n",
        "h = \\sigma_h(z_1)\n",
        "$$\n",
        "\n",
        "#### 2. Вычисление на выходном слое:\n",
        "$$\n",
        "z_2 = w_2 \\cdot h + b_2\n",
        "$$\n",
        "$$\n",
        "\\hat{y} = \\sigma_o(z_2)\n",
        "$$\n",
        "\n",
        "### Обобщённая формула выхода сети:\n",
        "$$\n",
        "\\hat{y} = \\sigma_o(w_2 \\cdot \\sigma_h(w_1 \\cdot x + b_1) + b_2)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 3. Обратное распространение ошибки (Backpropagation)\n",
        "\n",
        "Обратное распространение используется для вычисления градиентов функции потерь по параметрам модели с целью их дальнейшей корректировки.\n",
        "\n",
        "### Функция потерь\n",
        "Для задачи регрессии или бинарной классификации часто используется квадратичная функция потерь:\n",
        "$$\n",
        "L = \\frac{1}{2}(y - \\hat{y})^2\n",
        "$$\n",
        "где $ y $ — истинное значение, $ \\hat{y} $ — предсказанное значение.\n",
        "\n",
        "Градиент функции потерь по выходу сети:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "### Градиенты выходного слоя\n",
        "\n",
        "Частные производные:\n",
        "$$\n",
        "\\frac{\\partial \\hat{y}}{\\partial z_2} = \\sigma_o'(z_2), \\quad\n",
        "\\frac{\\partial z_2}{\\partial w_2} = h, \\quad\n",
        "\\frac{\\partial z_2}{\\partial b_2} = 1\n",
        "$$\n",
        "\n",
        "Полные градиенты:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2}\n",
        "$$\n",
        "\n",
        "### Градиенты скрытого слоя\n",
        "\n",
        "Частные производные:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial h} = w_2, \\quad\n",
        "\\frac{\\partial h}{\\partial z_1} = \\sigma_h'(z_1), \\quad\n",
        "\\frac{\\partial z_1}{\\partial w_1} = x, \\quad\n",
        "\\frac{\\partial z_1}{\\partial b_1} = 1\n",
        "$$\n",
        "\n",
        "Полные градиенты:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4. Обновление параметров\n",
        "\n",
        "Для обновления параметров используется метод градиентного спуска:\n",
        "\n",
        "$$\n",
        "w_1 \\leftarrow w_1 - \\eta \\cdot \\frac{\\partial L}{\\partial w_1}, \\quad\n",
        "b_1 \\leftarrow b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\n",
        "$$\n",
        "$$\n",
        "w_2 \\leftarrow w_2 - \\eta \\cdot \\frac{\\partial L}{\\partial w_2}, \\quad\n",
        "b_2 \\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
        "$$\n",
        "\n",
        "где $ \\eta $ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "\n",
        "## 5. Практические рекомендации\n",
        "\n",
        "### Рекомендуемый порядок вычислений:\n",
        "1. Начните с вычисления градиента функции потерь по выходу сети.\n",
        "2. Двигайтесь от выходного слоя к входному, применяя правило цепочки дифференцирования.\n",
        "3. Сохраняйте промежуточные значения для ускорения вычислений.\n",
        "\n",
        "### Проверка корректности градиентов:\n",
        "Для отладки можно использовать численную проверку градиентов:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} \\approx \\frac{L(w+\\epsilon) - L(w-\\epsilon)}{2\\epsilon}\n",
        "$$\n",
        "\n",
        "### Особенности функций активации:\n",
        "- Для ReLU ($ \\sigma(z) = \\max(0, z) $):\n",
        "  $$\n",
        "  \\sigma'(z) =\n",
        "  \\begin{cases}\n",
        "    1, & z > 0 \\\\\n",
        "    0, & z \\leq 0\n",
        "  \\end{cases}\n",
        "  $$\n",
        "- Для сигмоиды ($ \\sigma(z) = \\frac{1}{1+e^{-z}} $):\n",
        "  $$\n",
        "  \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "## 6. Пример вычислений\n",
        "\n",
        "**Исходные данные:**\n",
        "- $ x = 1.0 $, $ y = 0.5 $\n",
        "- $ w_1 = 0.3 $, $ b_1 = -0.2 $\n",
        "- $ w_2 = 0.4 $, $ b_2 = 0.1 $\n",
        "- Все функции активации — сигмоиды\n",
        "- Скорость обучения $ \\eta = 0.1 $\n",
        "\n",
        "### Прямой проход:\n",
        "1. $ z_1 = 0.3 \\cdot 1 - 0.2 = 0.1 $\n",
        "2. $ h = \\sigma(0.1) \\approx 0.525 $\n",
        "3. $ z_2 = 0.4 \\cdot 0.525 + 0.1 = 0.31 $\n",
        "4. $ \\hat{y} = \\sigma(0.31) \\approx 0.577 $\n",
        "\n",
        "### Обратный проход:\n",
        "1. $ \\frac{\\partial L}{\\partial \\hat{y}} = 0.577 - 0.5 = 0.077 $\n",
        "2. $ \\frac{\\partial \\hat{y}}{\\partial z_2} = 0.577 \\cdot (1 - 0.577) \\approx 0.244 $\n",
        "3. $ \\frac{\\partial L}{\\partial w_2} = 0.077 \\cdot 0.244 \\cdot 0.525 \\approx 0.0099 $\n",
        "4. Обновление веса $ w_2 $: $ 0.4 - 0.1 \\cdot 0.0099 \\approx 0.399 $\n"
      ],
      "metadata": {
        "id": "xdooks-FzfYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1 вход, 2 нейрона в скрытом слое, 1 выход)\n",
        "\n",
        "## 1. Архитектура нейронной сети\n",
        "\n",
        "Рассмотрим усовершенствованную архитектуру многослойного персептрона, состоящую из трёх слоёв:\n",
        "\n",
        "- **Входной слой**: содержит один нейрон, принимающий скалярный вход $ x $.\n",
        "- **Скрытый слой**: состоит из двух нейронов с функцией активации $ \\sigma_h $.\n",
        "- **Выходной слой**: содержит один нейрон с функцией активации $ \\sigma_o $.\n",
        "\n",
        "**Параметры модели:**\n",
        "- $ w_{11}, w_{12} $ — веса связи от входного нейрона к первому и второму скрытым нейронам соответственно,\n",
        "- $ b_{11}, b_{12} $ — смещения первого и второго скрытых нейронов,\n",
        "- $ w_{21}, w_{22} $ — веса связи от скрытых нейронов к выходному,\n",
        "- $ b_2 $ — смещение выходного нейрона.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Прямой проход (Forward Pass)\n",
        "\n",
        "Прямой проход реализует вычисление предсказания $ \\hat{y} $ на основе входного значения $ x $ и текущих параметров сети.\n",
        "\n",
        "### Последовательные вычисления:\n",
        "\n",
        "#### 1. Вычисления на скрытом слое:\n",
        "$$\n",
        "z_{11} = w_{11} \\cdot x + b_{11}, \\quad h_1 = \\sigma_h(z_{11})\n",
        "$$\n",
        "$$\n",
        "z_{12} = w_{12} \\cdot x + b_{12}, \\quad h_2 = \\sigma_h(z_{12})\n",
        "$$\n",
        "\n",
        "#### 2. Вычисления на выходном слое:\n",
        "$$\n",
        "z_2 = w_{21} \\cdot h_1 + w_{22} \\cdot h_2 + b_2\n",
        "$$\n",
        "$$\n",
        "\\hat{y} = \\sigma_o(z_2)\n",
        "$$\n",
        "\n",
        "### Обобщённая формула:\n",
        "$$\n",
        "\\hat{y} = \\sigma_o\\left(w_{21} \\cdot \\sigma_h(w_{11} \\cdot x + b_{11}) + w_{22} \\cdot \\sigma_h(w_{12} \\cdot x + b_{12}) + b_2\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 3. Обратное распространение ошибки (Backpropagation)\n",
        "\n",
        "Обратное распространение используется для вычисления градиентов функции потерь по всем обучаемым параметрам модели.\n",
        "\n",
        "### Функция потерь\n",
        "\n",
        "Для задачи регрессии или бинарной классификации часто используется квадратичная ошибка:\n",
        "$$\n",
        "L = \\frac{1}{2}(y - \\hat{y})^2\n",
        "$$\n",
        "где $ y $ — истинное значение, $ \\hat{y} $ — предсказанное значение.\n",
        "\n",
        "Градиент функции потерь по выходу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "### Градиенты выходного слоя\n",
        "\n",
        "Частные производные:\n",
        "$$\n",
        "\\frac{\\partial \\hat{y}}{\\partial z_2} = \\sigma_o'(z_2), \\quad\n",
        "\\frac{\\partial z_2}{\\partial w_{21}} = h_1, \\quad\n",
        "\\frac{\\partial z_2}{\\partial w_{22}} = h_2, \\quad\n",
        "\\frac{\\partial z_2}{\\partial b_2} = 1\n",
        "$$\n",
        "\n",
        "Полные градиенты:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{21}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{21}}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{22}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2}\n",
        "$$\n",
        "\n",
        "### Градиенты скрытого слоя\n",
        "\n",
        "Для каждого скрытого нейрона $ k=1,2 $:\n",
        "\n",
        "Частные производные:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial h_k} = w_{2k}, \\quad\n",
        "\\frac{\\partial h_k}{\\partial z_{1k}} = \\sigma_h'(z_{1k}), \\quad\n",
        "\\frac{\\partial z_{1k}}{\\partial w_{1k}} = x, \\quad\n",
        "\\frac{\\partial z_{1k}}{\\partial b_{1k}} = 1\n",
        "$$\n",
        "\n",
        "Полные градиенты:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{1k}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_{1k}} \\cdot \\frac{\\partial z_{1k}}{\\partial w_{1k}}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_{1k}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_{1k}} \\cdot \\frac{\\partial z_{1k}}{\\partial b_{1k}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4. Обновление параметров\n",
        "\n",
        "Для обновления всех параметров используется метод градиентного спуска:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}\n",
        "$$\n",
        "\n",
        "Конкретно для каждого параметра:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "w_{11} &\\leftarrow w_{11} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{11}}, \\\\\n",
        "b_{11} &\\leftarrow b_{11} - \\eta \\cdot \\frac{\\partial L}{\\partial b_{11}}, \\\\\n",
        "w_{12} &\\leftarrow w_{12} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{12}}, \\\\\n",
        "b_{12} &\\leftarrow b_{12} - \\eta \\cdot \\frac{\\partial L}{\\partial b_{12}}, \\\\\n",
        "w_{21} &\\leftarrow w_{21} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{21}}, \\\\\n",
        "w_{22} &\\leftarrow w_{22} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{22}}, \\\\\n",
        "b_2 &\\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 5. Практические рекомендации\n",
        "\n",
        "### Рекомендуемый порядок вычислений:\n",
        "1. Начните с вычисления градиента функции потерь по выходу сети.\n",
        "2. Двигайтесь от выходного слоя к входному, применяя правило цепочки дифференцирования.\n",
        "3. Сохраняйте промежуточные значения, полученные при прямом проходе.\n",
        "\n",
        "### Проверка корректности градиентов:\n",
        "Для отладки можно использовать численную проверку:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta+\\epsilon) - L(\\theta-\\epsilon)}{2\\epsilon}\n",
        "$$\n",
        "\n",
        "### Особенности реализации:\n",
        "- Каждый дополнительный нейрон увеличивает количество обучаемых параметров.\n",
        "- При использовании сигмоиды: $ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $\n",
        "- При использовании ReLU: $ \\sigma'(z) = 1 $ при $ z > 0 $, иначе $ 0 $\n",
        "\n",
        "\n",
        "\n",
        "## 6. Пример вычислений\n",
        "\n",
        "**Исходные данные:**\n",
        "- $ x = 1.5 $, $ y = 0.8 $\n",
        "- Параметры:\n",
        "  - $ w_{11} = 0.2 $, $ w_{12} = -0.3 $\n",
        "  - $ b_{11} = 0.1 $, $ b_{12} = -0.2 $\n",
        "  - $ w_{21} = 0.4 $, $ w_{22} = 0.5 $\n",
        "  - $ b_2 = 0.05 $\n",
        "- Все функции активации — сигмоиды\n",
        "- Скорость обучения $ \\eta = 0.1 $\n",
        "\n",
        "### Прямой проход:\n",
        "1. $ z_{11} = 0.2 \\cdot 1.5 + 0.1 = 0.4 $ → $ h_1 = \\sigma(0.4) \\approx 0.599 $\n",
        "2. $ z_{12} = -0.3 \\cdot 1.5 - 0.2 = -0.65 $ → $ h_2 = \\sigma(-0.65) \\approx 0.343 $\n",
        "3. $ z_2 = 0.4 \\cdot 0.599 + 0.5 \\cdot 0.343 + 0.05 \\approx 0.456 $\n",
        "4. $ \\hat{y} = \\sigma(0.456) \\approx 0.612 $\n",
        "\n",
        "### Обратный проход:\n",
        "1. $ \\frac{\\partial L}{\\partial \\hat{y}} = 0.612 - 0.8 = -0.188 $\n",
        "2. $ \\frac{\\partial \\hat{y}}{\\partial z_2} = 0.612 \\cdot (1 - 0.612) \\approx 0.237 $\n",
        "3. $ \\frac{\\partial L}{\\partial w_{21}} = -0.188 \\cdot 0.237 \\cdot 0.599 \\approx -0.027 $\n",
        "4. $ \\frac{\\partial L}{\\partial w_{22}} = -0.188 \\cdot 0.237 \\cdot 0.343 \\approx -0.015 $\n",
        "5. $ \\frac{\\partial L}{\\partial b_2} = -0.188 \\cdot 0.237 \\cdot 1 \\approx -0.045 $\n",
        "\n",
        "Для скрытых нейронов:\n",
        "- $ \\frac{\\partial L}{\\partial w_{11}} = -0.188 \\cdot 0.237 \\cdot 0.4 \\cdot 0.599 \\cdot (1 - 0.599) \\cdot 1.5 \\approx -0.0043 $\n",
        "- $ \\frac{\\partial L}{\\partial w_{12}} = -0.188 \\cdot 0.237 \\cdot 0.5 \\cdot 0.343 \\cdot (1 - 0.343) \\cdot 1.5 \\approx -0.0051 $\n",
        "\n",
        "### Обновление параметров:\n",
        "$$\n",
        "w_{21} = 0.4 - 0.1 \\cdot (-0.027) \\approx 0.4027\n",
        "$$\n",
        "$$\n",
        "w_{11} = 0.2 - 0.1 \\cdot (-0.0043) \\cdot 1.5 \\approx 0.2006\n",
        "$$\n"
      ],
      "metadata": {
        "id": "HnFT8qSgznwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2 входа, 3 нейрона в скрытом слое, 2 выхода)\n",
        "\n",
        "\n",
        "\n",
        "## 1. Архитектура сети\n",
        "\n",
        "Рассмотрим полноценную архитектуру многослойного персептрона, подходящую для многомерного входа и множественных выходов:\n",
        "\n",
        "- **Входной слой**: 2 нейрона (вектор $ \\mathbf{x} = [x_1, x_2] $)\n",
        "- **Скрытый слой**: 3 нейрона с функцией активации $ \\sigma_h $\n",
        "- **Выходной слой**: 2 нейрона с функцией активации $ \\sigma_o $\n",
        "\n",
        "**Параметры модели**:\n",
        "- **Матрица весов между входным и скрытым слоем** $ W_1 \\in \\mathbb{R}^{3 \\times 2} $:\n",
        "  $$\n",
        "  W_1 =\n",
        "  \\begin{bmatrix}\n",
        "  w_{111} & w_{112} \\\\\n",
        "  w_{121} & w_{122} \\\\\n",
        "  w_{131} & w_{132}\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "- **Вектор смещений скрытого слоя** $ \\mathbf{b}_1 \\in \\mathbb{R}^3 $: $ b_{11}, b_{12}, b_{13} $\n",
        "- **Матрица весов между скрытым и выходным слоем** $ W_2 \\in \\mathbb{R}^{2 \\times 3} $:\n",
        "  $$\n",
        "  W_2 =\n",
        "  \\begin{bmatrix}\n",
        "  w_{211} & w_{212} & w_{213} \\\\\n",
        "  w_{221} & w_{222} & w_{223}\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "- **Вектор смещений выходного слоя** $ \\mathbf{b}_2 \\in \\mathbb{R}^2 $: $ b_{21}, b_{22} $\n",
        "\n",
        "\n",
        "\n",
        "## 2. Forward Pass (Прямой проход)\n",
        "\n",
        "### Последовательные вычисления:\n",
        "\n",
        "#### 1. Скрытый слой:\n",
        "$$\n",
        "\\mathbf{z}_1 = W_1 \\cdot \\mathbf{x} + \\mathbf{b}_1\n",
        "\\Rightarrow\n",
        "\\begin{cases}\n",
        "z_{11} = w_{111} x_1 + w_{112} x_2 + b_{11} \\\\\n",
        "z_{12} = w_{121} x_1 + w_{122} x_2 + b_{12} \\\\\n",
        "z_{13} = w_{131} x_1 + w_{132} x_2 + b_{13}\n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        "\\mathbf{h} = \\sigma_h(\\mathbf{z}_1)\n",
        "\\Rightarrow\n",
        "\\begin{cases}\n",
        "h_1 = \\sigma_h(z_{11}) \\\\\n",
        "h_2 = \\sigma_h(z_{12}) \\\\\n",
        "h_3 = \\sigma_h(z_{13})\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "#### 2. Выходной слой:\n",
        "$$\n",
        "\\mathbf{z}_2 = W_2 \\cdot \\mathbf{h} + \\mathbf{b}_2\n",
        "\\Rightarrow\n",
        "\\begin{cases}\n",
        "z_{21} = w_{211} h_1 + w_{212} h_2 + w_{213} h_3 + b_{21} \\\\\n",
        "z_{22} = w_{221} h_1 + w_{222} h_2 + w_{223} h_3 + b_{22}\n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\sigma_o(\\mathbf{z}_2)\n",
        "\\Rightarrow\n",
        "\\begin{cases}\n",
        "\\hat{y}_1 = \\sigma_o(z_{21}) \\\\\n",
        "\\hat{y}_2 = \\sigma_o(z_{22})\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 3. Backpropagation (Обратное распространение ошибки)\n",
        "\n",
        "### Функция потерь\n",
        "\n",
        "Для задачи регрессии или многоклассовой классификации используем квадратичную функцию потерь:\n",
        "$$\n",
        "L = \\frac{1}{2} \\sum_{i=1}^2 (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "Градиенты по предсказанным значениям:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{y}_1} = \\hat{y}_1 - y_1, \\quad\n",
        "\\frac{\\partial L}{\\partial \\hat{y}_2} = \\hat{y}_2 - y_2\n",
        "$$\n",
        "\n",
        "\n",
        "### Градиенты выходного слоя\n",
        "\n",
        "Частные производные:\n",
        "$$\n",
        "\\frac{\\partial \\hat{y}_i}{\\partial z_{2i}} = \\sigma_o'(z_{2i}), \\quad\n",
        "\\frac{\\partial z_{2i}}{\\partial w_{2ij}} = h_j, \\quad\n",
        "\\frac{\\partial z_{2i}}{\\partial b_{2i}} = 1\n",
        "$$\n",
        "\n",
        "Полные градиенты:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{2ij}} = \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_{2i}} \\cdot \\frac{\\partial z_{2i}}{\\partial w_{2ij}}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_{2i}} = \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_{2i}} \\cdot \\frac{\\partial z_{2i}}{\\partial b_{2i}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Градиенты скрытого слоя\n",
        "\n",
        "Для каждого скрытого нейрона $ k=1,2,3 $:\n",
        "\n",
        "Частные производные:\n",
        "$$\n",
        "\\frac{\\partial z_{2i}}{\\partial h_k} = w_{2ik}, \\quad\n",
        "\\frac{\\partial h_k}{\\partial z_{1k}} = \\sigma_h'(z_{1k}), \\quad\n",
        "\\frac{\\partial z_{1k}}{\\partial w_{1kj}} = x_j, \\quad\n",
        "\\frac{\\partial z_{1k}}{\\partial b_{1k}} = 1\n",
        "$$\n",
        "\n",
        "Полные градиенты:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{1kj}} = \\sum_{i=1}^2 \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_{2i}} \\cdot \\frac{\\partial z_{2i}}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_{1k}} \\cdot \\frac{\\partial z_{1k}}{\\partial w_{1kj}} \\right)\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_{1k}} = \\sum_{i=1}^2 \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_{2i}} \\cdot \\frac{\\partial z_{2i}}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial z_{1k}} \\cdot \\frac{\\partial z_{1k}}{\\partial b_{1k}} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "## 4. Обновление параметров\n",
        "\n",
        "Используем градиентный спуск с learning rate $ \\eta $:\n",
        "\n",
        "$$\n",
        "W_1 \\leftarrow W_1 - \\eta \\cdot \\nabla_{W_1} L, \\quad\n",
        "\\mathbf{b}_1 \\leftarrow \\mathbf{b}_1 - \\eta \\cdot \\nabla_{\\mathbf{b}_1} L\n",
        "$$\n",
        "$$\n",
        "W_2 \\leftarrow W_2 - \\eta \\cdot \\nabla_{W_2} L, \\quad\n",
        "\\mathbf{b}_2 \\leftarrow \\mathbf{b}_2 - \\eta \\cdot \\nabla_{\\mathbf{b}_2} L\n",
        "$$\n",
        "\n",
        "Конкретно для отдельных параметров:\n",
        "$$\n",
        "w_{1kj} \\leftarrow w_{1kj} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{1kj}}, \\quad\n",
        "b_{1k} \\leftarrow b_{1k} - \\eta \\cdot \\frac{\\partial L}{\\partial b_{1k}}\n",
        "$$\n",
        "$$\n",
        "w_{2ij} \\leftarrow w_{2ij} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{2ij}}, \\quad\n",
        "b_{2i} \\leftarrow b_{2i} - \\eta \\cdot \\frac{\\partial L}{\\partial b_{2i}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Численная проверка градиентов:\n",
        "Для проверки корректности:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta+\\epsilon) - L(\\theta-\\epsilon)}{2\\epsilon}\n",
        "$$\n",
        "\n",
        "### Особенности реализации:\n",
        "- Используйте матричные операции для эффективности.\n",
        "- Для сигмоиды: $ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $\n",
        "- Для ReLU: $ \\sigma'(z) = 1 $ при $ z > 0 $, иначе $ 0 $\n",
        "\n",
        "\n",
        "\n",
        "## 6. Пример вычислений\n",
        "\n",
        "**Дано:**\n",
        "- Вход: $ \\mathbf{x} = [1.0, 0.5] $\n",
        "- Цель: $ \\mathbf{y} = [0.8, 0.3] $\n",
        "- Параметры:\n",
        "  $$\n",
        "  W_1 =\n",
        "  \\begin{bmatrix}\n",
        "  0.2 & -0.1 \\\\\n",
        "  0.3 & 0.4 \\\\\n",
        "  -0.2 & 0.1\n",
        "  \\end{bmatrix}, \\quad\n",
        "  \\mathbf{b}_1 = [0.1, -0.2, 0.0], \\quad\n",
        "  W_2 =\n",
        "  \\begin{bmatrix}\n",
        "  0.5 & 0.1 & -0.3 \\\\\n",
        "  0.2 & -0.4 & 0.1\n",
        "  \\end{bmatrix}, \\quad\n",
        "  \\mathbf{b}_2 = [-0.1, 0.2]\n",
        "  $$\n",
        "- Активации: сигмоиды\n",
        "- $ \\eta = 0.1 $\n",
        "\n",
        "### Прямой проход:\n",
        "1. $ \\mathbf{z}_1 = W_1 \\cdot \\mathbf{x} + \\mathbf{b}_1 = [0.2*1 + (-0.1)*0.5 + 0.1,\\; 0.3*1 + 0.4*0.5 - 0.2,\\; -0.2*1 + 0.1*0.5 + 0.0] = [0.25,\\; 0.3,\\; -0.15] $\n",
        "2. $ \\mathbf{h} = \\sigma_h(\\mathbf{z}_1) = [\\sigma(0.25),\\; \\sigma(0.3),\\; \\sigma(-0.15)] \\approx [0.562,\\; 0.575,\\; 0.462] $\n",
        "3. $ \\mathbf{z}_2 = W_2 \\cdot \\mathbf{h} + \\mathbf{b}_2 = [0.5*0.562 + 0.1*0.575 - 0.3*0.462 - 0.1,\\; 0.2*0.562 - 0.4*0.575 + 0.1*0.462 + 0.2] \\approx [0.049,\\; 0.114] $\n",
        "4. $ \\hat{\\mathbf{y}} = \\sigma_o(\\mathbf{z}_2) = [\\sigma(0.049),\\; \\sigma(0.114)] \\approx [0.512,\\; 0.528] $\n",
        "\n",
        "### Обратный проход:\n",
        "1. $ \\frac{\\partial L}{\\partial \\hat{y}_1} = 0.512 - 0.8 = -0.288 $,  \n",
        "   $ \\frac{\\partial L}{\\partial \\hat{y}_2} = 0.528 - 0.3 = 0.228 $\n",
        "2. $ \\frac{\\partial \\hat{y}_1}{\\partial z_{21}} = 0.512*(1 - 0.512) \\approx 0.250 $,  \n",
        "   $ \\frac{\\partial \\hat{y}_2}{\\partial z_{22}} = 0.528*(1 - 0.528) \\approx 0.250 $\n",
        "3. $ \\frac{\\partial L}{\\partial w_{211}} = -0.288 * 0.250 * 0.562 \\approx -0.041 $,  \n",
        "   $ \\frac{\\partial L}{\\partial w_{212}} = -0.288 * 0.250 * 0.575 \\approx -0.041 $,  \n",
        "   $ \\frac{\\partial L}{\\partial w_{213}} = -0.288 * 0.250 * 0.462 \\approx -0.033 $,  \n",
        "   $ \\frac{\\partial L}{\\partial b_{21}} = -0.288 * 0.250 \\approx -0.072 $\n",
        "\n",
        "(Аналогично вычисляются градиенты для второго выхода и скрытых нейронов.)\n",
        "\n",
        "### Обновление параметров:\n",
        "$$\n",
        "w_{211} = 0.5 - 0.1*(-0.041) \\approx 0.5041\n",
        "$$\n",
        "(Остальные параметры обновляются аналогично.)\n"
      ],
      "metadata": {
        "id": "W07G3b4Y0ZAZ"
      }
    }
  ]
}