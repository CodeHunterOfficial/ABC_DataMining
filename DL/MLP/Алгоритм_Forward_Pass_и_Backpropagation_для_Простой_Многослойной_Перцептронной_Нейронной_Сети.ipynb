{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM3dre8rTU1dUfMcvyXTT8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/DL/MLP/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_Forward_Pass_%D0%B8_Backpropagation_%D0%B4%D0%BB%D1%8F_%D0%9F%D1%80%D0%BE%D1%81%D1%82%D0%BE%D0%B9_%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D0%BE%D0%B9_%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9_%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9_%D0%A1%D0%B5%D1%82%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Алгоритм Forward Pass и Backpropagation для Простой Многослойной Перцептронной Нейронной Сети (MLP)\n",
        "\n",
        "#### Введение\n",
        "Мы рассмотрим простую многослойную перцептронную нейронную сеть (MLP), которая состоит из:\n",
        "- одного входного нейрона,\n",
        "- одного скрытого слоя с одним нейроном,\n",
        "- одного выходного нейрона.\n",
        "\n",
        "Эта архитектура позволяет нам разобрать процесс **forward pass** (прямого прохода) и **backpropagation** (обратного распространения ошибки) шаг за шагом, чтобы понять основные принципы работы нейронных сетей.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Архитектура сети\n",
        "\n",
        "Обозначим параметры сети:\n",
        "- $ x $: входное значение.\n",
        "- $ w_1 $: вес между входным и скрытым слоем.\n",
        "- $ b_1 $: смещение (bias) для скрытого слоя.\n",
        "- $ w_2 $: вес между скрытым и выходным слоем.\n",
        "- $ b_2 $: смещение (bias) для выходного слоя.\n",
        "- $ \\sigma(\\cdot) $: функция активации (например, сигмоидная или ReLU).\n",
        "\n",
        "Сеть вычисляет выходное значение $ y_{\\text{pred}} $ на основе входного значения $ x $. Мы также предполагаем, что у нас есть целевое значение $ y_{\\text{true}} $, которое используется для обучения сети.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Forward Pass (Прямой Проход)\n",
        "\n",
        "Forward pass — это процесс вычисления выходного значения сети на основе входных данных. Рассмотрим его шаг за шагом:\n",
        "\n",
        "#### Шаг 1: Вычисление взвешенной суммы для скрытого слоя\n",
        "Для скрытого слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_1 = w_1 \\cdot x + b_1\n",
        "$$\n",
        "где:\n",
        "- $ w_1 $ — вес между входным и скрытым слоем,\n",
        "- $ b_1 $ — смещение для скрытого слоя.\n",
        "\n",
        "#### Шаг 2: Применение функции активации для скрытого слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_1 $, чтобы получить выход скрытого слоя:\n",
        "$$\n",
        "a_1 = \\sigma(z_1)\n",
        "$$\n",
        "где $ a_1 $ — активация скрытого слоя.\n",
        "\n",
        "#### Шаг 3: Вычисление взвешенной суммы для выходного слоя\n",
        "Для выходного слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_2 = w_2 \\cdot a_1 + b_2\n",
        "$$\n",
        "где:\n",
        "- $ w_2 $ — вес между скрытым и выходным слоем,\n",
        "- $ b_2 $ — смещение для выходного слоя.\n",
        "\n",
        "#### Шаг 4: Применение функции активации для выходного слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_2 $, чтобы получить окончательный выход сети:\n",
        "$$\n",
        "y_{\\text{pred}} = \\sigma(z_2)\n",
        "$$\n",
        "где $ y_{\\text{pred}} $ — предсказанное значение.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Backpropagation (Обратное Распространение Ошибки)\n",
        "\n",
        "Backpropagation — это процесс вычисления градиентов потерь по параметрам сети ($ w_1, b_1, w_2, b_2 $) с использованием цепного правила дифференцирования. Предположим, что функция потерь — это среднеквадратичная ошибка (MSE):\n",
        "$$\n",
        "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "$$\n",
        "\n",
        "Рассмотрим шаги backpropagation:\n",
        "\n",
        "#### Шаг 1: Вычисление производной функции потерь по $ y_{\\text{pred}} $\n",
        "Производная функции потерь по предсказанному значению:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = y_{\\text{pred}} - y_{\\text{true}}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление производной $ y_{\\text{pred}} $ по $ z_2 $\n",
        "Производная активации выходного слоя:\n",
        "$$\n",
        "\\frac{\\partial y_{\\text{pred}}}{\\partial z_2} = \\sigma'(z_2)\n",
        "$$\n",
        "где $ \\sigma'(z_2) $ — производная функции активации.\n",
        "\n",
        "#### Шаг 3: Вычисление производной функции потерь по $ z_2 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Вычисление производных $ z_2 $ по параметрам выходного слоя\n",
        "- Производная $ z_2 $ по $ w_2 $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial w_2} = a_1\n",
        "$$\n",
        "- Производная $ z_2 $ по $ b_2 $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial b_2} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_2 $ и $ b_2 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot a_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "#### Шаг 5: Вычисление производной $ z_2 $ по $ a_1 $\n",
        "Производная $ z_2 $ по $ a_1 $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial a_1} = w_2\n",
        "$$\n",
        "\n",
        "#### Шаг 6: Вычисление производной $ a_1 $ по $ z_1 $\n",
        "Производная активации скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial a_1}{\\partial z_1} = \\sigma'(z_1)\n",
        "$$\n",
        "\n",
        "#### Шаг 7: Вычисление производной $ z_2 $ по $ z_1 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}\n",
        "$$\n",
        "\n",
        "#### Шаг 8: Вычисление производных $ z_1 $ по параметрам скрытого слоя\n",
        "- Производная $ z_1 $ по $ w_1 $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial w_1} = x\n",
        "$$\n",
        "- Производная $ z_1 $ по $ b_1 $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial b_1} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_1 $ и $ b_1 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot x\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4. Обновление параметров\n",
        "\n",
        "После вычисления градиентов параметры обновляются с использованием метода градиентного спуска:\n",
        "$$\n",
        "w_1 := w_1 - \\eta \\cdot \\frac{\\partial L}{\\partial w_1}\n",
        "$$\n",
        "$$\n",
        "b_1 := b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\n",
        "$$\n",
        "$$\n",
        "w_2 := w_2 - \\eta \\cdot \\frac{\\partial L}{\\partial w_2}\n",
        "$$\n",
        "$$\n",
        "b_2 := b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
        "$$\n",
        "где $ \\eta $ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "\n",
        "Реализация на питон\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Функция активации (сигмоида) и её производная\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(x, w1, b1, w2, b2):\n",
        "    # Скрытый слой\n",
        "    z1 = w1 * x + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    \n",
        "    # Выходной слой\n",
        "    z2 = w2 * a1 + b2\n",
        "    y_pred = sigmoid(z2)\n",
        "    \n",
        "    return z1, a1, z2, y_pred\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(x, y_true, y_pred, z1, a1, z2, w1, b1, w2, b2, learning_rate):\n",
        "    # Шаг 1: Производная функции потерь по y_pred\n",
        "    dL_dy_pred = y_pred - y_true\n",
        "    \n",
        "    # Шаг 2: Производная y_pred по z2\n",
        "    dy_pred_dz2 = sigmoid_derivative(z2)\n",
        "    \n",
        "    # Шаг 3: Производная функции потерь по z2\n",
        "    dL_dz2 = dL_dy_pred * dy_pred_dz2\n",
        "    \n",
        "    # Шаг 4: Градиенты для выходного слоя\n",
        "    dz2_dw2 = a1\n",
        "    dL_dw2 = dL_dz2 * dz2_dw2\n",
        "    dL_db2 = dL_dz2 * 1  # dz2_db2 = 1\n",
        "    \n",
        "    # Шаг 5: Производная z2 по a1\n",
        "    dz2_da1 = w2\n",
        "    \n",
        "    # Шаг 6: Производная a1 по z1\n",
        "    da1_dz1 = sigmoid_derivative(z1)\n",
        "    \n",
        "    # Шаг 7: Производная функции потерь по z1\n",
        "    dL_dz1 = dL_dz2 * dz2_da1 * da1_dz1\n",
        "    \n",
        "    # Шаг 8: Градиенты для скрытого слоя\n",
        "    dz1_dw1 = x\n",
        "    dL_dw1 = dL_dz1 * dz1_dw1\n",
        "    dL_db1 = dL_dz1 * 1  # dz1_db1 = 1\n",
        "    \n",
        "    # Обновление параметров\n",
        "    w1 -= learning_rate * dL_dw1\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    w2 -= learning_rate * dL_dw2\n",
        "    b2 -= learning_rate * dL_db2\n",
        "    \n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "# Визуализация изменения весов и смещений\n",
        "def plot_weights_and_biases(weights_history, biases_history):\n",
        "    epochs = range(len(weights_history['w1']))\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # График для весов\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(epochs, weights_history['w1'], label='w1')\n",
        "    plt.plot(epochs, weights_history['w2'], label='w2')\n",
        "    plt.title('Веса (Weights)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    # График для смещений\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(epochs, biases_history['b1'], label='b1')\n",
        "    plt.plot(epochs, biases_history['b2'], label='b2')\n",
        "    plt.title('Смещения (Biases)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Обучение сети\n",
        "def train_network(x, y_true, epochs, learning_rate):\n",
        "    # Инициализация параметров\n",
        "    w1, b1, w2, b2 = np.random.randn(), np.random.randn(), np.random.randn(), np.random.randn()\n",
        "    \n",
        "    # История для визуализации\n",
        "    weights_history = {'w1': [], 'w2': []}\n",
        "    biases_history = {'b1': [], 'b2': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        z1, a1, z2, y_pred = forward_pass(x, w1, b1, w2, b2)\n",
        "        \n",
        "        # Backpropagation\n",
        "        w1, b1, w2, b2 = backpropagation(x, y_true, y_pred, z1, a1, z2, w1, b1, w2, b2, learning_rate)\n",
        "        \n",
        "        # Запись истории\n",
        "        weights_history['w1'].append(w1)\n",
        "        weights_history['w2'].append(w2)\n",
        "        biases_history['b1'].append(b1)\n",
        "        biases_history['b2'].append(b2)\n",
        "        \n",
        "        # Вывод ошибки каждые 100 эпох\n",
        "        if epoch % 100 == 0:\n",
        "            loss = 0.5 * (y_pred - y_true) ** 2\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, y_pred = {y_pred:.4f}\")\n",
        "    \n",
        "    return weights_history, biases_history\n",
        "\n",
        "# Параметры обучения\n",
        "x = 0.5  # Входное значение\n",
        "y_true = 0.8  # Целевое значение\n",
        "epochs = 1000  # Количество эпох\n",
        "learning_rate = 0.5  # Скорость обучения\n",
        "\n",
        "# Запуск обучения\n",
        "weights_history, biases_history = train_network(x, y_true, epochs, learning_rate)\n",
        "\n",
        "# Визуализация\n",
        "plot_weights_and_biases(weights_history, biases_history)\n",
        "\n",
        "### Заключение\n",
        "\n",
        "1. **Forward pass** позволяет вычислить выход сети на основе входных данных.\n",
        "2. **Backpropagation** вычисляет градиенты функции потерь по параметрам сети, используя цепное правило.\n",
        "3. Параметры обновляются с использованием градиентного спуска.\n",
        "\n",
        "Это базовый алгоритм для простой MLP с одним скрытым слоем. Для более сложных сетей (например, с несколькими скрытыми слоями) процесс аналогичен, но требует большего количества вычислений.\n",
        "\n",
        "\n",
        "#Алгоритм Forward Pass и Backpropagation для Простой Многослойной Перцептронной Нейронной Сети (MLP) с Двумя Нейронами в Скрытом Слое\n",
        "\n",
        "#### Введение\n",
        "В этой лекции мы рассмотрим более сложную архитектуру многослойной перцептронной нейронной сети (MLP), которая состоит из:\n",
        "- одного входного нейрона,\n",
        "- одного скрытого слоя с **двумя нейронами**,\n",
        "- одного выходного нейрона.\n",
        "\n",
        "Мы детально разберем процесс **forward pass** (прямого прохода) и **backpropagation** (обратного распространения ошибки), чтобы понять, как работает обучение нейронной сети с несколькими нейронами в скрытом слое.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Архитектура сети\n",
        "\n",
        "Обозначим параметры сети:\n",
        "- $ x $: входное значение.\n",
        "- $ w_{11}, w_{12} $: веса между входным слоем и первым и вторым нейронами скрытого слоя соответственно.\n",
        "- $ b_1, b_2 $: смещения (bias) для первого и второго нейронов скрытого слоя.\n",
        "- $ w_3, w_4 $: веса между первым и вторым нейронами скрытого слоя и выходным нейроном.\n",
        "- $ b_3 $: смещение (bias) для выходного нейрона.\n",
        "- $ \\sigma(\\cdot) $: функция активации (например, сигмоидная или ReLU).\n",
        "\n",
        "Сеть вычисляет выходное значение $ y_{\\text{pred}} $ на основе входного значения $ x $. Мы также предполагаем, что у нас есть целевое значение $ y_{\\text{true}} $, которое используется для обучения сети.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Forward Pass (Прямой Проход)\n",
        "\n",
        "Forward pass — это процесс вычисления выходного значения сети на основе входных данных. Рассмотрим его шаг за шагом:\n",
        "\n",
        "#### Шаг 1: Вычисление взвешенных сумм для скрытого слоя\n",
        "Для каждого нейрона скрытого слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_1 = w_{11} \\cdot x + b_1\n",
        "$$\n",
        "$$\n",
        "z_2 = w_{12} \\cdot x + b_2\n",
        "$$\n",
        "где:\n",
        "- $ w_{11}, w_{12} $ — веса между входным слоем и нейронами скрытого слоя,\n",
        "- $ b_1, b_2 $ — смещения для нейронов скрытого слоя.\n",
        "\n",
        "#### Шаг 2: Применение функции активации для скрытого слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_1 $ и $ z_2 $, чтобы получить активации нейронов скрытого слоя:\n",
        "$$\n",
        "a_1 = \\sigma(z_1)\n",
        "$$\n",
        "$$\n",
        "a_2 = \\sigma(z_2)\n",
        "$$\n",
        "где $ a_1 $ и $ a_2 $ — активации первого и второго нейронов скрытого слоя.\n",
        "\n",
        "#### Шаг 3: Вычисление взвешенной суммы для выходного слоя\n",
        "Для выходного слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_3 = w_3 \\cdot a_1 + w_4 \\cdot a_2 + b_3\n",
        "$$\n",
        "где:\n",
        "- $ w_3, w_4 $ — веса между нейронами скрытого слоя и выходным нейроном,\n",
        "- $ b_3 $ — смещение для выходного нейрона.\n",
        "\n",
        "#### Шаг 4: Применение функции активации для выходного слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_3 $, чтобы получить окончательный выход сети:\n",
        "$$\n",
        "y_{\\text{pred}} = \\sigma(z_3)\n",
        "$$\n",
        "где $ y_{\\text{pred}} $ — предсказанное значение.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Backpropagation (Обратное Распространение Ошибки)\n",
        "\n",
        "Backpropagation — это процесс вычисления градиентов потерь по параметрам сети ($ w_{11}, w_{12}, b_1, b_2, w_3, w_4, b_3 $) с использованием цепного правила дифференцирования. Предположим, что функция потерь — это среднеквадратичная ошибка (MSE):\n",
        "$$\n",
        "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "$$\n",
        "\n",
        "Рассмотрим шаги backpropagation:\n",
        "\n",
        "#### Шаг 1: Вычисление производной функции потерь по $ y_{\\text{pred}} $\n",
        "Производная функции потерь по предсказанному значению:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = y_{\\text{pred}} - y_{\\text{true}}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление производной $ y_{\\text{pred}} $ по $ z_3 $\n",
        "Производная активации выходного слоя:\n",
        "$$\n",
        "\\frac{\\partial y_{\\text{pred}}}{\\partial z_3} = \\sigma'(z_3)\n",
        "$$\n",
        "где $ \\sigma'(z_3) $ — производная функции активации.\n",
        "\n",
        "#### Шаг 3: Вычисление производной функции потерь по $ z_3 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_3} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial z_3}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Вычисление производных $ z_3 $ по параметрам выходного слоя\n",
        "- Производная $ z_3 $ по $ w_3 $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial w_3} = a_1\n",
        "$$\n",
        "- Производная $ z_3 $ по $ w_4 $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial w_4} = a_2\n",
        "$$\n",
        "- Производная $ z_3 $ по $ b_3 $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial b_3} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_3, w_4, b_3 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_3} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_3} = \\frac{\\partial L}{\\partial z_3} \\cdot a_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_4} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_4} = \\frac{\\partial L}{\\partial z_3} \\cdot a_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3}\n",
        "$$\n",
        "\n",
        "#### Шаг 5: Вычисление производных $ z_3 $ по $ a_1 $ и $ a_2 $\n",
        "Производные $ z_3 $ по активациям скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial a_1} = w_3\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial a_2} = w_4\n",
        "$$\n",
        "\n",
        "#### Шаг 6: Вычисление производных $ a_1 $ и $ a_2 $ по $ z_1 $ и $ z_2 $\n",
        "Производные активаций скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial a_1}{\\partial z_1} = \\sigma'(z_1)\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial a_2}{\\partial z_2} = \\sigma'(z_2)\n",
        "$$\n",
        "\n",
        "#### Шаг 7: Вычисление производных $ z_3 $ по $ z_1 $ и $ z_2 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "#### Шаг 8: Вычисление производных $ z_1 $ и $ z_2 $ по параметрам скрытого слоя\n",
        "- Производная $ z_1 $ по $ w_{11} $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial w_{11}} = x\n",
        "$$\n",
        "- Производная $ z_2 $ по $ w_{12} $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial w_{12}} = x\n",
        "$$\n",
        "- Производная $ z_1 $ по $ b_1 $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial b_1} = 1\n",
        "$$\n",
        "- Производная $ z_2 $ по $ b_2 $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial b_2} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_{11}, w_{12}, b_1, b_2 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} = \\frac{\\partial L}{\\partial z_1} \\cdot x\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{12}} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{12}} = \\frac{\\partial L}{\\partial z_2} \\cdot x\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4. Обновление параметров\n",
        "\n",
        "После вычисления градиентов параметры обновляются с использованием метода градиентного спуска:\n",
        "$$\n",
        "w_{11} := w_{11} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{11}}\n",
        "$$\n",
        "$$\n",
        "w_{12} := w_{12} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{12}}\n",
        "$$\n",
        "$$\n",
        "b_1 := b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\n",
        "$$\n",
        "$$\n",
        "b_2 := b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
        "$$\n",
        "$$\n",
        "w_3 := w_3 - \\eta \\cdot \\frac{\\partial L}{\\partial w_3}\n",
        "$$\n",
        "$$\n",
        "w_4 := w_4 - \\eta \\cdot \\frac{\\partial L}{\\partial w_4}\n",
        "$$\n",
        "$$\n",
        "b_3 := b_3 - \\eta \\cdot \\frac{\\partial L}{\\partial b_3}\n",
        "$$\n",
        "где $ \\eta $ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "Давайте реализуем с нуля процесс forward pass и backpropagation для многослойного перцептрона (MLP) с двумя нейронами в скрытом слое на Python. Мы также добавим визуализацию, чтобы показать, как изменяются веса и смещения во время обучения.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Функция активации (сигмоида) и её производная\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(x, w11, b1, w12, b2, w3, w4, b3):\n",
        "    # Скрытый слой\n",
        "    z1 = w11 * x + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    \n",
        "    z2 = w12 * x + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    \n",
        "    # Выходной слой\n",
        "    z3 = w3 * a1 + w4 * a2 + b3\n",
        "    y_pred = sigmoid(z3)\n",
        "    \n",
        "    return z1, a1, z2, a2, z3, y_pred\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(x, y_true, y_pred, z1, a1, z2, a2, z3, w11, b1, w12, b2, w3, w4, b3, learning_rate):\n",
        "    # Шаг 1: Производная функции потерь по y_pred\n",
        "    dL_dy_pred = y_pred - y_true\n",
        "    \n",
        "    # Шаг 2: Производная y_pred по z3\n",
        "    dy_pred_dz3 = sigmoid_derivative(z3)\n",
        "    \n",
        "    # Шаг 3: Производная функции потерь по z3\n",
        "    dL_dz3 = dL_dy_pred * dy_pred_dz3\n",
        "    \n",
        "    # Шаг 4: Градиенты для выходного слоя\n",
        "    dz3_dw3 = a1\n",
        "    dz3_dw4 = a2\n",
        "    dL_dw3 = dL_dz3 * dz3_dw3\n",
        "    dL_dw4 = dL_dz3 * dz3_dw4\n",
        "    dL_db3 = dL_dz3 * 1  # dz3_db3 = 1\n",
        "    \n",
        "    # Шаг 5: Производные z3 по a1 и a2\n",
        "    dz3_da1 = w3\n",
        "    dz3_da2 = w4\n",
        "    \n",
        "    # Шаг 6: Производные a1 и a2 по z1 и z2\n",
        "    da1_dz1 = sigmoid_derivative(z1)\n",
        "    da2_dz2 = sigmoid_derivative(z2)\n",
        "    \n",
        "    # Шаг 7: Производные функции потерь по z1 и z2\n",
        "    dL_dz1 = dL_dz3 * dz3_da1 * da1_dz1\n",
        "    dL_dz2 = dL_dz3 * dz3_da2 * da2_dz2\n",
        "    \n",
        "    # Шаг 8: Градиенты для скрытого слоя\n",
        "    dz1_dw11 = x\n",
        "    dz2_dw12 = x\n",
        "    dL_dw11 = dL_dz1 * dz1_dw11\n",
        "    dL_dw12 = dL_dz2 * dz2_dw12\n",
        "    dL_db1 = dL_dz1 * 1  # dz1_db1 = 1\n",
        "    dL_db2 = dL_dz2 * 1  # dz2_db2 = 1\n",
        "    \n",
        "    # Обновление параметров\n",
        "    w11 -= learning_rate * dL_dw11\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    w12 -= learning_rate * dL_dw12\n",
        "    b2 -= learning_rate * dL_db2\n",
        "    w3 -= learning_rate * dL_dw3\n",
        "    w4 -= learning_rate * dL_dw4\n",
        "    b3 -= learning_rate * dL_db3\n",
        "    \n",
        "    return w11, b1, w12, b2, w3, w4, b3\n",
        "\n",
        "# Визуализация изменения весов и смещений\n",
        "def plot_weights_and_biases(weights_history, biases_history):\n",
        "    epochs = range(len(weights_history['w11']))\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # График для весов\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(epochs, weights_history['w11'], label='w11')\n",
        "    plt.plot(epochs, weights_history['w12'], label='w12')\n",
        "    plt.plot(epochs, weights_history['w3'], label='w3')\n",
        "    plt.plot(epochs, weights_history['w4'], label='w4')\n",
        "    plt.title('Веса (Weights)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    # График для смещений\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(epochs, biases_history['b1'], label='b1')\n",
        "    plt.plot(epochs, biases_history['b2'], label='b2')\n",
        "    plt.plot(epochs, biases_history['b3'], label='b3')\n",
        "    plt.title('Смещения (Biases)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Обучение сети\n",
        "def train_network(x, y_true, epochs, learning_rate):\n",
        "    # Инициализация параметров\n",
        "    w11, b1, w12, b2, w3, w4, b3 = (\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn()\n",
        "    )\n",
        "    \n",
        "    # История для визуализации\n",
        "    weights_history = {'w11': [], 'w12': [], 'w3': [], 'w4': []}\n",
        "    biases_history = {'b1': [], 'b2': [], 'b3': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        z1, a1, z2, a2, z3, y_pred = forward_pass(x, w11, b1, w12, b2, w3, w4, b3)\n",
        "        \n",
        "        # Backpropagation\n",
        "        w11, b1, w12, b2, w3, w4, b3 = backpropagation(\n",
        "            x, y_true, y_pred, z1, a1, z2, a2, z3,\n",
        "            w11, b1, w12, b2, w3, w4, b3, learning_rate\n",
        "        )\n",
        "        \n",
        "        # Запись истории\n",
        "        weights_history['w11'].append(w11)\n",
        "        weights_history['w12'].append(w12)\n",
        "        weights_history['w3'].append(w3)\n",
        "        weights_history['w4'].append(w4)\n",
        "        biases_history['b1'].append(b1)\n",
        "        biases_history['b2'].append(b2)\n",
        "        biases_history['b3'].append(b3)\n",
        "        \n",
        "        # Вывод ошибки каждые 100 эпох\n",
        "        if epoch % 100 == 0:\n",
        "            loss = 0.5 * (y_pred - y_true) ** 2\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, y_pred = {y_pred:.4f}\")\n",
        "    \n",
        "    return weights_history, biases_history\n",
        "\n",
        "# Параметры обучения\n",
        "x = 0.5  # Входное значение\n",
        "y_true = 0.8  # Целевое значение\n",
        "epochs = 1000  # Количество эпох\n",
        "learning_rate = 0.5  # Скорость обучения\n",
        "\n",
        "# Запуск обучения\n",
        "weights_history, biases_history = train_network(x, y_true, epochs, learning_rate)\n",
        "\n",
        "# Визуализация\n",
        "plot_weights_and_biases(weights_history, biases_history)\n",
        "\n",
        "\n",
        "### Заключение\n",
        "\n",
        "1. **Forward pass** позволяет вычислить выход сети на основе входных данных.\n",
        "2. **Backpropagation** вычисляет градиенты функции потерь по параметрам сети, используя цепное правило.\n",
        "3. Параметры обновляются с использованием градиентного спуска.\n",
        "\n",
        "\n",
        "#Алгоритм Forward Pass и Backpropagation для Простой Многослойной Перцептронной Нейронной Сети (MLP) с Двумя Нейронами в Каждом Слое\n",
        "\n",
        "#### Введение\n",
        "В этой лекции мы рассмотрим архитектуру многослойной перцептронной нейронной сети (MLP), которая состоит из:\n",
        "- одного входного слоя с **двумя нейронами**,\n",
        "- одного скрытого слоя с **двумя нейронами**,\n",
        "- одного выходного слоя с **одним нейроном**.\n",
        "\n",
        "Мы детально разберем процесс **forward pass** (прямого прохода) и **backpropagation** (обратного распространения ошибки), чтобы понять, как работает обучение нейронной сети с несколькими нейронами в каждом слое.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Архитектура сети\n",
        "\n",
        "Обозначим параметры сети:\n",
        "- $ x_1, x_2 $: входные значения.\n",
        "- $ w_{11}, w_{12}, w_{21}, w_{22} $: веса между входным и скрытым слоем ($ w_{ij} $ — вес от $ i $-го входного нейрона к $ j $-му нейрону скрытого слоя).\n",
        "- $ b_1, b_2 $: смещения (bias) для первого и второго нейронов скрытого слоя.\n",
        "- $ w_{31}, w_{32} $: веса между нейронами скрытого слоя и выходным нейроном.\n",
        "- $ b_3 $: смещение (bias) для выходного нейрона.\n",
        "- $ \\sigma(\\cdot) $: функция активации (например, сигмоидная или ReLU).\n",
        "\n",
        "Сеть вычисляет выходное значение $ y_{\\text{pred}} $ на основе входных значений $ x_1 $ и $ x_2 $. Мы также предполагаем, что у нас есть целевое значение $ y_{\\text{true}} $, которое используется для обучения сети.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Forward Pass (Прямой Проход)\n",
        "\n",
        "Forward pass — это процесс вычисления выходного значения сети на основе входных данных. Рассмотрим его шаг за шагом:\n",
        "\n",
        "#### Шаг 1: Вычисление взвешенных сумм для скрытого слоя\n",
        "Для каждого нейрона скрытого слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_1 = w_{11} \\cdot x_1 + w_{21} \\cdot x_2 + b_1\n",
        "$$\n",
        "$$\n",
        "z_2 = w_{12} \\cdot x_1 + w_{22} \\cdot x_2 + b_2\n",
        "$$\n",
        "где:\n",
        "- $ w_{11}, w_{12}, w_{21}, w_{22} $ — веса между входными нейронами и нейронами скрытого слоя,\n",
        "- $ b_1, b_2 $ — смещения для нейронов скрытого слоя.\n",
        "\n",
        "#### Шаг 2: Применение функции активации для скрытого слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_1 $ и $ z_2 $, чтобы получить активации нейронов скрытого слоя:\n",
        "$$\n",
        "a_1 = \\sigma(z_1)\n",
        "$$\n",
        "$$\n",
        "a_2 = \\sigma(z_2)\n",
        "$$\n",
        "где $ a_1 $ и $ a_2 $ — активации первого и второго нейронов скрытого слоя.\n",
        "\n",
        "#### Шаг 3: Вычисление взвешенной суммы для выходного слоя\n",
        "Для выходного слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_3 = w_{31} \\cdot a_1 + w_{32} \\cdot a_2 + b_3\n",
        "$$\n",
        "где:\n",
        "- $ w_{31}, w_{32} $ — веса между нейронами скрытого слоя и выходным нейроном,\n",
        "- $ b_3 $ — смещение для выходного нейрона.\n",
        "\n",
        "#### Шаг 4: Применение функции активации для выходного слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_3 $, чтобы получить окончательный выход сети:\n",
        "$$\n",
        "y_{\\text{pred}} = \\sigma(z_3)\n",
        "$$\n",
        "где $ y_{\\text{pred}} $ — предсказанное значение.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Backpropagation (Обратное Распространение Ошибки)\n",
        "\n",
        "Backpropagation — это процесс вычисления градиентов потерь по параметрам сети ($ w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2, w_{31}, w_{32}, b_3 $) с использованием цепного правила дифференцирования. Предположим, что функция потерь — это среднеквадратичная ошибка (MSE):\n",
        "$$\n",
        "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "$$\n",
        "\n",
        "Рассмотрим шаги backpropagation:\n",
        "\n",
        "#### Шаг 1: Вычисление производной функции потерь по $ y_{\\text{pred}} $\n",
        "Производная функции потерь по предсказанному значению:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = y_{\\text{pred}} - y_{\\text{true}}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление производной $ y_{\\text{pred}} $ по $ z_3 $\n",
        "Производная активации выходного слоя:\n",
        "$$\n",
        "\\frac{\\partial y_{\\text{pred}}}{\\partial z_3} = \\sigma'(z_3)\n",
        "$$\n",
        "где $ \\sigma'(z_3) $ — производная функции активации.\n",
        "\n",
        "#### Шаг 3: Вычисление производной функции потерь по $ z_3 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_3} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial z_3}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Вычисление производных $ z_3 $ по параметрам выходного слоя\n",
        "- Производная $ z_3 $ по $ w_{31} $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial w_{31}} = a_1\n",
        "$$\n",
        "- Производная $ z_3 $ по $ w_{32} $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial w_{32}} = a_2\n",
        "$$\n",
        "- Производная $ z_3 $ по $ b_3 $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial b_3} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_{31}, w_{32}, b_3 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{31}} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{31}} = \\frac{\\partial L}{\\partial z_3} \\cdot a_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{32}} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{32}} = \\frac{\\partial L}{\\partial z_3} \\cdot a_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3}\n",
        "$$\n",
        "\n",
        "#### Шаг 5: Вычисление производных $ z_3 $ по $ a_1 $ и $ a_2 $\n",
        "Производные $ z_3 $ по активациям скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial a_1} = w_{31}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial a_2} = w_{32}\n",
        "$$\n",
        "\n",
        "#### Шаг 6: Вычисление производных $ a_1 $ и $ a_2 $ по $ z_1 $ и $ z_2 $\n",
        "Производные активаций скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial a_1}{\\partial z_1} = \\sigma'(z_1)\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial a_2}{\\partial z_2} = \\sigma'(z_2)\n",
        "$$\n",
        "\n",
        "#### Шаг 7: Вычисление производных $ z_3 $ по $ z_1 $ и $ z_2 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "#### Шаг 8: Вычисление производных $ z_1 $ и $ z_2 $ по параметрам скрытого слоя\n",
        "- Производная $ z_1 $ по $ w_{11} $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial w_{11}} = x_1\n",
        "$$\n",
        "- Производная $ z_1 $ по $ w_{21} $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial w_{21}} = x_2\n",
        "$$\n",
        "- Производная $ z_2 $ по $ w_{12} $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial w_{12}} = x_1\n",
        "$$\n",
        "- Производная $ z_2 $ по $ w_{22} $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial w_{22}} = x_2\n",
        "$$\n",
        "- Производная $ z_1 $ по $ b_1 $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial b_1} = 1\n",
        "$$\n",
        "- Производная $ z_2 $ по $ b_2 $:\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial b_2} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} = \\frac{\\partial L}{\\partial z_1} \\cdot x_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{21}} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{21}} = \\frac{\\partial L}{\\partial z_1} \\cdot x_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{12}} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{12}} = \\frac{\\partial L}{\\partial z_2} \\cdot x_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{22}} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} = \\frac{\\partial L}{\\partial z_2} \\cdot x_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4. Обновление параметров\n",
        "\n",
        "После вычисления градиентов параметры обновляются с использованием метода градиентного спуска:\n",
        "$$\n",
        "w_{11} := w_{11} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{11}}\n",
        "$$\n",
        "$$\n",
        "w_{12} := w_{12} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{12}}\n",
        "$$\n",
        "$$\n",
        "w_{21} := w_{21} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{21}}\n",
        "$$\n",
        "$$\n",
        "w_{22} := w_{22} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{22}}\n",
        "$$\n",
        "$$\n",
        "b_1 := b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\n",
        "$$\n",
        "$$\n",
        "b_2 := b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
        "$$\n",
        "$$\n",
        "w_{31} := w_{31} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{31}}\n",
        "$$\n",
        "$$\n",
        "w_{32} := w_{32} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{32}}\n",
        "$$\n",
        "$$\n",
        "b_3 := b_3 - \\eta \\cdot \\frac{\\partial L}{\\partial b_3}\n",
        "$$\n",
        "где $ \\eta $ — скорость обучения (learning rate).\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Функция активации (сигмоида) и её производная\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(x1, x2, w11, w12, w21, w22, b1, b2, w31, w32, b3):\n",
        "    # Скрытый слой\n",
        "    z1 = w11 * x1 + w21 * x2 + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    \n",
        "    z2 = w12 * x1 + w22 * x2 + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    \n",
        "    # Выходной слой\n",
        "    z3 = w31 * a1 + w32 * a2 + b3\n",
        "    y_pred = sigmoid(z3)\n",
        "    \n",
        "    return z1, a1, z2, a2, z3, y_pred\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(\n",
        "    x1, x2, y_true, y_pred, z1, a1, z2, a2, z3,\n",
        "    w11, w12, w21, w22, b1, b2, w31, w32, b3, learning_rate\n",
        "):\n",
        "    # Шаг 1: Производная функции потерь по y_pred\n",
        "    dL_dy_pred = y_pred - y_true\n",
        "    \n",
        "    # Шаг 2: Производная y_pred по z3\n",
        "    dy_pred_dz3 = sigmoid_derivative(z3)\n",
        "    \n",
        "    # Шаг 3: Производная функции потерь по z3\n",
        "    dL_dz3 = dL_dy_pred * dy_pred_dz3\n",
        "    \n",
        "    # Шаг 4: Градиенты для выходного слоя\n",
        "    dz3_dw31 = a1\n",
        "    dz3_dw32 = a2\n",
        "    dL_dw31 = dL_dz3 * dz3_dw31\n",
        "    dL_dw32 = dL_dz3 * dz3_dw32\n",
        "    dL_db3 = dL_dz3 * 1  # dz3_db3 = 1\n",
        "    \n",
        "    # Шаг 5: Производные z3 по a1 и a2\n",
        "    dz3_da1 = w31\n",
        "    dz3_da2 = w32\n",
        "    \n",
        "    # Шаг 6: Производные a1 и a2 по z1 и z2\n",
        "    da1_dz1 = sigmoid_derivative(z1)\n",
        "    da2_dz2 = sigmoid_derivative(z2)\n",
        "    \n",
        "    # Шаг 7: Производные функции потерь по z1 и z2\n",
        "    dL_dz1 = dL_dz3 * dz3_da1 * da1_dz1\n",
        "    dL_dz2 = dL_dz3 * dz3_da2 * da2_dz2\n",
        "    \n",
        "    # Шаг 8: Градиенты для скрытого слоя\n",
        "    dz1_dw11 = x1\n",
        "    dz1_dw21 = x2\n",
        "    dz2_dw12 = x1\n",
        "    dz2_dw22 = x2\n",
        "    dL_dw11 = dL_dz1 * dz1_dw11\n",
        "    dL_dw21 = dL_dz1 * dz1_dw21\n",
        "    dL_dw12 = dL_dz2 * dz2_dw12\n",
        "    dL_dw22 = dL_dz2 * dz2_dw22\n",
        "    dL_db1 = dL_dz1 * 1  # dz1_db1 = 1\n",
        "    dL_db2 = dL_dz2 * 1  # dz2_db2 = 1\n",
        "    \n",
        "    # Обновление параметров\n",
        "    w11 -= learning_rate * dL_dw11\n",
        "    w21 -= learning_rate * dL_dw21\n",
        "    w12 -= learning_rate * dL_dw12\n",
        "    w22 -= learning_rate * dL_dw22\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    b2 -= learning_rate * dL_db2\n",
        "    w31 -= learning_rate * dL_dw31\n",
        "    w32 -= learning_rate * dL_dw32\n",
        "    b3 -= learning_rate * dL_db3\n",
        "    \n",
        "    return w11, w12, w21, w22, b1, b2, w31, w32, b3\n",
        "\n",
        "# Визуализация изменения весов и смещений\n",
        "def plot_weights_and_biases(weights_history, biases_history):\n",
        "    epochs = range(len(weights_history['w11']))\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # График для весов\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(epochs, weights_history['w11'], label='w11')\n",
        "    plt.plot(epochs, weights_history['w12'], label='w12')\n",
        "    plt.plot(epochs, weights_history['w21'], label='w21')\n",
        "    plt.plot(epochs, weights_history['w22'], label='w22')\n",
        "    plt.plot(epochs, weights_history['w31'], label='w31')\n",
        "    plt.plot(epochs, weights_history['w32'], label='w32')\n",
        "    plt.title('Веса (Weights)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    # График для смещений\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(epochs, biases_history['b1'], label='b1')\n",
        "    plt.plot(epochs, biases_history['b2'], label='b2')\n",
        "    plt.plot(epochs, biases_history['b3'], label='b3')\n",
        "    plt.title('Смещения (Biases)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    # График для ошибки\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(epochs, [0.5 * (y_true - y_pred) ** 2 for y_pred in biases_history['y_pred']], label='Loss')\n",
        "    plt.title('Ошибка (Loss)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Обучение сети\n",
        "def train_network(x1, x2, y_true, epochs, learning_rate):\n",
        "    # Инициализация параметров\n",
        "    w11, w12, w21, w22, b1, b2, w31, w32, b3 = (\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn()\n",
        "    )\n",
        "    \n",
        "    # История для визуализации\n",
        "    weights_history = {'w11': [], 'w12': [], 'w21': [], 'w22': [], 'w31': [], 'w32': []}\n",
        "    biases_history = {'b1': [], 'b2': [], 'b3': [], 'y_pred': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        z1, a1, z2, a2, z3, y_pred = forward_pass(x1, x2, w11, w12, w21, w22, b1, b2, w31, w32, b3)\n",
        "        \n",
        "        # Backpropagation\n",
        "        w11, w12, w21, w22, b1, b2, w31, w32, b3 = backpropagation(\n",
        "            x1, x2, y_true, y_pred, z1, a1, z2, a2, z3,\n",
        "            w11, w12, w21, w22, b1, b2, w31, w32, b3, learning_rate\n",
        "        )\n",
        "        \n",
        "        # Запись истории\n",
        "        weights_history['w11'].append(w11)\n",
        "        weights_history['w12'].append(w12)\n",
        "        weights_history['w21'].append(w21)\n",
        "        weights_history['w22'].append(w22)\n",
        "        weights_history['w31'].append(w31)\n",
        "        weights_history['w32'].append(w32)\n",
        "        biases_history['b1'].append(b1)\n",
        "        biases_history['b2'].append(b2)\n",
        "        biases_history['b3'].append(b3)\n",
        "        biases_history['y_pred'].append(y_pred)\n",
        "        \n",
        "        # Вывод ошибки каждые 100 эпох\n",
        "        if epoch % 100 == 0:\n",
        "            loss = 0.5 * (y_pred - y_true) ** 2\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, y_pred = {y_pred:.4f}\")\n",
        "    \n",
        "    return weights_history, biases_history\n",
        "\n",
        "# Параметры обучения\n",
        "x1, x2 = 0.5, 0.8  # Входные значения\n",
        "y_true = 0.9       # Целевое значение\n",
        "epochs = 1000      # Количество эпох\n",
        "learning_rate = 0.5  # Скорость обучения\n",
        "\n",
        "# Запуск обучения\n",
        "weights_history, biases_history = train_network(x1, x2, y_true, epochs, learning_rate)\n",
        "\n",
        "# Визуализация\n",
        "plot_weights_and_biases(weights_history, biases_history)\n",
        "\n",
        "### Заключение\n",
        "\n",
        "1. **Forward pass** позволяет вычислить выход сети на основе входных данных.\n",
        "2. **Backpropagation** вычисляет градиенты функции потерь по параметрам сети, используя цепное правило.\n",
        "3. Параметры обновляются с использованием градиентного спуска.\n",
        "\n",
        "Это базовый алгоритм для MLP с двумя нейронами в каждом слое. Для более сложных сетей (например, с большим количеством нейронов или слоев) процесс аналогичен, но требует большего количества вычислений.\n",
        "\n",
        "\n",
        "#Алгоритм Forward Pass и Backpropagation для MLP с Двумя Нейронами в Выходном Слое\n",
        "\n",
        "#### Введение\n",
        "В этой лекции мы рассмотрим архитектуру многослойной перцептронной нейронной сети (MLP), которая состоит из:\n",
        "- одного входного слоя с **двумя нейронами**,\n",
        "- одного скрытого слоя с **двумя нейронами**,\n",
        "- одного выходного слоя с **двумя нейронами**.\n",
        "\n",
        "Мы детально разберем процесс **forward pass** (прямого прохода) и **backpropagation** (обратного распространения ошибки), чтобы понять, как работает обучение нейронной сети с несколькими нейронами в каждом слое, включая выходной слой.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Архитектура сети\n",
        "\n",
        "Обозначим параметры сети:\n",
        "- $ x_1, x_2 $: входные значения.\n",
        "- $ w_{11}, w_{12}, w_{21}, w_{22} $: веса между входным и скрытым слоем ($ w_{ij} $ — вес от $ i $-го входного нейрона к $ j $-му нейрону скрытого слоя).\n",
        "- $ b_1, b_2 $: смещения (bias) для первого и второго нейронов скрытого слоя.\n",
        "- $ w_{31}, w_{32}, w_{41}, w_{42} $: веса между нейронами скрытого слоя и выходными нейронами ($ w_{ij} $ — вес от $ i $-го нейрона скрытого слоя к $ j $-му выходному нейрону).\n",
        "- $ b_3, b_4 $: смещения (bias) для первого и второго выходных нейронов.\n",
        "- $ \\sigma(\\cdot) $: функция активации (например, сигмоидная или ReLU).\n",
        "\n",
        "Сеть вычисляет два выходных значения $ y_{\\text{pred},1} $ и $ y_{\\text{pred},2} $ на основе входных значений $ x_1 $ и $ x_2 $. Мы также предполагаем, что у нас есть целевые значения $ y_{\\text{true},1} $ и $ y_{\\text{true},2} $, которые используются для обучения сети.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Forward Pass (Прямой Проход)\n",
        "\n",
        "Forward pass — это процесс вычисления выходных значений сети на основе входных данных. Рассмотрим его шаг за шагом:\n",
        "\n",
        "#### Шаг 1: Вычисление взвешенных сумм для скрытого слоя\n",
        "Для каждого нейрона скрытого слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_1 = w_{11} \\cdot x_1 + w_{21} \\cdot x_2 + b_1\n",
        "$$\n",
        "$$\n",
        "z_2 = w_{12} \\cdot x_1 + w_{22} \\cdot x_2 + b_2\n",
        "$$\n",
        "где:\n",
        "- $ w_{11}, w_{12}, w_{21}, w_{22} $ — веса между входными нейронами и нейронами скрытого слоя,\n",
        "- $ b_1, b_2 $ — смещения для нейронов скрытого слоя.\n",
        "\n",
        "#### Шаг 2: Применение функции активации для скрытого слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_1 $ и $ z_2 $, чтобы получить активации нейронов скрытого слоя:\n",
        "$$\n",
        "a_1 = \\sigma(z_1)\n",
        "$$\n",
        "$$\n",
        "a_2 = \\sigma(z_2)\n",
        "$$\n",
        "где $ a_1 $ и $ a_2 $ — активации первого и второго нейронов скрытого слоя.\n",
        "\n",
        "#### Шаг 3: Вычисление взвешенных сумм для выходного слоя\n",
        "Для каждого нейрона выходного слоя вычисляем взвешенную сумму входов:\n",
        "$$\n",
        "z_3 = w_{31} \\cdot a_1 + w_{41} \\cdot a_2 + b_3\n",
        "$$\n",
        "$$\n",
        "z_4 = w_{32} \\cdot a_1 + w_{42} \\cdot a_2 + b_4\n",
        "$$\n",
        "где:\n",
        "- $ w_{31}, w_{32}, w_{41}, w_{42} $ — веса между нейронами скрытого слоя и выходными нейронами,\n",
        "- $ b_3, b_4 $ — смещения для выходных нейронов.\n",
        "\n",
        "#### Шаг 4: Применение функции активации для выходного слоя\n",
        "Применяем функцию активации $ \\sigma(\\cdot) $ к $ z_3 $ и $ z_4 $, чтобы получить окончательные выходные значения сети:\n",
        "$$\n",
        "y_{\\text{pred},1} = \\sigma(z_3)\n",
        "$$\n",
        "$$\n",
        "y_{\\text{pred},2} = \\sigma(z_4)\n",
        "$$\n",
        "где $ y_{\\text{pred},1} $ и $ y_{\\text{pred},2} $ — предсказанные значения.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Backpropagation (Обратное Распространение Ошибки)\n",
        "\n",
        "Backpropagation — это процесс вычисления градиентов потерь по параметрам сети ($ w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2, w_{31}, w_{32}, w_{41}, w_{42}, b_3, b_4 $) с использованием цепного правила дифференцирования. Предположим, что функция потерь — это среднеквадратичная ошибка (MSE):\n",
        "$$\n",
        "L = \\frac{1}{2} \\left( (y_{\\text{pred},1} - y_{\\text{true},1})^2 + (y_{\\text{pred},2} - y_{\\text{true},2})^2 \\right)\n",
        "$$\n",
        "\n",
        "Рассмотрим шаги backpropagation:\n",
        "\n",
        "#### Шаг 1: Вычисление производной функции потерь по $ y_{\\text{pred},1} $ и $ y_{\\text{pred},2} $\n",
        "Производные функции потерь по предсказанным значениям:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{\\text{pred},1}} = y_{\\text{pred},1} - y_{\\text{true},1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{\\text{pred},2}} = y_{\\text{pred},2} - y_{\\text{true},2}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление производных $ y_{\\text{pred},1} $ и $ y_{\\text{pred},2} $ по $ z_3 $ и $ z_4 $\n",
        "Производные активаций выходного слоя:\n",
        "$$\n",
        "\\frac{\\partial y_{\\text{pred},1}}{\\partial z_3} = \\sigma'(z_3)\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial y_{\\text{pred},2}}{\\partial z_4} = \\sigma'(z_4)\n",
        "$$\n",
        "\n",
        "#### Шаг 3: Вычисление производных функции потерь по $ z_3 $ и $ z_4 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_3} = \\frac{\\partial L}{\\partial y_{\\text{pred},1}} \\cdot \\frac{\\partial y_{\\text{pred},1}}{\\partial z_3}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_4} = \\frac{\\partial L}{\\partial y_{\\text{pred},2}} \\cdot \\frac{\\partial y_{\\text{pred},2}}{\\partial z_4}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Вычисление производных $ z_3 $ и $ z_4 $ по параметрам выходного слоя\n",
        "- Производные $ z_3 $ и $ z_4 $ по $ w_{31}, w_{32}, w_{41}, w_{42} $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial w_{31}} = a_1, \\quad \\frac{\\partial z_3}{\\partial w_{41}} = a_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial z_4}{\\partial w_{32}} = a_1, \\quad \\frac{\\partial z_4}{\\partial w_{42}} = a_2\n",
        "$$\n",
        "- Производные $ z_3 $ и $ z_4 $ по $ b_3, b_4 $:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial b_3} = 1, \\quad \\frac{\\partial z_4}{\\partial b_4} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_{31}, w_{32}, w_{41}, w_{42}, b_3, b_4 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{31}} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{31}} = \\frac{\\partial L}{\\partial z_3} \\cdot a_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{41}} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{41}} = \\frac{\\partial L}{\\partial z_3} \\cdot a_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{32}} = \\frac{\\partial L}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial w_{32}} = \\frac{\\partial L}{\\partial z_4} \\cdot a_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{42}} = \\frac{\\partial L}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial w_{42}} = \\frac{\\partial L}{\\partial z_4} \\cdot a_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_4} = \\frac{\\partial L}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial b_4} = \\frac{\\partial L}{\\partial z_4}\n",
        "$$\n",
        "\n",
        "#### Шаг 5: Вычисление производных $ z_3 $ и $ z_4 $ по $ a_1 $ и $ a_2 $\n",
        "Производные $ z_3 $ и $ z_4 $ по активациям скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial z_3}{\\partial a_1} = w_{31}, \\quad \\frac{\\partial z_3}{\\partial a_2} = w_{41}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial z_4}{\\partial a_1} = w_{32}, \\quad \\frac{\\partial z_4}{\\partial a_2} = w_{42}\n",
        "$$\n",
        "\n",
        "#### Шаг 6: Вычисление производных $ a_1 $ и $ a_2 $ по $ z_1 $ и $ z_2 $\n",
        "Производные активаций скрытого слоя:\n",
        "$$\n",
        "\\frac{\\partial a_1}{\\partial z_1} = \\sigma'(z_1), \\quad \\frac{\\partial a_2}{\\partial z_2} = \\sigma'(z_2)\n",
        "$$\n",
        "\n",
        "#### Шаг 7: Вычисление производных $ z_3 $ и $ z_4 $ по $ z_1 $ и $ z_2 $\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} + \\frac{\\partial L}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} + \\frac{\\partial L}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "#### Шаг 8: Вычисление производных $ z_1 $ и $ z_2 $ по параметрам скрытого слоя\n",
        "- Производные $ z_1 $ и $ z_2 $ по $ w_{11}, w_{12}, w_{21}, w_{22} $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial w_{11}} = x_1, \\quad \\frac{\\partial z_1}{\\partial w_{21}} = x_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial z_2}{\\partial w_{12}} = x_1, \\quad \\frac{\\partial z_2}{\\partial w_{22}} = x_2\n",
        "$$\n",
        "- Производные $ z_1 $ и $ z_2 $ по $ b_1, b_2 $:\n",
        "$$\n",
        "\\frac{\\partial z_1}{\\partial b_1} = 1, \\quad \\frac{\\partial z_2}{\\partial b_2} = 1\n",
        "$$\n",
        "\n",
        "Тогда градиенты для $ w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2 $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} = \\frac{\\partial L}{\\partial z_1} \\cdot x_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{21}} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{21}} = \\frac{\\partial L}{\\partial z_1} \\cdot x_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{12}} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{12}} = \\frac{\\partial L}{\\partial z_2} \\cdot x_1\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{22}} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} = \\frac{\\partial L}{\\partial z_2} \\cdot x_2\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4. Обновление параметров\n",
        "\n",
        "После вычисления градиентов параметры обновляются с использованием метода градиентного спуска:\n",
        "$$\n",
        "w := w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "где $ \\eta $ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "\n",
        "Давайте реализуем с нуля процесс forward pass и backpropagation для многослойного перцептрона (MLP) с двумя нейронами в каждом слое, включая выходной слой. Мы также добавим визуализацию, чтобы показать, как изменяются веса и смещения во время обучения.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Функция активации (сигмоида) и её производная\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(x1, x2, w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4):\n",
        "    # Скрытый слой\n",
        "    z1 = w11 * x1 + w21 * x2 + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    \n",
        "    z2 = w12 * x1 + w22 * x2 + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    \n",
        "    # Выходной слой\n",
        "    z3 = w31 * a1 + w41 * a2 + b3\n",
        "    y_pred1 = sigmoid(z3)\n",
        "    \n",
        "    z4 = w32 * a1 + w42 * a2 + b4\n",
        "    y_pred2 = sigmoid(z4)\n",
        "    \n",
        "    return z1, a1, z2, a2, z3, y_pred1, z4, y_pred2\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(\n",
        "    x1, x2, y_true1, y_true2, y_pred1, y_pred2, z1, a1, z2, a2, z3, z4,\n",
        "    w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4, learning_rate\n",
        "):\n",
        "    # Шаг 1: Производные функции потерь по y_pred1 и y_pred2\n",
        "    dL_dy_pred1 = y_pred1 - y_true1\n",
        "    dL_dy_pred2 = y_pred2 - y_true2\n",
        "    \n",
        "    # Шаг 2: Производные y_pred1 и y_pred2 по z3 и z4\n",
        "    dy_pred1_dz3 = sigmoid_derivative(z3)\n",
        "    dy_pred2_dz4 = sigmoid_derivative(z4)\n",
        "    \n",
        "    # Шаг 3: Производные функции потерь по z3 и z4\n",
        "    dL_dz3 = dL_dy_pred1 * dy_pred1_dz3\n",
        "    dL_dz4 = dL_dy_pred2 * dy_pred2_dz4\n",
        "    \n",
        "    # Шаг 4: Градиенты для выходного слоя\n",
        "    dz3_dw31 = a1\n",
        "    dz3_dw41 = a2\n",
        "    dz4_dw32 = a1\n",
        "    dz4_dw42 = a2\n",
        "    dL_dw31 = dL_dz3 * dz3_dw31\n",
        "    dL_dw41 = dL_dz3 * dz3_dw41\n",
        "    dL_dw32 = dL_dz4 * dz4_dw32\n",
        "    dL_dw42 = dL_dz4 * dz4_dw42\n",
        "    dL_db3 = dL_dz3 * 1  # dz3_db3 = 1\n",
        "    dL_db4 = dL_dz4 * 1  # dz4_db4 = 1\n",
        "    \n",
        "    # Шаг 5: Производные z3 и z4 по a1 и a2\n",
        "    dz3_da1 = w31\n",
        "    dz3_da2 = w41\n",
        "    dz4_da1 = w32\n",
        "    dz4_da2 = w42\n",
        "    \n",
        "    # Шаг 6: Производные a1 и a2 по z1 и z2\n",
        "    da1_dz1 = sigmoid_derivative(z1)\n",
        "    da2_dz2 = sigmoid_derivative(z2)\n",
        "    \n",
        "    # Шаг 7: Производные функции потерь по z1 и z2\n",
        "    dL_dz1 = (\n",
        "        dL_dz3 * dz3_da1 * da1_dz1 +\n",
        "        dL_dz4 * dz4_da1 * da1_dz1\n",
        "    )\n",
        "    dL_dz2 = (\n",
        "        dL_dz3 * dz3_da2 * da2_dz2 +\n",
        "        dL_dz4 * dz4_da2 * da2_dz2\n",
        "    )\n",
        "    \n",
        "    # Шаг 8: Градиенты для скрытого слоя\n",
        "    dz1_dw11 = x1\n",
        "    dz1_dw21 = x2\n",
        "    dz2_dw12 = x1\n",
        "    dz2_dw22 = x2\n",
        "    dL_dw11 = dL_dz1 * dz1_dw11\n",
        "    dL_dw21 = dL_dz1 * dz1_dw21\n",
        "    dL_dw12 = dL_dz2 * dz2_dw12\n",
        "    dL_dw22 = dL_dz2 * dz2_dw22\n",
        "    dL_db1 = dL_dz1 * 1  # dz1_db1 = 1\n",
        "    dL_db2 = dL_dz2 * 1  # dz2_db2 = 1\n",
        "    \n",
        "    # Обновление параметров\n",
        "    w11 -= learning_rate * dL_dw11\n",
        "    w21 -= learning_rate * dL_dw21\n",
        "    w12 -= learning_rate * dL_dw12\n",
        "    w22 -= learning_rate * dL_dw22\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    b2 -= learning_rate * dL_db2\n",
        "    w31 -= learning_rate * dL_dw31\n",
        "    w41 -= learning_rate * dL_dw41\n",
        "    w32 -= learning_rate * dL_dw32\n",
        "    w42 -= learning_rate * dL_dw42\n",
        "    b3 -= learning_rate * dL_db3\n",
        "    b4 -= learning_rate * dL_db4\n",
        "    \n",
        "    return w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4\n",
        "\n",
        "# Визуализация изменения весов и смещений\n",
        "def plot_weights_and_biases(weights_history, biases_history):\n",
        "    epochs = range(len(weights_history['w11']))\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # График для весов\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(epochs, weights_history['w11'], label='w11')\n",
        "    plt.plot(epochs, weights_history['w12'], label='w12')\n",
        "    plt.plot(epochs, weights_history['w21'], label='w21')\n",
        "    plt.plot(epochs, weights_history['w22'], label='w22')\n",
        "    plt.plot(epochs, weights_history['w31'], label='w31')\n",
        "    plt.plot(epochs, weights_history['w32'], label='w32')\n",
        "    plt.plot(epochs, weights_history['w41'], label='w41')\n",
        "    plt.plot(epochs, weights_history['w42'], label='w42')\n",
        "    plt.title('Веса (Weights)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    # График для смещений\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(epochs, biases_history['b1'], label='b1')\n",
        "    plt.plot(epochs, biases_history['b2'], label='b2')\n",
        "    plt.plot(epochs, biases_history['b3'], label='b3')\n",
        "    plt.plot(epochs, biases_history['b4'], label='b4')\n",
        "    plt.title('Смещения (Biases)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    # График для ошибки\n",
        "    plt.subplot(3, 1, 3)\n",
        "    losses = [\n",
        "        0.5 * ((y_true1 - y_pred1) ** 2 + (y_true2 - y_pred2) ** 2)\n",
        "        for y_pred1, y_pred2 in zip(biases_history['y_pred1'], biases_history['y_pred2'])\n",
        "    ]\n",
        "    plt.plot(epochs, losses, label='Loss')\n",
        "    plt.title('Ошибка (Loss)')\n",
        "    plt.xlabel('Эпохи')\n",
        "    plt.ylabel('Значение')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Обучение сети\n",
        "def train_network(x1, x2, y_true1, y_true2, epochs, learning_rate):\n",
        "    # Инициализация параметров\n",
        "    w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4 = (\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn(),\n",
        "        np.random.randn(), np.random.randn()\n",
        "    )\n",
        "    \n",
        "    # История для визуализации\n",
        "    weights_history = {'w11': [], 'w12': [], 'w21': [], 'w22': [], 'w31': [], 'w32': [], 'w41': [], 'w42': []}\n",
        "    biases_history = {'b1': [], 'b2': [], 'b3': [], 'b4': [], 'y_pred1': [], 'y_pred2': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        z1, a1, z2, a2, z3, y_pred1, z4, y_pred2 = forward_pass(\n",
        "            x1, x2, w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4\n",
        "        )\n",
        "        \n",
        "        # Backpropagation\n",
        "        w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4 = backpropagation(\n",
        "            x1, x2, y_true1, y_true2, y_pred1, y_pred2, z1, a1, z2, a2, z3, z4,\n",
        "            w11, w12, w21, w22, b1, b2, w31, w32, w41, w42, b3, b4, learning_rate\n",
        "        )\n",
        "        \n",
        "        # Запись истории\n",
        "        weights_history['w11'].append(w11)\n",
        "        weights_history['w12'].append(w12)\n",
        "        weights_history['w21'].append(w21)\n",
        "        weights_history['w22'].append(w22)\n",
        "        weights_history['w31'].append(w31)\n",
        "        weights_history['w32'].append(w32)\n",
        "        weights_history['w41'].append(w41)\n",
        "        weights_history['w42'].append(w42)\n",
        "        biases_history['b1'].append(b1)\n",
        "        biases_history['b2'].append(b2)\n",
        "        biases_history['b3'].append(b3)\n",
        "        biases_history['b4'].append(b4)\n",
        "        biases_history['y_pred1'].append(y_pred1)\n",
        "        biases_history['y_pred2'].append(y_pred2)\n",
        "        \n",
        "        # Вывод ошибки каждые 100 эпох\n",
        "        if epoch % 100 == 0:\n",
        "            loss = 0.5 * ((y_true1 - y_pred1) ** 2 + (y_true2 - y_pred2) ** 2)\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, y_pred1 = {y_pred1:.4f}, y_pred2 = {y_pred2:.4f}\")\n",
        "    \n",
        "    return weights_history, biases_history\n",
        "\n",
        "# Параметры обучения\n",
        "x1, x2 = 0.5, 0.8  # Входные значения\n",
        "y_true1, y_true2 = 0.9, 0.2  # Целевые значения\n",
        "epochs = 2000      # Количество эпох\n",
        "learning_rate = 0.5  # Скорость обучения\n",
        "\n",
        "# Запуск обучения\n",
        "weights_history, biases_history = train_network(x1, x2, y_true1, y_true2, epochs, learning_rate)\n",
        "\n",
        "# Визуализация\n",
        "plot_weights_and_biases(weights_history, biases_history)\n",
        "\n",
        "### Заключение\n",
        "\n",
        "1. **Forward pass** позволяет вычислить выход сети на основе входных данных.\n",
        "2. **Backpropagation** вычисляет градиенты функции потерь по параметрам сети, используя цепное правило.\n",
        "3. Параметры обновляются с использованием градиентного спуска.\n",
        "\n",
        "Это базовый алгоритм для MLP с двумя нейронами в каждом слое, включая выходной слой. Для более сложных сетей (например, с большим количеством нейронов или слоев) процесс аналогичен, но требует большего количества вычислений.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PD8yGlDz4CKd"
      }
    }
  ]
}