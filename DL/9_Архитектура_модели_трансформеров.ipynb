{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8U22cBmNYuPT2Q/Y8T9aB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/DL/9_%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8_%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Архитектура модели трансформеров\n",
        "\n",
        "## Введение\n",
        "\n",
        "Современные задачи обработки естественного языка (NLP) требуют мощных моделей, способных эффективно обрабатывать большие объемы текстовых данных. Одним из наиболее значимых прорывов в этой области стало появление архитектуры **трансформеров** (Transformers). Эти модели стали основой для таких известных моделей, как BERT, GPT и T5, которые показали выдающиеся результаты на различных задачах NLP.\n",
        "\n",
        "В этой лекции мы подробно рассмотрим основные компоненты архитектуры трансформеров, их функциональность и особенности. Мы также поговорим о том, почему именно эта архитектура стала настолько популярной и успешной.\n",
        "\n",
        "\n",
        "\n",
        "## 1. История возникновения трансформеров\n",
        "\n",
        "До появления трансформеров, основными архитектурами для обработки последовательностей были рекуррентные нейронные сети (RNN), долгосрочная краткосрочная память (LSTM) и гейтед рекуррентные сети (GRU). Однако эти модели имели свои ограничения:\n",
        "\n",
        "- **Зависимость от порядка**: RNN обрабатывают данные последовательно, что делает их неэффективными при длинных последовательностях.\n",
        "- **Проблема с долгосрочной зависимостью**: LSTM и GRU решают проблему долгосрочной зависимости лучше, но все равно страдают от затруднений при обучении на длинных последовательностях.\n",
        "- **Параллельизация**: Поскольку RNN работают последовательно, параллельное выполнение операций становится невозможным, что замедляет обучение.\n",
        "\n",
        "Трансформеры, представленные в статье \"Attention is All You Need\" в 2017 году, предложили новый подход к обработке последовательностей, который устраняет многие из этих недостатков.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Основные принципы работы трансформера\n",
        "\n",
        "\n",
        "\n",
        "### 2.1. Модуль внимания (Attention Mechanism)\n",
        "\n",
        "Механизм внимания является ключевым компонентом архитектуры трансформеров и позволяет модели эффективно учитывать зависимости между различными частями входных данных. В данном разделе мы подробно рассмотрим, как работает механизм внимания, что означают матрицы $Q$, $K$, $V$, параметр $d_k$ и функция активации $\\text{softmax}$.\n",
        "\n",
        "#### Пример для понимания\n",
        "\n",
        "Предположим, у нас есть два предложения:\n",
        "1. \"Я люблю читать книги.\"\n",
        "2. \"Мне нравится играть в футбол.\"\n",
        "\n",
        "Когда модель должна перевести это предложение на другой язык, она может использовать механизм внимания, чтобы \"уделять внимание\" различным словам в зависимости от контекста. Например, если модель работает над переводом слова \"читать\", она будет уделять больше внимания слову \"книги\", чем другим словам в предложении. Это потому, что слово \"читать\" имеет сильную семантическую связь с \"книгами\".\n",
        "\n",
        "Теперь давайте разберем формально, как это реализуется в механизме внимания.\n",
        "\n",
        "#### Формула механизма внимания\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $Q$ — матрица запросов (queries),\n",
        "- $K$ — матрица ключей (keys),\n",
        "- $V$ — матрица значений (values),\n",
        "- $d_k$ — размерность ключей,\n",
        "- $\\text{softmax}$ — функция активации softmax, которая используется для нормализации весов.\n",
        "\n",
        "#### Разбор каждого элемента:\n",
        "\n",
        "##### 1. Матрица запросов ($Q$):\n",
        "\n",
        "Матрица запросов $Q$ представляет собой набор векторов запросов. Каждый вектор запроса соответствует определенному элементу входной последовательности, который модель пытается \"понять\" или \"перевести\". Например, если модель работает с предложением \"Я люблю читать книги.\", то каждый из слов \"Я\", \"люблю\", \"читать\", \"книги\" будет иметь свой вектор запроса в матрице $Q$.\n",
        "\n",
        "Формально, если у нас есть последовательность длины $T$ и размерность скрытого пространства $d_{model}$, то $Q \\in \\mathbb{R}^{T \\times d_{model}}$.\n",
        "\n",
        "##### 2. Матрица ключей ($K$):\n",
        "\n",
        "Матрица ключей $K$ также представляет собой набор векторов ключей, которые используются для вычисления степени совпадения между запросами и элементами входной последовательности. Эти ключи помогают модели определить, какие элементы входной последовательности наиболее релевантны для текущего запроса.\n",
        "\n",
        "Формально, $K \\in \\mathbb{R}^{T \\times d_{model}}$.\n",
        "\n",
        "##### 3. Матрица значений ($V$):\n",
        "\n",
        "Матрица значений $V$ содержит информацию, которую модель будет использовать для генерации выходного представления. После того как модель определила степень релевантности каждого элемента входной последовательности (используя матрицы $Q$ и $K$), она использует значения из матрицы $V$ для создания окончательного выходного представления.\n",
        "\n",
        "Формально, $V \\in \\mathbb{R}^{T \\times d_{model}}$.\n",
        "\n",
        "##### 4. Размерность ключей ($d_k$):\n",
        "\n",
        "Размерность ключей $d_k$ — это размерность векторов ключей и запросов. Обычно $d_k$ равен размерности скрытого пространства $d_{model}$. Однако в многослойном внимании (multi-head attention) $d_k$ может быть меньше, чтобы уменьшить количество параметров и улучшить производительность модели.\n",
        "\n",
        "##### 5. Функция активации softmax:\n",
        "\n",
        "Функция активации softmax применяется к результату скалярного произведения $QK^T$ для нормализации весов. Она преобразует эти веса в вероятности, которые показывают, насколько каждый элемент входной последовательности важен для текущего запроса.\n",
        "\n",
        "Формально, если у нас есть матрица $S \\in \\mathbb{R}^{T \\times T}$, где $S_{ij}$ — это скалярное произведение $q_i \\cdot k_j$, то softmax применяется по строкам этой матрицы:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(S)_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^T \\exp(S_{ik})}\n",
        "$$\n",
        "\n",
        "Это означает, что сумма всех весов в каждой строке будет равна 1, что делает их интерпретируемыми как вероятности.\n",
        "\n",
        "#### Как работает механизм внимания:\n",
        "\n",
        "1. **Вычисление скалярного произведения**: Для каждого запроса $q_i$ вычисляется скалярное произведение с каждым ключом $k_j$. Это дает нам матрицу $S \\in \\mathbb{R}^{T \\times T}$, где каждый элемент $S_{ij}$ показывает степень совпадения между запросом $i$ и ключом $j$.\n",
        "\n",
        "2. **Нормализация весов**: Скалярное произведение $S$ нормализуется с помощью функции softmax, чтобы получить матрицу весов $A \\in \\mathbb{R}^{T \\times T}$. Эта матрица показывает, насколько каждый элемент входной последовательности важен для текущего запроса.\n",
        "\n",
        "3. **Вычисление контекстного вектора**: Матрица весов $A$ умножается на матрицу значений $V$, чтобы получить контекстный вектор для каждого запроса. Этот контекстный вектор представляет собой комбинацию значений входной последовательности, взвешенных по их степени релевантности для текущего запроса.\n",
        "\n",
        "Формально, контекстный вектор $C$ вычисляется следующим образом:\n",
        "\n",
        "$$\n",
        "C = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Этот контекстный вектор затем используется для генерации выходного представления модели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Конкретный числовой пример механизма внимания\n",
        "\n",
        "Давайте рассмотрим конкретный числовой пример, чтобы лучше понять, как работает механизм внимания. Предположим, у нас есть простая последовательность из трех слов: \"Я\", \"люблю\", \"книги\". Мы будем использовать однослойное внимание для демонстрации.\n",
        "\n",
        "#### Шаг 1: Входные данные и проекция\n",
        "\n",
        "Представим каждое слово в виде вектора размерности $d_{model} = 4$. Для простоты возьмем следующие случайные векторы:\n",
        "\n",
        "- \"Я\" -> $x_1 = [0.1, 0.2, 0.3, 0.4]$\n",
        "- \"люблю\" -> $x_2 = [0.5, 0.6, 0.7, 0.8]$\n",
        "- \"книги\" -> $x_3 = [0.9, 1.0, 1.1, 1.2]$\n",
        "\n",
        "Матрица входных данных $X$ будет выглядеть так:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь мы используем обучаемые параметры для получения матриц запросов $Q$, ключей $K$ и значений $V$:\n",
        "\n",
        "$$\n",
        "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
        "$$\n",
        "\n",
        "где $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
        "\n",
        "Для простоты предположим, что $d_k = 4$ (то есть размерность скрытого пространства равна размерности ключей). Возьмем случайные значения для $W_Q, W_K, W_V$:\n",
        "\n",
        "$$\n",
        "W_Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}, \\quad\n",
        "W_K = \\begin{bmatrix}\n",
        "0.2 & 0.3 & 0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 & 0.8 & 0.9 \\\\\n",
        "1.0 & 1.1 & 1.2 & 1.3 \\\\\n",
        "1.4 & 1.5 & 1.6 & 1.7\n",
        "\\end{bmatrix}, \\quad\n",
        "W_V = \\begin{bmatrix}\n",
        "0.3 & 0.4 & 0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 & 0.9 & 1.0 \\\\\n",
        "1.1 & 1.2 & 1.3 & 1.4 \\\\\n",
        "1.5 & 1.6 & 1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим матрицы $Q$, $K$ и $V$:\n",
        "\n",
        "$$\n",
        "Q = XW_Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1.4 & 1.6 & 1.8 & 2.0 \\\\\n",
        "3.4 & 3.8 & 4.2 & 4.6 \\\\\n",
        "5.4 & 6.0 & 6.6 & 7.2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K = XW_K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 & 0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 & 0.8 & 0.9 \\\\\n",
        "1.0 & 1.1 & 1.2 & 1.3 \\\\\n",
        "1.4 & 1.5 & 1.6 & 1.7\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1.8 & 2.0 & 2.2 & 2.4 \\\\\n",
        "4.2 & 4.6 & 5.0 & 5.4 \\\\\n",
        "6.6 & 7.2 & 7.8 & 8.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V = XW_V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 & 0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 & 0.9 & 1.0 \\\\\n",
        "1.1 & 1.2 & 1.3 & 1.4 \\\\\n",
        "1.5 & 1.6 & 1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "2.2 & 2.4 & 2.6 & 2.8 \\\\\n",
        "5.0 & 5.4 & 5.8 & 6.2 \\\\\n",
        "7.8 & 8.4 & 9.0 & 9.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление скалярного произведения\n",
        "\n",
        "Вычислим скалярное произведение $QK^T$:\n",
        "\n",
        "$$\n",
        "QK^T = \\begin{bmatrix}\n",
        "1.4 & 1.6 & 1.8 & 2.0 \\\\\n",
        "3.4 & 3.8 & 4.2 & 4.6 \\\\\n",
        "5.4 & 6.0 & 6.6 & 7.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1.8 & 4.2 & 6.6 \\\\\n",
        "2.0 & 4.6 & 7.2 \\\\\n",
        "2.2 & 5.0 & 7.8 \\\\\n",
        "2.4 & 5.4 & 8.4\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "14.8 & 34.4 & 54.0 \\\\\n",
        "34.4 & 81.2 & 128.0 \\\\\n",
        "54.0 & 128.0 & 202.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 3: Нормализация весов с помощью softmax\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $QK^T / \\sqrt{d_k}$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(S)_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^T \\exp(S_{ik})}\n",
        "$$\n",
        "\n",
        "Для $d_k = 4$:\n",
        "\n",
        "$$\n",
        "\\frac{QK^T}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{4}} \\begin{bmatrix}\n",
        "14.8 & 34.4 & 54.0 \\\\\n",
        "34.4 & 81.2 & 128.0 \\\\\n",
        "54.0 & 128.0 & 202.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "7.4 & 17.2 & 27.0 \\\\\n",
        "17.2 & 40.6 & 64.0 \\\\\n",
        "27.0 & 64.0 & 101.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь применим softmax:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "7.4 & 17.2 & 27.0 \\\\\n",
        "17.2 & 40.6 & 64.0 \\\\\n",
        "27.0 & 64.0 & 101.0\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.0000 & 0.0001 & 0.9999 \\\\\n",
        "0.0001 & 0.0047 & 0.9952 \\\\\n",
        "0.0002 & 0.0133 & 0.9865\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Вычисление контекстного вектора\n",
        "\n",
        "Умножим матрицу весов на матрицу значений $V$:\n",
        "\n",
        "$$\n",
        "C = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\approx \\begin{bmatrix}\n",
        "0.0000 & 0.0001 & 0.9999 \\\\\n",
        "0.0001 & 0.0047 & 0.9952 \\\\\n",
        "0.0002 & 0.0133 & 0.9865\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "2.2 & 2.4 & 2.6 & 2.8 \\\\\n",
        "5.0 & 5.4 & 5.8 & 6.2 \\\\\n",
        "7.8 & 8.4 & 9.0 & 9.6\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "7.8 & 8.4 & 9.0 & 9.6 \\\\\n",
        "7.8 & 8.4 & 9.0 & 9.6 \\\\\n",
        "7.8 & 8.4 & 9.0 & 9.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, контекстный вектор для каждого слова в последовательности будет приближенно равен $[7.8, 8.4, 9.0, 9.6]$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 2.2. Масштабируемое точечное произведение (Scaled Dot-Product Attention)\n",
        "\n",
        "Масштабируемое точечное произведение внимания (Scaled Dot-Product Attention) — это конкретный тип механизма внимания, который используется в архитектуре трансформеров. Этот механизм позволяет модели эффективно учитывать зависимости между различными элементами входной последовательности и масштабирует значения, чтобы избежать слишком больших или маленьких значений во время обучения.\n",
        "\n",
        "#### Принцип работы\n",
        "\n",
        "Основная идея заключается в вычислении скалярного произведения между матрицей запросов $Q$ и матрицей ключей $K$, что позволяет определить степень совпадения между запросами и ключами. Затем результат этого скалярного произведения нормализуется с помощью функции softmax для получения весов, которые показывают, насколько каждый элемент входной последовательности важен для текущего запроса. Наконец, эти веса используются для взвешивания значений из матрицы $V$, чтобы получить контекстный вектор.\n",
        "\n",
        "Формула масштабируемого точечного произведения внимания:\n",
        "\n",
        "$$\n",
        "\\text{Scaled Dot-Product Attention} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $Q$ — матрица запросов,\n",
        "- $K$ — матрица ключей,\n",
        "- $V$ — матрица значений,\n",
        "- $d_k$ — размерность ключей,\n",
        "- $\\text{softmax}$ — функция активации softmax, которая используется для нормализации весов.\n",
        "\n",
        "#### Подробное объяснение каждого компонента:\n",
        "\n",
        "1. **Скалярное произведение $QK^T$**:\n",
        "   - Скалярное произведение между матрицей запросов $Q$ и матрицей ключей $K$ дает нам матрицу, где каждый элемент $S_{ij}$ показывает степень совпадения между запросом $i$ и ключом $j$.\n",
        "   - Это произведение может привести к большим значениям, особенно если размерность $d_k$ велика. Чтобы избежать проблем с градиентами при обучении, результат скалярного произведения делится на $\\sqrt{d_k}$.\n",
        "\n",
        "2. **Масштабирование $\\frac{QK^T}{\\sqrt{d_k}}$**:\n",
        "   - Масштабирование помогает стабилизировать процесс обучения, предотвращая слишком большие или маленькие значения в выходных данных.\n",
        "   - Например, если размерность $d_k$ равна 64, то результат скалярного произведения будет делиться на $\\sqrt{64} = 8$.\n",
        "\n",
        "3. **Нормализация с помощью softmax**:\n",
        "   - Функция softmax применяется к каждой строке результата скалярного произведения, чтобы преобразовать его в вероятности.\n",
        "   - Это означает, что сумма всех весов в каждой строке будет равна 1, что делает их интерпретируемыми как вероятности.\n",
        "   - Формально, если у нас есть матрица $S \\in \\mathbb{R}^{T \\times T}$, где $S_{ij}$ — это скалярное произведение $q_i \\cdot k_j$, то softmax применяется по строкам этой матрицы:\n",
        "\n",
        "  $$\n",
        "     \\text{softmax}(S)_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^T \\exp(S_{ik})}\n",
        "  $$\n",
        "\n",
        "4. **Взвешивание значений $V$**:\n",
        "   - После нормализации весов с помощью softmax, они умножаются на матрицу значений $V$ для получения контекстного вектора.\n",
        "   - Каждый элемент контекстного вектора представляет собой комбинацию значений входной последовательности, взвешенных по их степени релевантности для текущего запроса.\n",
        "\n",
        "Таким образом, масштабируемое точечное произведение внимания позволяет модели эффективно учитывать зависимости между различными элементами входной последовательности и создавать точные выходные представления.\n",
        "\n",
        "\n",
        "\n",
        "### Конкретный числовой пример для масштабируемого точечного произведения внимания (Scaled Dot-Product Attention)\n",
        "\n",
        "Давайте рассмотрим конкретный числовой пример, чтобы лучше понять, как работает масштабируемое точечное произведение внимания. Предположим, у нас есть простая последовательность из трех слов: \"Я\", \"люблю\", \"книги\". Мы будем использовать однослойное внимание для демонстрации.\n",
        "\n",
        "#### Шаг 1: Входные данные и проекция\n",
        "\n",
        "Представим каждое слово в виде вектора размерности $d_{model} = 4$. Для простоты возьмем следующие случайные векторы:\n",
        "\n",
        "- \"Я\" -> $x_1 = [0.1, 0.2, 0.3, 0.4]$\n",
        "- \"люблю\" -> $x_2 = [0.5, 0.6, 0.7, 0.8]$\n",
        "- \"книги\" -> $x_3 = [0.9, 1.0, 1.1, 1.2]$\n",
        "\n",
        "Матрица входных данных $X$ будет выглядеть так:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь мы используем обучаемые параметры для получения матриц запросов $Q$, ключей $K$ и значений $V$:\n",
        "\n",
        "$$\n",
        "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
        "$$\n",
        "\n",
        "где $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
        "\n",
        "Для простоты предположим, что $d_k = 2$ (размерность скрытого пространства меньше размерности модели). Возьмем случайные значения для $W_Q, W_K, W_V$:\n",
        "\n",
        "$$\n",
        "W_Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}, \\quad\n",
        "W_K = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_V = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим матрицы $Q$, $K$ и $V$:\n",
        "\n",
        "$$\n",
        "Q = XW_Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.5 & 0.6 \\\\\n",
        "1.1 & 1.2 \\\\\n",
        "1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K = XW_K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V = XW_V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление скалярного произведения\n",
        "\n",
        "Вычислим скалярное произведение $QK^T$:\n",
        "\n",
        "$$\n",
        "QK^T = \\begin{bmatrix}\n",
        "0.5 & 0.6 \\\\\n",
        "1.1 & 1.2 \\\\\n",
        "1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.6 & 1.4 & 2.2 \\\\\n",
        "0.7 & 1.5 & 2.3\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.82 & 1.84 & 2.86 \\\\\n",
        "1.62 & 3.64 & 5.66 \\\\\n",
        "2.42 & 5.44 & 8.46\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 3: Масштабирование\n",
        "\n",
        "Размерность ключей $d_k = 2$, поэтому масштабируем результат на $\\sqrt{d_k} = \\sqrt{2} \\approx 1.414$:\n",
        "\n",
        "$$\n",
        "\\frac{QK^T}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}\n",
        "0.82 & 1.84 & 2.86 \\\\\n",
        "1.62 & 3.64 & 5.66 \\\\\n",
        "2.42 & 5.44 & 8.46\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "0.58 & 1.30 & 2.02 \\\\\n",
        "1.15 & 2.57 & 4.00 \\\\\n",
        "1.71 & 3.85 & 6.00\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Нормализация весов с помощью softmax\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $\\frac{QK^T}{\\sqrt{d_k}}$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(S)_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^T \\exp(S_{ik})}\n",
        "$$\n",
        "\n",
        "Для первой строки:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.58 & 1.30 & 2.02\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Для второй строки:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "1.15 & 2.57 & 4.00\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.03 & 0.11 & 0.86\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Для третьей строки:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "1.71 & 3.85 & 6.00\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, нормализованная матрица весов:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67 \\\\\n",
        "0.03 & 0.11 & 0.86 \\\\\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 5: Взвешивание значений $V$\n",
        "\n",
        "Умножим матрицу весов на матрицу значений $V$:\n",
        "\n",
        "$$\n",
        "C = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67 \\\\\n",
        "0.03 & 0.11 & 0.86 \\\\\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "1.84 & 1.92 \\\\\n",
        "2.07 & 2.16 \\\\\n",
        "2.21 & 2.30\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Этот числовой пример показывает, как механизм масштабируемого точечного произведения внимания позволяет модели фокусироваться на наиболее релевантных частях входной последовательности. В данном случае модель уделяет больше внимания последнему слову \"книги\" для всех элементов последовательности, что может быть полезно в задачах перевода или анализа текста.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 2.3. Многослойное внимание (Multi-Head Attention)\n",
        "\n",
        "Многослойное внимание (multi-head attention) — это расширение механизма внимания, которое позволяет модели улучшить представление информации за счет использования нескольких параллельных механизмов внимания. Каждый из этих механизмов фокусируется на разных аспектах данных, что позволяет модели улавливать более сложные зависимости и взаимосвязи между элементами входной последовательности.\n",
        "\n",
        "#### Принцип работы\n",
        "\n",
        "Основная идея многослойного внимания заключается в том, что вместо одного механизма внимания используются несколько \"голов\" (heads). Каждая голова выполняет собственный механизм внимания с отдельными параметрами, а затем результаты всех голов объединяются в одно общее представление. Это позволяет модели лучше учитывать различные аспекты входной последовательности и создавать более точные выходные представления.\n",
        "\n",
        "Формула многослойного внимания:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, head_2, ..., head_h)W^O\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$,\n",
        "- $W_i^Q$, $W_i^K$, $W_i^V$ — параметры для каждого головного механизма,\n",
        "- $W^O$ — проекционная матрица для объединения всех головных механизмов.\n",
        "\n",
        "#### Подробное объяснение каждого компонента:\n",
        "\n",
        "1. **Отдельные головы (Heads)**:\n",
        "   - Для каждого головного механизма используются отдельные параметры $W_i^Q$, $W_i^K$, $W_i^V$, которые применяются к исходным матрицам запросов $Q$, ключей $K$ и значений $V$.\n",
        "   - Эти параметры позволяют каждой голове выделять разные аспекты данных. Например, одна голова может фокусироваться на семантических связях между словами, а другая — на грамматических структурах.\n",
        "   - Формально, для каждой головы $i$:\n",
        "\n",
        "  $$\n",
        "     Q_i = QW_i^Q, \\quad K_i = KW_i^K, \\quad V_i = VW_i^V\n",
        "  $$\n",
        "\n",
        "     Где $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$.\n",
        "\n",
        "   - Здесь $d_k$ и $d_v$ — размерности ключей и значений соответственно. Обычно $d_k = d_v$, но они могут быть различными.\n",
        "\n",
        "2. **Механизм внимания для каждой головы**:\n",
        "   - После применения параметров $W_i^Q$, $W_i^K$, $W_i^V$, каждая голова выполняет механизм внимания на своих преобразованных матрицах $Q_i$, $K_i$, $V_i$.\n",
        "   - Формально, для каждой головы $i$:\n",
        "\n",
        "  $$\n",
        "     head_i = \\text{Attention}(Q_i, K_i, V_i)\n",
        "  $$\n",
        "\n",
        "   - Механизм внимания для каждой головы может быть масштабируемым точечным произведением внимания (scaled dot-product attention), который мы рассмотрели ранее:\n",
        "\n",
        "  $$\n",
        "     \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i\n",
        "  $$\n",
        "\n",
        "   - Таким образом, каждая голова вычисляет свой контекстный вектор, основываясь на различных аспектах входной последовательности.\n",
        "\n",
        "3. **Объединение результатов всех голов**:\n",
        "   - После того как все головы выполнили свои механизмы внимания, их результаты объединяются в одно общее представление.\n",
        "   - Это достигается путем конкатенации всех контекстных векторов $head_i$ по оси последней размерности:\n",
        "\n",
        "  $$\n",
        "     \\text{Concat}(head_1, head_2, ..., head_h)\n",
        "  $$\n",
        "\n",
        "   - Результат конкатенации имеет форму $T \\times h \\cdot d_v$, где $T$ — длина последовательности, $h$ — количество голов, $d_v$ — размерность значений.\n",
        "\n",
        "4. **Проекционная матрица $W^O$**:\n",
        "   - После конкатенации результатов всех голов используется проекционная матрица $W^O$, чтобы привести объединенное представление обратно к исходной размерности $d_{model}$.\n",
        "   - Формально, проекционная матрица $W^O \\in \\mathbb{R}^{h \\cdot d_v \\times d_{model}}$.\n",
        "   - Умножение результата конкатенации на $W^O$ дает окончательное выходное представление:\n",
        "\n",
        "  $$\n",
        "     \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, head_2, ..., head_h)W^O\n",
        "  $$\n",
        "\n",
        "   - Этот шаг позволяет модели сохранить согласованность размерностей и интегрировать информацию из всех голов в единое представление.\n",
        "\n",
        "#### Преимущества многослойного внимания:\n",
        "\n",
        "1. **Улучшенное представление информации**:\n",
        "   - Использование нескольких голов позволяет модели улавливать различные аспекты данных, что делает ее более мощной и гибкой.\n",
        "   - Например, одна голова может фокусироваться на семантических связях, другая — на грамматической структуре, третья — на лексических особенностях и т.д.\n",
        "\n",
        "2. **Параллелизация**:\n",
        "   - Поскольку каждая голова работает независимо, механизмы внимания можно выполнять параллельно, что значительно ускоряет обучение и инференс (выполнение модели на новых данных).\n",
        "\n",
        "3. **Снижение вероятности переобучения**:\n",
        "   - Использование нескольких голов увеличивает количество параметров модели, что помогает избежать переобучения на малых наборах данных.\n",
        "\n",
        "4. **Гибкость**:\n",
        "   - Многослойное внимание можно адаптировать для различных задач NLP, таких как машинный перевод, классификация текста, суммаризация текста и другие.\n",
        "\n",
        "\n",
        "Таким образом, многослойное внимание является важным компонентом архитектуры трансформеров и позволяет модели эффективно улавливать сложные зависимости и взаимосвязи между элементами входной последовательности. Благодаря использованию нескольких параллельных механизмов внимания, каждый из которых фокусируется на разных аспектах данных, модель становится более мощной и гибкой, что делает ее подходящей для решения широкого спектра задач обработки естественного языка.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Конкретный числовой пример многослойного внимания (Multi-Head Attention)\n",
        "\n",
        "Давайте рассмотрим конкретный числовой пример для многослойного внимания. Предположим, у нас есть простая последовательность из трех слов: \"Я\", \"люблю\", \"книги\". Мы будем использовать двухголовое внимание (two-head attention) для демонстрации.\n",
        "\n",
        "#### Шаг 1: Входные данные и проекция\n",
        "\n",
        "Представим каждое слово в виде вектора размерности $d_{model} = 4$. Для простоты возьмем следующие случайные векторы:\n",
        "\n",
        "- \"Я\" -> $x_1 = [0.1, 0.2, 0.3, 0.4]$\n",
        "- \"люблю\" -> $x_2 = [0.5, 0.6, 0.7, 0.8]$\n",
        "- \"книги\" -> $x_3 = [0.9, 1.0, 1.1, 1.2]$\n",
        "\n",
        "Матрица входных данных $X$ будет выглядеть так:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь мы используем обучаемые параметры для получения матриц запросов $Q$, ключей $K$ и значений $V$:\n",
        "\n",
        "$$\n",
        "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
        "$$\n",
        "\n",
        "где $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$.\n",
        "\n",
        "Для простоты предположим, что $d_k = 2$ (размерность скрытого пространства меньше размерности модели). Возьмем случайные значения для параметров голов:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "W_1^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}, \\quad\n",
        "W_1^K = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_1^V = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "W_2^Q = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2^K = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2^V = \\begin{bmatrix}\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9 \\\\\n",
        "1.0 & 1.1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим матрицы $Q_i$, $K_i$ и $V_i$ для каждой головы:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "Q_1 = XW_1^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.5 & 0.6 \\\\\n",
        "1.1 & 1.2 \\\\\n",
        "1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K_1 = XW_1^K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_1 = XW_1^V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "Q_2 = XW_2^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K_2 = XW_2^K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_2 = XW_2^V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9 \\\\\n",
        "1.0 & 1.1\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.8 & 0.9 \\\\\n",
        "1.6 & 1.7 \\\\\n",
        "2.4 & 2.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Вычисление скалярного произведения для каждой головы\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "Q_1K_1^T = \\begin{bmatrix}\n",
        "0.5 & 0.6 \\\\\n",
        "1.1 & 1.2 \\\\\n",
        "1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.6 & 1.4 & 2.2 \\\\\n",
        "0.7 & 1.5 & 2.3\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.82 & 1.84 & 2.86 \\\\\n",
        "1.62 & 3.64 & 5.66 \\\\\n",
        "2.42 & 5.44 & 8.46\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "Q_2K_2^T = \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 & 1.5 & 2.3 \\\\\n",
        "0.8 & 1.6 & 2.4\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1.06 & 2.34 & 3.62 \\\\\n",
        "2.18 & 4.80 & 7.42 \\\\\n",
        "3.30 & 7.26 & 11.22\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 3: Масштабирование\n",
        "\n",
        "Размерность ключей $d_k = 2$, поэтому масштабируем результат на $\\sqrt{d_k} = \\sqrt{2} \\approx 1.414$:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "\\frac{Q_1K_1^T}{\\sqrt{d_k}} \\approx \\begin{bmatrix}\n",
        "0.58 & 1.30 & 2.02 \\\\\n",
        "1.15 & 2.57 & 4.00 \\\\\n",
        "1.71 & 3.85 & 6.00\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "\\frac{Q_2K_2^T}{\\sqrt{d_k}} \\approx \\begin{bmatrix}\n",
        "0.75 & 1.65 & 2.56 \\\\\n",
        "1.54 & 3.40 & 5.25 \\\\\n",
        "2.33 & 5.13 & 7.93\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 4: Нормализация весов с помощью softmax\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $\\frac{Q_iK_i^T}{\\sqrt{d_k}}$:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.58 & 1.30 & 2.02 \\\\\n",
        "1.15 & 2.57 & 4.00 \\\\\n",
        "1.71 & 3.85 & 6.00\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67 \\\\\n",
        "0.03 & 0.11 & 0.86 \\\\\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.75 & 1.65 & 2.56 \\\\\n",
        "1.54 & 3.40 & 5.25 \\\\\n",
        "2.33 & 5.13 & 7.93\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.07 & 0.22 & 0.71 \\\\\n",
        "0.03 & 0.10 & 0.87 \\\\\n",
        "0.02 & 0.07 & 0.91\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 5: Взвешивание значений $V_i$\n",
        "\n",
        "Умножим матрицу весов на матрицу значений $V_i$:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "C_1 = \\text{softmax}\\left(\\frac{Q_1K_1^T}{\\sqrt{d_k}}\\right)V_1\n",
        "\\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67 \\\\\n",
        "0.03 & 0.11 & 0.86 \\\\\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "1.84 & 1.92 \\\\\n",
        "2.07 & 2.16 \\\\\n",
        "2.21 & 2.30\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "C_2 = \\text{softmax}\\left(\\frac{Q_2K_2^T}{\\sqrt{d_k}}\\right)V_2\n",
        "\\approx \\begin{bmatrix}\n",
        "0.07 & 0.22 & 0.71 \\\\\n",
        "0.03 & 0.10 & 0.87 \\\\\n",
        "0.02 & 0.07 & 0.91\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.8 & 0.9 \\\\\n",
        "1.6 & 1.7 \\\\\n",
        "2.4 & 2.5\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "1.90 & 1.99 \\\\\n",
        "2.13 & 2.24 \\\\\n",
        "2.27 & 2.37\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 6: Объединение результатов всех голов\n",
        "\n",
        "Конкатенация результатов всех голов по оси последней размерности:\n",
        "\n",
        "$$\n",
        "\\text{Concat}(C_1, C_2) = \\begin{bmatrix}\n",
        "1.84 & 1.92 & 1.90 & 1.99 \\\\\n",
        "2.07 & 2.16 & 2.13 & 2.24 \\\\\n",
        "2.21 & 2.30 & 2.27 & 2.37\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 7: Проекционная матрица $W^O$\n",
        "\n",
        "Используем проекционную матрицу $W^O$ для приведения объединенного представления обратно к исходной размерности $d_{model}$. Предположим, что:\n",
        "\n",
        "$$\n",
        "W^O = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выполним умножение:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(C_1, C_2)W^O\n",
        "\\approx \\begin{bmatrix}\n",
        "1.84 & 1.92 & 1.90 & 1.99 \\\\\n",
        "2.07 & 2.16 & 2.13 & 2.24 \\\\\n",
        "2.21 & 2.30 & 2.27 & 2.37\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "4.66 & 4.86 & 5.06 & 5.26 \\\\\n",
        "5.26 & 5.46 & 5.66 & 5.86 \\\\\n",
        "5.86 & 6.06 & 6.26 & 6.46\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, этот числовой пример показывает, как механизм многослойного внимания позволяет модели фокусироваться на различных аспектах входной последовательности. Каждая голова выделяет разные зависимости между элементами последовательности, а затем их результаты объединяются в одно общее представление. Это делает модель более мощной и гибкой, что позволяет ей эффективно решать различные задачи обработки естественного языка.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 3. Структура трансформера\n",
        "\n",
        "Трансформер — это архитектура нейронной сети, которая используется для обработки последовательностей данных, таких как тексты. Основные компоненты трансформера включают **энкодер** (encoder) и **декодер** (decoder). В зависимости от задачи, может использоваться только энкодер или оба блока вместе.\n",
        "\n",
        "### 3.1. Энкодер\n",
        "\n",
        "Энкодер является первой частью трансформера и предназначен для преобразования входной последовательности в более абстрактное представление. Каждый слой энкодера состоит из нескольких ключевых компонентов, которые обеспечивают эффективное обучение и обработку данных:\n",
        "\n",
        "#### Многослойное внимание (Multi-Head Attention)\n",
        "\n",
        "Многослойное внимание позволяет модели фокусироваться на различных частях входной последовательности. Этот механизм основан на параллельном выполнении нескольких голов внимания, каждая из которых выделяет разные аспекты данных.\n",
        "\n",
        "Каждая голова выполняет следующие шаги:\n",
        "1. Применяет обучаемые параметры $W_i^Q$, $W_i^K$, $W_i^V$ к исходным матрицам запросов $Q$, ключей $K$ и значений $V$.\n",
        "2. Выполняет механизм внимания для каждой головы с помощью масштабируемого точечного произведения внимания (scaled dot-product attention).\n",
        "3. Объединяет результаты всех голов с помощью конкатенации и проекционной матрицы $W^O$.\n",
        "\n",
        "Формально, многослойное внимание можно записать как:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, head_2, ..., head_h)W^O\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$,\n",
        "- $W_i^Q$, $W_i^K$, $W_i^V$ — параметры для каждого головного механизма,\n",
        "- $W^O$ — проекционная матрица для объединения всех головных механизмов.\n",
        "\n",
        "#### Передача сигнала через сеть (Feed-Forward Neural Network)\n",
        "\n",
        "После применения многослойного внимания выход передается через полноценную нейронную сеть (feed-forward neural network). Это позволяет добавить нелинейность в модель и улучшить ее способность к обучению сложных зависимостей.\n",
        "\n",
        "Слои нейронной сети обычно состоят из двух линейных преобразований с функцией активации ReLU между ними:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $W_1 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ и $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ — весовые матрицы,\n",
        "- $b_1 \\in \\mathbb{R}^{d_{ff}}$ и $b_2 \\in \\mathbb{R}^{d_{model}}$ — векторы смещений,\n",
        "- $d_{ff}$ — размерность скрытого слоя нейронной сети (обычно значительно больше, чем $d_{model}$).\n",
        "\n",
        "#### Слои нормализации (Layer Normalization) и остаточные соединения (Residual Connections)\n",
        "\n",
        "Чтобы стабилизировать процесс обучения и улучшить производительность модели, между компонентами энкодера применяются слои нормализации и остаточные соединения.\n",
        "\n",
        "**Слой нормализации (Layer Normalization)**:\n",
        "- Нормализует значения по оси последней размерности, чтобы сделать их менее чувствительными к изменению входных данных.\n",
        "- Формально, нормализация выполняется по формуле:\n",
        "\n",
        "$$\n",
        "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $\\mu$ и $\\sigma^2$ — среднее и дисперсия значений по оси последней размерности,\n",
        "- $\\gamma$ и $\\beta$ — обучаемые параметры,\n",
        "- $\\epsilon$ — малое значение для предотвращения деления на ноль.\n",
        "\n",
        "**Остаточные соединения (Residual Connections)**:\n",
        "- Добавляют входные данные к выходу компонента, чтобы сохранить информацию о начальных данных и улучшить градиентный спуск.\n",
        "- Формально, остаточное соединение выполняется по формуле:\n",
        "\n",
        "$$\n",
        "\\text{Residual}(x) = x + f(x)\n",
        "$$\n",
        "\n",
        "Где $f(x)$ — это выход компонента (например, многослойного внимания или нейронной сети).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 3.2. Декодер\n",
        "\n",
        "\n",
        "Декодер является важной частью архитектуры трансформеров (например, в модели Transformer), которая используется для генерации последовательности на основе входного представления, созданного энкодером. Декодер имеет несколько ключевых компонентов и механизмов, которые позволяют модели учитывать как входные данные от энкодера, так и уже сгенерированные слова при создании следующего слова.\n",
        "\n",
        "#### 1. Многослойное внимание на декодере\n",
        "\n",
        "**Многослойное внимание на декодере** работает аналогично многослойному вниманию в энкодере, но с некоторыми отличиями. В декодере этот механизм фокусируется на различных частях уже сгенерированной части текста. Это означает, что модель может \"смотреть\" на предыдущие слова в последовательности, чтобы определить, какое слово должно быть сгенерировано следующим.\n",
        "\n",
        "- **Запросы $Q$**: В этом контексте запросы представляют собой текущее состояние декодера.\n",
        "- **Ключи $K$ и значения $V$**: Эти параметры берутся из того же слоя декодера, что и запросы, поскольку механизм внимания работает внутри одной последовательности.\n",
        "\n",
        "Формально, механизм можно записать как:\n",
        "$$\n",
        "\\text{SelfAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "где $d_k$ — размерность ключей.\n",
        "\n",
        "#### 2. Механизм внимания на декодере (Decoder Attention)\n",
        "\n",
        "**Механизм внимания на декодере** позволяет модели учитывать не только уже сгенерированные слова, но и входные данные от энкодера. Этот механизм использует два набора данных: выход энкодера и уже сгенерированные слова.\n",
        "\n",
        "- **Запросы $Q$**: Запросы берутся из текущего слоя декодера.\n",
        "- **Ключи $K$ и значения $V$**: Эти параметры берутся из выхода энкодера.\n",
        "\n",
        "Формально, механизм можно записать как:\n",
        "$$\n",
        "\\text{DecoderAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Здесь важно отметить, что механизмы внимания на декодере могут работать как с маской, так и без неё. При обучении модели с использованием автоперевода или других задач, где порядок слов важен, часто применяется маска, чтобы модель не могла \"подглядывать\" в будущее. Это делается для того, чтобы модель училась предсказывать следующее слово, основываясь только на уже виденных словах.\n",
        "\n",
        "#### 3. Передача сигнала через сеть (Feed-Forward Neural Network)\n",
        "\n",
        "**Передача сигнала через сеть** добавляет нелинейность в модель и работает аналогично нейронной сети в энкодере. После применения механизма внимания сигнал проходит через полностью связную нейронную сеть, состоящую из двух линейных слоёв и функции активации ReLU между ними.\n",
        "\n",
        "Формально, это можно записать как:\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "где $W_1$, $W_2$ — весовые матрицы, а $b_1$, $b_2$ — векторы смещений.\n",
        "\n",
        "#### Слои нормализации и остаточные соединения\n",
        "\n",
        "Как и в энкодере, между компонентами декодера также применяются слои нормализации и остаточные соединения. Эти элементы помогают стабилизировать процесс обучения и улучшить производительность модели.\n",
        "\n",
        "- **Нормализация**: Нормализация применяется к каждому слою после вычисления внимания и передачи сигнала через сеть. Она помогает стабилизировать градиенты и ускоряет процесс обучения.\n",
        "- **Остаточные соединения**: Остаточные соединения позволяют пропускать прямой сигнал через несколько слоёв, что помогает решить проблему затухания градиентов и улучшает обобщение модели.\n",
        "\n",
        "\n",
        "\n",
        " ### Конкретный числовой пример структуры трансформера\n",
        "\n",
        "Давайте рассмотрим конкретный числовой пример для работы энкодера и декодера в трансформере. Предположим, у нас есть простая последовательность из трех слов: \"Я\", \"люблю\", \"книги\". Мы будем использовать однослойный энкодер и однослойный декодер для демонстрации.\n",
        "\n",
        "#### Шаг 1: Входные данные и проекция\n",
        "\n",
        "Представим каждое слово в виде вектора размерности $d_{model} = 4$. Для простоты возьмем следующие случайные векторы:\n",
        "\n",
        "- \"Я\" -> $x_1 = [0.1, 0.2, 0.3, 0.4]$\n",
        "- \"люблю\" -> $x_2 = [0.5, 0.6, 0.7, 0.8]$\n",
        "- \"книги\" -> $x_3 = [0.9, 1.0, 1.1, 1.2]$\n",
        "\n",
        "Матрица входных данных $X$ будет выглядеть так:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### Шаг 2: Энкодер\n",
        "\n",
        "##### Многослойное внимание (Multi-Head Attention)\n",
        "\n",
        "Предположим, что мы используем двухголовое внимание (two-head attention). Возьмем случайные значения для параметров голов:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "W_1^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}, \\quad\n",
        "W_1^K = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_1^V = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "W_2^Q = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2^K = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2^V = \\begin{bmatrix}\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9 \\\\\n",
        "1.0 & 1.1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычислим матрицы $Q_i$, $K_i$ и $V_i$ для каждой головы:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "Q_1 = XW_1^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.5 & 0.6 \\\\\n",
        "1.1 & 1.2 \\\\\n",
        "1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K_1 = XW_1^K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_1 = XW_1^V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "Q_2 = XW_2^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K_2 = XW_2^K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_2 = XW_2^V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9 \\\\\n",
        "1.0 & 1.1\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.8 & 0.9 \\\\\n",
        "1.6 & 1.7 \\\\\n",
        "2.4 & 2.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление скалярного произведения для каждой головы\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "Q_1K_1^T = \\begin{bmatrix}\n",
        "0.5 & 0.6 \\\\\n",
        "1.1 & 1.2 \\\\\n",
        "1.7 & 1.8\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.6 & 1.4 & 2.2 \\\\\n",
        "0.7 & 1.5 & 2.3\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.82 & 1.84 & 2.86 \\\\\n",
        "1.62 & 3.64 & 5.66 \\\\\n",
        "2.42 & 5.44 & 8.46\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "Q_2K_2^T = \\begin{bmatrix}\n",
        "0.6 & 0.7 \\\\\n",
        "1.4 & 1.5 \\\\\n",
        "2.2 & 2.3\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 & 1.5 & 2.3 \\\\\n",
        "0.8 & 1.6 & 2.4\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1.06 & 2.34 & 3.62 \\\\\n",
        "2.18 & 4.80 & 7.42 \\\\\n",
        "3.30 & 7.26 & 11.22\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Масштабирование и нормализация весов с помощью softmax\n",
        "\n",
        "Размерность ключей $d_k = 2$, поэтому масштабируем результат на $\\sqrt{d_k} = \\sqrt{2} \\approx 1.414$:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "\\frac{Q_1K_1^T}{\\sqrt{d_k}} \\approx \\begin{bmatrix}\n",
        "0.58 & 1.30 & 2.02 \\\\\n",
        "1.15 & 2.57 & 4.00 \\\\\n",
        "1.71 & 3.85 & 6.00\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $\\frac{Q_1K_1^T}{\\sqrt{d_k}}$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.58 & 1.30 & 2.02 \\\\\n",
        "1.15 & 2.57 & 4.00 \\\\\n",
        "1.71 & 3.85 & 6.00\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67 \\\\\n",
        "0.03 & 0.11 & 0.86 \\\\\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "\\frac{Q_2K_2^T}{\\sqrt{d_k}} \\approx \\begin{bmatrix}\n",
        "0.75 & 1.65 & 2.56 \\\\\n",
        "1.54 & 3.40 & 5.25 \\\\\n",
        "2.33 & 5.13 & 7.93\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $\\frac{Q_2K_2^T}{\\sqrt{d_k}}$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.75 & 1.65 & 2.56 \\\\\n",
        "1.54 & 3.40 & 5.25 \\\\\n",
        "2.33 & 5.13 & 7.93\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "0.07 & 0.22 & 0.71 \\\\\n",
        "0.03 & 0.10 & 0.87 \\\\\n",
        "0.02 & 0.07 & 0.91\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Взвешивание значений $V_i$\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "C_1 = \\text{softmax}\\left(\\frac{Q_1K_1^T}{\\sqrt{d_k}}\\right)V_1\n",
        "\\approx \\begin{bmatrix}\n",
        "0.09 & 0.24 & 0.67 \\\\\n",
        "0.03 & 0.11 & 0.86 \\\\\n",
        "0.01 & 0.05 & 0.94\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 & 0.8 \\\\\n",
        "1.5 & 1.6 \\\\\n",
        "2.3 & 2.4\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "1.84 & 1.92 \\\\\n",
        "2.07 & 2.16 \\\\\n",
        "2.21 & 2.30\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "C_2 = \\text{softmax}\\left(\\frac{Q_2K_2^T}{\\sqrt{d_k}}\\right)V_2\n",
        "\\approx \\begin{bmatrix}\n",
        "0.07 & 0.22 & 0.71 \\\\\n",
        "0.03 & 0.10 & 0.87 \\\\\n",
        "0.02 & 0.07 & 0.91\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.8 & 0.9 \\\\\n",
        "1.6 & 1.7 \\\\\n",
        "2.4 & 2.5\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "1.90 & 1.99 \\\\\n",
        "2.13 & 2.24 \\\\\n",
        "2.27 & 2.37\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Объединение результатов всех голов\n",
        "\n",
        "Конкатенация результатов всех голов по оси последней размерности:\n",
        "\n",
        "$$\n",
        "\\text{Concat}(C_1, C_2) = \\begin{bmatrix}\n",
        "1.84 & 1.92 & 1.90 & 1.99 \\\\\n",
        "2.07 & 2.16 & 2.13 & 2.24 \\\\\n",
        "2.21 & 2.30 & 2.27 & 2.37\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Проекционная матрица $W^O$\n",
        "\n",
        "Используем проекционную матрицу $W^O$ для приведения объединенного представления обратно к исходной размерности $d_{model}$. Предположим, что:\n",
        "\n",
        "$$\n",
        "W^O = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выполним умножение:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(C_1, C_2)W^O\n",
        "\\approx \\begin{bmatrix}\n",
        "1.84 & 1.92 & 1.90 & 1.99 \\\\\n",
        "2.07 & 2.16 & 2.13 & 2.24 \\\\\n",
        "2.21 & 2.30 & 2.27 & 2.37\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "4.66 & 4.86 & 5.06 & 5.26 \\\\\n",
        "5.26 & 5.46 & 5.66 & 5.86 \\\\\n",
        "5.86 & 6.06 & 6.26 & 6.46\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Передача сигнала через сеть (Feed-Forward Neural Network)\n",
        "\n",
        "Добавим нелинейную трансформацию с помощью нейронной сети:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "\n",
        "Предположим, что:\n",
        "\n",
        "$$\n",
        "W_1 = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2 = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_1 = \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix}, \\quad\n",
        "b_2 = \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выполним вычисления:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, \\begin{bmatrix}\n",
        "4.66 & 4.86 & 5.06 & 5.26 \\\\\n",
        "5.26 & 5.46 & 5.66 & 5.86 \\\\\n",
        "5.86 & 6.06 & 6.26 & 6.46\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "+ \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix})\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "+ \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "После того как энкодер преобразовал входную последовательность в абстрактное представление, декодер использует это представление для генерации новой последовательности слов. Декодер имеет схожую структуру с энкодером, но содержит дополнительные компоненты, которые позволяют модели учитывать уже сгенерированную часть текста при создании следующего слова.\n",
        "\n",
        "#### Шаг 3: Декодер\n",
        "\n",
        "##### Входные данные для декодера\n",
        "\n",
        "Допустим, что мы хотим перевести фразу \"Я люблю книги\" на другой язык (например, английский). Предположим, что первое слово, которое модель должна сгенерировать, — это \"I\". Мы будем использовать этот вектор для входных данных декодера:\n",
        "\n",
        "- \"I\" -> $y_1 = [0.1, 0.2, 0.3, 0.4]$\n",
        "\n",
        "Матрица входных данных для декодера $Y$ будет выглядеть так:\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Многослойное внимание на декодере\n",
        "\n",
        "Как и в энкодере, декодер использует многослойное внимание для обработки входной последовательности. Для простоты возьмем те же параметры голов, что и для энкодера:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "W_1^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}, \\quad\n",
        "W_1^K = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_1^V = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "W_2^Q = \\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2^K = \\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2^V = \\begin{bmatrix}\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9 \\\\\n",
        "1.0 & 1.1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычислим матрицы $Q_i$, $K_i$ и $V_i$ для каждой головы:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "Q_1 = YW_1^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.5 & 0.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K_1 = YW_1^K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_1 = YW_1^V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "Q_2 = YW_2^Q = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.6 & 0.7\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K_2 = YW_2^K = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 \\\\\n",
        "0.9 & 1.0\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_2 = YW_2^V = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.4 & 0.5 \\\\\n",
        "0.6 & 0.7 \\\\\n",
        "0.8 & 0.9 \\\\\n",
        "1.0 & 1.1\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Вычисление скалярного произведения для каждой головы\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "Q_1K_1^T = \\begin{bmatrix}\n",
        "0.5 & 0.6\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.6 \\\\\n",
        "0.7\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.82\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "Q_2K_2^T = \\begin{bmatrix}\n",
        "0.6 & 0.7\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 \\\\\n",
        "0.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "1.06\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Масштабирование и нормализация весов с помощью softmax\n",
        "\n",
        "Размерность ключей $d_k = 2$, поэтому масштабируем результат на $\\sqrt{d_k} = \\sqrt{2} \\approx 1.414$:\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "\\frac{Q_1K_1^T}{\\sqrt{d_k}} \\approx \\begin{bmatrix}\n",
        "0.58\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $\\frac{Q_1K_1^T}{\\sqrt{d_k}}$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.58\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "1.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "\\frac{Q_2K_2^T}{\\sqrt{d_k}} \\approx \\begin{bmatrix}\n",
        "0.75\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Применим функцию softmax к каждой строке матрицы $\\frac{Q_2K_2^T}{\\sqrt{d_k}}$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\begin{bmatrix}\n",
        "0.75\n",
        "\\end{bmatrix}\\right)\n",
        "\\approx \\begin{bmatrix}\n",
        "1.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Взвешивание значений $V_i$\n",
        "\n",
        "**Голова 1:**\n",
        "\n",
        "$$\n",
        "C_1 = \\text{softmax}\\left(\\frac{Q_1K_1^T}{\\sqrt{d_k}}\\right)V_1\n",
        "\\approx \\begin{bmatrix}\n",
        "1.0\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Голова 2:**\n",
        "\n",
        "$$\n",
        "C_2 = \\text{softmax}\\left(\\frac{Q_2K_2^T}{\\sqrt{d_k}}\\right)V_2\n",
        "\\approx \\begin{bmatrix}\n",
        "1.0\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Объединение результатов всех голов\n",
        "\n",
        "Конкатенация результатов всех голов по оси последней размерности:\n",
        "\n",
        "$$\n",
        "\\text{Concat}(C_1, C_2) = \\begin{bmatrix}\n",
        "0.7 & 0.8 & 0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Проекционная матрица $W^O$\n",
        "\n",
        "Используем проекционную матрицу $W^O$ для приведения объединенного представления обратно к исходной размерности $d_{model}$. Возьмем ту же проекционную матрицу, что и для энкодера:\n",
        "\n",
        "$$\n",
        "W^O = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выполним умножение:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(C_1, C_2)W^O\n",
        "\\approx \\begin{bmatrix}\n",
        "0.7 & 0.8 & 0.8 & 0.9\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "\\approx \\begin{bmatrix}\n",
        "1.78 & 1.90 & 2.02 & 2.14\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "##### Передача сигнала через сеть (Feed-Forward Neural Network)\n",
        "\n",
        "Добавим нелинейную трансформацию с помощью нейронной сети:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "\n",
        "Предположим, что:\n",
        "\n",
        "$$\n",
        "W_1 = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}, \\quad\n",
        "W_2 = \\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_1 = \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix}, \\quad\n",
        "b_2 = \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Выполним вычисления:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, \\begin{bmatrix}\n",
        "1.78 & 1.90 & 2.02 & 2.14\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "+ \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix})\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
        "0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
        "1.3 & 1.4 & 1.5 & 1.6\n",
        "\\end{bmatrix}\n",
        "+ \\begin{bmatrix}\n",
        "0.1 \\\\\n",
        "0.2 \\\\\n",
        "0.3 \\\\\n",
        "0.4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, этот числовой пример показывает, как работает полный цикл трансформера, включая энкодер и декодер. Энкодер преобразует входную последовательность в абстрактное представление, а декодер использует это представление для генерации новой последовательности текста. Использование многослойного внимания, нейронной сети и различных техник нормализации и остаточных соединений позволяет модели эффективно решать задачи обработки естественного языка, такие как машинный перевод.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 4. Преимущества трансформеров\n",
        "\n",
        "Трансформеры (Transformers) — это архитектура нейронных сетей, которая была введена в 2017 году и быстро стала доминирующей моделью для задач обработки естественного языка (NLP). В этом разделе мы подробно рассмотрим основные преимущества трансформеров по сравнению с другими моделями, такими как рекуррентные нейронные сети (RNN).\n",
        "\n",
        "### 4.1. Параллелизация\n",
        "\n",
        "#### Описание механизма\n",
        "Одним из ключевых преимуществ трансформеров является их способность к параллельной обработке данных. В отличие от RNN, которые должны последовательно обрабатывать элементы входной последовательности, трансформеры могут обрабатывать все элементы одновременно. Это возможно благодаря тому, что трансформеры не имеют рекуррентных связей, которые ограничивают возможность параллельного выполнения операций.\n",
        "\n",
        "В RNN каждая ячейка зависит от предыдущей, что делает невозможным обработку нескольких элементов одновременно. Например, чтобы вычислить выходное значение на шаге t, необходимо знать выходное значение на шаге t-1. Это приводит к значительным времененным затратам при обучении и инференсе, особенно на длинных последовательностях.\n",
        "\n",
        "Трансформеры же используют механизм внимания (attention mechanism), который позволяет модели сразу учитывать все элементы последовательности без необходимости последовательной обработки. Таким образом, все элементы могут быть обработаны параллельно, что значительно ускоряет процесс обучения и инференса.\n",
        "\n",
        "#### Пример\n",
        "Представьте себе задачу машинного перевода, где входная последовательность состоит из 100 слов. В случае RNN каждое слово будет обрабатываться последовательно, что может занять значительное время, особенно если модель имеет много слоев. В случае трансформера все 100 слов будут обработаны одновременно, что значительно сокращает время обработки.\n",
        "\n",
        "#### Результаты\n",
        "Эта способность к параллельной обработке данных делает трансформеры гораздо более эффективными на больших объемах данных. В экспериментах было показано, что трансформеры могут обучаться в несколько раз быстрее, чем RNN, что делает их идеальными для задач, требующих обработки больших текстовых корпусов.\n",
        "\n",
        "### 4.2. Устойчивость к длинным последовательностям\n",
        "\n",
        "#### Описание механизма\n",
        "Другим важным преимуществом трансформеров является их способность эффективно работать с длинными последовательностями. Механизмы внимания позволяют модели учитывать связи между всеми элементами последовательности, даже если они находятся далеко друг от друга. Это делает трансформеры более устойчивыми к длинным последовательностям, чем RNN.\n",
        "\n",
        "В RNN проблема заключается в том, что информация теряется при прохождении через большое количество временных шагов. Это связано с эффектом затухания градиентов (vanishing gradient problem), когда градиенты становятся слишком маленькими, чтобы эффективно обновлять веса модели. Это приводит к тому, что RNN плохо справляются с длинными последовательностями.\n",
        "\n",
        "Трансформеры решают эту проблему за счет использования механизма внимания. Механизм внимания позволяет модели \"смотреть\" на все элементы последовательности одновременно и определять, какие из них наиболее важны для текущего контекста. Это позволяет модели учитывать долгосрочные зависимости между элементами последовательности, что делает ее более устойчивой к длинным последовательностям.\n",
        "\n",
        "#### Пример\n",
        "Рассмотрим задачу анализа текста, где входная последовательность состоит из нескольких тысяч слов. В случае RNN информация о первых словах будет потеряна к моменту, когда модель дойдет до конца последовательности. В случае трансформера модель сможет эффективно учитывать все слова, даже те, которые находятся на большом расстоянии друг от друга.\n",
        "\n",
        "#### Результаты\n",
        "Это свойство делает трансформеры идеальными для задач, где важна долгосрочная зависимость, таких как машинный перевод, анализ длинных текстов и другие задачи NLP.\n",
        "\n",
        "### 4.3. Гибкость\n",
        "\n",
        "#### Описание механизма\n",
        "Трансформеры обладают высокой гибкостью и могут быть адаптированы для решения различных задач NLP. Благодаря своей универсальной архитектуре, трансформеры можно использовать для таких задач, как машинный перевод, классификация текста, суммаризация текста, вопросно-ответные системы и многие другие.\n",
        "\n",
        "Основная причина такой гибкости заключается в том, что трансформеры не зависят от специфической структуры данных, как это часто бывает в других моделях. Они могут работать с различными типами текстовых данных, будь то предложения, абзацы или даже целые документы.\n",
        "\n",
        "Кроме того, трансформеры могут быть легко адаптированы для работы с дополнительными данными, такими как изображения или звуковые сигналы. Это делает их универсальным инструментом для разработчиков, работающих в различных областях, таких как компьютерное зрение, анализ временных рядов и другие.\n",
        "\n",
        "#### Пример\n",
        "Представьте себе задачу классификации текста, где нужно определить, относится ли данный текст к определенной категории. Трансформеры могут быть легко адаптированы для этой задачи, просто добавив выходной слой, который будет выполнять классификацию. Аналогично, трансформеры могут быть использованы для задачи машинного перевода, просто заменив выходной слой на слой декодирования.\n",
        "\n",
        "#### Результаты\n",
        "Эта гибкость делает трансформеры одним из самых популярных инструментов в области машинного обучения. Они широко используются в различных задачах, начиная от простых задач классификации и заканчивая сложными задачами, такими как создание искусственного интеллекта для игры в шахматы или Go.\n",
        "\n",
        "\n",
        "## 5. Заключение\n",
        "\n",
        "Архитектура трансформеров представляет собой важный шаг вперед в развитии моделей обработки естественного языка. Благодаря своей способности эффективно обрабатывать длинные последовательности и возможности параллельного выполнения операций, трансформеры достигли выдающихся результатов на множестве задач NLP.\n",
        "\n",
        "На данный момент трансформеры являются одним из самых перспективных направлений в области машинного обучения, и их использование продолжает расширяться в различных областях, таких как компьютерное зрение, анализ временных рядов и другие. Исследователи активно работают над улучшением трансформеров, разрабатывая новые архитектуры и методы обучения, что позволяет им решать все более сложные задачи.\n",
        "\n",
        "\n",
        "\n",
        "## Вопросы для обсуждения:\n",
        "\n",
        "### 1. Какие проблемы решаются с помощью механизма внимания?\n",
        "\n",
        "Механизм внимания решает проблему зависимости между элементами последовательности, особенно на больших расстояниях. В рекуррентных нейронных сетях (RNN) информация может теряться при прохождении через большое количество временных шагов, что приводит к проблеме затухания градиентов. Механизм внимания позволяет модели учитывать связи между всеми элементами последовательности, даже если они находятся далеко друг от друга. Это особенно полезно в задачах, где важна долгосрочная зависимость, такие как машинный перевод, анализ длинных текстов и другие.\n",
        "\n",
        "### 2. Почему трансформеры более эффективны, чем RNN на длинных последовательностях?\n",
        "\n",
        "Трансформеры более эффективны на длинных последовательностях благодаря двум основным факторам: параллелизации и механизму внимания. Во-первых, трансформеры могут обрабатывать все элементы последовательности одновременно, что значительно ускоряет процесс обучения и инференса. Во-вторых, механизм внимания позволяет модели учитывать долгосрочные зависимости между элементами последовательности, что делает их более устойчивыми к длинным последовательностям. В RNN эти зависимости теряются из-за эффекта затухания градиентов, что делает их менее эффективными на длинных последовательностях.\n",
        "\n",
        "### 3. Какие задачи NLP можно решить с помощью трансформеров?\n",
        "\n",
        "Трансформеры могут быть использованы для решения широкого спектра задач NLP, включая:\n",
        "- **Машинный перевод**: Трансформеры могут эффективно переводить тексты с одного языка на другой, учитывая долгосрочные зависимости между словами.\n",
        "- **Классификация текста**: Трансформеры могут быть использованы для классификации текстов на различные категории, такие как позитивные/негативные отзывы, новости по темам и другие.\n",
        "- **Суммаризация текста**: Трансформеры могут создавать краткие резюме длинных текстов, сохраняя ключевые моменты.\n",
        "- **Вопросно-ответные системы**: Трансформеры могут быть использованы для создания систем, которые отвечают на вопросы пользователя на основе предоставленных данных.\n",
        "- **Извлечение информации**: Трансформеры могут извлекать важную информацию из текста, такую как имена людей, организации, даты и другие.\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "aLZQzoph__XN"
      }
    }
  ]
}