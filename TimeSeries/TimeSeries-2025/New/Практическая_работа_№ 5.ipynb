{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPXYYW2EU77PKbrnorsXFFA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/TimeSeries/TimeSeries-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Практическая работа № 5**  \n",
        "## **Глубокое обучение для табличных данных и временных рядов**\n",
        "\n",
        "### **1. Цель и задачи работы**\n",
        "\n",
        "**Цель работы** — сформировать у обучающегося целостное понимание возможностей и ограничений моделей глубокого обучения при работе с **табличными данными**, включая как **стандартные задачи машинного обучения** (регрессия, классификация), так и **специфические сценарии временных рядов**. Работа направлена на освоение методологии проектирования, настройки, диагностики и интерпретации глубоких архитектур в условиях структурированных данных, где доминируют числовые и категориальные признаки, а временная зависимость может присутствовать или отсутствовать.\n",
        "\n",
        "**Основные задачи работы:**  \n",
        "1. Использовать два типа данных:  \n",
        "   - **Табличный датасет из Практической работы № 1** (временной ряд, преобразованный в регрессионную задачу с лагами и инженерными признаками);  \n",
        "   - **Дополнительный табличный датасет без временной структуры** (например, из репозитория UCI или Kaggle: регрессия — Boston, California Housing; классификация — Adult, Bank Marketing).  \n",
        "2. Реализовать и сравнить следующие архитектуры глубокого обучения:  \n",
        "   - **MLP (многослойный перцептрон)** — базовая архитектура для табличных данных;  \n",
        "   - **1D-CNN** — применённая к последовательности лагов (для временных рядов) или к эмбеддингам категориальных признаков;  \n",
        "   - **RNN-семейство**: Vanilla RNN, LSTM, GRU — только для задач с временной структурой;  \n",
        "   - **Двунаправленные модели**: BiLSTM, BiGRU — для offline-анализа временных рядов;  \n",
        "   - **Encoder–Decoder** — для многопшагового прогнозирования временных рядов;  \n",
        "   - **Гибридные архитектуры**:  \n",
        "     - **Tabular + Temporal**: MLP для статических признаков + LSTM для временных;  \n",
        "     - **CNN + RNN**: извлечение локальных паттернов → моделирование динамики;  \n",
        "     - **Residual MLP/LSTM**: с skip-соединениями для стабилизации обучения.  \n",
        "3. Для каждой модели и каждого датасета:  \n",
        "   - Обеспечить корректную предобработку (масштабирование числовых признаков, эмбеддинги категориальных);  \n",
        "   - Применить **строгое разделение данных** (для временных рядов — `TimeSeriesSplit`; для табличных — стратифицированное разбиение);  \n",
        "   - Настроить гиперпараметры через **валидацию без утечки будущего**.  \n",
        "4. Оценить качество:  \n",
        "   - Для регрессии: MAE, RMSE, MASE, R²;  \n",
        "   - Для классификации: Accuracy, F1, AUC-ROC.  \n",
        "5. Провести **углублённую диагностику обучения**:  \n",
        "   - Графики train/val loss и метрик;  \n",
        "   - Анализ переобучения;  \n",
        "   - Мониторинг градиентов и норм весов.  \n",
        "6. Исследовать **интерпретируемость**:  \n",
        "   - Для MLP/CNN — Saliency Maps, Integrated Gradients;  \n",
        "   - Для RNN/LSTM — анализ скрытых состояний и вентилей;  \n",
        "   - Для гибридов — вклад временных vs. статических признаков.  \n",
        "7. Сравнить все глубокие модели между собой и с **лучшими классическими моделями** из Практической работы № 3 (LightGBM, XGBoost, Ridge и др.).  \n",
        "8. Оценить **вычислительную эффективность**: время обучения, потребление памяти, скорость инференса.  \n",
        "9. Разработать **модульную кодовую базу** на PyTorch или TensorFlow с поддержкой переключения между задачами (табличная/временная) и архитектурами.  \n",
        "10. Опубликовать воспроизводимый пайплайн: данные, конфигурации, веса, визуализации, сравнительные таблицы.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Теоретические предпосылки**\n",
        "\n",
        "На протяжении десятилетий табличные данные оставались доменом классических алгоритмов — деревьев решений, ансамблей и линейных моделей. Глубокое обучение долгое время считалось неэффективным в этой области из-за:  \n",
        "- отсутствия пространственной или последовательной структуры, на которой строятся CNN/RNN;  \n",
        "- высокого риска переобучения при малом объёме данных;  \n",
        "- сложности работы с разнородными типами признаков (числовые, категориальные, разреженные).\n",
        "\n",
        "Однако последние годы показали, что при правильной архитектуре и регуляризации **глубокие модели могут конкурировать, а иногда и превосходить градиентный бустинг**, особенно в задачах с:  \n",
        "- длинными последовательностями (временные ряды);  \n",
        "- сложными нелинейными взаимодействиями признаков;  \n",
        "- наличием мультимодальных или иерархических структур.\n",
        "\n",
        "**MLP**, несмотря на простоту, остаётся сильной базовой линией для табличных данных. С добавлением dropout, batch normalization и внимательной инициализации он демонстрирует удивительную устойчивость.\n",
        "\n",
        "**1D-CNN** находят применение двумя способами:  \n",
        "- как **свёртки по временной оси** — для извлечения локальных паттернов в лагах;  \n",
        "- как **свёртки по эмбеддингам категориальных признаков**, выстроенных в искусственную «последовательность».\n",
        "\n",
        "**Рекуррентные сети (LSTM, GRU)** применимы исключительно в контексте **временной или упорядоченной структуры данных**. Их способность моделировать долгосрочные зависимости делает их незаменимыми в задачах прогнозирования, но бесполезными в чисто табличных задачах без порядка.\n",
        "\n",
        "**Encoder–Decoder** архитектура — естественное решение для **многопшагового прогнозирования**, где требуется сгенерировать последовательность выходов. В отличие от рекурсивной стратегии, она обучается end-to-end и избегает накопления ошибки.\n",
        "\n",
        "**Гибридные модели** открывают путь к объединению статического и динамического контекста: например, прогнозирование спроса с учётом как исторических продаж (LSTM), так и характеристик товара и магазина (MLP).\n",
        "\n",
        "Ключевой тезис настоящей работы: **глубокое обучение для табличных данных — это не универсальный инструмент, а контекстно-зависимый выбор, требующий понимания природы данных и задачи**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Порядок выполнения работы**\n",
        "\n",
        "#### **3.1. Подготовка данных**\n",
        "\n",
        "- **Датасет 1 (временной ряд)**: финальная таблица из Практической работы № 1 (целевая переменная + лаги + инженерные признаки);  \n",
        "- **Датасет 2 (табличный)**: например, `California Housing` (регрессия) или `Adult Income` (классификация);  \n",
        "- Предобработка:  \n",
        "  - Числовые признаки → масштабирование (StandardScaler/RobustScaler);  \n",
        "  - Категориальные → эмбеддинги или one-hot (в зависимости от архитектуры);  \n",
        "  - Для временных рядов — формирование окон без утечки будущего.\n",
        "\n",
        "#### **3.2. Реализация архитектур**\n",
        "\n",
        "- Все модели реализуются в едином интерфейсе (`BaseTabularModel`);  \n",
        "- Поддержка GPU и CPU;  \n",
        "- Для временных рядов — вход в формате `(batch, seq_len, features)`;  \n",
        "- Для табличных — `(batch, num_features)`.  \n",
        "\n",
        "Примеры:  \n",
        "- **MLP**: `Linear → ReLU → Dropout → ... → Output`;  \n",
        "- **1D-CNN**: `Conv1d → ReLU → AdaptiveAvgPool1d → Flatten → Linear`;  \n",
        "- **LSTM**: `LSTM → Dropout → Linear`;  \n",
        "- **Encoder–Decoder**: отдельные модули для кодирования и декодирования последовательности.\n",
        "\n",
        "#### **3.3. Обучение и валидация**\n",
        "\n",
        "- **Временные ряды**: разделение `train = до T−180`, `val = T−180:T−90`, `test = T−90:T`; валидация через временные окна;  \n",
        "- **Табличные данные**: 80/10/10, стратификация по целевой переменной;  \n",
        "- **Оптимизация**: Adam, начальный learning rate = 1e−3, ReduceLROnPlateau;  \n",
        "- **Регуляризация**: dropout (0.2–0.5), weight decay, gradient clipping (для RNN);  \n",
        "- **Остановка**: early stopping по val loss (patience = 10).\n",
        "\n",
        "#### **3.4. Эксперименты и оценка**\n",
        "\n",
        "- Для каждой модели и датасета:  \n",
        "  - Запуск с фиксированным seed;  \n",
        "  - Сохранение лучшей эпохи по валидации;  \n",
        "  - Оценка на тесте — **только один раз**;  \n",
        "- Метрики: MAE, RMSE, R² (регрессия); Accuracy, F1 (классификация);  \n",
        "- Время обучения (сек), память (MB), время инференса (мс на наблюдение).\n",
        "\n",
        "#### **3.5. Интерпретация и диагностика**\n",
        "\n",
        "- **Saliency Maps**: градиенты прогноза по входным признакам;  \n",
        "- **Integrated Gradients**: более устойчивая оценка вклада признаков;  \n",
        "- **Для LSTM/GRU**: визуализация forget/input gates на реальных последовательностях;  \n",
        "- **Скрытые состояния**: кластеризация с помощью t-SNE — отражают ли они семантику данных?\n",
        "\n",
        "#### **3.6. Сравнение и выводы**\n",
        "\n",
        "- Сравнительная таблица: «Модель × Датасет × Метрика × Время × Интерпретируемость»;  \n",
        "- Ранжирование по взвешенной оценке;  \n",
        "- Ответ на вопрос: **когда глубокое обучение оправдано для табличных данных?**  \n",
        "- Сравнение с бустингом (LightGBM/XGBoost) — лидерами в классическом ML.\n",
        "\n",
        "#### **3.7. Публикация и воспроизводимость**\n",
        "\n",
        "- Код — в публичном GitHub/GitLab репозитории;  \n",
        "- Данные — под лицензией CC BY 4.0;  \n",
        "- Веса моделей, логи обучения, конфигурации — архивированы;  \n",
        "- Dockerfile или `requirements.txt` для воспроизведения;  \n",
        "- При необходимости — развёртывание демо на Hugging Face Spaces.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Дополнительные исследовательские задания**\n",
        "\n",
        "1. **Сравнение MLP с табличными специализированными архитектурами**: NODE, TabNet, DCN (без реализации, через библиотеки);  \n",
        "2. **Анализ влияния размера датасета**: при каком количестве наблюдений DL начинает выигрывать у бустинга?  \n",
        "3. **Робастность к зашумлённым признакам**: сравнение устойчивости MLP, LSTM и LightGBM к добавлению шума;  \n",
        "4. **Transfer learning между табличными задачами**: возможно ли?  \n",
        "5. **Ансамблирование DL и классических моделей**: stacking MLP + LightGBM.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Требования к отчёту**\n",
        "\n",
        "Отчёт оформляется в соответствии с ГОСТ 7.32–2017 и включает:  \n",
        "1. **Введение** — эволюция DL для табличных данных, мотивация, связь с предыдущими работами;  \n",
        "2. **Методология** — описание датасетов, архитектур, стратегий обучения и оценки;  \n",
        "3. **Результаты** — таблицы, графики обучения, интерпретационные визуализации, сравнение с ML;  \n",
        "4. **Обсуждение** — когда и почему использовать MLP, LSTM или CNN для табличных данных; анализ компромиссов;  \n",
        "5. **Заключение** — практические рекомендации по выбору архитектуры в зависимости от типа данных, объёма и задачи;  \n",
        "6. **Список литературы** — с ссылками на оригинальные работы по LSTM, GRU, TabNet, и современные обзоры по DL для табличных данных;  \n",
        "7. **Приложения** — фрагменты кода, архитектурные схемы, скриншоты визуализаций, сравнительные метрики.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Критерии оценивания**\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|----------|\n",
        "| **Отлично** | Реализованы все архитектуры на двух типах данных; корректная предобработка и разделение; глубокая диагностика и интерпретация; сравнение с классическим ML; модульный код; полная воспроизводимость; отчёт по ГОСТ. |\n",
        "| **Хорошо** | Реализованы 6–8 моделей на обоих датасетах; есть метрики и базовая интерпретация; отсутствуют лишь отдельные элементы (например, гибридные архитектуры или сравнение с ML). |\n",
        "| **Удовлетворительно** | Работа проведена только на одном типе данных (только временные ряды или только табличные); реализованы 3–5 моделей; есть прогноз и метрики, но без диагностики. |\n",
        "| **Неудовлетворительно** | Отсутствует сравнительный анализ, данные не соответствуют требованиям, нет разделения на выборки, отчёт не представлен. |\n"
      ],
      "metadata": {
        "id": "g4QIdpZuhtwO"
      }
    }
  ]
}