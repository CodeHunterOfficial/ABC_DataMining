{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMALMMfeQrBRC7il6e73HAW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/TimeSeries/TimeSeries-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Практическая работа № 6**  \n",
        "## **Трансформеры и BERT архитектуры для табличных данных и временных рядов**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Цель и задачи работы**\n",
        "\n",
        "**Цель работы** — сформировать у обучающегося целостное и критически осмысленное представление о роли архитектур на основе механизма внимания в задачах анализа структурированных данных. Особое внимание уделяется не прямому переносу языковых моделей, а **специализированной адаптации идей BERT и трансформера** к природе числовых последовательностей и табличных признаков. Работа направлена на освоение методологии проектирования, обучения, интерпретации и сравнения современных моделей, построенных после 2020 года, включая как **специализированные трансформеры**, так и **BERT-вдохновлённые подходы с маскированным самообучением**.\n",
        "\n",
        "**Основные задачи работы:**  \n",
        "1. Использовать два типа данных:  \n",
        "   - **Временной ряд**, обработанный в Практической работе № 1 (не менее 720 последовательных наблюдений, с лагами, скользящими окнами, циклическими кодировками);  \n",
        "   - **Табличный датасет без временной структуры** (например, California Housing, Adult Income).  \n",
        "2. Реализовать и сравнить следующие категории моделей:  \n",
        "   - **Специализированные трансформеры для временных рядов**:  \n",
        "     - Informer (ProbSparse Attention),  \n",
        "     - Autoformer (автокорреляция),  \n",
        "     - FEDformer (частотно-доменный attention),  \n",
        "     - TimesNet (1D→2D периодическое преобразование),  \n",
        "     - TSMixer (MLP-смеси по времени и признакам);  \n",
        "   - **BERT-вдохновлённые модели для временных рядов**:  \n",
        "     - PatchTST (представление ряда как последовательности «патчей» + Transformer encoder),  \n",
        "     - TimeBERT (маскированное обучение с временными эмбеддингами),  \n",
        "     - TS2Vec (контрастивное обучение временных представлений);  \n",
        "   - **Трансформеры для табличных данных**:  \n",
        "     - TabTransformer (Transformer над эмбеддингами категориальных признаков),  \n",
        "     - FT-Transformer (унифицированная токенизация всех признаков).  \n",
        "3. Для каждой модели:  \n",
        "   - Обеспечить корректную предобработку (масштабирование, кодирование категорий, разбиение на патчи или окна);  \n",
        "   - Применить **строгое разделение данных**: для временных рядов — `TimeSeriesSplit`, для табличных — стратифицированное разбиение;  \n",
        "   - Исключить **утечку будущего**, особенно в моделях с маскированием.  \n",
        "4. Провести обучение в двух режимах:  \n",
        "   - **Supervised-only** (для моделей без самообучения);  \n",
        "   - **Self-supervised pretraining + fine-tuning** (для BERT-подобных моделей).  \n",
        "5. Оценить качество:  \n",
        "   - Для регрессии: MAE, RMSE, MASE, SMAPE;  \n",
        "   - Для классификации: Accuracy, F1, AUC-ROC — **только на независимой тестовой выборке**.  \n",
        "6. Проанализировать:  \n",
        "   - **Вычислительную эффективность**: время обучения, память, масштабируемость по длине последовательности;  \n",
        "   - **Интерпретируемость**: attention maps, важность патчей, вклад категориальных признаков;  \n",
        "   - **Эффективность самообучения**: прирост качества при ограниченных labelled данных.  \n",
        "7. Сравнить все модели между собой и с **лучшими решениями из Практических работ № 3 (классический ML) и № 5 (глубокое обучение без трансформеров)**.  \n",
        "8. Разработать модульную кодовую базу на PyTorch с единым интерфейсом для всех архитектур.  \n",
        "9. Опубликовать воспроизводимый пайплайн: код, конфигурации, веса, визуализации, сравнительные таблицы.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Теоретические предпосылки**\n",
        "\n",
        "Появление BERT в 2018 году ознаменовало сдвиг от поверхностного к **контекстуальному представлению данных**. Однако его прямое применение к числовым последовательностям методологически некорректно: временные ряды не обладают словарём, грамматикой или семантикой, свойственными естественному языку. Тем не менее, **ключевые идеи BERT** — маскированное моделирование, глубокий encoder, позиционная осведомлённость — могут быть **переосмыслены** для структурированных данных.\n",
        "\n",
        "Современные архитектуры решают эту задачу по-разному.  \n",
        "- **PatchTST** трактует временной ряд как последовательность «токенов», где токеном выступает **патч** — непересекающееся окно из k точек. Это позволяет применить стандартный Transformer encoder, но с **улучшенной позиционной кодировкой**, учитывающей как глобальный, так и локальный порядок.  \n",
        "- **TimeBERT** вводит **богатые временные эмбеддинги**: абсолютное время, относительное смещение, циклические компоненты (день недели, месяц), что даёт модели понимание временной структуры. Обучение ведётся через восстановление маскированных наблюдений — аналог MLM в BERT.  \n",
        "- **TS2Vec** отказывается от attention в пользу **контрастивного обучения**: близкие по времени сегменты должны иметь схожие представления в скрытом пространстве, что обеспечивает устойчивость к шуму и обобщение.\n",
        "\n",
        "Для **специализированных трансформеров** характерен отказ от dogmatic self-attention:  \n",
        "- **Autoformer** заменяет его на **автокорреляцию** — естественный механизм для временных зависимостей;  \n",
        "- **FEDformer** работает в **частотной области**, выделяя глобальные тренды и сезонности;  \n",
        "- **TimesNet** преобразует 1D-ряд в 2D-матрицу, чтобы применить **2D-CNN** к обнаруженным периодам;  \n",
        "- **TSMixer** вообще отказывается от attention, используя **MLP по временным и признаковым измерениям** — простота, скорость, эффективность.\n",
        "\n",
        "Для **табличных данных** трансформеры применяются через **трансформацию признаков в последовательность**:  \n",
        "- **TabTransformer** строит attention-взаимодействия только между категориальными признаками, оставляя числовые вне encoder’а;  \n",
        "- **FT-Transformer** унифицирует все признаки через линейную токенизацию, создавая симметричную архитектуру.\n",
        "\n",
        "Важно подчеркнуть: **ни одна из этих моделей не использует предобученные веса BERT из Hugging Face**. Все они — **специализированные архитектуры**, обучаемые с нуля на числовых данных. Это принципиальное отличие от наивного «подставить числа вместо слов».\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Порядок выполнения работы**\n",
        "\n",
        "#### **3.1. Подготовка данных**\n",
        "\n",
        "- **Временной ряд**: формирование окон с учётом горизонта прогноза; для BERT-подобных моделей — разбиение на патчи;  \n",
        "- **Табличные данные**: числовые признаки → масштабирование; категориальные → эмбеддинги или one-hot;  \n",
        "- **Маскирование**: для самообучения — случайное скрытие 15–25% патчей или точек (только на обучающей части).\n",
        "\n",
        "#### **3.2. Реализация моделей**\n",
        "\n",
        "- Все архитектуры реализуются в едином интерфейсе (`BaseTransformerModel`) с методами `fit()` и `predict()`;  \n",
        "- Используются официальные или воспроизведённые по статьям реализации;  \n",
        "- Поддержка GPU и CPU; фиксация random seed для воспроизводимости.\n",
        "\n",
        "#### **3.3. Обучение и валидация**\n",
        "\n",
        "- **Временные ряды**:  \n",
        "  - train = до \\( T - 180 \\),  \n",
        "  - val = \\( T - 180 : T - 90 \\),  \n",
        "  - test = последние 90 точек (только для финальной оценки);  \n",
        "- **Табличные данные**: 80/10/10 с стратификацией;  \n",
        "- **Оптимизация**: AdamW, learning rate scheduling, gradient clipping;  \n",
        "- **Регуляризация**: dropout, weight decay, early stopping.\n",
        "\n",
        "#### **3.4. Эксперименты**\n",
        "\n",
        "- Горизонты прогнозирования: \\( h = 1, 7, 30 \\);  \n",
        "- Для BERT-подобных моделей: сравнение с и без pretraining;  \n",
        "- Фиксация:  \n",
        "  - Тестовые метрики,  \n",
        "  - Время обучения (сек),  \n",
        "  - Потребление памяти (MB),  \n",
        "  - Стабильность по разным seed.\n",
        "\n",
        "#### **3.5. Интерпретация**\n",
        "\n",
        "- **PatchTST/Informer/TabTransformer**: визуализация attention-карт;  \n",
        "- **TimesNet**: какие периоды активны в CNN-слоях?  \n",
        "- **TimeBERT**: качество восстановления маскированных точек;  \n",
        "- **FT-Transformer**: важность признаков через attention weights.\n",
        "\n",
        "#### **3.6. Сравнение и выводы**\n",
        "\n",
        "- Сравнительная таблица: «Модель × Тип данных × MAE × Время × Интерпретируемость»;  \n",
        "- Статистическое сравнение с помощью теста Дикболда–Мариано;  \n",
        "- Ответ на ключевой вопрос:  \n",
        "  > **Когда современные трансформеры и BERT-подобные подходы оправдывают свою сложность?**  \n",
        "- Рекомендации по выбору архитектуры в зависимости от объёма данных, горизонта, требований к интерпретируемости.\n",
        "\n",
        "#### **3.7. Публикация и воспроизводимость**\n",
        "\n",
        "- Код — в публичном репозитории (GitHub/GitLab) под лицензией MIT;  \n",
        "- Данные — под CC BY 4.0;  \n",
        "- Веса, конфигурации, логи — архивированы;  \n",
        "- Dockerfile или `requirements.txt` для точного воспроизведения;  \n",
        "- При наличии — веб-демо на Hugging Face Spaces или Streamlit.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Дополнительные исследовательские задания**\n",
        "\n",
        "1. **Влияние длины патча**: как качество PatchTST зависит от k = 4, 8, 16, 32?  \n",
        "2. **Обучение с малым числом меток**: сравнение supervised-only и self-supervised подходов при 10%, 30%, 50% labelled данных.  \n",
        "3. **Робастность к пропускам**: как модели ведут себя при 10–30% пропущенных значений?  \n",
        "4. **Гибридизация**: объединение представлений из FT-Transformer и прогнозов LightGBM через стекинг.  \n",
        "5. **Анализ утечки**: что происходит, если в маскирующей модели случайно разрешить доступ к будущему?\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Требования к отчёту**\n",
        "\n",
        "Отчёт оформляется в соответствии с ГОСТ 7.32–2017 и содержит:  \n",
        "1. **Введение** — от BERT к специализированным трансформерам; мотивация, связь с предыдущими работами;  \n",
        "2. **Методология** — описание данных, архитектур, стратегий обучения, предотвращения утечки;  \n",
        "3. **Результаты** — таблицы, графики прогнозов, attention maps, сравнение с ML/DL;  \n",
        "4. **Обсуждение** — критический анализ: attention vs. автокорреляция vs. частотный анализ; эффективность самообучения;  \n",
        "5. **Заключение** — практические рекомендации: когда использовать PatchTST, когда TSMixer, когда не использовать трансформеры вовсе;  \n",
        "6. **Список литературы** — оригинальные статьи (2020–2024) по каждой модели;  \n",
        "7. **Приложения** — архитектурные схемы, фрагменты кода, скриншоты визуализаций, сравнительные метрики.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Критерии оценивания**\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|----------|\n",
        "| **Отлично** | Реализованы ≥7 моделей (включая BERT-подобные); корректная валидация без утечки; глубокая интерпретация (attention maps, патчи, эмбеддинги); сравнение с ML/DL; модульный код; полная воспроизводимость; отчёт по ГОСТ. |\n",
        "| **Хорошо** | Реализованы 4–6 моделей; есть метрики и базовая интерпретация; отсутствуют лишь отдельные элементы (например, сравнение с прошлыми работами). |\n",
        "| **Удовлетворительно** | Реализованы 2–3 модели (например, только Informer и PatchTST); есть прогноз, но без диагностики утечки или интерпретации. |\n",
        "| **Неудовлетворительно** | Применён стандартный BERT из Hugging Face без адаптации; нет разделения данных; отчёт не предоставлен. |\n"
      ],
      "metadata": {
        "id": "u4esmXdooFBQ"
      }
    }
  ]
}