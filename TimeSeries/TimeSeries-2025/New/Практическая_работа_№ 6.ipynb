{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP85/zZ/Ph0/hheunUCI2KZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/TimeSeries/TimeSeries-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Практическая работа № 6**  \n",
        "## **Трансформеры и BERT-вдохновлённые архитектуры для табличных данных и временных рядов**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Цель и задачи работы**\n",
        "\n",
        "**Цель работы** — формирование у обучающегося системного представления о методологии сравнительного анализа современных архитектур на основе механизма внимания и BERT-парадигмы, применяемых к задачам анализа структурированных данных, включая как табличные датасеты, так и временные ряды. Работа направлена на освоение практических навыков проектирования, настройки, диагностики и интерпретации специализированных трансформерных моделей, построенных после 2020 года, при условии соблюдения строгих требований к предобработке, временной валидации, отсутствию утечки будущего и воспроизводимости результатов в соответствии с передовыми научно-методическими стандартами.\n",
        "\n",
        "**Основные задачи работы:**  \n",
        "1. Использовать два типа данных:  \n",
        "   – **Временной ряд**, обработанный в Практической работе № 1, содержащий не менее 720 последовательных наблюдений, с подтверждённой стационарностью, стабилизированной дисперсией и богатым набором инженерных признаков (лаги, скользящие окна, циклические кодировки);  \n",
        "   – **Табличный датасет без временной структуры**, содержащий смешанные типы признаков (например, California Housing для регрессии или Adult Income для классификации).  \n",
        "2. Формализовать задачу машинного обучения с учётом природы данных: для временных рядов — как задачу прогнозирования с соблюдением временного порядка; для табличных данных — как стандартную задачу регрессии или классификации.  \n",
        "3. Определить архитектуры и гиперпараметры для всех моделей и провести их обучение с использованием корректной валидации: `TimeSeriesSplit` для временных рядов, стратифицированное разбиение — для табличных данных.  \n",
        "4. Реализовать и сравнить следующие категории моделей:  \n",
        "   – **Специализированные трансформеры для временных рядов**:  \n",
        "     • Informer (ProbSparse Attention),  \n",
        "     • Autoformer (автокорреляция),  \n",
        "     • FEDformer (частотно-доменный attention),  \n",
        "     • TimesNet (1D→2D периодическое преобразование),  \n",
        "     • TSMixer (MLP-смеси по времени и признакам);  \n",
        "   – **BERT-вдохновлённые модели для временных рядов**:  \n",
        "     • PatchTST (представление ряда как последовательности «патчей» + Transformer encoder),  \n",
        "     • TimeBERT (маскированное обучение с временными эмбеддингами),  \n",
        "     • TS2Vec (контрастивное обучение временных представлений);  \n",
        "   – **Трансформеры для табличных данных**:  \n",
        "     • TabTransformer (Transformer над эмбеддингами категориальных признаков),  \n",
        "     • FT-Transformer (унифицированная токенизация всех признаков).  \n",
        "5. Обеспечить корректную предобработку: масштабирование числовых признаков, кодирование категориальных, разбиение на патчи или окна, применение маскирования только на обучающей части.  \n",
        "6. Провести обучение в двух режимах:  \n",
        "   – **Supervised-only** (для моделей без самообучения);  \n",
        "   – **Self-supervised pretraining + fine-tuning** (для BERT-подобных моделей).  \n",
        "7. Оценить качество прогнозов/классификации по метрикам:  \n",
        "   – Для регрессии: MAE, RMSE, MASE, SMAPE;  \n",
        "   – Для классификации: Accuracy, F1, AUC-ROC — **на независимой тестовой выборке, не участвовавшей в настройке**.  \n",
        "8. Проанализировать:  \n",
        "   – **Вычислительную эффективность**: время обучения, память, масштабируемость по длине последовательности;  \n",
        "   – **Интерпретируемость**: attention maps, важность патчей, вклад категориальных признаков;  \n",
        "   – **Эффективность самообучения**: прирост качества при ограниченных labelled данных.  \n",
        "9. Сравнить все модели между собой и с **лучшими решениями из Практических работ № 3 (классический ML) и № 5 (глубокое обучение без трансформеров)**.  \n",
        "10. Оценить вычислительную сложность: время обучения, прогнозирования и потребление памяти.  \n",
        "11. Разработать модульный фреймворк на PyTorch с единым интерфейсом для всех архитектур и типов данных.  \n",
        "12. Обеспечить воспроизводимость и открытость результатов: опубликовать код, конфигурации, веса, визуализации и аналитический отчёт.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Теоретические предпосылки**\n",
        "\n",
        "Появление BERT в 2018 году ознаменовало сдвиг от поверхностного к **контекстуальному представлению данных**. Однако его прямое применение к числовым последовательностям методологически некорректно: временные ряды не обладают словарём, грамматикой или семантикой, свойственными естественному языку. Тем не менее, **ключевые идеи BERT** — маскированное моделирование, глубокий encoder, позиционная осведомлённость — могут быть **переосмыслены** для структурированных данных.\n",
        "\n",
        "Современные архитектуры решают эту задачу по-разному.  \n",
        "– **PatchTST** трактует временной ряд как последовательность «токенов», где токеном выступает **патч** — непересекающееся окно из k точек. Это позволяет применить стандартный Transformer encoder, но с **улучшенной позиционной кодировкой**, учитывающей как глобальный, так и локальный порядок.  \n",
        "– **TimeBERT** вводит **богатые временные эмбеддинги**: абсолютное время, относительное смещение, циклические компоненты (день недели, месяц), что даёт модели понимание временной структуры. Обучение ведётся через восстановление маскированных наблюдений — аналог MLM в BERT.  \n",
        "– **TS2Vec** отказывается от attention в пользу **контрастивного обучения**: близкие по времени сегменты должны иметь схожие представления в скрытом пространстве, что обеспечивает устойчивость к шуму и обобщение.\n",
        "\n",
        "Для **специализированных трансформеров** характерен отказ от dogmatic self-attention:  \n",
        "– **Autoformer** заменяет его на **автокорреляцию** — естественный механизм для временных зависимостей;  \n",
        "– **FEDformer** работает в **частотной области**, выделяя глобальные тренды и сезонности;  \n",
        "– **TimesNet** преобразует 1D-ряд в 2D-матрицу, чтобы применить **2D-CNN** к обнаруженным периодам;  \n",
        "– **TSMixer** вообще отказывается от attention, используя **MLP по временным и признаковым измерениям** — простота, скорость, эффективность.\n",
        "\n",
        "Для **табличных данных** трансформеры применяются через **трансформацию признаков в последовательность**:  \n",
        "– **TabTransformer** строит attention-взаимодействия только между категориальными признаками, оставляя числовые вне encoder’а;  \n",
        "– **FT-Transformer** унифицирует все признаки через линейную токенизацию, создавая симметричную архитектуру.\n",
        "\n",
        "Важно подчеркнуть: **ни одна из этих моделей не использует предобученные веса BERT из Hugging Face**. Все они — **специализированные архитектуры**, обучаемые с нуля на числовых данных. Это принципиальное отличие от наивного «подставить числа вместо слов».\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Порядок выполнения работы**\n",
        "\n",
        "Работа выполняется в несколько последовательных этапов, каждый из которых направлен на достижение конкретных образовательных и исследовательских результатов.\n",
        "\n",
        "#### **3.1. Использование данных из Практической работы № 1 и дополнительного табличного датасета**\n",
        "\n",
        "**Задача:** Обеспечить методологическую преемственность и введение разнообразных сценариев.  \n",
        "**Требования к выполнению:**  \n",
        "– **Временной ряд**: финальная таблица из Практической работы № 1:  \n",
        "  • Объём: не менее 720 последовательных наблюдений;  \n",
        "  • Наличие целевой переменной и не менее пяти дополнительных признаков (включая лаги \\( y_{t-1}, y_{t-7}, y_{t-30} \\), скользящие средние, циклические кодировки);  \n",
        "  • Подтверждённая стационарность (результаты ADF/KPSS);  \n",
        "  • Сохранённые параметры преобразований (например, \\( \\lambda \\) для Бокса–Кокса).  \n",
        "– **Табличный датасет**: например, `California Housing` (регрессия) или `Adult Income` (классификация):  \n",
        "  • Содержит числовые и категориальные признаки;  \n",
        "  • Объём ≥10 000 наблюдений.  \n",
        "– Все признаки представлены в табличной форме без пропусков на момент обучения.\n",
        "\n",
        "#### **3.2. Формализация задачи и разделение данных**\n",
        "\n",
        "**Задача:** Корректно сформулировать задачу машинного обучения с учётом природы данных.  \n",
        "**Требования к выполнению:**  \n",
        "– Для временных рядов:  \n",
        "  • Целевая переменная: \\( y_t \\); признаки \\( X_t \\): все доступные лаги и инженерные признаки;  \n",
        "  • Разделение: тренировочная + валидационная часть — всё до \\( T - 90 \\); финальный тест — последние 90 наблюдений (только для итоговой оценки);  \n",
        "  • Для кросс-валидации: `TimeSeriesSplit(n_splits=5, test_size=60)` — без перемешивания.  \n",
        "– Для табличных данных:  \n",
        "  • Стандартная задача регрессии/классификации;  \n",
        "  • Разделение: 80/10/10 с стратификацией по целевой переменной.\n",
        "\n",
        "#### **3.3. Определение архитектур и настройка моделей**\n",
        "\n",
        "**Задача:** Привести каждую модель к её оптимальной конфигурации с использованием корректной валидации.  \n",
        "**Требования к выполнению:**  \n",
        "– Для всех моделей определяются гиперпараметры на основе рекомендаций литературы:  \n",
        "  • **Informer**: `factor=5`, `d_model=512`;  \n",
        "  • **PatchTST**: `patch_len=16`, `stride=16`;  \n",
        "  • **TabTransformer**: `num_heads=8`, `num_encoder_layers=6`;  \n",
        "  • **TimesNet**: `top_k=5`, `d_model=128`.  \n",
        "– Настройка:  \n",
        "  • Обучение с фиксированным seed;  \n",
        "  • Оптимизатор: AdamW (lr=1e-4), learning rate scheduling;  \n",
        "  • Регуляризация: dropout (0.1–0.3), weight decay, gradient clipping;  \n",
        "  • Остановка: early stopping по val loss (patience=10).  \n",
        "– Все процедуры выполняются только на тренировочной части с корректной валидацией.  \n",
        "– Фиксация: оптимальные гиперпараметры, значение метрики на валидации, время обучения.\n",
        "\n",
        "#### **3.4. Обучение в режимах supervised и self-supervised**\n",
        "\n",
        "**Задача:** Реализовать корректное обучение BERT-подобных моделей.  \n",
        "**Требования к выполнению:**  \n",
        "– Для моделей **без самообучения** (Informer, TSMixer): прямое supervised обучение;  \n",
        "– Для моделей **с самообучением** (PatchTST, TimeBERT, TS2Vec):  \n",
        "  • Этап 1: pretraining на всём тренировочном ряду с маскированием 15–25% патчей/точек;  \n",
        "  • Этап 2: fine-tuning на задачу прогнозирования;  \n",
        "– Маскирование применяется **только к обучающей части**, без доступа к будущему;  \n",
        "– Сравнение качества с и без pretraining.\n",
        "\n",
        "#### **3.5. Построение и настройка моделей**\n",
        "\n",
        "**Задача:** Реализовать единообразный интерфейс для всех трансформерных моделей.  \n",
        "**Требования к выполнению:**  \n",
        "– Все модели реализуются на PyTorch с единым API:  \n",
        "  • `model.fit(X_train, y_train)`;  \n",
        "  • `model.predict(X_test)`.  \n",
        "– Поддержка GPU и CPU;  \n",
        "– Для временных рядов — вход в формате `(batch, seq_len, features)`;  \n",
        "– Для табличных — `(batch, num_features)`.  \n",
        "– Используются официальные или воспроизведённые по статьям реализации.\n",
        "\n",
        "#### **3.6. Диагностика адекватности моделей**\n",
        "\n",
        "**Задача:** Убедиться, что модель корректно обучается и не допускает утечки будущего.  \n",
        "**Требования к выполнению:**  \n",
        "– **Анализ утечки**: проверка, что маскирование не использует информацию из будущего;  \n",
        "– **Графики обучения**: train/val loss по эпохам;  \n",
        "– **Стабильность**: повторный запуск с разными seed — насколько стабильны метрики?  \n",
        "– **Масштабируемость**: измерение времени обучения при увеличении длины ряда (T = 100, 500, 1000).  \n",
        "– Диагностика обязательна для всех моделей и обоих типов данных.\n",
        "\n",
        "#### **3.7. Оценка качества и статистическое сравнение**\n",
        "\n",
        "**Задача:** Обоснованно сравнить модели по объективным критериям.  \n",
        "**Требования к выполнению:**  \n",
        "– Расчёт метрик **на финальной тестовой выборке** для каждого датасета и горизонта (\\( h = 1, 7, 30 \\)):  \n",
        "  • Регрессия: MAE, RMSE, MASE, SMAPE;  \n",
        "  • Классификация: Accuracy, F1, AUC-ROC.  \n",
        "– Применение **теста Дикболда–Мариано** для попарного сравнения прогнозов топ-5 моделей;  \n",
        "– Составление сравнительных таблиц:  \n",
        "  • Модели × датасеты × метрики × время обучения × время прогноза;  \n",
        "  • Интерпретируемость (attention maps, важность патчей);  \n",
        "  • Эффективность самообучения (прирост качества).  \n",
        "– Ранжирование моделей по взвешенной оценке: точность (50%), интерпретируемость (30%), вычислительная эффективность (20%).\n",
        "\n",
        "#### **3.8. Анализ интерпретируемости моделей**\n",
        "\n",
        "**Задача:** Оценить способность модели объяснять свои предсказания.  \n",
        "**Требования к выполнению:**  \n",
        "– Для **PatchTST/Informer/TabTransformer**: визуализация attention-карт;  \n",
        "– Для **TimesNet**: анализ активных периодов в CNN-слоях;  \n",
        "– Для **TimeBERT**: оценка качества восстановления маскированных точек;  \n",
        "– Для **FT-Transformer**: расчёт важности признаков через attention weights;  \n",
        "– Проведение **локальной интерпретации**: heatmaps для отдельных прогнозов;  \n",
        "– Результаты интерпретации включаются в сравнительные таблицы.\n",
        "\n",
        "#### **3.9. Анализ вычислительной эффективности**\n",
        "\n",
        "**Задача:** Оценить практическую применимость моделей в условиях ограниченных ресурсов.  \n",
        "**Требования к выполнению:**  \n",
        "– Фиксация времени:  \n",
        "  • Обучение (в секундах),  \n",
        "  • Прогнозирование (в миллисекундах на наблюдение);  \n",
        "– Оценка потребления памяти (опционально, через `memory_profiler`);  \n",
        "– Сравнение сложности: O(N) vs. O(N²) по длине последовательности;  \n",
        "– Включение столбца «Вычислительная сложность» в сравнительные таблицы: низкая/средняя/высокая.\n",
        "\n",
        "#### **3.10. Разработка модульного фреймворка**\n",
        "\n",
        "**Задача:** Создать единый интерфейс для всех архитектур и типов данных.  \n",
        "**Требования к выполнению:**  \n",
        "– Единый базовый класс `BaseTransformerModel` с методами `fit()`, `predict()`, `interpret()`;  \n",
        "– Поддержка переключения между задачами (табличная/временная) через конфигурацию;  \n",
        "– Автоматическая предобработка признаков (масштабирование, патчинг, эмбеддинги);  \n",
        "– Экспорт/загрузка весов моделей.\n",
        "\n",
        "#### **3.11. Публикация результатов исследования**\n",
        "\n",
        "**Задача:** Обеспечить открытость и воспроизводимость работы.  \n",
        "**Требования к выполнению:**  \n",
        "– Для кода и результатов составляется метаописание, включающее:  \n",
        "  • Список моделей и гиперпараметров;  \n",
        "  • Оптимальные гиперпараметры и метрики на валидации;  \n",
        "  • Сравнительные таблицы по всем моделям и датасетам;  \n",
        "  • Ключевые выводы и рекомендации;  \n",
        "  • Инструкцию по запуску фреймворка.  \n",
        "– Все материалы публикуются в публичном репозитории (GitHub/GitLab);  \n",
        "– Веса моделей и логи обучения — архивированы;  \n",
        "– Использование открытых лицензий: MIT (код), CC BY 4.0 (данные).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Дополнительные исследовательские задания**\n",
        "\n",
        "1. Влияние длины патча: как качество PatchTST зависит от \\( k = 4, 8, 16, 32 \\)?  \n",
        "2. Обучение с малым числом меток: сравнение supervised-only и self-supervised подходов при 10%, 30%, 50% labelled данных.  \n",
        "3. Робастность к пропускам: как модели ведут себя при 10–30% пропущенных значений?  \n",
        "4. Гибридизация: объединение представлений из FT-Transformer и прогнозов LightGBM через стекинг.  \n",
        "5. Анализ утечки: что происходит, если в маскирующей модели случайно разрешить доступ к будущему?  \n",
        "6. Сравнение скорости масштабирования: как время обучения растёт с длиной ряда (T = 100, 500, 1000, 2000)?\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Требования к отчёту**\n",
        "\n",
        "Отчёт оформляется в соответствии с ГОСТ 7.32–2017 и должен содержать следующие разделы:  \n",
        "1. **Введение** — от BERT к специализированным трансформерам; актуальность, обзор литературы, указание на преемственность с Практическими работами № 1, № 3, № 5.  \n",
        "2. **Методология** — описание данных, архитектур, стратегий обучения, предотвращения утечки будущего, метрик.  \n",
        "3. **Результаты экспериментов** — сравнительные таблицы, графики прогнозов, attention maps, результаты теста Дикболда–Мариано, диагностика утечки.  \n",
        "4. **Обсуждение** — критический анализ: attention vs. автокорреляция vs. частотный анализ; эффективность самообучения; сравнение с классическим и глубоким ML.  \n",
        "5. **Заключение** — итоговые выводы, практические рекомендации по выбору архитектуры в зависимости от объёма данных, горизонта, требований к интерпретируемости и вычислительным ресурсам.  \n",
        "6. **Список использованных источников** — оформлен в соответствии с ГОСТ Р 7.0.5–2008.  \n",
        "7. **Приложения** — архитектурные схемы, фрагменты кода, скриншоты визуализаций, таблицы гиперпараметров.  \n",
        "Отчёт сопровождается ссылками на репозиторий с кодом и опубликованные данные.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Критерии оценивания**\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|----------|\n",
        "| **Отлично** | Полное выполнение всех этапов; обучение ≥7 моделей (включая BERT-подобные); корректная валидация без утечки будущего; углублённая диагностика (attention maps, патчи, эмбеддинги); сравнение с ML/DL; модульный фреймворк; публикация всех материалов; отчёт, соответствующий ГОСТ. |\n",
        "| **Хорошо** | Выполнение основных этапов; обучение 4–6 моделей; наличие сравнительных таблиц и базовой интерпретации; отсутствие лишь отдельных элементов (например, анализа утечки или сравнения с прошлыми работами). |\n",
        "| **Удовлетворительно** | Реализованы 2–3 модели (например, только Informer и PatchTST); есть прогноз и метрики, но без диагностики утечки или интерпретации. |\n",
        "| **Неудовлетворительно** | Применён стандартный BERT из Hugging Face без адаптации; нет корректного разделения данных; отсутствует сравнительный анализ; отчёт не представлен. |\n"
      ],
      "metadata": {
        "id": "u4esmXdooFBQ"
      }
    }
  ]
}