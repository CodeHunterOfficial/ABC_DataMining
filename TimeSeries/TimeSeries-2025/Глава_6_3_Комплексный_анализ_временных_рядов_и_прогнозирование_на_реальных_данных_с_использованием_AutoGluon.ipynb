{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPNDWvk63VhFekNtxT9r1zW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/TimeSeries/TimeSeries-2025/%D0%93%D0%BB%D0%B0%D0%B2%D0%B0_6_3_%D0%9A%D0%BE%D0%BC%D0%BF%D0%BB%D0%B5%D0%BA%D1%81%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D1%85_%D1%80%D1%8F%D0%B4%D0%BE%D0%B2_%D0%B8_%D0%BF%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BD%D0%B0_%D1%80%D0%B5%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85_%D1%81_%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%D0%BC_AutoGluon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Комплексный анализ временных рядов и прогнозирование на реальных данных с использованием AutoGluon**\n",
        "\n",
        "**Тема:** Построение end-to-end решения для прогнозирования цен на недвижимость, включая предобработку данных, feature engineering, обучение модели и глубокий анализ результатов.\n",
        "\n",
        "**Цель:** Продемонстрировать полный цикл работы Data Scientist на практике, от сырых данных до готовых выводов для научной статьи.\n",
        "\n",
        "\n",
        "\n",
        "### **Структура лекции (соответствует разделам кода):**\n",
        "\n",
        "1.  **Введение и подготовка среды**\n",
        "2.  **Предварительный анализ и предобработка данных (Data Preprocessing)**\n",
        "3.  **Разведочный анализ данных (EDA - Exploratory Data Analysis)**\n",
        "4.  **Статистический анализ и проверка гипотез**\n",
        "5.  **Feature Engineering и подготовка данных для модели**\n",
        "6.  **Прогнозирование с помощью AutoGluon**\n",
        "7.  **Диагностика модели и анализ остатков**\n",
        "8.  **Интерпретация модели и финальные результаты**\n",
        "\n",
        "\n",
        "\n",
        "### **#1. Введение и подготовка среды**\n",
        "\n",
        "```python\n",
        "# %%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "...\n",
        "```\n",
        "\n",
        "*   **Импорт библиотек:** Код начинается с импорта всех необходимых библиотек. Это стандартная практика.\n",
        "    *   `pandas`, `numpy`: для работы с данными.\n",
        "    *   `matplotlib`, `seaborn`, `plotly`: для визуализации. Использование `plotly` говорит о стремлении к интерактивным и публикационным графикам.\n",
        "    *   `scipy.stats`, `statsmodels`: для статистического анализа и анализа временных рядов (тесты, ACF/PACF, декомпозиция).\n",
        "    *   `sklearn`: для метрик и кросс-валидации.\n",
        "    *   `autogluon.tabular`: основной инструмент для автоматического машинного обучения. Он автоматически подберет и обучит ансамбль моделей (GBM, CAT, XGB и т.д.).\n",
        "    *   `shap`: для интерпретируемости модели и анализа важности признаков.\n",
        "    *   `tqdm`: для отображения прогресса.\n",
        "*   **Настройки отображения:** Увеличение максимального числа колонок и ширины вывода для `pandas`, установка стилей графиков. Это важно для удобства анализа.\n",
        "*   **Google Colab:** Код предназначен для запуска в среде Google Colab, о чем говорит монтаж Google Drive. Это популярный выбор для экспериментов из-за бесплатного доступа к GPU/TPU.\n",
        "\n",
        "**Ключевой вывод:** Подготовлена полноценная среда для анализа, покрывающая все этапы: от обработки данных до продвинутого ML и визуализации.\n",
        "\n",
        "\n",
        "\n",
        "### **#2. Предварительный анализ и предобработка данных**\n",
        "\n",
        "**Цель:** Превратить сырые данные в пригодный для анализа и моделирования набор.\n",
        "\n",
        "```python\n",
        "# Загрузка данных\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/tatarstan_dataset.csv')\n",
        "```\n",
        "*   **Загрузка данных:** Данные загружаются из CSV-файла, предположительно содержащего информацию о продажах недвижимости в Татарстане.\n",
        "\n",
        "```python\n",
        "print(\"Размер датасета:\", df.shape)\n",
        "df.info()\n",
        "df.describe().round(2)\n",
        "missing_df = pd.DataFrame({'Количество': missing_data, 'Процент': missing_percent})\n",
        "```\n",
        "*   **Первичный анализ:** Сразу же оценивается размер данных, типы столбцов, наличие пропусков. `df.describe()` дает представление о распределении числовых признаков. Это первый и обязательный шаг.\n",
        "\n",
        "```python\n",
        "# Обработка пропущенных значений\n",
        "def handle_missing_values(df):\n",
        "    # Числовые признаки - медиана\n",
        "    # Категориальные признаки - мода\n",
        "```\n",
        "*   **Обработка пропусков:** Пропуски в числовых признаках заполняются **медианой** (устойчива к выбросам), в категориальных — **модой** (наиболее частым значением). Это стандартные и надежные стратегии.\n",
        "\n",
        "```python\n",
        "# Создание datetime колонки\n",
        "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "df = df.sort_values('datetime').reset_index(drop=True)\n",
        "```\n",
        "*   **Работа с временем:** Создание единого столбца `datetime` и сортировка по нему — **критически важный шаг для временных рядов**. Без этого последующий анализ лагов и скользящих статистик будет некорректным.\n",
        "\n",
        "```python\n",
        "# Анализ и обработка выбросов\n",
        "def detect_outliers(df, column, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(df[column]))\n",
        "...\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "```\n",
        "*   **Борьба с выбросами:** Применяются два метода.\n",
        "    1.  **Z-Score** (`detect_outliers`): для детектирования и анализа выбросов.\n",
        "    2.  **IQR (Interquartile Range)** (`remove_outliers_iqr`): для их непосредственного удаления. Метод IQR считается более робастным, чем z-score, особенно для ненормальных распределений. Удаляются значения, выходящие за пределы `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]`.\n",
        "\n",
        "**Ключевой вывод:** Данные были тщательно очищены от пропусков и выбросов, а также приведены к формату, пригодному для анализа временных рядов.\n",
        "\n",
        "\n",
        "\n",
        "### **#3. Разведочный анализ данных (EDA)**\n",
        "\n",
        "**Цель:** Понимание данных, выявление закономерностей, аномалий и взаимосвязей.\n",
        "\n",
        "```python\n",
        "# Распределение целевой переменной\n",
        "sns.histplot(df_clean['price'], kde=True, bins=50)\n",
        "sns.histplot(np.log1p(df_clean['price']), kde=True, bins=50)\n",
        "```\n",
        "*   **Анализ целевой переменной (Price):** Исходное распределение цен скорее всего имеет **длинный хвост (right-skewed)**. Логарифмирование (`np.log1p`) помогает приблизить распределение к нормальному, что часто полезно для линейных моделей.\n",
        "\n",
        "```python\n",
        "# Временной ряд средней цены\n",
        "daily_prices = df_clean.groupby('date')['price'].agg(['mean', 'median', 'count']).reset_index()\n",
        "fig = make_subplots(rows=2, cols=1, subplot_titles=('Средняя цена по дням', 'Количество объявлений по дням'))\n",
        "```\n",
        "*   **Анализ временных рядов:** Строятся два ключевых графика: динамика средней цены и динамика количества объявлений. Важно искать **тренды, сезонность и выбросы**. Например, падение количества объявлений может объяснять всплески волатильности цен.\n",
        "\n",
        "```python\n",
        "# Сезонность и тренды\n",
        "decomposition = seasonal_decompose(ts_series, period=30, model='additive')\n",
        "```\n",
        "*   **Декомпозиция временного ряда:** Классический метод разделения ряда на составляющие:\n",
        "    *   **Тренд (Trend):** Долгосрочная направленность.\n",
        "    *   **Сезонность (Seasonality):** Периодические колебания (например, ежеквартальные).\n",
        "    *   **Остаток (Resid):** Случайный шум, который не объясняется моделью.\n",
        "\n",
        "```python\n",
        "# Анализ корреляций\n",
        "correlation_matrix = df_clean[numeric_cols].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "```\n",
        "*   **Матрица корреляций:** Позволяет быстро увидеть линейные зависимости между числовыми признаками. Например, ожидаема высокая корреляция между `area` и `price`.\n",
        "\n",
        "```python\n",
        "# Анализ категориальных переменных\n",
        "categorical_cols = ['region', 'building_type', 'object_type']\n",
        "df_clean[col].value_counts().plot(kind='bar')\n",
        "```\n",
        "*   **Анализ категориальных признаков:** Столбчатые диаграммы показывают распределение объектов по регионам, типам зданий и т.д. Помогает выявить дисбаланс классов.\n",
        "\n",
        "```python\n",
        "# Географическое распределение цен\n",
        "plt.scatter(df_clean['geo_lon'], df_clean['geo_lat'], c=df_clean['price'], cmap='viridis')\n",
        "```\n",
        "*   **Гео-визуализация:** Точечная карта, где цвет точки означает цену. Позволяет визуально идентифицировать дорогие и дешевые районы (например, центр города vs окраины).\n",
        "\n",
        "**Ключевой вывод:** EDA выявило особенности данных: ненормальное распределение цены, наличие тренда и сезонности, географическую зависимость цены и корреляции между признаками.\n",
        "\n",
        "\n",
        "\n",
        "### **#4. Статистический анализ и проверка гипотез**\n",
        "\n",
        "**Цель:** Подтвердить или опровергнуть качественные предположения статистическими методами.\n",
        "\n",
        "```python\n",
        "# Тест на стационарность (ADF и KPSS)\n",
        "def test_stationarity(timeseries):\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    kpsstest = kpss(timeseries, regression='c')\n",
        "```\n",
        "*   **Тесты на стационарность:** Крайне важны для прогнозирования.\n",
        "    *   **ADF (Augmented Dickey-Fuller):** Нулевая гипотеза — ряд нестационарен. Малое p-value (<0.05) позволяет отклонить H0 и считать ряд стационарным.\n",
        "    *   **KPSS (Kwiatkowski-Phillips-Schmidt-Shin):** Нулевая гипотеза — ряд стационарен. Малое p-value говорит *против* стационарности.\n",
        "    *   Обычно ряд требует преобразований (дифференцирования) если ADF говорит о нестационарности, а KPSS это подтверждает.\n",
        "\n",
        "```python\n",
        "# Автокорреляция и частичная автокорреляция\n",
        "plot_acf(ts_series.dropna(), ax=ax1, lags=40)\n",
        "plot_pacf(ts_series.dropna(), ax=ax2, lags=40)\n",
        "```\n",
        "*   **ACF/PACF:** Графики автокорреляции помогают определить параметры для классических моделей временных рядов (ARIMA). ACF показывает общую корреляцию с лагами, PACF — только \"чистую\" корреляцию с конкретным лагом.\n",
        "\n",
        "```python\n",
        "# Проверка статистических гипотез\n",
        "f_stat, p_value = stats.f_oneway(*region_prices)\n",
        "```\n",
        "*   **ANOVA (Analysis of Variance):** Проверяет гипотезу о том, что средние значения цен **статистически значимо различаются** между разными группами (регионами, типами зданий). Малое p-value (<0.05) подтверждает, что признак действительно важен для объяснения различий в цене.\n",
        "\n",
        "**Ключевой вывод:** Статистические тесты подтвердили нестационарность ряда и статистическую значимость различий цен в зависимости от категориальных признаков.\n",
        "\n",
        "\n",
        "\n",
        "### **#5. Feature Engineering и подготовка данных**\n",
        "\n",
        "**Цель:** Создать новые информативные признаки, которые помогут модели лучше уловить закономерности.\n",
        "\n",
        "```python\n",
        "# Преобразование Бокса-Кокса\n",
        "def apply_boxcox(series):\n",
        "    transformed, lmbda = stats.boxcox(series_positive)\n",
        "```\n",
        "*   **Преобразование Бокса-Кокса:** Более продвинутый способ, чем логарифмирование, для нормализации распределения и стабилизации дисперсии. Подбирает оптимальный параметр `lambda` автоматически.\n",
        "\n",
        "```python\n",
        "# Создание временных признаков\n",
        "def create_time_features(df):\n",
        "    df['year'] = df['datetime'].dt.year\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    ...\n",
        "    df['season'] = df['month'] % 12 // 3 + 1\n",
        "```\n",
        "*   **Временные признаки:** Из datetime извлекаются составляющие (год, месяц, день недели, время года и т.д.). Это помогает модели выучить **сезонные паттерны** (например, цены выше весной, ниже осенью).\n",
        "\n",
        "```python\n",
        "# Создание лаговых и скользящих статистик\n",
        "def create_lag_features(df, group_col='region', target_col='price', lags=[1, 7, 30]):\n",
        "    df[f'{target_col}_lag_{lag}'] = df.groupby(group_col)[target_col].shift(lag)\n",
        "    df[f'{target_col}_rolling_mean_{window}'] = ...rolling(window...).mean()\n",
        "```\n",
        "*   **Лаги и скользящие окна:** Это **ключевой прием** для прогнозирования временных рядов табличными методами.\n",
        "    *   **Лаг (Lag):** Цена 1, 7 или 30 дней назад. Модель видит, что было в прошлом.\n",
        "    *   **Скользящее среднее (Rolling Mean):** Сглаживает ряд, показывает тренд.\n",
        "    *   **Скользящее стандартное отклонение (Rolling Std):** Показывает волатильность.\n",
        "    *   Группировка по `region` перед сдвигом — блестящая идея! Она создает эти признаки в разрезе каждого региона, что гораздо осмысленнее, чем по всему датасету.\n",
        "\n",
        "```python\n",
        "# Кодирование категориальных переменных\n",
        "le_region = LabelEncoder()\n",
        "df['region_encoded'] = le_region.fit_transform(df['region'])\n",
        "df = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n",
        "```\n",
        "*   **Кодирование признаков:**\n",
        "    *   **Label Encoding** для региона, так как это порядковый признак с множеством категорий.\n",
        "    *   **One-Hot Encoding** для других категориальных признаков, так как у них мало уникальных значений.\n",
        "\n",
        "**Ключевой вывод:** Создан богатый набор признаков, включая временные, лаговые и статистические, которые являются залогом успешного прогноза.\n",
        "\n",
        "\n",
        "\n",
        "### **#6. Прогнозирование с AutoGluon**\n",
        "\n",
        "**Цель:** Использовать мощь AutoML для автоматического построения и оптимизации ансамбля моделей.\n",
        "\n",
        "```python\n",
        "# Разделение на train/test с учетом временного порядка\n",
        "split_date = df_final['datetime'].quantile(0.8)\n",
        "train_data = df_final[df_final['datetime'] < split_date]\n",
        "test_data = df_final[df_final['datetime'] >= split_date]\n",
        "```\n",
        "*   **Временное разделение:** Важно! Никогда не нужно делать случайное разделение для временных рядов. Данные должны быть разделены по времени: старые данные для обучения, новые — для тестирования. Это симулирует реальный процесс прогнозирования.\n",
        "\n",
        "```python\n",
        "predictor = TabularPredictor(...).fit(\n",
        "    train_dataset,\n",
        "    time_limit=7200,  # 2 часа\n",
        "    presets='best_quality',\n",
        "    hyperparameters={...},\n",
        "    num_bag_folds=5,\n",
        "    num_stack_levels=2\n",
        ")\n",
        "```\n",
        "*   **Обучение AutoGluon:**\n",
        "    *   `time_limit`: Ограничение по времени обучения.\n",
        "    *   `presets='best_quality'`: Настройка на максимальное качество, а не на скорость.\n",
        "    *   `hyperparameters`: Можно тонко настраивать конкретные модели.\n",
        "    *   `num_bag_folds=5`: **Бэггинг** — обучение нескольких копий модели на разных подвыборках.\n",
        "    *   `num_stack_levels=2`: **Стекинг** — использование predictions первых-level моделей как features для meta-модели второго уровня. Мощный прием для увеличения точности.\n",
        "\n",
        "```python\n",
        "leaderboard = predictor.leaderboard()\n",
        "```\n",
        "*   **Leaderboard:** AutoGluon автоматически сравнивает все обученные модели и показывает их метрики. Позволяет увидеть, какая модель (или ансамбль) показала лучший результат.\n",
        "\n",
        "**Ключевой вывод:** AutoGluon использован как высокоэффективный инструмент для автоматического построения сложного ансамбля моделей, избавляя исследователя от ручного подбора.\n",
        "\n",
        "\n",
        "\n",
        "### **#7. Диагностика модели и анализ остатков**\n",
        "\n",
        "**Цель:** Убедиться, что модель адекватна, и понять природу ее ошибок.\n",
        "\n",
        "```python\n",
        "# Анализ остатков\n",
        "plt.scatter(y_pred, residuals, alpha=0.6)\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plot_acf(residuals, lags=40)\n",
        "```\n",
        "*   **Диагностика остатков (Residual Analysis):**\n",
        "    *   **Остатки vs Прогнозы:** Остатки должны быть случайно разбросаны вокруг нуля без каких-либо паттернов (например, в форме \"воронки\"). Паттерны означают, что модель не уловила какую-то закономерность.\n",
        "    *   **Q-Q plot:** Проверка нормальности распределения остатков. Отклонение точек от прямой линии говорит о ненормальности.\n",
        "    *   **Автокорреляция остатков:** Остатки не должны иметь значимой автокорреляции. Наличие автокорреляции (выходящие за синюю область столбцы) — признак того, что модель не использовала всю информацию из временных зависимостей.\n",
        "\n",
        "```python\n",
        "# Статистические тесты для остатков\n",
        "shapiro_test = stats.shapiro(residuals)\n",
        "```\n",
        "*   **Тест Шапиро-Уилка:** Формальная проверка гипотезы о нормальности распределения остатков.\n",
        "\n",
        "```python\n",
        "# Анализ важности признаков\n",
        "feature_importance = predictor.feature_importance(test_dataset)\n",
        "```\n",
        "*   **Важность признаков:** Показывает, какие признаки были наиболее важны для модели при принятии решений. Обычно это лаговые признаки и площадь.\n",
        "\n",
        "```python\n",
        "# SHAP значения\n",
        "explainer = shap.Explainer(predictor.predict, sample_data)\n",
        "shap_values = explainer(sample_data)\n",
        "shap.summary_plot(shap_values.values, sample_data, plot_type=\"bar\")\n",
        "```\n",
        "*   **SHAP (SHapley Additive exPlanations):** Передовой метод интерпретации моделей. Показывает не только важность, но и **направление влияния** каждого признака на прогноз (положительное или отрицательное).\n",
        "\n",
        "**Ключевой вывод:** Проведена комплексная диагностика, которая показывает, насколько модель надежна, и помогает понять, как она принимает решения.\n",
        "\n",
        "\n",
        "\n",
        "### **#8. Финальные результаты и выводы для статьи**\n",
        "\n",
        "```python\n",
        "# Финальные метрики\n",
        "final_metrics = {'RMSE': ..., 'MAE': ..., 'R2': ..., 'MAPE': ...}\n",
        "```\n",
        "*   **Метрики качества:** Финальная оценка модели на тестовой выборке.\n",
        "    *   **RMSE, MAE:** Ошибки в единицах целевой переменной (рубли). Показывают, \"на сколько рублей в среднем ошибается модель\".\n",
        "    *   **R² (R-Squared):** Доля дисперсии, объясненная моделью. Ближе к 1 — лучше.\n",
        "    *   **MAPE (Mean Absolute Percentage Error):** Средняя абсолютная процентная ошибка. Легко интерпретировать бизнесу.\n",
        "\n",
        "```python\n",
        "# Сравнение с baseline моделями\n",
        "baseline_mae = mean_absolute_error(y_test, [y_train.mean()] * len(y_test))\n",
        "print(f\"Улучшение MAE: {(1 - final_metrics['MAE']/baseline_mae) * 100:.2f}%\")\n",
        "```\n",
        "*   **Сравнение с Baseline:** Обязательный шаг. Baseline — это простейшая модель (например, прогноз всегда средним значением из тренировочного набора). Любая сложная модель должна быть лучше baseline. Процент улучшения — ключевой результат.\n",
        "\n",
        "```python\n",
        "# Генерация отчета\n",
        "print(\"АНАЛИТИЧЕСКИЙ ОТЧЕТ\")\n",
        "...\n",
        "```\n",
        "*   **Подготовка к публикации:** Код генерирует сводный отчет и набор графиков, которые можно напрямую использовать в научной статье или отчете для заказчика.\n",
        "\n",
        "**Ключевой вывод всей работы:** Представлен полный, воспроизводимый и хорошо документированный цикл анализа данных, который привел к созданию эффективной модели прогнозирования с измеримым улучшением качества относительно наивных методов. Код является отличным примером для изучения best practices в Data Science."
      ],
      "metadata": {
        "id": "zm5a7hxqJZCO"
      }
    }
  ]
}