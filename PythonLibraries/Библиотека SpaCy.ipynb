{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/PythonLibraries/%D0%91%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0%20SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Введение**"
      ],
      "metadata": {
        "id": "-4RsYEfjUQKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy — это современная библиотека для обработки естественного языка (Natural Language Processing, NLP), которая сочетает высокую производительность с простотой использования. Она предназначена для профессионального использования в приложениях, связанных с извлечением информации, машинным обучением и анализом больших объемов текстовых данных. В отличие от более старых библиотек, таких как NLTK, SpaCy предоставляет готовые к использованию модели и инструменты для токенизации, синтаксического анализа, частеречной разметки, лемматизации, распознавания именованных сущностей (NER) и других задач."
      ],
      "metadata": {
        "id": "G3uqCbhGUcnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка SpaCy\n",
        "Чтобы начать работу с SpaCy, необходимо установить библиотеку и загрузить языковую модель. Установка библиотеки осуществляется через пакетный менеджер pip:\n"
      ],
      "metadata": {
        "id": "XI6NZJmZUgpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "id": "tm05BY7OUb2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После этого требуется загрузить модель для выбранного языка. Например, для работы с английским языком можно использовать команду: python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "EJ8YOZjJU3QM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Языковые модели в SpaCy**"
      ],
      "metadata": {
        "id": "wAc8Dpq4UOPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Языковые модели в SpaCy содержат различные компоненты для обработки текста. Каждая модель включает в себя:\n",
        "\n",
        "Токенизатор: Разбивает текст на отдельные слова и знаки препинания.\n",
        "\n",
        "**Частеречную** разметку (POS tagging): Определяет часть речи для каждого токена.\n",
        "\n",
        "**Лемматизатор**: Приводит слова к их исходной форме.\n",
        "\n",
        "**Синтаксический анализатор**: Строит дерево зависимостей предложений.\n",
        "\n",
        "**Модуль распознавания именованных сущностей (NER)**: Определяет имена людей, места, даты и другие сущности.\n",
        "\n",
        "Теперь давайте рассмотрим основные возможности SpaCy на практике.\n",
        "\n",
        "\n",
        "\n",
        "**1. Токенизация**\n",
        "**Токенизация** — это процесс, при котором текст разбивается на отдельные слова, знаки препинания и другие значимые элементы, называемые токенами. В SpaCy токенизация выполняется автоматически при обработке текста с помощью загруженной языковой модели.\n",
        "\n",
        "Пример токенизации текста:"
      ],
      "metadata": {
        "id": "20A1Js5IVMAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка английской языковой модели\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Пример текста\n",
        "text = \"Hello, world! Welcome to the world of SpaCy.\"\n",
        "\n",
        "# Обработка текста с помощью модели\n",
        "doc = nlp(text)\n",
        "\n",
        "# Вывод всех токенов\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0NJdIpNVJRl",
        "outputId": "629b325a-5e94-4d6f-cdf1-f2fbf348c9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "world\n",
            "!\n",
            "Welcome\n",
            "to\n",
            "the\n",
            "world\n",
            "of\n",
            "SpaCy\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "Мы загружаем модель английского языка с помощью функции spacy.load().\n",
        "\n",
        "\n",
        "Передаем текст в модель, которая создает объект doc, представляющий собой обработанный текст.\n",
        "\n",
        "Затем мы выводим каждый токен в тексте через цикл for, используя свойство text для каждого токена.\n",
        "\n",
        "**2. Лемматизация**\n",
        "**Лемматизация** — это процесс приведения слов к их исходной форме, называемой леммой. Леммы позволяют нормализовать слова для их дальнейшего анализа, игнорируя грамматические изменения (например, спряжение глаголов).\n",
        "\n",
        "Пример лемматизации:"
      ],
      "metadata": {
        "id": "KBWCwK72Vvwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация токенов\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, Lemma: {token.lemma_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_e-iseHV74E",
        "outputId": "aef8cad7-0671-4988-a835-62bbff3105a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Hello, Lemma: Hello\n",
            "Token: ,, Lemma: ,\n",
            "Token: world, Lemma: world\n",
            "Token: !, Lemma: !\n",
            "Token: Welcome, Lemma: welcome\n",
            "Token: to, Lemma: to\n",
            "Token: the, Lemma: the\n",
            "Token: world, Lemma: world\n",
            "Token: of, Lemma: of\n",
            "Token: SpaCy, Lemma: SpaCy\n",
            "Token: ., Lemma: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Объяснение:*\n",
        "\n",
        "Мы выводим исходный текст каждого токена, а также его лемму с помощью свойства lemma_.\n",
        "\n",
        "Лемматизация полезна при работе с текстами, чтобы сгруппировать разные формы одного и того же слова.\n",
        "\n",
        "**3. Частеречная разметка (POS-tagging)**\n",
        "Частеречная разметка (POS-tagging) — это процесс, при котором каждому слову в предложении присваивается его часть речи, например, существительное, глагол, прилагательное и т. д. Это позволяет лучше понять синтаксическую структуру текста и его грамматические зависимости.\n",
        "\n",
        "Пример частеречной разметки:"
      ],
      "metadata": {
        "id": "8G5VFbOiV_9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Часть речи для каждого токена\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, POS: {token.pos_}, Detailed POS: {token.tag_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l00-frNfWQD1",
        "outputId": "577527d2-a4bf-47b5-e95e-04c056fdff76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Hello, POS: PROPN, Detailed POS: NNP\n",
            "Token: ,, POS: PUNCT, Detailed POS: ,\n",
            "Token: world, POS: NOUN, Detailed POS: NN\n",
            "Token: !, POS: PUNCT, Detailed POS: .\n",
            "Token: Welcome, POS: INTJ, Detailed POS: UH\n",
            "Token: to, POS: ADP, Detailed POS: IN\n",
            "Token: the, POS: DET, Detailed POS: DT\n",
            "Token: world, POS: NOUN, Detailed POS: NN\n",
            "Token: of, POS: ADP, Detailed POS: IN\n",
            "Token: SpaCy, POS: PROPN, Detailed POS: NNP\n",
            "Token: ., POS: PUNCT, Detailed POS: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Мы выводим часть речи для каждого токена с помощью свойства pos_ (например, NOUN — существительное, VERB — глагол).\n",
        "\n",
        "Также выводим детализированную информацию о части речи с помощью tag_, которая даёт более точные грамматические характеристики."
      ],
      "metadata": {
        "id": "iTQMgJTgWSzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Синтаксический анализ (Dependency Parsing)**\n",
        "\n",
        "Синтаксический анализ позволяет определить грамматические отношения между словами в предложении. В процессе синтаксического анализа строится дерево зависимостей, где каждое слово имеет связь с другими словами.\n",
        "\n",
        "Пример синтаксического анализа:"
      ],
      "metadata": {
        "id": "w2OvXJSmWVBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Синтаксические зависимости и связи между токенами\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, Head: {token.head.text}, Dependency: {token.dep_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2xZ7WuqWYIk",
        "outputId": "e791e446-615e-4af0-c72d-120d2122037c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Hello, Head: Hello, Dependency: ROOT\n",
            "Token: ,, Head: Hello, Dependency: punct\n",
            "Token: world, Head: Hello, Dependency: appos\n",
            "Token: !, Head: Hello, Dependency: punct\n",
            "Token: Welcome, Head: Welcome, Dependency: ROOT\n",
            "Token: to, Head: Welcome, Dependency: prep\n",
            "Token: the, Head: world, Dependency: det\n",
            "Token: world, Head: to, Dependency: pobj\n",
            "Token: of, Head: world, Dependency: prep\n",
            "Token: SpaCy, Head: of, Dependency: pobj\n",
            "Token: ., Head: Welcome, Dependency: punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Свойство head показывает \"главное\" слово, к которому относится данный токен.\n",
        "Свойство dep_ описывает тип грамматической зависимости между токенами (например, nsubj — подлежащее, obj — дополнение).\n",
        "\n",
        "**5. Распознавание именованных сущностей (NER)**\n",
        "\n",
        "Распознавание именованных сущностей (NER) — это процесс идентификации и классификации ключевых сущностей в тексте, таких как имена людей, компании, города, даты и суммы денег. SpaCy поддерживает распознавание таких сущностей и присваивает им метки.\n",
        "\n",
        "Пример распознавания именованных сущностей:"
      ],
      "metadata": {
        "id": "O_RfQ0PMWe1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Распознавание именованных сущностей\n",
        "for ent in doc.ents:\n",
        "    print(f'Entity: {ent.text}, Label: {ent.label_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_QJoD1aWmPv",
        "outputId": "4c4f9d83-0c57-4920-cf44-e668ce4b00ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: SpaCy, Label: PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Мы используем атрибут ents, который хранит список всех распознанных сущностей в документе.\n",
        "\n",
        "Свойство label_ показывает тип сущности (например, PERSON — человек, ORG — организация, DATE — дата).\n",
        "\n",
        "В нашем примере нет явных именованных сущностей, кроме \"SpaCy\", которая распознана как организация (ORG).\n",
        "\n",
        "**6. Работа с предложениями**\n",
        "SpaCy также умеет разбивать текст на предложения. Это полезно для анализа текста на уровне предложений, например, при генерации текста или выделении отдельных предложений из абзацев.\n",
        "\n",
        "Пример разбивки на предложения:"
      ],
      "metadata": {
        "id": "JvXJ-2IxWoG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбивка текста на предложения\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9jAi6h8W0yS",
        "outputId": "d1670578-47e0-4ec2-b2b3-1c1db743eaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world!\n",
            "Welcome to the world of SpaCy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Атрибут sents объекта doc содержит список всех предложений в тексте.\n",
        "Мы проходим по каждому предложению и выводим его.\n",
        "\n",
        "**7. Работа с несколькими языками**\n",
        "SpaCy поддерживает множество языков, включая русский, испанский, немецкий и другие. Для каждого языка доступны\n",
        "\n",
        "предобученные модели.\n",
        "\n",
        "Пример работы с русским языком:\n",
        "\n",
        "Сначала нужно загрузить русскую модель:\n",
        "\n"
      ],
      "metadata": {
        "id": "WxHWEfpSW3bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWlYP74TXF2Q",
        "outputId": "e277d7b4-8855-4092-9d4b-15090de6febe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "После загрузки модели можно использовать её для обработки текстов на русском языке."
      ],
      "metadata": {
        "id": "3nPlDwg-Xybe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка русской модели\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Пример русского текста\n",
        "doc = nlp(\"Привет, мир! Добро пожаловать в мир SpaCy.\")\n",
        "\n",
        "# Вывод токенов и частей речи\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, POS: {token.pos_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzhi2_GIXCb3",
        "outputId": "165c0a70-91f9-4421-ce87-fbb4d2368ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'ru_core_news_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Привет, POS: NOUN\n",
            "Token: ,, POS: PUNCT\n",
            "Token: мир, POS: NOUN\n",
            "Token: !, POS: PUNCT\n",
            "Token: Добро, POS: NOUN\n",
            "Token: пожаловать, POS: VERB\n",
            "Token: в, POS: ADP\n",
            "Token: мир, POS: NOUN\n",
            "Token: SpaCy, POS: PROPN\n",
            "Token: ., POS: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Углубленное изучение возможностей библиотеки SpaCy**\n",
        "\n",
        "**1. Кастомизация пайплайна SpaCy**\n",
        "Кастомизация пайплайна в SpaCy позволяет адаптировать стандартные компоненты обработки текста под специфические требования вашего проекта. Вы можете добавлять новые компоненты, изменять порядок существующих или полностью заменять их.\n",
        "\n",
        "Пример кастомизации пайплайна\n",
        "\n",
        "Предположим, что у вас есть текст, и вы хотите добавить свой компонент, который будет выделять определенные ключевые слова:"
      ],
      "metadata": {
        "id": "LuOh9KFFX8OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Загрузка модели\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Регистрация расширения для ключевых слов\n",
        "Doc.set_extension(\"keywords\", default=[])\n",
        "\n",
        "# Кастомный компонент для выделения ключевых слов\n",
        "@Language.component(\"keyword_extractor\")\n",
        "def keyword_extractor(doc):\n",
        "    keywords = []\n",
        "    for token in doc:\n",
        "        if token.text.lower() in [\"spacy\", \"nlp\", \"machine learning\"]:\n",
        "            keywords.append(token.text)\n",
        "    doc._.keywords = keywords  # Теперь это будет работать\n",
        "    return doc\n",
        "\n",
        "# Регистрация компонента в пайплайне\n",
        "nlp.add_pipe(\"keyword_extractor\", last=True)\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(\"SpaCy is an amazing library for NLP and machine learning.\")\n",
        "print(doc._.keywords)  # Вывод ключевых слов"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xrAT3pKYF7x",
        "outputId": "9493a0c8-b68a-48ff-97fd-6d77d8c54697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SpaCy', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Мы создаем новый компонент keyword_extractor, который добавляет в документ список ключевых слов.\n",
        "Ключевые слова добавляются в атрибут ._.keywords документа.\n",
        "\n",
        "Компонент регистрируется в пайплайне с помощью add_pipe(), а затем используется для обработки текста.\n",
        "\n",
        "**2. Обучение кастомных моделей NER**\n",
        "\n",
        "Создание кастомных моделей NER позволяет распознавать специфические сущности, которые могут быть не представлены в стандартных моделях SpaCy. Например, для медицинских текстов это могут быть названия лекарств.\n",
        "\n",
        "Пример обучения модели NER\n",
        "\n",
        "Подготовка данных: Данные должны быть размечены в формате, который SpaCy принимает для обучения моделей. Обычно это формат JSON."
      ],
      "metadata": {
        "id": "sNUyvBv0YrPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA = [\n",
        "    (\"Aspirin is a pain reliever.\", {\"entities\": [(0, 7, \"MEDICINE\")]}),\n",
        "    (\"Tylenol is also known as acetaminophen.\", {\"entities\": [(0, 7, \"MEDICINE\"), (30, 43, \"MEDICINE\")]}),\n",
        "]"
      ],
      "metadata": {
        "id": "q0dw0gjAYzyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение модели:\n"
      ],
      "metadata": {
        "id": "4M8ynmbDY1pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "\n",
        "# Загрузка базовой модели\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Добавление новой метки\n",
        "ner.add_label(\"MEDICINE\")\n",
        "\n",
        "# Обучение модели\n",
        "optimizer = nlp.begin_training()\n",
        "for epoch in range(30):\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "        nlp.update([example], drop=0.5, losses={})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Ga6hBQY1BP",
        "outputId": "31178417-f52f-4872-85bb-cc353eedec8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Tylenol is also known as acetaminophen.\" with entities \"[(0, 7, 'MEDICINE'), (30, 43, 'MEDICINE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Мы создаем тренировочные данные с размеченными сущностями и добавляем новую метку \"MEDICINE\".\n",
        "\n",
        "Обучение происходит через 30 эпох, где каждый текст обновляет модель.\n",
        "\n",
        "**3. Обработка текстов на разных языках**\n",
        "\n",
        "SpaCy поддерживает множество языков, и каждая языковая модель имеет свои особенности. Чтобы использовать SpaCy для обработки текстов на другом языке, просто загрузите соответствующую языковую модель.\n",
        "\n",
        "Пример работы с русским языком:\n",
        "\n",
        "python -m spacy download ru_core_news_sm\n",
        "\n",
        "Затем в коде:"
      ],
      "metadata": {
        "id": "ox3luYniY_H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка русской модели\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Пример текста на русском языке\n",
        "doc = nlp(\"Привет, мир! Добро пожаловать в мир SpaCy.\")\n",
        "\n",
        "# Вывод токенов и частей речи\n",
        "for token in doc:\n",
        "    print(f'Token: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qkDvYMVZKf4",
        "outputId": "6d47add4-e89f-4716-a239-b2d38ded29ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Привет, POS: NOUN, Lemma: привет\n",
            "Token: ,, POS: PUNCT, Lemma: ,\n",
            "Token: мир, POS: NOUN, Lemma: мир\n",
            "Token: !, POS: PUNCT, Lemma: !\n",
            "Token: Добро, POS: NOUN, Lemma: добро\n",
            "Token: пожаловать, POS: VERB, Lemma: пожаловать\n",
            "Token: в, POS: ADP, Lemma: в\n",
            "Token: мир, POS: NOUN, Lemma: мир\n",
            "Token: SpaCy, POS: PROPN, Lemma: spacy\n",
            "Token: ., POS: PUNCT, Lemma: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Использование SpaCy с библиотеками для машинного обучения**\n",
        "\n",
        "SpaCy может быть использован в сочетании с популярными библиотеками для машинного обучения, такими как Scikit-learn. Это позволяет подготавливать текстовые данные для обучения классификаторов и других моделей.\n",
        "\n",
        "Пример использования SpaCy с Scikit-learn:"
      ],
      "metadata": {
        "id": "IG7UoYboZRh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Данные для классификации\n",
        "data = [\"I love SpaCy!\", \"SpaCy is amazing.\", \"I hate bugs.\", \"Bugs are annoying.\"]\n",
        "labels = [\"positive\", \"positive\", \"negative\", \"negative\"]\n",
        "\n",
        "# Создание модели\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(data, labels)\n",
        "\n",
        "# Прогнозирование\n",
        "predicted = model.predict([\"I enjoy working with SpaCy.\"])\n",
        "print(predicted)  # Вывод: ['positive']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuA7PgDEZUkY",
        "outputId": "1fae983d-f161-4250-8ab8-ad1e02b763c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Мы используем CountVectorizer для преобразования текстов в числовые векторы.\n",
        "\n",
        "Затем применяется MultinomialNB для классификации.\n",
        "\n",
        "**5. Тональность текста и анализ сентимента**\n",
        "\n",
        "Хотя SpaCy не имеет встроенных моделей анализа сентимента, вы можете использовать её для предобработки текста перед применением других библиотек.\n",
        "\n",
        "Пример анализа сентимента с TextBlob:"
      ],
      "metadata": {
        "id": "ACOSYnOIZVhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Предобработка текста с помощью SpaCy\n",
        "doc = nlp(\"SpaCy is a great library for NLP!\")\n",
        "\n",
        "# Применение TextBlob для анализа сентимента\n",
        "blob = TextBlob(doc.text)\n",
        "print(blob.sentiment)  # Вывод: Sentiment(polarity=0.5, subjectivity=0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlyo5WSbZc_N",
        "outputId": "fc2f98eb-4f0a-44ef-a0e0-c7dad4b39b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=1.0, subjectivity=0.75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Кастомные токенизаторы и правила токенизации**\n",
        "\n",
        "Кастомные токенизаторы могут быть полезны, если стандартные правила токенизации не подходят для вашего текста. Например, вы можете создать токенизатор, который будет учитывать специфические знаки препинания или технические термины.\n",
        "\n",
        "Пример создания кастомного токенизатора:"
      ],
      "metadata": {
        "id": "n-B83-KiZfBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "# Создание кастомного токенизатора\n",
        "infixes = [r'(?<=[a-z])(?=[A-Z])']  # Пример для разбиения camelCase\n",
        "infix_regex = compile_infix_regex(infixes)\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab, infix_finditer=infix_regex.finditer)\n",
        "\n",
        "doc = tokenizer(\"ThisIsAnExample\")\n",
        "print([token.text for token in doc])  # ['This', 'Is', 'An', 'Example']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "427cXMv4Zgtp",
        "outputId": "16b912b7-d563-4377-e752-3ad745babdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'Is', 'An', 'Example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Семантическая обработка и сходство текстов**\n",
        "\n",
        "SpaCy может вычислять семантическое сходство между словами и предложениями с помощью word embeddings.\n",
        "\n",
        "Пример вычисления сходства:"
      ],
      "metadata": {
        "id": "FG6qlprqZk8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравнение сходства между токенами\n",
        "doc1 = nlp(\"I love programming.\")\n",
        "doc2 = nlp(\"I enjoy coding.\")\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(f'Similarity: {similarity}')  # Выводит значение от 0 до 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W7q_jA0ZkAw",
        "outputId": "c4fb19cd-69cf-42c6-d15d-24297f13e7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: 0.974075717051845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-77b3796bcb05>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Работа с большими объёмами данных**\n",
        "\n",
        "При работе с большими наборами данных важно оптимизировать использование памяти и производительность.\n",
        "\n",
        "Пример пакетной обработки:"
      ],
      "metadata": {
        "id": "WnxBKV8yZpxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]  # Массив больших данных\n",
        "docs = list(nlp.pipe(texts))  # Используем pipe для пакетной обработки"
      ],
      "metadata": {
        "id": "hknRwgCGZr7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Интеграция с Gensim для анализа тем**\n",
        "\n",
        "SpaCy может быть интегрирован с Gensim для выполнения тематического моделирования.\n",
        "\n",
        "Пример анализа тем с Gensim:"
      ],
      "metadata": {
        "id": "S9AzVcUMZzzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Подготовка данных\n",
        "texts = [[\"spaCy\", \"is\", \"great\"], [\"nlp\", \"with\", \"python\"], [\"machine\", \"learning\"]]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Модель LDA\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)"
      ],
      "metadata": {
        "id": "gVkAgnrdZzY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Обработка структуры текста**\n",
        "\n",
        "SpaCy может быть настроен для обработки структур текста, таких как абзацы и заголовки.\n",
        "\n",
        "Пример обработки структуры текста:"
      ],
      "metadata": {
        "id": "Fm7WhNZ2Z5vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение текста на абзацы\n",
        "text = \"This is the first paragraph.\\n\\nThis is the second paragraph.\"\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "for paragraph in paragraphs:\n",
        "    doc = nlp(paragraph)\n",
        "    print(f\"Paragraph: {paragraph}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS5QDTBbZ9vp",
        "outputId": "b7565118-32a3-43ff-924a-2324d9626aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph: This is the first paragraph.\n",
            "Paragraph: This is the second paragraph.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Кастомные модели на основе правила**\n",
        "\n",
        "Кастомные модели на основе правил могут быть использованы для решения простых задач, где не требуется машинное обучение.\n",
        "\n",
        "Пример создания модели на основе правил:"
      ],
      "metadata": {
        "id": "HHDrmr-9Z86n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Простой пример нахождения чисел в тексте\n",
        "text = \"The price is 100 dollars and 50 cents.\"\n",
        "doc = nlp(text)\n",
        "numbers = [token.text for token\n",
        "\n",
        " in doc if token.like_num]\n",
        "print(numbers)  # ['100', '50']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P4avQU_aFKM",
        "outputId": "99edf862-7e66-4d86-fcff-999bb59aaea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['100', '50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Морфологический анализ**\n",
        "\n",
        "SpaCy может выполнять морфологический анализ, что полезно для языков с богатой морфологией, таких как русский.\n",
        "\n",
        "Пример морфологического анализа:"
      ],
      "metadata": {
        "id": "WSspB8IUaICF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "doc = nlp(\"Мальчик играет в футбол.\")\n",
        "for token in doc:\n",
        "    print(f'Слово: {token.text}, Падеж: {token.morph.get(\"Case\")}, Число: {token.morph.get(\"Number\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZX2lkhzaMIz",
        "outputId": "62b4ebe5-4853-472a-9faa-55f52ba408ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слово: Мальчик, Падеж: ['Nom'], Число: ['Sing']\n",
            "Слово: играет, Падеж: [], Число: ['Sing']\n",
            "Слово: в, Падеж: [], Число: []\n",
            "Слово: футбол, Падеж: ['Acc'], Число: ['Sing']\n",
            "Слово: ., Падеж: [], Число: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Заключение**\n",
        "\n",
        "Библиотека SpaCy предлагает богатый функционал для обработки естественного языка. От кастомизации пайплайнов до интеграции с другими библиотеками и анализом семантики — возможности SpaCy практически безграничны. Каждая из рассмотренных тем открывает новые горизонты для обработки текстовых данных, что делает SpaCy мощным инструментом для разработчиков и исследователей в области NLP.\n"
      ],
      "metadata": {
        "id": "43gNfMvbazw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вопросы для самопроверки**\n",
        "\n",
        "1.Что такое SpaCy и в чем его основные преимущества по сравнению с другими библиотеками для обработки естественного языка?\n",
        "\n",
        "2.Как установить SpaCy и загрузить языковую модель?\n",
        "\n",
        "3.Что такое токенизация и как она реализована в SpaCy?\n",
        "\n",
        "4.Как выполнить лемматизацию текста с помощью SpaCy?\n",
        "\n",
        "5.В чем разница между частеречной разметкой (POS-tagging) и синтаксическим анализом?\n",
        "\n",
        "6.Как работает распознавание именованных сущностей (NER) в SpaCy и какие типы сущностей можно распознавать?\n",
        "\n",
        "7.Как разбить текст на предложения с помощью SpaCy?\n",
        "\n",
        "8.Как можно кастомизировать пайплайн SpaCy для добавления новых компонентов?\n",
        "\n",
        "9.Как обучить кастомную модель NER на основе размеченных данных?\n",
        "\n",
        "10.Как использовать nlp.pipe() для пакетной обработки текстов и какие преимущества это дает?\n",
        "\n",
        "11.Как можно извлечь числовые значения из текста с помощью SpaCy?\n",
        "\n",
        "12.В чем заключается морфологический анализ и как его можно выполнить в SpaCy?\n",
        "\n",
        "13.Как использовать SpaCy для анализа сентимента текста?\n",
        "\n",
        "14.Как реализовать кастомный токенизатор для специфических текстов?\n",
        "\n",
        "15.Как вычислить семантическое сходство между двумя текстами с помощью SpaCy?\n",
        "\n",
        "16.Как можно использовать SpaCy в сочетании с библиотеками машинного обучения, такими как Scikit-learn?\n",
        "\n",
        "17.Как визуализировать деревья зависимостей с помощью displacy?\n",
        "\n",
        "18.Какие существуют способы извлечения информации из текстов с помощью SpaCy?\n",
        "\n",
        "19.Как обрабатывать тексты на нескольких языках с использованием SpaCy?\n",
        "\n",
        "20.Какие методы оптимизации производительности можно использовать при работе с большими объемами данных в SpaCy?"
      ],
      "metadata": {
        "id": "gg1FvatjbAYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задачи для самостоятельной работы**\n",
        "1. Установите SpaCy и загрузите английскую языковую модель. Проверьте, что установка прошла успешно, обработав простой текст."
      ],
      "metadata": {
        "id": "Dn8dLIxnbUYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CDtxXhhrbY-J",
        "outputId": "c0e968fc-a9ed-4f68-dfbd-2493b58b76a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.7.1\n",
            "    Uninstalling en-core-web-sm-3.7.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.7.1\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Токенизация текста на английском языке"
      ],
      "metadata": {
        "id": "sYrgjZlRcAnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка модели\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Текст для токенизации\n",
        "text = \"SpaCy is an amazing library for NLP and machine learning.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Вывод токенов в виде списка\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5MZiaeScEEs",
        "outputId": "e56f50fe-e2cf-47ed-bb68-f2f4dfa9132e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SpaCy', 'is', 'an', 'amazing', 'library', 'for', 'NLP', 'and', 'machine', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Лемматизация набора предложений"
      ],
      "metadata": {
        "id": "TEzIv2tscI7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация\n",
        "lemmas = [(token.text, token.lemma_) for token in doc]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_x7hl49cKxt",
        "outputId": "c8ce8736-0a41-4c9a-d1b7-a0663b831a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('SpaCy', 'SpaCy'), ('is', 'be'), ('an', 'an'), ('amazing', 'amazing'), ('library', 'library'), ('for', 'for'), ('NLP', 'NLP'), ('and', 'and'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Частеречная разметка (POS-tagging)"
      ],
      "metadata": {
        "id": "LplPOnJ5cNGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0NU37p-cRge",
        "outputId": "0fa9afd5-5b60-4d9d-b220-126ebf192650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('SpaCy', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('library', 'NOUN'), ('for', 'ADP'), ('NLP', 'PROPN'), ('and', 'CCONJ'), ('machine', 'NOUN'), ('learning', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Синтаксический анализ и дерево зависимостей"
      ],
      "metadata": {
        "id": "o9GOASdhcT18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод дерева зависимостей\n",
        "for token in doc:\n",
        "    print(f'{token.text} --> {token.dep_} --> {token.head.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUvAW3SjcaP0",
        "outputId": "57754659-f846-43fa-e3df-568b32d09400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy --> nsubj --> is\n",
            "is --> ROOT --> is\n",
            "an --> det --> library\n",
            "amazing --> amod --> library\n",
            "library --> attr --> is\n",
            "for --> prep --> library\n",
            "NLP --> pobj --> for\n",
            "and --> cc --> NLP\n",
            "machine --> compound --> learning\n",
            "learning --> conj --> NLP\n",
            ". --> punct --> is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Распознавание именованных сущностей (NER)"
      ],
      "metadata": {
        "id": "A88tmV-6cdbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NER\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.text} ({ent.label_})')"
      ],
      "metadata": {
        "id": "wNfIOjNacesc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Разбиение текста на предложения"
      ],
      "metadata": {
        "id": "WUNOSKrTchwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиение на предложения\n",
        "sentences = list(doc.sents)\n",
        "for sentence in sentences:\n",
        "    print(sentence.text)"
      ],
      "metadata": {
        "id": "kTtLO86dcgQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Загрузка русской языковой модели и токенизация русского текста"
      ],
      "metadata": {
        "id": "Hh618lzZc3Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceTFKXO_c6sq",
        "outputId": "39d3b644-7ffa-4290-df8d-ffea6870840d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ru-core-news-sm==3.8.0) (2.0.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.7.2)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка русской модели\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Русский текст для токенизации\n",
        "text_ru = \"SpaCy — это отличная библиотека для обработки естественного языка.\"\n",
        "doc_ru = nlp_ru(text_ru)\n",
        "\n",
        "# Вывод токенов и их частей речи\n",
        "for token in doc_ru:\n",
        "    print(f'{token.text} ({token.pos_})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C54dpbXbc8qu",
        "outputId": "bcccb462-2469-41c9-9730-cfbb4ec6adc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy (PROPN)\n",
            "— (PUNCT)\n",
            "это (PART)\n",
            "отличная (ADJ)\n",
            "библиотека (NOUN)\n",
            "для (ADP)\n",
            "обработки (NOUN)\n",
            "естественного (ADJ)\n",
            "языка (NOUN)\n",
            ". (PUNCT)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Кастомный компонент для выделения ключевых слов"
      ],
      "metadata": {
        "id": "G3i082nZdCxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language\n",
        "@Language.component(\"keyword_extractor\")\n",
        "def keyword_extractor(doc):\n",
        "    keywords = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
        "    doc._.keywords = keywords\n",
        "    return doc\n",
        "\n",
        "# Регистрация компонента в пайплайне\n",
        "nlp.add_pipe(\"keyword_extractor\", last=True)\n",
        "\n",
        "# Обработка текста с выделением ключевых слов\n",
        "doc = nlp(\"SpaCy is an amazing library for NLP and machine learning.\")\n",
        "print(doc._.keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAVT20FZdGnX",
        "outputId": "5cdba14d-5535-41a1-f126-c3e32bc0987e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SpaCy', 'library', 'NLP', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Пакетная обработка текстов с использованием nlp.pipe()**\n"
      ],
      "metadata": {
        "id": "VytEL3l0dNdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Text one.\", \"Text two.\", \"Text three.\"]\n",
        "docs = list(nlp.pipe(texts))\n",
        "\n",
        "# Измерение времени выполнения (не забудьте импортировать time)\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "docs = list(nlp.pipe(texts))\n",
        "end_time = time.time()\n",
        "print(f\"Время выполнения: {end_time - start_time} секунд\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_uSUNAadXxd",
        "outputId": "a21c0a9f-26df-4a9e-9ff9-21eff3d88992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Время выполнения: 0.01977705955505371 секунд\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Анализ сентимента с использованием SpaCy и TextBlob (требует установки TextBlob)**"
      ],
      "metadata": {
        "id": "uvj2Jrhgdffs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y1T3N__diSe",
        "outputId": "42075643-c101-4e0e-ca57-b0c8d495bbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.1->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.1->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.1->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text_blob = TextBlob(\"SpaCy is an amazing library for NLP.\")\n",
        "print(text_blob.sentiment)  # Выводит полярность и субъективность"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaWiaxMWdlgz",
        "outputId": "cc311867-d02d-4405-d942-da12cbb57592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.6000000000000001, subjectivity=0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Извлечение чисел из текста"
      ],
      "metadata": {
        "id": "5g0CWlgAdo42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [token.text for token in doc if token.like_num]\n",
        "print(numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkpuxB6Wdqq4",
        "outputId": "a396f72e-e9d8-42e8-d101-19f6d9b1ce79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Кастомный токенизатор для специфических текстов (например, медицинских терминов)"
      ],
      "metadata": {
        "id": "R4FQQJe5dwXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    return nlp.tokenizer\n",
        "\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "\n",
        "text_medical = \"Aspirin is used to reduce fever and relieve pain.\"\n",
        "doc_medical = nlp(text_medical)\n",
        "tokens_medical = [token.text for token in doc_medical]\n",
        "print(tokens_medical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eVjWG39d0HF",
        "outputId": "3cbd651a-eff5-46b8-bd32-cf8e7d0c58e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aspirin', 'is', 'used', 'to', 'reduce', 'fever', 'and', 'relieve', 'pain', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Семантический анализ сходства между двумя предложениями"
      ],
      "metadata": {
        "id": "DBT7msGSdzX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I love natural language processing.\")\n",
        "doc2 = nlp(\"Natural language processing is fascinating.\")\n",
        "\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(f\"Сходство: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn60Pa3nd5BB",
        "outputId": "ad99a98c-a917-434c-d084-35da8d9aa83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сходство: 0.6726497349273787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-8e23c0d057a4>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = doc1.similarity(doc2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Анализ частоты слов в тексте (самостоятельно)\n",
        "17. Обработка текстов на нескольких языках\n",
        "18. Модель классификации текста с использованием Scikit-learn (требует больше кода и данных)\n",
        "19. Морфологический анализ для русского текста (используйте атрибуты токенов)\n",
        "20. Извлечение имен собственных из текста в формате JSON (используйте json библиотеку)\n",
        "21. Кастомные правила для выделения паттернов в тексте (например, регулярные выражения)\n",
        "22. Анализ структуры текста (разбиение на абзацы и предложения)\n",
        "23. Визуализация деревьев зависимостей с помощью displacy (требует установки displacy)\n",
        "24. Обучение кастомной модели для распознавания специфических сущностей\n",
        "25. Анализ текста на предмет наличия негативных и позитивных слов\n",
        "26. Система рекомендаций на основе анализа текстов\n",
        "27. Анализ тональности текстов и визуализация результатов\n",
        "28. Автоматизированный процесс извлечения информации из текстов\n",
        "29. Приложение для обработки вводимого текста\n",
        "30. Оптимизация производительности обработки текста"
      ],
      "metadata": {
        "id": "CxUyZtpgd8WJ"
      }
    }
  ]
}