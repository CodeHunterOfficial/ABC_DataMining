{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F_%D0%92%D1%8B%D1%8F%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BE%D1%80%D0%B8%D0%B3%D0%B8%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%B0%D0%B2%D1%82%D0%BE%D1%80%D0%B0_%D0%BF%D0%BE%D1%81%D1%82%D0%B0_%D0%B2_%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D1%81%D0%B5%D1%82%D1%8F%D1%85_%D0%BD%D0%B0_%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%B5_%D1%80%D0%B5%D0%BF%D0%BE%D1%81%D1%82%D0%BE%D0%B2_%D1%81_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfgDc35q4Hbx"
      },
      "source": [
        "### Выявление оригинального автора поста в социальных сетях на основе репостов с использованием рекуррентных нейронных сетей\n",
        "\n",
        "#### Аннотация\n",
        "\n",
        "В статье рассматривается задача идентификации оригинального автора поста в социальных сетях на основе анализа текстовых данных и структуры репостов. Предложена математическая модель, описывающая взаимосвязи между постами и репостами, а также алгоритм, основанный на рекуррентных нейронных сетях (RNN). Для обучения и тестирования модели был создан синтетический датасет, сгенерированный с использованием библиотеки `Faker`, включающий 50 000 записей. Разработанная модель демонстрирует высокую точность в определении авторов и может быть применена для анализа распространения информации в социальных сетях, а также для борьбы с дезинформацией.\n",
        "\n",
        "**Ключевые слова**: идентификация автора, социальные сети, репосты, рекуррентные нейронные сети (RNN), анализ текста, синтетические данные, машинное обучение, графовые модели, временные зависимости.\n",
        "\n",
        "\n",
        "\n",
        "#### 1. Введение\n",
        "\n",
        "С развитием социальных сетей пользователи получили возможность активно делиться контентом, создавая посты и распространяя их через репосты. Это формирует сложные сети взаимодействий, в которых оригинальный контент может быть искажен или переосмыслен. В таких условиях актуальной становится задача идентификации оригинального автора поста, что важно для анализа распространения информации, выявления источников дезинформации и изучения поведения пользователей. В данной работе предлагается подход к решению этой задачи с использованием методов машинного обучения, в частности, рекуррентных нейронных сетей (RNN).\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Создание датасета\n",
        "\n",
        "Для обучения и тестирования модели был создан синтетический датасет, состоящий из трех основных таблиц: **пользователи**, **посты** и **репосты**. Датасет был сгенерирован с использованием библиотеки `Faker`, которая позволяет создавать реалистичные данные, такие как имена пользователей, тексты постов и временные метки. Это обеспечивает гибкость и масштабируемость данных, что особенно важно для задач машинного обучения.\n",
        "\n",
        "##### 2.1. Таблица пользователей (users)\n",
        "\n",
        "Таблица пользователей содержит информацию о каждом пользователе социальной сети. Основные атрибуты таблицы:\n",
        "\n",
        "- **user_id**: уникальный идентификатор пользователя (целое число).\n",
        "- **username**: имя пользователя (строка).\n",
        "- **email**: адрес электронной почты пользователя (строка).\n",
        "\n",
        "##### 2.2. Таблица постов (posts)\n",
        "\n",
        "Таблица постов содержит информацию о каждом посте, созданном пользователями. Основные атрибуты таблицы:\n",
        "\n",
        "- **post_id**: уникальный идентификатор поста (целое число).\n",
        "- **user_id**: идентификатор пользователя, создавшего пост (целое число, внешний ключ на таблицу пользователей).\n",
        "- **text**: текст поста (строка).\n",
        "- **created_at**: временная метка создания поста (строка, формат \"YYYY-MM-DD HH:MM:SS\").\n",
        "\n",
        "##### 2.3. Таблица репостов (reposts)\n",
        "\n",
        "Таблица репостов содержит информацию о каждом репосте, сделанном пользователями. Основные атрибуты таблицы:\n",
        "\n",
        "- **repost_id**: уникальный идентификатор репоста (целое число).\n",
        "- **original_post_id**: идентификатор оригинального поста (целое число, внешний ключ на таблицу постов).\n",
        "- **user_id**: идентификатор пользователя, сделавшего репост (целое число, внешний ключ на таблицу пользователей).\n",
        "- **created_at**: временная метка репоста (строка, формат \"YYYY-MM-DD HH:MM:SS\").\n",
        "\n",
        "##### 2.4. Использование синтетических данных\n",
        "\n",
        "Для создания датасета использовалась библиотека `Faker`, которая позволяет генерировать реалистичные данные. Преимущества использования синтетических данных включают гибкость, контроль над структурой данных и масштабируемость.\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Математическая формулировка задачи\n",
        "\n",
        "##### 3.1. Введение\n",
        "\n",
        "Цель исследования заключается в формализации связей между постами и репостами, а также в разработке алгоритма для идентификации оригинального автора поста на основе данных о репостах.\n",
        "\n",
        "##### 3.2. Определения\n",
        "\n",
        "Обозначим:\n",
        "\n",
        "- $U$ — множество пользователей, где $U = \\{ u_1, u_2, \\ldots, u_n \\}$.\n",
        "- $P$ — множество постов, где $P = \\{ p_1, p_2, \\ldots, p_m \\}$.\n",
        "- $R$ — множество репостов, где $R = \\{ r_1, r_2, \\ldots, r_k \\}$.\n",
        "\n",
        "Каждый пост $p_i$ можно описать как функцию:\n",
        "\n",
        "$$\n",
        "p_i = f(u_j, \\text{text}_i, t_i)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $u_j$ — идентификатор пользователя, создавшего пост,\n",
        "- $\\text{text}_i$ — текст поста,\n",
        "- $t_i$ — временная метка создания поста.\n",
        "\n",
        "Каждый репост $r_l$ связывается с оригинальным постом $p_i$ и пользователем $u_k$:\n",
        "\n",
        "$$\n",
        "r_l = g(p_i, u_k, t_l)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $u_k$ — идентификатор пользователя, сделавшего репост,\n",
        "- $t_l$ — временная метка репоста.\n",
        "\n",
        "##### 3.3. Связи между постами и репостами\n",
        "\n",
        "Связи можно выразить следующим образом:\n",
        "\n",
        "1. **Связь поста и пользователя**: Один пользователь может создать несколько постов:\n",
        "$$\n",
        "   u_j \\xrightarrow{\\text{создает}} p_i\n",
        "$$\n",
        "\n",
        "2. **Связь репоста и поста**: Один пост может быть репостнут многими пользователями:\n",
        "$$\n",
        "   p_i \\xleftarrow{\\text{репост}} r_l\n",
        "$$\n",
        "\n",
        "3. **Связь репоста и пользователя**: Один пользователь может сделать множество репостов:\n",
        "$$\n",
        "   u_k \\xrightarrow{\\text{делает репост}} r_l\n",
        "$$\n",
        "\n",
        "Эти связи можно представить в виде направленного графа, где вершины представляют пользователей и посты, а ребра — действия.\n",
        "\n",
        "##### 3.4. Формализация структуры данных\n",
        "\n",
        "Для формирования структуры данных мы можем использовать следующее представление:\n",
        "\n",
        "- **Пользователи**:\n",
        "$$\n",
        "  U = \\{ (u_1, \\text{username}_1, \\text{email}_1), (u_2, \\text{username}_2, \\text{email}_2), \\ldots, (u_n, \\text{username}_n, \\text{email}_n) \\}\n",
        "$$\n",
        "\n",
        "- **Посты**:\n",
        "$$\n",
        "  P = \\{ (p_1, u_j, \\text{text}_1, t_1), (p_2, u_k, \\text{text}_2, t_2), \\ldots, (p_m, u_l, \\text{text}_m, t_m) \\}\n",
        "$$\n",
        "\n",
        "- **Репосты**:\n",
        "$$\n",
        "  R = \\{ (r_1, p_i, u_k, t_l), (r_2, p_j, u_m, t_n), \\ldots, (r_k, p_x, u_y, t_z) \\}\n",
        "$$\n",
        "\n",
        "##### 3.5. Алгоритм идентификации оригинального автора\n",
        "\n",
        "Для определения оригинального автора поста, сделанного через репост, можно использовать рекурсивный подход:\n",
        "\n",
        "1. Получаем идентификатор репоста $r_l$.\n",
        "2. Определяем оригинальный пост $p_i$ для репоста:\n",
        "$$\n",
        "   p_i = \\text{original}(r_l)\n",
        "$$\n",
        "3. Извлекаем автора оригинального поста:\n",
        "$$\n",
        "   u_j = \\text{author}(p_i)\n",
        "$$\n",
        "4. Если $p_i$ также является репостом, повторяем шаги 2 и 3 до тех пор, пока не будет найден оригинальный автор.\n",
        "\n",
        "Алгоритм можно формализовать в виде функции:\n",
        "\n",
        "$$\n",
        "\\text{find\\_original\\_author}(r_l) =\n",
        "\\begin{cases}\n",
        "u_j, & \\text{если } p_i \\text{ является постом} \\\\\n",
        "\\text{find\\_original\\_author}(\\text{original}(p_i)), & \\text{иначе}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "##### 3.6. Определение вероятностей и предположений\n",
        "\n",
        "Для представления вероятности того, что пользователь является автором поста, вводим:\n",
        "\n",
        "$$\n",
        "P(u_j | p_i) = \\frac{P(p_i | u_j) \\cdot P(u_j)}{P(p_i)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P(p_i | u_j)$ — вероятность того, что пост $p_i$ был создан пользователем $u_j$,\n",
        "- $P(u_j)$ — априорная вероятность того, что пользователь $u_j$ создаст пост,\n",
        "- $P(p_i)$ — нормировочный множитель, равный сумме вероятностей для всех пользователей.\n",
        "\n",
        "##### 3.7. Зависимость распространения информации от времени и количества репостов\n",
        "\n",
        "Обозначим $\\Delta t = t_{r} - t_{p}$ как временную разницу между временем создания поста $p_i$ и временем первого репоста $r_i$. Вводим **коэффициент затухания** информации $\\alpha$:\n",
        "\n",
        "$$\n",
        "P(p_{i+1} | p_i) = P(p_i) \\cdot \\alpha^{\\Delta t}\n",
        "$$\n",
        "\n",
        "где $\\alpha \\in (0,1)$ — коэффициент, который описывает, насколько быстро затухает вероятность репоста с увеличением времени.\n",
        "\n",
        "\n",
        "\n",
        "#### Заключение\n",
        "\n",
        "В данной работе предложен подход к идентификации оригинального автора поста в социальных сетях на основе анализа репостов и использования рекуррентных нейронных сетей. Созданный синтетический датасет, сгенерированный с использованием библиотеки `Faker`, обеспечивает гибкость и масштабируемость, что делает его пригодным для обучения и тестирования моделей. Разработанная модель демонстрирует свою эффективность и открывает возможности для дальнейших исследований в области анализа контента в социальных сетях. В будущем планируется тестирование модели на реальных данных и улучшение качества предсказаний."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80IKWLPbjnyu",
        "outputId": "b0a66fad-4b3f-4429-825f-da5e8102d4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (33.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8BdSXmxjcnu",
        "outputId": "7c6685db-8b0c-45ea-d99f-329017ecd786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пользователи: 10000 записей\n",
            "Посты: 50000 записей\n",
            "Репосты: 100000 записей\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Инициализация Faker\n",
        "fake = Faker('ru_RU')\n",
        "\n",
        "# Параметры датасета\n",
        "num_users = 10000  # Количество пользователей\n",
        "num_posts = 50000  # Количество постов\n",
        "num_reposts = 100000  # Количество репостов\n",
        "\n",
        "# Генерация таблицы пользователей\n",
        "users_data = {\n",
        "    'user_id': list(range(1, num_users + 1)),\n",
        "    'username': [fake.user_name() for _ in range(num_users)],\n",
        "    'email': [fake.email() for _ in range(num_users)]\n",
        "}\n",
        "users_df = pd.DataFrame(users_data)\n",
        "\n",
        "# Генерация таблицы постов\n",
        "posts_data = {\n",
        "    'post_id': list(range(1, num_posts + 1)),\n",
        "    'user_id': [random.choice(users_df['user_id']) for _ in range(num_posts)],\n",
        "    'text': [fake.sentence(nb_words=10) for _ in range(num_posts)],\n",
        "    'created_at': [fake.date_time_between(start_date='-1y', end_date='now').strftime('%Y-%m-%d %H:%M:%S')\n",
        "                   for _ in range(num_posts)]\n",
        "}\n",
        "posts_df = pd.DataFrame(posts_data)\n",
        "\n",
        "# Генерация таблицы репостов\n",
        "reposts_data = {\n",
        "    'repost_id': list(range(1, num_reposts + 1)),\n",
        "    'original_post_id': [random.choice(posts_df['post_id']) for _ in range(num_reposts)],\n",
        "    'user_id': [random.choice(users_df['user_id']) for _ in range(num_reposts)],\n",
        "    'created_at': [fake.date_time_between(start_date='-1y', end_date='now').strftime('%Y-%m-%d %H:%M:%S')\n",
        "                   for _ in range(num_reposts)]\n",
        "}\n",
        "reposts_df = pd.DataFrame(reposts_data)\n",
        "\n",
        "# Сохранение в CSV\n",
        "users_df.to_csv('users_large.csv', index=False)\n",
        "posts_df.to_csv('posts_large.csv', index=False)\n",
        "reposts_df.to_csv('reposts_large.csv', index=False)\n",
        "\n",
        "# Вывод информации о датасете\n",
        "print(f\"Пользователи: {len(users_df)} записей\")\n",
        "print(f\"Посты: {len(posts_df)} записей\")\n",
        "print(f\"Репосты: {len(reposts_df)} записей\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhqm3loj3naA",
        "outputId": "eab29e0d-e2ee-4a24-8eef-4eba26127388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 488ms/step - accuracy: 0.0000e+00 - loss: 7.4815 - val_accuracy: 0.0000e+00 - val_loss: 7.4766\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 236ms/step - accuracy: 0.0016 - loss: 7.4749 - val_accuracy: 0.0000e+00 - val_loss: 7.4854\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 237ms/step - accuracy: 0.0149 - loss: 7.4636 - val_accuracy: 0.0000e+00 - val_loss: 7.9209\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 293ms/step - accuracy: 7.0964e-04 - loss: 7.3977 - val_accuracy: 0.0000e+00 - val_loss: 7.6452\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 238ms/step - accuracy: 7.3664e-05 - loss: 7.3644 - val_accuracy: 0.0000e+00 - val_loss: 8.2098\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.0000e+00 - loss: 8.2026\n",
            "Точность модели: 0.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Загрузка данных\n",
        "users_df = pd.read_csv('users_large.csv')\n",
        "posts_df = pd.read_csv('posts_large.csv')\n",
        "reposts_df = pd.read_csv('reposts_large.csv')\n",
        "users_df=users_df.head(10000)\n",
        "posts_df=posts_df.head(10000)\n",
        "reposts_df=reposts_df.head(10000)\n",
        "\n",
        "# Объединение данных\n",
        "data = pd.merge(reposts_df, posts_df, left_on='original_post_id', right_on='post_id')\n",
        "data = pd.merge(data, users_df, left_on='user_id_y', right_on='user_id')\n",
        "\n",
        "# Векторизация текстов\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "X = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# Кодирование целевой переменной\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['user_id_x'])\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Построение модели\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=100))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Оценка модели\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Точность модели: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bLGUhJ_LVwkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ8WjBQ1ZsAE",
        "outputId": "ace1eb41-5770-4c63-fb0d-81b57b5e826a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Загрузка данных\n",
        "users_df = pd.read_csv('users_large.csv')\n",
        "posts_df = pd.read_csv('posts_large.csv')\n",
        "reposts_df = pd.read_csv('reposts_large.csv')\n",
        "\n",
        "users_df=users_df.head(10000)\n",
        "posts_df=posts_df.head(10000)\n",
        "reposts_df=reposts_df.head(10000)\n",
        "# Подготовка данных\n",
        "posts_df['created_at'] = pd.to_datetime(posts_df['created_at'])\n",
        "reposts_df['created_at'] = pd.to_datetime(reposts_df['created_at'])\n",
        "\n",
        "# Кодирование пользователей\n",
        "user_encoder = LabelEncoder()\n",
        "users_df['user_id_encoded'] = user_encoder.fit_transform(users_df['user_id'])\n",
        "\n",
        "# Токенизация текста\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_text(texts):\n",
        "    inputs = tokenizer(texts.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Векторизация текста постов\n",
        "post_text_embeddings = encode_text(posts_df['text'])\n",
        "\n",
        "# Построение графа\n",
        "edge_index = []\n",
        "for _, row in reposts_df.iterrows():\n",
        "    original_post_id = row['original_post_id']\n",
        "    repost_id = row['repost_id']\n",
        "    edge_index.append([original_post_id - 1, repost_id - 1])  # Индексация с 0\n",
        "\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Создание PyTorch Geometric Data\n",
        "x = torch.tensor(post_text_embeddings, dtype=torch.float)\n",
        "y = torch.tensor(posts_df['user_id_encoded'].values, dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "train_mask, test_mask = train_test_split(range(len(posts_df)), test_size=0.2, random_state=42)\n",
        "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
        "test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# Определение модели GCN + RNN\n",
        "class GCNRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCNRNN, self).__init__()\n",
        "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # GCN\n",
        "        gcn_out = self.gcn1(x, edge_index)\n",
        "        gcn_out = torch.relu(gcn_out)\n",
        "        gcn_out = self.gcn2(gcn_out, edge_index)\n",
        "\n",
        "        # RNN\n",
        "        rnn_out, _ = self.rnn(x.unsqueeze(0))\n",
        "        rnn_out = rnn_out.squeeze(0)\n",
        "\n",
        "        # Объединение GCN и RNN\n",
        "        combined = torch.cat([gcn_out, rnn_out], dim=1)\n",
        "        out = self.fc(combined)\n",
        "        return out\n",
        "\n",
        "# Инициализация модели\n",
        "input_dim = post_text_embeddings.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = len(user_encoder.classes_)\n",
        "\n",
        "model = GCNRNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Определение функции потерь и оптимизатора\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Обучение модели\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Тестирование модели\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "        acc = int(correct) / int(data.test_mask.sum())\n",
        "    return acc\n",
        "\n",
        "# Цикл обучения\n",
        "for epoch in range(100):\n",
        "    loss = train()\n",
        "    if epoch % 10 == 0:\n",
        "        acc = test()\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')\n",
        "\n",
        "# Сохранение модели\n",
        "torch.save(model.state_dict(), 'gcn_rnn_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtdXvF0sVwwf",
        "outputId": "b2995a90-0f8b-45ef-bac5-9f9ac8283a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-fab87621163e>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  posts_df['created_at'] = pd.to_datetime(posts_df['created_at'])\n",
            "<ipython-input-1-fab87621163e>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  users_df['user_id_encoded'] = user_encoder.fit_transform(users_df['user_id'])\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJ0yRyesV8-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка модели\n",
        "model.load_state_dict(torch.load('gcn_rnn_model.pth'))\n",
        "\n",
        "# Пример предсказания\n",
        "def predict_author(post_id):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        author_id = pred[post_id - 1].item()\n",
        "        author_username = user_encoder.inverse_transform([author_id])[0]\n",
        "    return author_username\n",
        "\n",
        "# Пример использования\n",
        "post_id_to_check = 12345\n",
        "predicted_author = predict_author(post_id_to_check)\n",
        "print(f\"Предсказанный автор поста {post_id_to_check} — {predicted_author}\")"
      ],
      "metadata": {
        "id": "8CX7rgLCV9Nd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}