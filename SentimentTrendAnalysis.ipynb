{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM7L/NJOVFV6Aq8qFNJK7Rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/SentimentTrendAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "5_7Othow1Dwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Исходные данные\n",
        "data = {\n",
        "    \"video_id\": [\n",
        "        \"vid1\", \"vid1\", \"vid2\", \"vid2\", \"vid3\",\n",
        "        \"vid4\", \"vid4\", \"vid5\", \"vid5\", \"vid6\",\n",
        "        \"vid7\", \"vid8\", \"vid9\", \"vid10\", \"vid10\"\n",
        "    ],\n",
        "    \"comment_id\": [\n",
        "        \"cid1\", \"cid2\", \"cid3\", \"cid4\", \"cid5\",\n",
        "        \"cid6\", \"cid7\", \"cid8\", \"cid9\", \"cid10\",\n",
        "        \"cid11\", \"cid12\", \"cid13\", \"cid14\", \"cid15\"\n",
        "    ],\n",
        "    \"text\": [\n",
        "        \"I love this video, it's amazing!\",\n",
        "        \"This is the worst video I've ever seen.\",\n",
        "        \"Nice content, keep it up!\",\n",
        "        \"I don't like this, it's boring.\",\n",
        "        \"Great job, very informative.\",\n",
        "        \"The video was okay, but could be better.\",\n",
        "        \"Absolutely fantastic! Can't get enough of this.\",\n",
        "        \"Not my cup of tea, too slow-paced.\",\n",
        "        \"Loved every second of it, please make more!\",\n",
        "        \"Terrible quality, not worth my time.\",\n",
        "        \"Incredible visuals and sound design!\",\n",
        "        \"Meh, nothing special about it.\",\n",
        "        \"Highly recommend this to everyone!\",\n",
        "        \"Boring and predictable, didn't enjoy it.\",\n",
        "        \"One of the best videos I've seen this year!\"\n",
        "    ],\n",
        "    \"author\": [\n",
        "        \"user1\", \"user2\", \"user3\", \"user4\", \"user5\",\n",
        "        \"user6\", \"user7\", \"user8\", \"user9\", \"user10\",\n",
        "        \"user11\", \"user12\", \"user13\", \"user14\", \"user15\"\n",
        "    ],\n",
        "    \"published_at\": [\n",
        "        \"2023-10-01T12:00:00Z\", \"2023-10-02T13:00:00Z\", \"2023-10-03T14:00:00Z\",\n",
        "        \"2023-10-04T15:00:00Z\", \"2023-10-05T16:00:00Z\", \"2023-10-06T17:00:00Z\",\n",
        "        \"2023-10-07T18:00:00Z\", \"2023-10-08T19:00:00Z\", \"2023-10-09T20:00:00Z\",\n",
        "        \"2023-10-10T21:00:00Z\", \"2023-10-11T22:00:00Z\", \"2023-10-12T23:00:00Z\",\n",
        "        \"2023-10-13T00:00:00Z\", \"2023-10-14T01:00:00Z\", \"2023-10-15T02:00:00Z\"\n",
        "    ],\n",
        "    \"like_count\": [10, 2, 15, 1, 20, 5, 30, 3, 25, 1, 18, 4, 35, 2, 40],\n",
        "    \"sentiment\": [\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\",\n",
        "        \"neutral\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"neutral\", \"positive\", \"negative\", \"positive\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Создаем исходный DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Функция для генерации новых данных\n",
        "def generate_new_data(original_df, target_size):\n",
        "    new_data = []\n",
        "    video_ids = original_df['video_id'].unique()\n",
        "    authors = original_df['author'].unique()\n",
        "    texts = original_df['text'].unique()\n",
        "    sentiments = original_df['sentiment'].unique()\n",
        "\n",
        "    while len(new_data) < target_size:\n",
        "        video_id = random.choice(video_ids)\n",
        "        comment_id = f\"cid{len(new_data) + 1}\"\n",
        "        text = random.choice(texts)\n",
        "        author = random.choice(authors)\n",
        "        published_at = (datetime.now() - timedelta(days=random.randint(0, 365))).isoformat() + \"Z\"\n",
        "        like_count = random.randint(0, 50)\n",
        "        sentiment = random.choice(sentiments)\n",
        "\n",
        "        new_data.append({\n",
        "            \"video_id\": video_id,\n",
        "            \"comment_id\": comment_id,\n",
        "            \"text\": text,\n",
        "            \"author\": author,\n",
        "            \"published_at\": published_at,\n",
        "            \"like_count\": like_count,\n",
        "            \"sentiment\": sentiment\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(new_data)\n",
        "\n",
        "# Генерация нового DataFrame с 200 строками\n",
        "new_df = generate_new_data(df, 200)\n",
        "\n",
        "# Сохранение в CSV\n",
        "new_df.to_csv('comments_dataset.csv', index=False)\n",
        "\n",
        "# Проверка первых 5 строк\n",
        "print(new_df.head())"
      ],
      "metadata": {
        "id": "iNHy93lt4s22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7ikeOZA034t"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, language='en', remove_html=True, remove_stopwords=True,\n",
        "                 lemmatize=True, stem=False, lowercase=True):\n",
        "        \"\"\"\n",
        "        Инициализация параметров предобработки текста.\n",
        "\n",
        "        :param language: Язык текста ('en' или 'ru').\n",
        "        :param remove_html: Удалять ли HTML-теги.\n",
        "        :param remove_stopwords: Удалять ли стоп-слова.\n",
        "        :param lemmatize: Применять ли лемматизацию.\n",
        "        :param stem: Применять ли стемминг.\n",
        "        :param lowercase: Приводить ли текст к нижнему регистру.\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.remove_html = remove_html\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.lemmatize = lemmatize\n",
        "        self.stem = stem\n",
        "        self.lowercase = lowercase\n",
        "        self.stop_words = set(stopwords.words('english' if language == 'en' else 'russian'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Применение предобработки к текстовым данным.\n",
        "\n",
        "        :param X: Список текстов для предобработки.\n",
        "        :return: Список предобработанных текстов.\n",
        "        \"\"\"\n",
        "        return [self._preprocess_text(text) for text in X]\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Предобработка одного текста.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :return: Предобработанный текст.\n",
        "        \"\"\"\n",
        "        # 1. Удаление HTML-тегов\n",
        "        if self.remove_html:\n",
        "            text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "        # 2. Приведение к нижнему регистру\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "\n",
        "        # 3. Удаление пунктуации и специальных символов\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # 4. Токенизация текста\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # 5. Удаление стоп-слов\n",
        "        if self.remove_stopwords:\n",
        "            tokens = [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "        # 6. Лемматизация\n",
        "        if self.lemmatize:\n",
        "            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        # 7. Стемминг\n",
        "        if self.stem:\n",
        "            tokens = [self.stemmer.stem(word) for word in tokens]\n",
        "\n",
        "        # Возвращаем обработанный текст\n",
        "        return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class UniversalTextVectorizer:\n",
        "    def __init__(self, vectorization_type='tf_idf', max_features=1000, language='en'):\n",
        "        self.vectorization_type = vectorization_type\n",
        "        self.max_features = max_features\n",
        "        self.language = language\n",
        "        self.vectorizer = None\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        if self.vectorization_type == 'one_hot':\n",
        "            self.vectorizer = CountVectorizer(max_features=self.max_features, binary=True)\n",
        "        elif self.vectorization_type == 'bow':\n",
        "            self.vectorizer = CountVectorizer(max_features=self.max_features)\n",
        "        elif self.vectorization_type == 'tf_idf':\n",
        "            self.vectorizer = TfidfVectorizer(max_features=self.max_features)\n",
        "        elif self.vectorization_type == 'word2vec':\n",
        "            self.vectorizer = self._train_word2vec(X)\n",
        "        elif self.vectorization_type == 'doc2vec':\n",
        "            self.vectorizer = self._train_doc2vec(X)\n",
        "        elif self.vectorization_type == 'fasttext_gensim':\n",
        "            self.vectorizer = self._train_fasttext(X)\n",
        "        elif self.vectorization_type == 'bert':\n",
        "            return self._transform_with_bert(X)\n",
        "        elif self.vectorization_type == 'multilingual_bert':\n",
        "            return self._transform_with_multilingual_bert(X)\n",
        "        elif self.vectorization_type == 'use':\n",
        "            raise NotImplementedError(\"Universal Sentence Encoder implementation is not provided in this version\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported vectorization type: {self.vectorization_type}\")\n",
        "\n",
        "        if self.vectorization_type in ['one_hot', 'bow', 'tf_idf']:\n",
        "            return self.vectorizer.fit_transform(X)\n",
        "        else:\n",
        "            # For Word2Vec, Doc2Vec, and FastText, compute document-level embeddings\n",
        "            return self.transform(X)\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not hasattr(self, 'vectorizer') or self.vectorizer is None:\n",
        "            raise ValueError(\"Vectorizer has not been fitted yet. Call fit_transform first.\")\n",
        "\n",
        "        if self.vectorization_type in ['one_hot', 'bow', 'tf_idf']:\n",
        "            return self.vectorizer.transform(X)\n",
        "        elif self.vectorization_type == 'word2vec':\n",
        "            return np.array([self._get_word2vec_vector(doc) for doc in X])\n",
        "        elif self.vectorization_type == 'doc2vec':\n",
        "            return np.array([self.vectorizer.infer_vector(doc.split()) for doc in X])\n",
        "        elif self.vectorization_type == 'fasttext_gensim':\n",
        "            return np.array([self._get_fasttext_vector(doc) for doc in X])\n",
        "        elif self.vectorization_type == 'bert':\n",
        "            return self._transform_with_bert(X)\n",
        "        elif self.vectorization_type == 'multilingual_bert':\n",
        "            return self._transform_with_multilingual_bert(X)\n",
        "        elif self.vectorization_type == 'use':\n",
        "            raise NotImplementedError(\"Universal Sentence Encoder implementation is not provided in this version\")\n",
        "\n",
        "    def _train_word2vec(self, X):\n",
        "        tokenized_X = [doc.split() for doc in X]\n",
        "        model = Word2Vec(sentences=tokenized_X, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "    def _get_word2vec_vector(self, doc):\n",
        "        \"\"\"\n",
        "        Compute the average Word2Vec vector for a document.\n",
        "        \"\"\"\n",
        "        tokens = doc.split()\n",
        "        vectors = [self.vectorizer.wv[token] for token in tokens if token in self.vectorizer.wv]\n",
        "        return np.mean(vectors, axis=0) if vectors else np.zeros(self.vectorizer.vector_size)\n",
        "\n",
        "    def _train_doc2vec(self, X):\n",
        "        tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(X)]\n",
        "        model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4, epochs=40)\n",
        "        return model\n",
        "\n",
        "    def _train_fasttext(self, X):\n",
        "        tokenized_X = [doc.split() for doc in X]\n",
        "        model = FastText(sentences=tokenized_X, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "        return model\n",
        "\n",
        "    def _get_fasttext_vector(self, doc):\n",
        "        \"\"\"\n",
        "        Compute the average FastText vector for a document.\n",
        "        \"\"\"\n",
        "        tokens = doc.split()\n",
        "        vectors = [self.vectorizer.wv[token] for token in tokens if token in self.vectorizer.wv]\n",
        "        return np.mean(vectors, axis=0) if vectors else np.zeros(self.vectorizer.vector_size)\n",
        "\n",
        "    def _transform_with_bert(self, X):\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        embeddings = []\n",
        "        for text in X:\n",
        "            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "            outputs = model(**inputs)\n",
        "            embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def _transform_with_multilingual_bert(self, X):\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        embeddings = []\n",
        "        for text in X:\n",
        "            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "            outputs = model(**inputs)\n",
        "            embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
        "        return np.vstack(embeddings)"
      ],
      "metadata": {
        "id": "KT2KZPgS04fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import time\n",
        "import os\n",
        "import base64\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from io import BytesIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        ")\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "class DLTextClassificationPipeline:\n",
        "    def __init__(self, X=None, y=None, datasetPath=None, label_column=None, text_column=None,\n",
        "                 vectorization_type='tf_idf', output_dir=\"dl_classification_models\",\n",
        "                 max_features=1000, max_sequence_length=200, language='en', sample_size=50,\n",
        "                 model_list=None, return_type='path'):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.datasetPath = datasetPath\n",
        "        self.label_column = label_column\n",
        "        self.text_column = text_column\n",
        "        self.vectorization_type = vectorization_type\n",
        "        self.output_dir = output_dir\n",
        "        self.max_features = max_features\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.language = language\n",
        "        self.sample_size = sample_size\n",
        "        self.return_type = return_type\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.model_paths = {}\n",
        "        self.graph_paths = {}\n",
        "\n",
        "        # Доступные модели глубокого обучения\n",
        "        self.all_models = {\n",
        "            \"MLP\": self.build_mlp,\n",
        "            \"CNN\": self.build_cnn,\n",
        "            \"RNN\": self.build_rnn,\n",
        "            \"LSTM\": self.build_lstm,\n",
        "            \"GRU\": self.build_gru,\n",
        "            \"BiRNN\": self.build_birnn,\n",
        "            \"BiLSTM\": self.build_bilstm,\n",
        "            \"CNN_LSTM\": self.build_cnn_lstm,\n",
        "            \"Transformer\": self.build_transformer\n",
        "        }\n",
        "        # Параметры моделей\n",
        "        self.model_params = {\n",
        "            \"MLP\": {\"embedding_dim\": 64, \"hidden_units\": [128, 64], \"dropout_rate\": 0.3},\n",
        "            \"CNN\": {\"embedding_dim\": 64, \"filters\": 64, \"kernel_size\": 5, \"pool_size\": 4},\n",
        "            \"RNN\": {\"embedding_dim\": 64, \"rnn_units\": 64, \"dropout_rate\": 0.2},\n",
        "            \"LSTM\": {\"embedding_dim\": 64, \"lstm_units\": 64, \"dropout_rate\": 0.2},\n",
        "            \"GRU\": {\"embedding_dim\": 64, \"gru_units\": 64, \"dropout_rate\": 0.2},\n",
        "            \"BiRNN\": {\"embedding_dim\": 64, \"rnn_units\": 64, \"dropout_rate\": 0.2},\n",
        "            \"BiLSTM\": {\"embedding_dim\": 64, \"lstm_units\": 64, \"dropout_rate\": 0.2},\n",
        "            \"CNN_LSTM\": {\n",
        "                \"embedding_dim\": 64,\n",
        "                \"filters\": 64,\n",
        "                \"kernel_size\": 5,\n",
        "                \"pool_size\": 4,\n",
        "                \"lstm_units\": 64\n",
        "            },\n",
        "            \"Transformer\": {\n",
        "                \"model_name\": \"bert-base-uncased\",\n",
        "                \"max_length\": self.max_sequence_length,\n",
        "                \"trainable\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Фильтрация моделей по списку\n",
        "        if model_list is None:\n",
        "            self.models = self.all_models\n",
        "        else:\n",
        "            available = set(self.all_models.keys())\n",
        "            for model in model_list:\n",
        "                if model not in available:\n",
        "                    raise ValueError(f\"Модель '{model}' недоступна. Доступные модели: {available}\")\n",
        "            self.models = {name: self.all_models[name] for name in model_list}\n",
        "\n",
        "        self.results = []\n",
        "        self.confusion_matrices = {}\n",
        "        self.roc_curves = {}\n",
        "        self.tokenizer = None\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        df = df.dropna(subset=[self.label_column])\n",
        "        df = df.drop_duplicates()\n",
        "        return df\n",
        "\n",
        "    def load_and_split_data(self):\n",
        "        if self.X is not None and self.y is not None:\n",
        "            print(\"Данные уже загружены через X и y.\")\n",
        "            data_loaded = True\n",
        "        elif self.datasetPath is None:\n",
        "            print(\"Загрузка данных из fetch_20newsgroups...\")\n",
        "            newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "            self.X = newsgroups.data\n",
        "            self.y = newsgroups.target\n",
        "            self.label_mapping = dict(enumerate(newsgroups.target_names))\n",
        "            print(f\"Маппинг меток: {self.label_mapping}\")\n",
        "            data_loaded = True\n",
        "        elif self.datasetPath is not None and self.label_column is not None:\n",
        "            if not os.path.exists(self.datasetPath):\n",
        "                raise FileNotFoundError(f\"Файл не найден: {self.datasetPath}\")\n",
        "            print(f\"Загрузка данных из {self.datasetPath}...\")\n",
        "            df = pd.read_csv(self.datasetPath)\n",
        "            print(df.head(5))\n",
        "            print(df.columns)\n",
        "            if self.label_column not in df.columns:\n",
        "                raise ValueError(f\"Целевой столбец '{self.label_column}' не найден в датасете.\")\n",
        "            if self.text_column is not None and self.text_column not in df.columns:\n",
        "                raise ValueError(f\"Текстовый столбец '{self.text_column}' не найден в датасете.\")\n",
        "\n",
        "            self.X = df[self.text_column].values if self.text_column else df.drop(columns=[self.label_column]).values.flatten()\n",
        "            self.y = df[self.label_column].values\n",
        "\n",
        "            self.label_encoder = LabelEncoder()\n",
        "            self.y = self.label_encoder.fit_transform(self.y)\n",
        "            self.label_mapping = dict(zip(range(len(self.label_encoder.classes_)), self.label_encoder.classes_))\n",
        "            print(f\"Маппинг меток: {self.label_mapping}\")\n",
        "            data_loaded = True\n",
        "        else:\n",
        "            raise ValueError(\"Необходимо передать либо X и y, либо datasetPath и label_column.\")\n",
        "\n",
        "        if data_loaded and self.sample_size is not None:\n",
        "            if not (0 < self.sample_size <= 100):\n",
        "                raise ValueError(\"Параметр sample_size должен быть в диапазоне от 0 до 100.\")\n",
        "            sample_fraction = self.sample_size / 100.0\n",
        "            sample_size = int(len(self.X) * sample_fraction)\n",
        "            self.X = self.X[:sample_size]\n",
        "            self.y = self.y[:sample_size]\n",
        "\n",
        "        # Проверяем количество уникальных классов\n",
        "        unique_classes = np.unique(self.y)\n",
        "        self.n_classes = len(unique_classes)\n",
        "        print(f\"Обнаружено классов: {self.n_classes} ({unique_classes})\")\n",
        "\n",
        "        # Стратифицированное разделение\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
        "        )\n",
        "\n",
        "        # Преобразуем в one-hot encoding\n",
        "        self.y_train_cat = tf.keras.utils.to_categorical(self.y_train, num_classes=self.n_classes)\n",
        "        self.y_test_cat = tf.keras.utils.to_categorical(self.y_test, num_classes=self.n_classes)\n",
        "\n",
        "    def train_model(self, name, model, X_train=None, y_train=None, epochs=10, batch_size=32):\n",
        "        start_time = time.time()\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            os.path.join(self.output_dir, f\"{name}_best.keras\"),\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        )\n",
        "\n",
        "        if name == \"Transformer\":\n",
        "            model, train_dataset, test_dataset = model\n",
        "            history = model.fit(\n",
        "                train_dataset.shuffle(1000).batch(batch_size),\n",
        "                epochs=epochs,\n",
        "                validation_data=test_dataset.batch(batch_size),\n",
        "                callbacks=[early_stopping, checkpoint],\n",
        "                verbose=1\n",
        "            )\n",
        "        else:\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[early_stopping, checkpoint],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        model_path = os.path.join(self.output_dir, f\"{name}_model.keras\")\n",
        "        model.save(model_path)\n",
        "        self.model_paths[name] = model_path\n",
        "\n",
        "        # Предсказания\n",
        "        if name == \"Transformer\":\n",
        "            y_pred = np.argmax(model.predict(test_dataset.batch(batch_size)), axis=1)\n",
        "            y_prob = model.predict(test_dataset.batch(batch_size))\n",
        "        else:\n",
        "            y_pred = np.argmax(model.predict(self.X_test_pad), axis=1)\n",
        "            y_prob = model.predict(self.X_test_pad)\n",
        "\n",
        "        # Метрики\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        precision = precision_score(self.y_test, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(self.y_test, y_pred, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(self.y_test, y_pred, average='weighted', zero_division=0)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        self.results.append({\n",
        "            \"Model\": name,\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1-Score\": f1,\n",
        "            \"Training Time (s)\": elapsed_time,\n",
        "            \"Epochs\": len(history.history['loss'])\n",
        "        })\n",
        "\n",
        "        # Confusion matrix\n",
        "        self.confusion_matrices[name] = confusion_matrix(self.y_test, y_pred)\n",
        "\n",
        "        # ROC curves (только для бинарной и мультиклассовой классификации)\n",
        "        if self.n_classes > 1:\n",
        "            fpr, tpr, roc_auc = {}, {}, {}\n",
        "            for i in range(y_prob.shape[1]):\n",
        "                try:\n",
        "                    if len(np.unique(self.y_test_cat[:, i])) > 1:  # Нужно как минимум 2 разных значения\n",
        "                        fpr[i], tpr[i], _ = roc_curve(self.y_test_cat[:, i], y_prob[:, i])\n",
        "                        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "                except Exception as e:\n",
        "                    print(f\"Не удалось вычислить ROC для класса {i}: {str(e)}\")\n",
        "            self.roc_curves[name] = (fpr, tpr, roc_auc)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        preprocessor = TextPreprocessor(\n",
        "            language=self.language,\n",
        "            remove_html=True,\n",
        "            remove_stopwords=True,\n",
        "            lemmatize=True,\n",
        "            stem=False,\n",
        "            lowercase=True\n",
        "        )\n",
        "\n",
        "        self.X_train = preprocessor.transform(self.X_train)\n",
        "        self.X_test = preprocessor.transform(self.X_test)\n",
        "\n",
        "    def vectorize_text(self):\n",
        "        if self.vectorization_type == \"transformer\":\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_params[\"Transformer\"][\"model_name\"])\n",
        "            return\n",
        "\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.max_features)\n",
        "        self.tokenizer.fit_on_texts(self.X_train)\n",
        "\n",
        "        self.X_train_seq = self.tokenizer.texts_to_sequences(self.X_train)\n",
        "        self.X_test_seq = self.tokenizer.texts_to_sequences(self.X_test)\n",
        "\n",
        "        self.X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(self.X_train_seq, maxlen=self.max_sequence_length)\n",
        "        self.X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(self.X_test_seq, maxlen=self.max_sequence_length)\n",
        "\n",
        "        # Сохраняем токенизатор\n",
        "        tokenizer_path = os.path.join(self.output_dir, \"tokenizer.pkl\")\n",
        "        joblib.dump(self.tokenizer, tokenizer_path)\n",
        "        self.model_paths['Tokenizer'] = tokenizer_path\n",
        "\n",
        "    def build_mlp(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"MLP\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.Dropout(self.model_params[\"MLP\"][\"dropout_rate\"]))\n",
        "        for units in self.model_params[\"MLP\"][\"hidden_units\"]:\n",
        "            model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "            model.add(tf.keras.layers.Dropout(self.model_params[\"MLP\"][\"dropout_rate\"]))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_cnn(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"CNN\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.Conv1D(self.model_params[\"CNN\"][\"filters\"],\n",
        "                      self.model_params[\"CNN\"][\"kernel_size\"],\n",
        "                      activation='relu'))\n",
        "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_rnn(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"RNN\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.SimpleRNN(self.model_params[\"RNN\"][\"rnn_units\"], dropout=self.model_params[\"RNN\"][\"dropout_rate\"]))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_lstm(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"LSTM\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.LSTM(self.model_params[\"LSTM\"][\"lstm_units\"], dropout=self.model_params[\"LSTM\"][\"dropout_rate\"]))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_gru(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"GRU\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.GRU(self.model_params[\"GRU\"][\"gru_units\"], dropout=self.model_params[\"GRU\"][\"dropout_rate\"]))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_birnn(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"BiRNN\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(self.model_params[\"BiRNN\"][\"rnn_units\"], dropout=self.model_params[\"BiRNN\"][\"dropout_rate\"])))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_bilstm(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"BiLSTM\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.model_params[\"BiLSTM\"][\"lstm_units\"], dropout=self.model_params[\"BiLSTM\"][\"dropout_rate\"])))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_cnn_lstm(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(self.max_features, self.model_params[\"CNN_LSTM\"][\"embedding_dim\"], input_length=self.max_sequence_length))\n",
        "        model.add(tf.keras.layers.Conv1D(self.model_params[\"CNN_LSTM\"][\"filters\"],\n",
        "                      self.model_params[\"CNN_LSTM\"][\"kernel_size\"],\n",
        "                      activation='relu'))\n",
        "        model.add(tf.keras.layers.LSTM(self.model_params[\"CNN_LSTM\"][\"lstm_units\"]))\n",
        "        model.add(tf.keras.layers.Dense(self.n_classes, activation='softmax'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_transformer(self):\n",
        "        # Загрузка предобученной модели и токенизатора\n",
        "        transformer_model = TFAutoModel.from_pretrained(self.model_params[\"Transformer\"][\"model_name\"])\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.model_params[\"Transformer\"][\"model_name\"])\n",
        "\n",
        "        # Токенизация текста\n",
        "        train_encodings = tokenizer(self.X_train.tolist(), truncation=True, padding=True, max_length=self.max_sequence_length)\n",
        "        test_encodings = tokenizer(self.X_test.tolist(), truncation=True, padding=True, max_length=self.max_sequence_length)\n",
        "\n",
        "        # Создание tf.data.Dataset\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(train_encodings),\n",
        "            self.y_train_cat\n",
        "        ))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            dict(test_encodings),\n",
        "            self.y_test_cat\n",
        "        ))\n",
        "\n",
        "        # Входные слои\n",
        "        input_ids = tf.keras.layers.Input(shape=(self.max_sequence_length,), dtype=tf.int32, name=\"input_ids\")\n",
        "        attention_mask = tf.keras.layers.Input(shape=(self.max_sequence_length,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "        # Получение выходов трансформера\n",
        "        transformer_model.trainable = self.model_params[\"Transformer\"][\"trainable\"]\n",
        "        sequence_output = transformer_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "        # Добавление классификатора поверх трансформера\n",
        "        cls_token = sequence_output[:, 0, :]\n",
        "        output = tf.keras.layers.Dense(self.n_classes, activation='softmax')(cls_token)\n",
        "\n",
        "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "        return model, train_dataset, test_dataset\n",
        "\n",
        "    def train_models(self, epochs=10, batch_size=32):\n",
        "        \"\"\"Этот метод остается БЕЗ ИЗМЕНЕНИЙ\"\"\"\n",
        "        for name, builder in self.models.items():\n",
        "            print(f\"\\nTraining {name} model...\")\n",
        "            if name == \"Transformer\":\n",
        "                model, train_dataset, test_dataset = builder()\n",
        "                history = self.train_model(name, (model, train_dataset, test_dataset),\n",
        "                          epochs=epochs, batch_size=batch_size)\n",
        "            else:\n",
        "                model = builder()\n",
        "                history = self.train_model(name, model, self.X_train_pad, self.y_train_cat,\n",
        "                          epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "            self.plot_training_history(name, history)\n",
        "\n",
        "    def plot_training_history(self, name, history):\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title(f'{name} Model Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'{name} Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if self.return_type == 'path':\n",
        "            graph_path = os.path.join(self.output_dir, f\"{name}_training_history.png\")\n",
        "            plt.savefig(graph_path)\n",
        "            self.graph_paths[f'{name}_history'] = graph_path\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def save_results(self):\n",
        "        self.results_df = pd.DataFrame(self.results)\n",
        "        results_path = os.path.join(self.output_dir, \"dl_model_results.csv\")\n",
        "        self.results_df.to_csv(results_path, index=False)\n",
        "        self.graph_paths['results_csv'] = results_path\n",
        "\n",
        "    def plot_confusion_matrices(self):\n",
        "        num_models = len(self.confusion_matrices)\n",
        "        fig, axes = plt.subplots(num_models, 1, figsize=(10, 5 * num_models))\n",
        "        for i, (name, conf_matrix) in enumerate(self.confusion_matrices.items()):\n",
        "            ax = axes[i] if num_models > 1 else axes\n",
        "            sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
        "            ax.set_title(f\"Confusion Matrix for {name}\")\n",
        "            ax.set_xlabel(\"Predicted Labels\")\n",
        "            ax.set_ylabel(\"True Labels\")\n",
        "        plt.tight_layout()\n",
        "        if self.return_type == 'path':\n",
        "            graph_path = os.path.join(self.output_dir, \"dl_confusion_matrices.png\")\n",
        "            plt.savefig(graph_path)\n",
        "            self.graph_paths['confusion_matrices'] = graph_path\n",
        "            plt.close(fig)\n",
        "            return graph_path\n",
        "        else:\n",
        "            return fig\n",
        "\n",
        "    def plot_roc_curves(self):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for name, (fpr, tpr, roc_auc) in self.roc_curves.items():\n",
        "            for i in range(self.n_classes):\n",
        "                plt.plot(fpr[i], tpr[i], label=f\"{name} (Class {i}, AUC={roc_auc[i]:.2f})\")\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curves for All Models\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        if self.return_type == 'path':\n",
        "            graph_path = os.path.join(self.output_dir, \"dl_roc_curves.png\")\n",
        "            plt.savefig(graph_path)\n",
        "            self.graph_paths['roc_curves'] = graph_path\n",
        "            plt.close()\n",
        "            return graph_path\n",
        "        else:\n",
        "            return plt.gcf()\n",
        "\n",
        "    def plot_metric_comparison(self, metric_name):\n",
        "        metric_df = self.results_df[[\"Model\", metric_name]].set_index(\"Model\")\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        metric_df.plot(kind='bar', figsize=(12, 6))\n",
        "        plt.title(f\"Comparison of Model {metric_name}\")\n",
        "        plt.ylabel(metric_name)\n",
        "        plt.xlabel(\"Model\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        if self.return_type == 'path':\n",
        "            graph_path = os.path.join(self.output_dir, f\"dl_{metric_name}_comparison.png\")\n",
        "            plt.savefig(graph_path)\n",
        "            self.graph_paths[f'{metric_name}_comparison'] = graph_path\n",
        "            plt.close()\n",
        "            return graph_path\n",
        "        else:\n",
        "            return plt.gcf()\n",
        "\n",
        "    def generate_html_report(self):\n",
        "        html_content = \"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html lang=\"en\">\n",
        "        <head>\n",
        "            <meta charset=\"UTF-8\">\n",
        "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "            <title>Deep Learning Text Classification Report</title>\n",
        "            <style>\n",
        "                body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "                h1, h2, h3, h4 { color: #333; }\n",
        "                table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }\n",
        "                th, td { border: 1px solid #ddd; padding: 8px; text-align: center; }\n",
        "                th { background-color: #f4f4f4; }\n",
        "                img { max-width: 100%; height: auto; margin-bottom: 10px; border: 1px solid #eee; }\n",
        "                .model-section {\n",
        "                    margin-bottom: 40px;\n",
        "                    padding: 15px;\n",
        "                    background: #f9f9f9;\n",
        "                    border-radius: 5px;\n",
        "                }\n",
        "                .best-model {\n",
        "                    background-color: #e9f7ef;\n",
        "                    padding: 20px;\n",
        "                    border-left: 5px solid #28a745;\n",
        "                    margin-bottom: 30px;\n",
        "                    border-radius: 5px;\n",
        "                }\n",
        "                .plot-grid {\n",
        "                    display: grid;\n",
        "                    grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));\n",
        "                    gap: 20px;\n",
        "                    margin-top: 20px;\n",
        "                }\n",
        "                .plot-container {\n",
        "                    padding: 10px;\n",
        "                    background: white;\n",
        "                    border-radius: 5px;\n",
        "                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
        "                }\n",
        "                @media (max-width: 600px) {\n",
        "                    .plot-grid { grid-template-columns: 1fr; }\n",
        "                }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h1>Deep Learning Text Classification Report</h1>\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Общая информация о данных\n",
        "        html_content += \"<div class='model-section'>\"\n",
        "        html_content += \"<h2>Dataset Information</h2>\"\n",
        "        html_content += f\"<p><strong>Total samples:</strong> {len(self.X)}</p>\"\n",
        "        html_content += f\"<p><strong>Number of classes:</strong> {self.n_classes}</p>\"\n",
        "        html_content += f\"<p><strong>Class distribution:</strong> {dict(zip(self.label_encoder.classes_, np.bincount(self.y)))}</p>\"\n",
        "        html_content += \"</div>\"\n",
        "\n",
        "        # 2. Таблица с метриками всех моделей\n",
        "        html_content += \"<div class='model-section'>\"\n",
        "        html_content += \"<h2>Model Performance Summary</h2>\"\n",
        "        html_content += self.results_df.to_html(index=False, classes=\"metrics-table\")\n",
        "        html_content += \"</div>\"\n",
        "\n",
        "        # 3. Детали по каждой модели\n",
        "        for model_name in self.models.keys():\n",
        "            html_content += f\"<div class='model-section'>\"\n",
        "            html_content += f\"<h3>{model_name} Model</h3>\"\n",
        "\n",
        "            # Метрики модели\n",
        "            model_metrics = self.results_df[self.results_df['Model'] == model_name]\n",
        "            html_content += model_metrics.to_html(index=False, classes=\"metrics-table\")\n",
        "\n",
        "            # Графики модели\n",
        "            html_content += \"<div class='plot-grid'>\"\n",
        "\n",
        "            # 1. График обучения\n",
        "            history_path = os.path.join(self.output_dir, f\"{model_name}_training_history.png\")\n",
        "            if os.path.exists(history_path):\n",
        "                with open(history_path, \"rb\") as f:\n",
        "                    history_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                html_content += f\"\"\"\n",
        "                <div class='plot-container'>\n",
        "                    <h4>Training History</h4>\n",
        "                    <img src=\"data:image/png;base64,{history_base64}\" alt=\"Training History\">\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "            # 2. Матрица ошибок\n",
        "            if model_name in self.confusion_matrices:\n",
        "                conf_matrix_path = os.path.join(self.output_dir, f\"{model_name}_confusion_matrix.png\")\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(self.confusion_matrices[model_name], annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "                plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "                plt.savefig(conf_matrix_path)\n",
        "                plt.close()\n",
        "                with open(conf_matrix_path, \"rb\") as f:\n",
        "                    conf_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                html_content += f\"\"\"\n",
        "                <div class='plot-container'>\n",
        "                    <h4>Confusion Matrix</h4>\n",
        "                    <img src=\"data:image/png;base64,{conf_base64}\" alt=\"Confusion Matrix\">\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "            # 3. ROC-кривые (если есть)\n",
        "            if hasattr(self, 'roc_curves') and model_name in self.roc_curves:\n",
        "                roc_path = os.path.join(self.output_dir, f\"{model_name}_roc_curve.png\")\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                fpr, tpr, roc_auc = self.roc_curves[model_name]\n",
        "                for i in fpr:\n",
        "                    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "                plt.plot([0, 1], [0, 1], 'k--')\n",
        "                plt.title(f'ROC Curves - {model_name}')\n",
        "                plt.legend()\n",
        "                plt.savefig(roc_path)\n",
        "                plt.close()\n",
        "                with open(roc_path, \"rb\") as f:\n",
        "                    roc_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                html_content += f\"\"\"\n",
        "                <div class='plot-container'>\n",
        "                    <h4>ROC Curves</h4>\n",
        "                    <img src=\"data:image/png;base64,{roc_base64}\" alt=\"ROC Curves\">\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "            html_content += \"</div></div>\"  # Закрываем plot-grid и model-section\n",
        "\n",
        "        # 4. Лучшая модель\n",
        "        best_model = self.results_df.loc[self.results_df['F1-Score'].idxmax()]\n",
        "        best_model_name = best_model['Model']\n",
        "        html_content += \"<div class='best-model'>\"\n",
        "        html_content += f\"<h2>🌟 Best Performing Model: {best_model_name} 🌟</h2>\"\n",
        "\n",
        "        # Таблица метрик лучшей модели\n",
        "        html_content += best_model.to_frame().to_html(header=False, classes=\"metrics-table\")\n",
        "\n",
        "        # Графики лучшей модели\n",
        "        html_content += \"<div class='plot-grid'>\"\n",
        "\n",
        "        # 1. История обучения\n",
        "        history_path = os.path.join(self.output_dir, f\"{best_model_name}_training_history.png\")\n",
        "        if os.path.exists(history_path):\n",
        "            with open(history_path, \"rb\") as f:\n",
        "                history_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "            html_content += f\"\"\"\n",
        "            <div class='plot-container'>\n",
        "                <h4>Training History</h4>\n",
        "                <img src=\"data:image/png;base64,{history_base64}\" alt=\"Training History\">\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        # 2. Матрица ошибок\n",
        "        if best_model_name in self.confusion_matrices:\n",
        "            conf_matrix_path = os.path.join(self.output_dir, f\"{best_model_name}_confusion_matrix.png\")\n",
        "            with open(conf_matrix_path, \"rb\") as f:\n",
        "                conf_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "            html_content += f\"\"\"\n",
        "            <div class='plot-container'>\n",
        "                <h4>Confusion Matrix</h4>\n",
        "                <img src=\"data:image/png;base64,{conf_base64}\" alt=\"Confusion Matrix\">\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        # 3. ROC-кривые\n",
        "        if hasattr(self, 'roc_curves') and best_model_name in self.roc_curves:\n",
        "            roc_path = os.path.join(self.output_dir, f\"{best_model_name}_roc_curve.png\")\n",
        "            with open(roc_path, \"rb\") as f:\n",
        "                roc_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "            html_content += f\"\"\"\n",
        "            <div class='plot-container'>\n",
        "                <h4>ROC Curves</h4>\n",
        "                <img src=\"data:image/png;base64,{roc_base64}\" alt=\"ROC Curves\">\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html_content += \"</div></div>\"  # Закрываем plot-grid и best-model\n",
        "\n",
        "        # Закрываем HTML\n",
        "        html_content += \"\"\"\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Сохраняем отчет\n",
        "        report_path = os.path.join(self.output_dir, \"classification_report.html\")\n",
        "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        print(f\"Report generated: {report_path}\")\n",
        "        return html_content\n",
        "\n",
        "    def run_pipeline(self, epochs=10, batch_size=32):\n",
        "        # Загрузка данных и разделение на обучающую и тестовую выборки\n",
        "        self.load_and_split_data()\n",
        "\n",
        "        # Предобработка текста\n",
        "        self.preprocess_text()\n",
        "\n",
        "        # Векторизация текста\n",
        "        self.vectorize_text()\n",
        "\n",
        "        # Обучение моделей\n",
        "        self.train_models(epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "        # Сохранение результатов\n",
        "        self.save_results()\n",
        "\n",
        "        # Построение графиков\n",
        "        self.plot_confusion_matrices()\n",
        "        self.plot_roc_curves()\n",
        "\n",
        "        # Графики сравнения метрик\n",
        "        for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Training Time (s)']:\n",
        "            self.plot_metric_comparison(metric)\n",
        "\n",
        "        # Генерация HTML-отчета\n",
        "        html_content = self.generate_html_report()\n",
        "\n",
        "        # Возвращаем результаты\n",
        "        return {\n",
        "            'results_df': self.results_df,\n",
        "            'model_paths': self.model_paths,\n",
        "            'tokenizer_path': self.model_paths.get('Tokenizer'),\n",
        "            'graph_paths': self.graph_paths,\n",
        "            'html_content': html_content\n",
        "        }"
      ],
      "metadata": {
        "id": "9R4RobDz1TWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ZoiDscb02dOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация пайплайна с данными\n",
        "pipeline = DLTextClassificationPipeline(\n",
        "    datasetPath=\"/content/comments_dataset.csv\",  # передается в родительский класс\n",
        "    text_column=\"text\",           # передается в родительский класс\n",
        "    label_column=\"sentiment\",     # передается в родительский класс\n",
        "    model_list=[\"LSTM\"],  # какие модели использовать\n",
        "    output_dir=\"results\"   # куда сохранять результаты\n",
        ")\n",
        "\n",
        "# Запуск пайплайна\n",
        "results = pipeline.run_pipeline(epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "pJSCYzw32EYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SocialMediaAnalysisPipeline(DLTextClassificationPipeline):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.temporal_analysis_results = {}\n",
        "        self.full_dataset = None\n",
        "\n",
        "    def load_and_split_data(self):\n",
        "        \"\"\"Загрузка данных с проверкой всех необходимых столбцов\"\"\"\n",
        "        try:\n",
        "            if self.datasetPath:\n",
        "                # Загрузка данных\n",
        "                self.full_dataset = pd.read_csv(self.datasetPath)\n",
        "\n",
        "                # Проверка наличия необходимых столбцов\n",
        "                required_columns = {self.text_column, self.label_column, 'published_at'}\n",
        "                if not required_columns.issubset(self.full_dataset.columns):\n",
        "                    missing = required_columns - set(self.full_dataset.columns)\n",
        "                    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "                # Преобразование дат\n",
        "                self.full_dataset['published_at'] = pd.to_datetime(\n",
        "                    self.full_dataset['published_at'],\n",
        "                    errors='coerce'\n",
        "                )\n",
        "                # Удаление строк с некорректными датами\n",
        "                self.full_dataset = self.full_dataset.dropna(subset=['published_at'])\n",
        "\n",
        "                # Извлечение текста и меток\n",
        "                self.X = self.full_dataset[self.text_column].values\n",
        "                self.y = self.full_dataset[self.label_column].values\n",
        "\n",
        "                # Кодирование меток\n",
        "                self.label_encoder = LabelEncoder()\n",
        "                self.y = self.label_encoder.fit_transform(self.y)\n",
        "                self.label_mapping = dict(zip(self.label_encoder.classes_,\n",
        "                                           range(len(self.label_encoder.classes_))))\n",
        "\n",
        "                # Разделение данных\n",
        "                self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "                    self.X, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
        "                )\n",
        "\n",
        "                # One-hot кодирование\n",
        "                self.n_classes = len(np.unique(self.y))\n",
        "                self.y_train_cat = tf.keras.utils.to_categorical(self.y_train, num_classes=self.n_classes)\n",
        "                self.y_test_cat = tf.keras.utils.to_categorical(self.y_test, num_classes=self.n_classes)\n",
        "            else:\n",
        "                super().load_and_split_data()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def analyze_temporal_trends(self):\n",
        "        \"\"\"Анализ временных тенденций с защитой от ошибок\"\"\"\n",
        "        try:\n",
        "            if self.full_dataset is None or 'published_at' not in self.full_dataset.columns:\n",
        "                print(\"Skipping temporal analysis - no datetime data available\")\n",
        "                return\n",
        "\n",
        "            df = self.full_dataset.copy()\n",
        "            df['sentiment'] = self.y\n",
        "\n",
        "            # Извлекаем компоненты даты\n",
        "            df['date'] = df['published_at'].dt.date\n",
        "            df['hour'] = df['published_at'].dt.hour\n",
        "            df['day_of_week'] = df['published_at'].dt.day_name()\n",
        "\n",
        "            # Анализ по разным периодам\n",
        "            self.temporal_analysis_results = {\n",
        "                'daily': df.groupby(['date', 'sentiment']).size().unstack(fill_value=0),\n",
        "                'hourly': df.groupby(['hour', 'sentiment']).size().unstack(fill_value=0),\n",
        "                'weekly': df.groupby(['day_of_week', 'sentiment']).size().unstack(fill_value=0)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in temporal analysis: {str(e)}\")\n",
        "\n",
        "    def plot_temporal_trends(self):\n",
        "        \"\"\"Визуализация временных тенденций\"\"\"\n",
        "        try:\n",
        "            if not self.temporal_analysis_results:\n",
        "                print(\"No temporal data to visualize\")\n",
        "                return\n",
        "\n",
        "            os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "            for trend_type, data in self.temporal_analysis_results.items():\n",
        "                plt.figure(figsize=(12, 6))\n",
        "\n",
        "                if trend_type == 'daily':\n",
        "                    data.plot(kind='line', title=f'{trend_type.capitalize()} Sentiment Trends')\n",
        "                else:\n",
        "                    if trend_type == 'weekly':\n",
        "                        week_order = ['Monday', 'Tuesday', 'Wednesday',\n",
        "                                    'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "                        data = data.loc[week_order]\n",
        "                    data.plot(kind='bar', stacked=True,\n",
        "                            title=f'{trend_type.capitalize()} Sentiment Distribution')\n",
        "\n",
        "                plt.ylabel('Number of Comments')\n",
        "                plt.tight_layout()\n",
        "                path = os.path.join(self.output_dir, f\"{trend_type}_trends.png\")\n",
        "                plt.savefig(path)\n",
        "                plt.close()\n",
        "                self.graph_paths[f\"{trend_type}_trends\"] = path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting temporal trends: {str(e)}\")\n",
        "\n",
        "    def save_model_results(self):\n",
        "        \"\"\"Сохранение результатов моделей\"\"\"\n",
        "        try:\n",
        "            if not hasattr(self, 'results'):\n",
        "                self.results = []\n",
        "\n",
        "            # Создаем DataFrame с результатами\n",
        "            self.results_df = pd.DataFrame(self.results)\n",
        "\n",
        "            # Сохраняем в файл\n",
        "            results_path = os.path.join(self.output_dir, \"model_results.csv\")\n",
        "            self.results_df.to_csv(results_path, index=False)\n",
        "            self.graph_paths['results_csv'] = results_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model results: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_full_report(self):\n",
        "        \"\"\"Генерация полного отчета со всеми разделами, включая лучшую модель\"\"\"\n",
        "        try:\n",
        "            # 1. Сохраняем результаты моделей\n",
        "            self.save_model_results()\n",
        "\n",
        "            # 2. Создаем базовую структуру отчета\n",
        "            html_content = \"\"\"\n",
        "            <!DOCTYPE html>\n",
        "            <html>\n",
        "            <head>\n",
        "                <title>Complete Social Media Analysis Report</title>\n",
        "                <style>\n",
        "                    body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "                    h1, h2, h3 { color: #333; }\n",
        "                    .section { margin-bottom: 30px; padding: 15px;\n",
        "                              background: #f9f9f9; border-radius: 5px; }\n",
        "                    .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));\n",
        "                            gap: 20px; margin-bottom: 20px; }\n",
        "                    .plot-container { background: white; padding: 10px;\n",
        "                                    border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n",
        "                    table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }\n",
        "                    th, td { border: 1px solid #ddd; padding: 8px; text-align: center; }\n",
        "                    th { background-color: #f2f2f2; }\n",
        "                    .best-model { background: #e8f5e9; padding: 20px;\n",
        "                                border-left: 5px solid #2e7d32; margin: 30px 0; }\n",
        "                    .model-metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n",
        "                                  gap: 20px; }\n",
        "                </style>\n",
        "            </head>\n",
        "            <body>\n",
        "                <h1>Complete Social Media Analysis Report</h1>\n",
        "            \"\"\"\n",
        "\n",
        "            # 3. Добавляем информацию о датасете\n",
        "            html_content += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Dataset Information</h2>\n",
        "                <p><strong>Total samples:</strong> \"\"\" + str(len(self.X)) + \"\"\"</p>\n",
        "                <p><strong>Number of classes:</strong> \"\"\" + str(self.n_classes) + \"\"\"</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            # 4. Добавляем результаты классификации\n",
        "            if hasattr(self, 'results_df'):\n",
        "                html_content += \"\"\"\n",
        "                <div class=\"section\">\n",
        "                    <h2>Classification Results</h2>\n",
        "                    \"\"\" + self.results_df.to_html(index=False, classes=\"metrics-table\") + \"\"\"\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "            # 5. Добавляем графики обучения\n",
        "            html_content += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Model Training History</h2>\n",
        "                <div class=\"grid\">\n",
        "            \"\"\"\n",
        "            for model_name in self.models.keys():\n",
        "                history_path = os.path.join(self.output_dir, f\"{model_name}_training_history.png\")\n",
        "                if os.path.exists(history_path):\n",
        "                    with open(history_path, \"rb\") as f:\n",
        "                        img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                    html_content += f\"\"\"\n",
        "                    <div class=\"plot-container\">\n",
        "                        <h3>{model_name}</h3>\n",
        "                        <img src=\"data:image/png;base64,{img}\" style=\"width:100%\">\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "            html_content += \"</div></div>\"\n",
        "\n",
        "            # 6. Добавляем временные тенденции (если есть)\n",
        "            if any(f\"{t}_trends\" in self.graph_paths for t in ['daily', 'hourly', 'weekly']):\n",
        "                html_content += \"\"\"\n",
        "                <div class=\"section\">\n",
        "                    <h2>Temporal Sentiment Analysis</h2>\n",
        "                    <div class=\"grid\">\n",
        "                \"\"\"\n",
        "                for trend_type in ['daily', 'hourly', 'weekly']:\n",
        "                    if f\"{trend_type}_trends\" in self.graph_paths:\n",
        "                        with open(self.graph_paths[f\"{trend_type}_trends\"], \"rb\") as f:\n",
        "                            img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                        html_content += f\"\"\"\n",
        "                        <div class=\"plot-container\">\n",
        "                            <h3>{trend_type.capitalize()} Trends</h3>\n",
        "                            <img src=\"data:image/png;base64,{img}\" style=\"width:100%\">\n",
        "                        </div>\n",
        "                        \"\"\"\n",
        "                html_content += \"</div></div>\"\n",
        "\n",
        "            # 7. Добавляем раздел лучшей модели\n",
        "            if hasattr(self, 'results_df') and not self.results_df.empty:\n",
        "                best_model = self.results_df.loc[self.results_df['F1-Score'].idxmax()]\n",
        "                best_model_name = best_model['Model']\n",
        "\n",
        "                html_content += f\"\"\"\n",
        "                <div class=\"best-model\">\n",
        "                    <h2>Best Performing Model: {best_model_name}</h2>\n",
        "                    <div class=\"model-metrics\">\n",
        "                        <div>\n",
        "                            <h3>Metrics</h3>\n",
        "                            <table>\n",
        "                                <tr><th>F1-Score</th><td>{best_model['F1-Score']:.4f}</td></tr>\n",
        "                                <tr><th>Accuracy</th><td>{best_model['Accuracy']:.4f}</td></tr>\n",
        "                                <tr><th>Precision</th><td>{best_model['Precision']:.4f}</td></tr>\n",
        "                                <tr><th>Recall</th><td>{best_model['Recall']:.4f}</td></tr>\n",
        "                                <tr><th>Training Time</th><td>{best_model['Training Time (s)']:.2f} sec</td></tr>\n",
        "                            </table>\n",
        "                        </div>\n",
        "                \"\"\"\n",
        "\n",
        "                # Графики обучения лучшей модели\n",
        "                history_path = os.path.join(self.output_dir, f\"{best_model_name}_training_history.png\")\n",
        "                if os.path.exists(history_path):\n",
        "                    with open(history_path, \"rb\") as f:\n",
        "                        img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                    html_content += f\"\"\"\n",
        "                    <div class=\"plot-container\">\n",
        "                        <h3>Training History</h3>\n",
        "                        <img src=\"data:image/png;base64,{img}\" style=\"width:100%\">\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "\n",
        "                # Матрица ошибок лучшей модели\n",
        "                conf_matrix_path = os.path.join(self.output_dir, f\"{best_model_name}_confusion.png\")\n",
        "                if os.path.exists(conf_matrix_path):\n",
        "                    with open(conf_matrix_path, \"rb\") as f:\n",
        "                        img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                    html_content += f\"\"\"\n",
        "                    <div class=\"plot-container\">\n",
        "                        <h3>Confusion Matrix</h3>\n",
        "                        <img src=\"data:image/png;base64,{img}\" style=\"width:100%\">\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "\n",
        "                html_content += \"</div></div>\"\n",
        "\n",
        "            # 8. Закрываем HTML\n",
        "            html_content += \"</body></html>\"\n",
        "\n",
        "            # 9. Сохраняем отчет\n",
        "            report_path = os.path.join(self.output_dir, \"full_report.html\")\n",
        "            with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(html_content)\n",
        "\n",
        "            print(f\"Report successfully generated: {report_path}\")\n",
        "            return html_content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating report: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self, epochs=10, batch_size=32):\n",
        "        \"\"\"Полный защищенный цикл анализа\"\"\"\n",
        "        try:\n",
        "            # 1. Загрузка данных\n",
        "            self.load_and_split_data()\n",
        "\n",
        "            # 2. Предобработка текста\n",
        "            self.preprocess_text()\n",
        "            self.vectorize_text()\n",
        "\n",
        "            # 3. Обучение моделей\n",
        "            self.train_models(epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "            # 4. Анализ временных тенденций\n",
        "            self.analyze_temporal_trends()\n",
        "\n",
        "            # 5. Визуализация результатов\n",
        "            self.plot_temporal_trends()\n",
        "\n",
        "            # 6. Генерация отчета\n",
        "            return self.generate_full_report()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Pipeline failed: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "oqID3fko1BJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация пайплайна\n",
        "pipeline = SocialMediaAnalysisPipeline(\n",
        "    datasetPath=\"/content/comments_dataset.csv\",\n",
        "    text_column=\"text\",\n",
        "    label_column=\"sentiment\",\n",
        "    model_list=['CNN','RNN', 'LSTM','GRU','BiRNN']\n",
        ")\n",
        "#        ['MLP', 'CNN','RNN', 'LSTM','GRU','BiRNN','BiLSTM', 'CNN_LSTM',  'Transformer']\n",
        "# Запуск полного анализа\n",
        "results = pipeline.run_pipeline(epochs=10)"
      ],
      "metadata": {
        "id": "MPemxICVCf4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}