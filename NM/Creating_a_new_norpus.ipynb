{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPafvitFTU8SItIMJTecz3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NM/Creating_a_new_norpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx langdetect PyPDF2 bs4 langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE78RxT-OlKe",
        "outputId": "90aaa3f1-9a75-4045-bbe5-6884cdf45a5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/981.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=67750f7e7e15f90ee9614da2919114ac295819f9c181e0db6f9ec5d4e659fe26\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: python-docx, PyPDF2, langdetect, bs4\n",
            "Successfully installed PyPDF2-3.0.1 bs4-0.0.2 langdetect-1.0.9 python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyaR3s-WMJ72",
        "outputId": "1d43494b-7f96-4384-ee07-6938527db4f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите путь к папке с книгами: /content/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Файл .config пропущен из-за неподдерживаемого формата.\n",
            "WARNING:root:Файл sample_data пропущен из-за неподдерживаемого формата.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from docx import Document\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "import nltk  # Для разделения на предложения\n",
        "from langdetect import detect  # Для определения языка\n",
        "import json  # Для сохранения в JSON\n",
        "import xml.etree.ElementTree as ET  # Для сохранения в XML\n",
        "from PyPDF2 import PdfReader  # Для чтения PDF\n",
        "from bs4 import BeautifulSoup  # Для парсинга HTML\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "\n",
        "class BookCorpusProcessor:\n",
        "    def __init__(self, books_folder: str, output_base: str = \"tajik_books\"):\n",
        "        \"\"\"\n",
        "        Инициализация класса.\n",
        "\n",
        "        :param books_folder: Путь к папке с книгами.\n",
        "        :param output_base: Базовое имя выходных файлов (без расширений).\n",
        "        \"\"\"\n",
        "        self.books_folder = books_folder\n",
        "        self.output_base = output_base\n",
        "        self.processed_books: List[str] = []\n",
        "\n",
        "    def clean_text(self, text: str, custom_patterns: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Очищает текст от лишних символов и номеров страниц.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :param custom_patterns: Список пользовательских регулярных выражений для очистки текста.\n",
        "        :return: Очищенный текст.\n",
        "        \"\"\"\n",
        "        patterns = [\n",
        "            r\"^\\s*\\d+\\s*$\",  # Удалить номера страниц\n",
        "            r\"[^\\w\\s\\.,!?;:()«»“”'\\\"\\\\/-]\",  # Удалить специальные символы\n",
        "            r\"\\s+\",  # Заменить множественные пробелы на один пробел\n",
        "            r\"^\\s*$\",  # Удалить пустые строки\n",
        "            r\"\\t+\",  # Удалить табуляции\n",
        "            r\"\\.{2,}\",  # Заменить многоточия на точку\n",
        "            r\"<[^>]*>\",  # Удалить HTML-теги\n",
        "        ]\n",
        "\n",
        "        if custom_patterns:\n",
        "            patterns.extend(custom_patterns)\n",
        "\n",
        "        for pattern in patterns:\n",
        "            text = re.sub(pattern, \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_metadata(self, filename: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Извлекает метаданные (название и автора) из имени файла.\n",
        "\n",
        "        :param filename: Имя файла.\n",
        "        :return: Словарь с метаданными.\n",
        "        \"\"\"\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        parts = base_name.split(\"_\", 1)\n",
        "\n",
        "        if len(parts) == 2:\n",
        "            title, author = parts\n",
        "            return {\"title\": title.strip(), \"author\": author.strip()}\n",
        "        else:\n",
        "            return {\"title\": base_name.strip(), \"author\": \"Неизвестный\"}\n",
        "\n",
        "    def process_docx_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .docx файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            doc = Document(file_path)\n",
        "            paragraphs = [paragraph.text for paragraph in doc.paragraphs]\n",
        "            raw_text = \"\\n\".join(paragraphs)\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_txt_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .txt файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                raw_text = file.read()\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_pdf_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .pdf файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            reader = PdfReader(file_path)\n",
        "            raw_text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_html_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .html файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                soup = BeautifulSoup(file.read(), 'html.parser')\n",
        "\n",
        "            # Удаляем скрипты и стили\n",
        "            for script_or_style in soup([\"script\", \"style\"]):\n",
        "                script_or_style.decompose()\n",
        "\n",
        "            raw_text = soup.get_text(separator=\"\\n\")\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_all_books(self):\n",
        "        \"\"\"\n",
        "        Обрабатывает все файлы в указанной папке и сохраняет результат.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.books_folder):\n",
        "            logging.error(\"Указанный путь не существует.\")\n",
        "            return\n",
        "\n",
        "        all_books = []\n",
        "        for filename in os.listdir(self.books_folder):\n",
        "            file_path = os.path.join(self.books_folder, filename)\n",
        "\n",
        "            if filename.endswith(\".docx\"):\n",
        "                processed_text = self.process_docx_file(file_path)\n",
        "            elif filename.endswith(\".txt\"):\n",
        "                processed_text = self.process_txt_file(file_path)\n",
        "            elif filename.endswith(\".pdf\"):\n",
        "                processed_text = self.process_pdf_file(file_path)\n",
        "            elif filename.endswith(\".html\"):\n",
        "                processed_text = self.process_html_file(file_path)\n",
        "            else:\n",
        "                logging.warning(f\"Файл {filename} пропущен из-за неподдерживаемого формата.\")\n",
        "                continue\n",
        "\n",
        "            if processed_text:\n",
        "                all_books.append(processed_text)\n",
        "                self.processed_books.append(filename)\n",
        "                logging.info(f\"Обработан файл: {filename}\")\n",
        "            else:\n",
        "                logging.warning(f\"Файл {filename} пропущен из-за ошибки.\")\n",
        "\n",
        "        # Сохранение в TXT\n",
        "        with open(f\"{self.output_base}.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            txt_file.write(\"\\n\".join(all_books))\n",
        "\n",
        "        # Сохранение в JSON\n",
        "        json_data = []\n",
        "        for book in all_books:\n",
        "            title, rest = book.split(\"\\n\", 1)\n",
        "            author, text = rest.split(\"# -----\\n\", 1)\n",
        "            json_data.append({\n",
        "                \"title\": title.split(\":\")[1].strip(),\n",
        "                \"author\": author.split(\":\")[1].strip(),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "        with open(f\"{self.output_base}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Сохранение в XML\n",
        "        root = ET.Element(\"books\")\n",
        "        for book in all_books:\n",
        "            title, rest = book.split(\"\\n\", 1)\n",
        "            author, text = rest.split(\"# -----\\n\", 1)\n",
        "            book_elem = ET.SubElement(root, \"book\")\n",
        "            ET.SubElement(book_elem, \"title\").text = title.split(\":\")[1].strip()\n",
        "            ET.SubElement(book_elem, \"author\").text = author.split(\":\")[1].strip()\n",
        "            ET.SubElement(book_elem, \"text\").text = text.strip()\n",
        "\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(f\"{self.output_base}.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "        logging.info(f\"Все книги успешно объединены в {self.output_base}.txt, {self.output_base}.json и {self.output_base}.xml.\")\n",
        "        logging.info(f\"Обработано книг: {len(self.processed_books)}.\")\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Разделяет текст на предложения.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :return: Текст, разделенный на предложения.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            sentences = nltk.sent_tokenize(text, language=\"russian\")\n",
        "            return \"\\n\".join(sentences)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Ошибка при разделении текста на предложения: {e}\")\n",
        "            return text\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Определяет язык текста.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :return: Язык текста (например, 'ru', 'tg', 'en').\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return detect(text[:1000])  # Проверяем первые 1000 символов\n",
        "        except:\n",
        "            return \"unknown\"\n",
        "\n",
        "\n",
        "# Пример использования класса\n",
        "if __name__ == \"__main__\":\n",
        "    # Путь к папке с книгами\n",
        "    books_folder = input(\"Введите путь к папке с книгами: \").strip()\n",
        "\n",
        "    # Создание экземпляра класса\n",
        "    processor = BookCorpusProcessor(books_folder, output_base=\"tajik_books\")\n",
        "\n",
        "    # Обработка всех книг\n",
        "    processor.process_all_books()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class CleanCorpusProcessor:\n",
        "    def __init__(self, input_file: str, output_base: str = \"clean_corpus\"):\n",
        "        \"\"\"\n",
        "        Инициализация класса.\n",
        "        :param input_file: Путь к файлу (JSON, TXT или XML), созданному BookCorpusProcessor.\n",
        "        :param output_base: Базовое имя выходных файлов.\n",
        "        \"\"\"\n",
        "        self.input_file = input_file\n",
        "        self.output_base = output_base\n",
        "        self.stop_words = set(stopwords.words('russian') + stopwords.words('english'))\n",
        "        self.spell_correction_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.processed_books: List[Dict] = []\n",
        "\n",
        "    def load_data(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из файла (JSON, TXT или XML).\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.input_file):\n",
        "            logging.error(f\"Файл {self.input_file} не найден.\")\n",
        "            return []\n",
        "\n",
        "        _, ext = os.path.splitext(self.input_file)\n",
        "        if ext == \".json\":\n",
        "            return self.load_from_json()\n",
        "        elif ext == \".txt\":\n",
        "            return self.load_from_txt()\n",
        "        elif ext == \".xml\":\n",
        "            return self.load_from_xml()\n",
        "        else:\n",
        "            logging.error(f\"Неподдерживаемый формат файла: {ext}\")\n",
        "            return []\n",
        "\n",
        "    def load_from_json(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из JSON-файла.\n",
        "        \"\"\"\n",
        "        with open(self.input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "\n",
        "    def load_from_txt(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из TXT-файла.\n",
        "        \"\"\"\n",
        "        books = []\n",
        "        with open(self.input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read().strip().split(\"\\n\\n\")\n",
        "        for book in content:\n",
        "            try:\n",
        "                title, rest = book.split(\"\\n\", 1)\n",
        "                author, text = rest.split(\"# -----\\n\", 1)\n",
        "                books.append({\n",
        "                    \"title\": title.split(\":\")[1].strip(),\n",
        "                    \"author\": author.split(\":\")[1].strip(),\n",
        "                    \"text\": text.strip()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Ошибка при чтении книги из TXT: {e}\")\n",
        "        return books\n",
        "\n",
        "    def load_from_xml(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из XML-файла.\n",
        "        \"\"\"\n",
        "        books = []\n",
        "        tree = ET.parse(self.input_file)\n",
        "        root = tree.getroot()\n",
        "        for book_elem in root.findall(\"book\"):\n",
        "            title = book_elem.find(\"title\").text.strip() if book_elem.find(\"title\") is not None else \"Unknown Title\"\n",
        "            author = book_elem.find(\"author\").text.strip() if book_elem.find(\"author\") is not None else \"Unknown Author\"\n",
        "            text = book_elem.find(\"text\").text.strip() if book_elem.find(\"text\") is not None else \"\"\n",
        "            books.append({\"title\": title, \"author\": author, \"text\": text})\n",
        "        return books\n",
        "\n",
        "    def correct_spelling(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Исправляет опечатки в тексте.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "            corrected_sentences = [self.spell_correction_model(sentence)[0]['generated_text'] for sentence in sentences]\n",
        "            return \" \".join(corrected_sentences)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Ошибка при исправлении опечаток: {e}\")\n",
        "            return text\n",
        "\n",
        "    def lemmatize_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Лемматизирует текст.\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return \" \".join(lemmatized_tokens)\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Удаляет стоп-слова из текста.\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return \" \".join(filtered_tokens)\n",
        "\n",
        "    def process_book(self, book: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Обрабатывает одну книгу: исправляет опечатки, лемматизирует и удаляет стоп-слова.\n",
        "        \"\"\"\n",
        "        title = book.get(\"title\", \"Unknown Title\")\n",
        "        author = book.get(\"author\", \"Unknown Author\")\n",
        "        raw_text = book.get(\"text\", \"\")\n",
        "\n",
        "        # Исправление опечаток\n",
        "        corrected_text = self.correct_spelling(raw_text)\n",
        "\n",
        "        # Лемматизация\n",
        "        lemmatized_text = self.lemmatize_text(corrected_text)\n",
        "\n",
        "        # Удаление стоп-слов\n",
        "        clean_text = self.remove_stopwords(lemmatized_text)\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"author\": author,\n",
        "            \"language\": detect(raw_text[:1000]) if raw_text else \"unknown\",\n",
        "            \"text\": clean_text\n",
        "        }\n",
        "\n",
        "    def process_all_books(self):\n",
        "        \"\"\"\n",
        "        Обрабатывает все книги из входного файла и сохраняет результат.\n",
        "        \"\"\"\n",
        "        books_data = self.load_data()\n",
        "        if not books_data:\n",
        "            logging.error(\"Нет данных для обработки.\")\n",
        "            return\n",
        "\n",
        "        for book in books_data:\n",
        "            processed_book = self.process_book(book)\n",
        "            if processed_book:\n",
        "                self.processed_books.append(processed_book)\n",
        "                logging.info(f\"Обработана книга: {processed_book['title']}\")\n",
        "\n",
        "        # Сохранение в JSON\n",
        "        with open(f\"{self.output_base}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(self.processed_books, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Сохранение в XML\n",
        "        root = ET.Element(\"books\")\n",
        "        for book in self.processed_books:\n",
        "            book_elem = ET.SubElement(root, \"book\")\n",
        "            ET.SubElement(book_elem, \"title\").text = book[\"title\"]\n",
        "            ET.SubElement(book_elem, \"author\").text = book[\"author\"]\n",
        "            ET.SubElement(book_elem, \"language\").text = book[\"language\"]\n",
        "            ET.SubElement(book_elem, \"text\").text = book[\"text\"]\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(f\"{self.output_base}.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "        logging.info(f\"Все книги успешно обработаны и сохранены в {self.output_base}.json и {self.output_base}.xml.\")\n",
        "        logging.info(f\"Обработано книг: {len(self.processed_books)}.\")\n",
        "\n",
        "# Пример использования класса\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = input(\"Введите путь к файлу (JSON, TXT или XML): \").strip()\n",
        "    processor = CleanCorpusProcessor(input_file, output_base=\"clean_corpus\")\n",
        "    processor.process_all_books()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZMcNzm4tiO4",
        "outputId": "cbe7010b-402c-4eb1-f6c3-d7da6a1220ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите путь к файлу (JSON, TXT или XML): /content/tajik_books.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube tweepy readability-lxml lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp57PsAka3c2",
        "outputId": "25df0fb2-06cd-4247-c219-144abcaf034d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.14.0)\n",
            "Requirement already satisfied: readability-lxml in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (5.3.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.12.14)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from readability import Document  # Для извлечения основного текста\n",
        "from pytube import YouTube  # Для работы с YouTube\n",
        "import tweepy  # Для работы с Twitter\n",
        "import xml.etree.ElementTree as ET  # Для создания XML\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "class NewsCorpusProcessor:\n",
        "    def __init__(self, output_base: str = \"news_corpus\"):\n",
        "        \"\"\"\n",
        "        Инициализация класса.\n",
        "\n",
        "        :param output_base: Базовое имя выходных файлов (без расширений).\n",
        "        \"\"\"\n",
        "        self.output_base = output_base\n",
        "        self.processed_items: List[Dict] = []\n",
        "\n",
        "    def clean_text(self, text: str, custom_patterns: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Очищает текст от HTML-тегов и лишних символов.\n",
        "        :param text: Исходный текст.\n",
        "        :param custom_patterns: Список пользовательских регулярных выражений для очистки текста.\n",
        "        :return: Очищенный текст.\n",
        "        \"\"\"\n",
        "        # Удаление HTML-тегов с помощью BeautifulSoup\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "        plain_text = soup.get_text(separator=\" \")\n",
        "\n",
        "        # Определение базовых шаблонов для очистки текста\n",
        "        patterns = [\n",
        "            r\"^\\s*\\d+\\s*$\",  # Удалить номера страниц\n",
        "            r\"[^\\w\\s\\.,!?;:()«»“”'\\\"\\\\/-]\",  # Удалить специальные символы\n",
        "            r\"\\s+\",  # Заменить множественные пробелы на один пробел\n",
        "            r\"^\\s*$\",  # Удалить пустые строки\n",
        "            r\"\\t+\",  # Удалить табуляции\n",
        "            r\"\\.{2,}\",  # Заменить многоточия на точку\n",
        "        ]\n",
        "        if custom_patterns:\n",
        "            patterns.extend(custom_patterns)\n",
        "\n",
        "        # Применение шаблонов для очистки текста\n",
        "        for pattern in patterns:\n",
        "            plain_text = re.sub(pattern, \" \", plain_text, flags=re.MULTILINE)\n",
        "\n",
        "        return plain_text.strip()\n",
        "\n",
        "    def clean_content(self, data: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Очищает содержимое словаря с ключами 'title', 'author' и 'content'.\n",
        "        :param data: Словарь с данными.\n",
        "        :return: Очищенный словарь.\n",
        "        \"\"\"\n",
        "        cleaned_data = {\n",
        "            \"title\": self.clean_text(data.get(\"title\", \"\")),\n",
        "            \"author\": self.clean_text(data.get(\"author\", \"\")),\n",
        "            \"content\": self.clean_text(data.get(\"content\", \"\"))\n",
        "        }\n",
        "        return cleaned_data\n",
        "\n",
        "    def extract_web_content(self, url: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Извлекает заголовок, автора и основной текстовый контент из веб-страницы.\n",
        "        :param url: URL веб-страницы.\n",
        "        :return: Словарь с заголовком, автором и текстом.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Парсим HTML с помощью BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Извлекаем заголовок\n",
        "            title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "\n",
        "            # Извлекаем автора (если указан)\n",
        "            author_tag = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
        "            author = author_tag[\"content\"].strip() if author_tag else \"Unknown Author\"\n",
        "\n",
        "            # Используем Document для извлечения основного текста\n",
        "            doc = Document(response.text)\n",
        "            raw_text = doc.summary()  # Получаем основной текст статьи\n",
        "\n",
        "            # Создаем словарь с сырыми данными\n",
        "            raw_data = {\n",
        "                \"title\": title,\n",
        "                \"author\": author,\n",
        "                \"content\": raw_text\n",
        "            }\n",
        "\n",
        "            # Чистим содержимое\n",
        "            cleaned_data = self.clean_content(raw_data)\n",
        "\n",
        "            return cleaned_data\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при извлечении контента из {url}: {e}\")\n",
        "            return {\"title\": \"Error\", \"author\": \"Unknown\", \"content\": \"\"}\n",
        "\n",
        "    def extract_youtube_transcript(self, video_url: str) -> str:\n",
        "        \"\"\"\n",
        "        Извлекает транскрипт видео с YouTube.\n",
        "\n",
        "        :param video_url: URL видео на YouTube.\n",
        "        :return: Транскрипт видео.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            yt = YouTube(video_url)\n",
        "            transcript = yt.captions.get_by_language_code('ru')  # Получаем русский транскрипт\n",
        "            if transcript:\n",
        "                raw_text = transcript.generate_srt_captions()\n",
        "                cleaned_text = self.clean_text(raw_text, custom_patterns=[r\"\\d+\\n\\d+:\\d+:\\d+,?\\d* --> \\d+:\\d+:\\d+,?\\d*\"])\n",
        "                return cleaned_text\n",
        "            else:\n",
        "                logging.warning(f\"Транскрипт для {video_url} не найден.\")\n",
        "                return \"\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при извлечении транскрипта из {video_url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_twitter_posts(self, username: str, count: int = 10) -> List[str]:\n",
        "        \"\"\"\n",
        "        Извлекает последние твиты пользователя.\n",
        "\n",
        "        :param username: Имя пользователя Twitter.\n",
        "        :param count: Количество твитов для извлечения.\n",
        "        :return: Список твитов.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            client = tweepy.Client(bearer_token=\"YOUR_TWITTER_BEARER_TOKEN\")\n",
        "            user = client.get_user(username=username)\n",
        "            tweets = client.get_users_tweets(id=user.data.id, max_results=count, tweet_fields=[\"text\"])\n",
        "            cleaned_tweets = [self.clean_text(tweet['text']) for tweet in tweets.data]\n",
        "            return cleaned_tweets\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при извлечении твитов пользователя {username}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_meta_posts(self, page_id: str, access_token: str, count: int = 10) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Извлекает последние посты со страницы Facebook или Instagram через Meta Graph API.\n",
        "\n",
        "        :param page_id: ID страницы Facebook или Instagram.\n",
        "        :param access_token: Токен доступа Meta.\n",
        "        :param count: Количество постов для извлечения.\n",
        "        :return: Список постов.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = f\"https://graph.facebook.com/{page_id}/posts?fields=message&limit={count}&access_token={access_token}\"\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get(\"data\", [])\n",
        "            posts = [{\"message\": post.get(\"message\", \"\")} for post in data]\n",
        "            cleaned_posts = [{\"message\": self.clean_text(post[\"message\"])} for post in posts]\n",
        "            return cleaned_posts\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при извлечении постов из Meta для страницы {page_id}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_vk_posts(self, group_id: str, count: int = 10) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Извлекает последние посты из группы ВКонтакте через VK API.\n",
        "\n",
        "        :param group_id: ID группы ВКонтакте.\n",
        "        :param count: Количество постов для извлечения.\n",
        "        :return: Список постов.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            vk_api_url = \"https://api.vk.com/method/wall.get\"\n",
        "            params = {\n",
        "                \"owner_id\": f\"-{group_id}\",  # Минус перед ID для групп\n",
        "                \"count\": count,\n",
        "                \"access_token\": \"YOUR_VK_ACCESS_TOKEN\",\n",
        "                \"v\": \"5.131\"\n",
        "            }\n",
        "            response = requests.get(vk_api_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get(\"response\", {}).get(\"items\", [])\n",
        "            posts = [{\"text\": post.get(\"text\", \"\")} for post in data]\n",
        "            cleaned_posts = [{\"text\": self.clean_text(post[\"text\"])} for post in posts]\n",
        "            return cleaned_posts\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при извлечении постов из ВКонтакте для группы {group_id}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_all_sources(self, sources: List[Dict]):\n",
        "        \"\"\"\n",
        "        Обрабатывает все источники данных (сайты, YouTube, Twitter, Meta, VK).\n",
        "\n",
        "        :param sources: Список источников данных.\n",
        "        \"\"\"\n",
        "        all_data = []\n",
        "        for source in sources:\n",
        "            if source[\"type\"] == \"web\":\n",
        "                content = self.extract_web_content(source[\"url\"])\n",
        "                if content:\n",
        "                    all_data.append({\"source\": \"web\", \"url\": source[\"url\"], \"content\": content})\n",
        "            elif source[\"type\"] == \"youtube\":\n",
        "                transcript = self.extract_youtube_transcript(source[\"url\"])\n",
        "                if transcript:\n",
        "                    all_data.append({\"source\": \"youtube\", \"url\": source[\"url\"], \"content\": transcript})\n",
        "            elif source[\"type\"] == \"twitter\":\n",
        "                tweets = self.extract_twitter_posts(source[\"username\"], count=source.get(\"count\", 10))\n",
        "                if tweets:\n",
        "                    all_data.extend([{\"source\": \"twitter\", \"username\": source[\"username\"], \"content\": tweet} for tweet in tweets])\n",
        "            elif source[\"type\"] == \"meta\":\n",
        "                posts = self.extract_meta_posts(\n",
        "                    page_id=source[\"page_id\"],\n",
        "                    access_token=source[\"access_token\"],\n",
        "                    count=source.get(\"count\", 10)\n",
        "                )\n",
        "                if posts:\n",
        "                    all_data.extend([{\"source\": \"meta\", \"page_id\": source[\"page_id\"], \"content\": post[\"message\"]} for post in posts])\n",
        "            elif source[\"type\"] == \"vk\":\n",
        "                posts = self.extract_vk_posts(group_id=source[\"group_id\"], count=source.get(\"count\", 10))\n",
        "                if posts:\n",
        "                    all_data.extend([{\"source\": \"vk\", \"group_id\": source[\"group_id\"], \"content\": post[\"text\"]} for post in posts])\n",
        "\n",
        "        # Сохранение в JSON\n",
        "        self.save_to_json(all_data)\n",
        "\n",
        "        # Сохранение в TXT\n",
        "        self.save_to_txt(all_data)\n",
        "\n",
        "        # Сохранение в XML\n",
        "        self.save_to_xml(all_data)\n",
        "\n",
        "        logging.info(f\"Все данные успешно сохранены в {self.output_base}.json, {self.output_base}.txt и {self.output_base}.xml.\")\n",
        "        logging.info(f\"Обработано источников: {len(all_data)}.\")\n",
        "\n",
        "    def save_to_json(self, data: List[Dict]):\n",
        "        \"\"\"\n",
        "        Сохраняет данные в JSON-файл.\n",
        "\n",
        "        :param data: Список словарей с данными.\n",
        "        \"\"\"\n",
        "        with open(f\"{self.output_base}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def save_to_txt(self, data: List[Dict]):\n",
        "        with open(f\"{self.output_base}.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            for item in data:\n",
        "                txt_file.write(f\"Title: {item.get('title', 'N/A')}\\n\")\n",
        "                txt_file.write(f\"Author: {item.get('author', 'N/A')}\\n\")\n",
        "                txt_file.write(f\"Content: {item.get('content', '')}\\n\\n\")\n",
        "\n",
        "\n",
        "    def save_to_xml(self, data: List[Dict], default_title=\"N/A\", default_author=\"N/A\", default_content=\"\"):\n",
        "        # Validate input\n",
        "        if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n",
        "            raise ValueError(\"Invalid input: 'data' must be a list of dictionaries.\")\n",
        "\n",
        "        if not hasattr(self, \"output_base\") or not self.output_base:\n",
        "            raise ValueError(\"Output base filename is not set.\")\n",
        "\n",
        "        # Initialize XML structure\n",
        "        root = ET.Element(\"news_corpus\")\n",
        "        for item in data:\n",
        "            entry = ET.SubElement(root, \"entry\")\n",
        "\n",
        "            # Handle title, author, and content\n",
        "            ET.SubElement(entry, \"title\").text = str(item.get(\"title\", default_title))\n",
        "\n",
        "            if isinstance(item.get(\"author\"), dict):  # Handle nested author dictionary\n",
        "                author_elem = ET.SubElement(entry, \"author\")\n",
        "                for key, value in item[\"author\"].items():\n",
        "                    ET.SubElement(author_elem, key).text = str(value)\n",
        "            else:\n",
        "                ET.SubElement(entry, \"author\").text = str(item.get(\"author\", default_author))\n",
        "\n",
        "            ET.SubElement(entry, \"content\").text = str(item.get(\"content\", default_content))\n",
        "\n",
        "            # Dynamically add other fields\n",
        "            for key, value in item.items():\n",
        "                if key not in [\"title\", \"author\", \"content\"]:\n",
        "                    ET.SubElement(entry, key).text = str(value)\n",
        "\n",
        "        # Save the XML file\n",
        "        try:\n",
        "            tree = ET.ElementTree(root)\n",
        "            tree.write(f\"{self.output_base}.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving XML: {e}\")\n",
        "\n",
        "# Пример использования класса\n",
        "if __name__ == \"__main__\":\n",
        "    # Создание экземпляра класса\n",
        "    processor = NewsCorpusProcessor(output_base=\"social_media_news\")\n",
        "\n",
        "   # Список источников данных\n",
        "    sources = [\n",
        "        {\"type\": \"web\", \"url\": \"https://en.wikipedia.org/wiki/Web_scraping\"}\n",
        "    ]\n",
        "\n",
        "    # Обработка всех источников\n",
        "    processor.process_all_sources(sources)"
      ],
      "metadata": {
        "id": "acruVlXUXRpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "   # Список источников данных\n",
        "    sources = [\n",
        "        {\"type\": \"web\", \"url\": \"https://example.com/article1\"},\n",
        "        {\"type\": \"youtube\", \"url\": \"https://www.youtube.com/watch?v=VIDEO_ID\"},\n",
        "        {\"type\": \"twitter\", \"username\": \"twitter_username\", \"count\": 5},\n",
        "        {\"type\": \"meta\", \"page_id\": \"PAGE_ID\", \"access_token\": \"META_ACCESS_TOKEN\", \"count\": 5},\n",
        "        {\"type\": \"vk\", \"group_id\": \"GROUP_ID\", \"count\": 5}\n",
        "    ]\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-wwy0RagoXb5"
      }
    }
  ]
}