{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/hJWdWhTThr1m+qoAIoEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NM/Creating_a_new_korpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx langdetect PyPDF2 bs4 langdetect readability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE78RxT-OlKe",
        "outputId": "7c96d3d0-fab3-4c8c-ae31-a5f1bc74b852"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Collecting readability\n",
            "  Downloading readability-0.3.2.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Building wheels for collected packages: readability\n",
            "  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for readability: filename=readability-0.3.2-py3-none-any.whl size=36384 sha256=d97401060a24f14c2c11ce940aa8c8de24876048f903a7eea101d75ced4a1347\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/a8/01/0b6587e224d9731dae317fdad11b081f0e8b7be7d8367fc6eb\n",
            "Successfully built readability\n",
            "Installing collected packages: readability\n",
            "Successfully installed readability-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyaR3s-WMJ72",
        "outputId": "1d43494b-7f96-4384-ee07-6938527db4f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите путь к папке с книгами: /content/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Файл .config пропущен из-за неподдерживаемого формата.\n",
            "WARNING:root:Файл sample_data пропущен из-за неподдерживаемого формата.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from docx import Document\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "import nltk  # Для разделения на предложения\n",
        "from langdetect import detect  # Для определения языка\n",
        "import json  # Для сохранения в JSON\n",
        "import xml.etree.ElementTree as ET  # Для сохранения в XML\n",
        "from PyPDF2 import PdfReader  # Для чтения PDF\n",
        "from bs4 import BeautifulSoup  # Для парсинга HTML\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "\n",
        "class BookCorpusProcessor:\n",
        "    def __init__(self, books_folder: str, output_base: str = \"tajik_books\"):\n",
        "        \"\"\"\n",
        "        Инициализация класса.\n",
        "\n",
        "        :param books_folder: Путь к папке с книгами.\n",
        "        :param output_base: Базовое имя выходных файлов (без расширений).\n",
        "        \"\"\"\n",
        "        self.books_folder = books_folder\n",
        "        self.output_base = output_base\n",
        "        self.processed_books: List[str] = []\n",
        "\n",
        "    def clean_text(self, text: str, custom_patterns: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Очищает текст от лишних символов и номеров страниц.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :param custom_patterns: Список пользовательских регулярных выражений для очистки текста.\n",
        "        :return: Очищенный текст.\n",
        "        \"\"\"\n",
        "        patterns = [\n",
        "            r\"^\\s*\\d+\\s*$\",  # Удалить номера страниц\n",
        "            r\"[^\\w\\s\\.,!?;:()«»“”'\\\"\\\\/-]\",  # Удалить специальные символы\n",
        "            r\"\\s+\",  # Заменить множественные пробелы на один пробел\n",
        "            r\"^\\s*$\",  # Удалить пустые строки\n",
        "            r\"\\t+\",  # Удалить табуляции\n",
        "            r\"\\.{2,}\",  # Заменить многоточия на точку\n",
        "            r\"<[^>]*>\",  # Удалить HTML-теги\n",
        "        ]\n",
        "\n",
        "        if custom_patterns:\n",
        "            patterns.extend(custom_patterns)\n",
        "\n",
        "        for pattern in patterns:\n",
        "            text = re.sub(pattern, \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_metadata(self, filename: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Извлекает метаданные (название и автора) из имени файла.\n",
        "\n",
        "        :param filename: Имя файла.\n",
        "        :return: Словарь с метаданными.\n",
        "        \"\"\"\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        parts = base_name.split(\"_\", 1)\n",
        "\n",
        "        if len(parts) == 2:\n",
        "            title, author = parts\n",
        "            return {\"title\": title.strip(), \"author\": author.strip()}\n",
        "        else:\n",
        "            return {\"title\": base_name.strip(), \"author\": \"Неизвестный\"}\n",
        "\n",
        "    def process_docx_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .docx файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            doc = Document(file_path)\n",
        "            paragraphs = [paragraph.text for paragraph in doc.paragraphs]\n",
        "            raw_text = \"\\n\".join(paragraphs)\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_txt_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .txt файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                raw_text = file.read()\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_pdf_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .pdf файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            reader = PdfReader(file_path)\n",
        "            raw_text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_html_file(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Обрабатывает один .html файл, извлекает текст и метаданные.\n",
        "\n",
        "        :param file_path: Путь к файлу.\n",
        "        :return: Обработанный текст с метаданными или пустая строка при ошибке.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                soup = BeautifulSoup(file.read(), 'html.parser')\n",
        "\n",
        "            # Удаляем скрипты и стили\n",
        "            for script_or_style in soup([\"script\", \"style\"]):\n",
        "                script_or_style.decompose()\n",
        "\n",
        "            raw_text = soup.get_text(separator=\"\\n\")\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "            metadata = self.extract_metadata(os.path.basename(file_path))\n",
        "            metadata_str = f\"# Название: {metadata['title']}\\n# Автор: {metadata['author']}\\n# -----\\n\"\n",
        "            return metadata_str + cleaned_text + \"\\n\\n\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_all_books(self):\n",
        "        \"\"\"\n",
        "        Обрабатывает все файлы в указанной папке и сохраняет результат.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.books_folder):\n",
        "            logging.error(\"Указанный путь не существует.\")\n",
        "            return\n",
        "\n",
        "        all_books = []\n",
        "        for filename in os.listdir(self.books_folder):\n",
        "            file_path = os.path.join(self.books_folder, filename)\n",
        "\n",
        "            if filename.endswith(\".docx\"):\n",
        "                processed_text = self.process_docx_file(file_path)\n",
        "            elif filename.endswith(\".txt\"):\n",
        "                processed_text = self.process_txt_file(file_path)\n",
        "            elif filename.endswith(\".pdf\"):\n",
        "                processed_text = self.process_pdf_file(file_path)\n",
        "            elif filename.endswith(\".html\"):\n",
        "                processed_text = self.process_html_file(file_path)\n",
        "            else:\n",
        "                logging.warning(f\"Файл {filename} пропущен из-за неподдерживаемого формата.\")\n",
        "                continue\n",
        "\n",
        "            if processed_text:\n",
        "                all_books.append(processed_text)\n",
        "                self.processed_books.append(filename)\n",
        "                logging.info(f\"Обработан файл: {filename}\")\n",
        "            else:\n",
        "                logging.warning(f\"Файл {filename} пропущен из-за ошибки.\")\n",
        "\n",
        "        # Сохранение в TXT\n",
        "        with open(f\"{self.output_base}.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            txt_file.write(\"\\n\".join(all_books))\n",
        "\n",
        "        # Сохранение в JSON\n",
        "        json_data = []\n",
        "        for book in all_books:\n",
        "            title, rest = book.split(\"\\n\", 1)\n",
        "            author, text = rest.split(\"# -----\\n\", 1)\n",
        "            json_data.append({\n",
        "                \"title\": title.split(\":\")[1].strip(),\n",
        "                \"author\": author.split(\":\")[1].strip(),\n",
        "                \"text\": text.strip()\n",
        "            })\n",
        "        with open(f\"{self.output_base}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Сохранение в XML\n",
        "        root = ET.Element(\"books\")\n",
        "        for book in all_books:\n",
        "            title, rest = book.split(\"\\n\", 1)\n",
        "            author, text = rest.split(\"# -----\\n\", 1)\n",
        "            book_elem = ET.SubElement(root, \"book\")\n",
        "            ET.SubElement(book_elem, \"title\").text = title.split(\":\")[1].strip()\n",
        "            ET.SubElement(book_elem, \"author\").text = author.split(\":\")[1].strip()\n",
        "            ET.SubElement(book_elem, \"text\").text = text.strip()\n",
        "\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(f\"{self.output_base}.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "        logging.info(f\"Все книги успешно объединены в {self.output_base}.txt, {self.output_base}.json и {self.output_base}.xml.\")\n",
        "        logging.info(f\"Обработано книг: {len(self.processed_books)}.\")\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Разделяет текст на предложения.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :return: Текст, разделенный на предложения.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            sentences = nltk.sent_tokenize(text, language=\"russian\")\n",
        "            return \"\\n\".join(sentences)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Ошибка при разделении текста на предложения: {e}\")\n",
        "            return text\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Определяет язык текста.\n",
        "\n",
        "        :param text: Исходный текст.\n",
        "        :return: Язык текста (например, 'ru', 'tg', 'en').\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return detect(text[:1000])  # Проверяем первые 1000 символов\n",
        "        except:\n",
        "            return \"unknown\"\n",
        "\n",
        "\n",
        "# Пример использования класса\n",
        "if __name__ == \"__main__\":\n",
        "    # Путь к папке с книгами\n",
        "    books_folder = input(\"Введите путь к папке с книгами: \").strip()\n",
        "\n",
        "    # Создание экземпляра класса\n",
        "    processor = BookCorpusProcessor(books_folder, output_base=\"tajik_books\")\n",
        "\n",
        "    # Обработка всех книг\n",
        "    processor.process_all_books()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CleanCorpusProcessor"
      ],
      "metadata": {
        "id": "274SepY8yBh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Dict\n",
        "import nltk\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class CleanCorpusProcessor:\n",
        "    def __init__(self, input_file: str, output_base: str = \"clean_corpus\"):\n",
        "        \"\"\"\n",
        "        Инициализация класса.\n",
        "        :param input_file: Путь к файлу (JSON, TXT или XML), созданному BookCorpusProcessor.\n",
        "        :param output_base: Базовое имя выходных файлов.\n",
        "        \"\"\"\n",
        "        self.input_file = input_file\n",
        "        self.output_base = output_base\n",
        "        self.stop_words = set(stopwords.words('russian') + stopwords.words('english'))\n",
        "        self.spell_correction_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.processed_books: List[Dict] = []\n",
        "\n",
        "    def load_data(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из файла (JSON, TXT или XML).\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.input_file):\n",
        "            logging.error(f\"Файл {self.input_file} не найден.\")\n",
        "            return []\n",
        "\n",
        "        _, ext = os.path.splitext(self.input_file)\n",
        "        if ext == \".json\":\n",
        "            return self.load_from_json()\n",
        "        elif ext == \".txt\":\n",
        "            return self.load_from_txt()\n",
        "        elif ext == \".xml\":\n",
        "            return self.load_from_xml()\n",
        "        else:\n",
        "            logging.error(f\"Неподдерживаемый формат файла: {ext}\")\n",
        "            return []\n",
        "\n",
        "    def load_from_json(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из JSON-файла.\n",
        "        \"\"\"\n",
        "        with open(self.input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "\n",
        "    def load_from_txt(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из TXT-файла.\n",
        "        \"\"\"\n",
        "        books = []\n",
        "        with open(self.input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read().strip().split(\"\\n\\n\")\n",
        "        for book in content:\n",
        "            try:\n",
        "                title, rest = book.split(\"\\n\", 1)\n",
        "                author, text = rest.split(\"# -----\\n\", 1)\n",
        "                books.append({\n",
        "                    \"title\": title.split(\":\")[1].strip(),\n",
        "                    \"author\": author.split(\":\")[1].strip(),\n",
        "                    \"text\": text.strip()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Ошибка при чтении книги из TXT: {e}\")\n",
        "        return books\n",
        "\n",
        "    def load_from_xml(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Загружает данные из XML-файла.\n",
        "        \"\"\"\n",
        "        books = []\n",
        "        tree = ET.parse(self.input_file)\n",
        "        root = tree.getroot()\n",
        "        for book_elem in root.findall(\"book\"):\n",
        "            title = book_elem.find(\"title\").text.strip() if book_elem.find(\"title\") is not None else \"Unknown Title\"\n",
        "            author = book_elem.find(\"author\").text.strip() if book_elem.find(\"author\") is not None else \"Unknown Author\"\n",
        "            text = book_elem.find(\"text\").text.strip() if book_elem.find(\"text\") is not None else \"\"\n",
        "            books.append({\"title\": title, \"author\": author, \"text\": text})\n",
        "        return books\n",
        "\n",
        "    def correct_spelling(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Исправляет опечатки в тексте.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "            corrected_sentences = [self.spell_correction_model(sentence)[0]['generated_text'] for sentence in sentences]\n",
        "            return \" \".join(corrected_sentences)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Ошибка при исправлении опечаток: {e}\")\n",
        "            return text\n",
        "\n",
        "    def lemmatize_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Лемматизирует текст.\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return \" \".join(lemmatized_tokens)\n",
        "\n",
        "    def remove_stopwords(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Удаляет стоп-слова из текста.\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return \" \".join(filtered_tokens)\n",
        "\n",
        "    def process_book(self, book: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Обрабатывает одну книгу: исправляет опечатки, лемматизирует и удаляет стоп-слова.\n",
        "        \"\"\"\n",
        "        title = book.get(\"title\", \"Unknown Title\")\n",
        "        author = book.get(\"author\", \"Unknown Author\")\n",
        "        raw_text = book.get(\"text\", \"\")\n",
        "\n",
        "        # Исправление опечаток\n",
        "        corrected_text = self.correct_spelling(raw_text)\n",
        "\n",
        "        # Лемматизация\n",
        "        lemmatized_text = self.lemmatize_text(corrected_text)\n",
        "\n",
        "        # Удаление стоп-слов\n",
        "        clean_text = self.remove_stopwords(lemmatized_text)\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"author\": author,\n",
        "            \"language\": detect(raw_text[:1000]) if raw_text else \"unknown\",\n",
        "            \"text\": clean_text\n",
        "        }\n",
        "\n",
        "    def process_all_books(self):\n",
        "        \"\"\"\n",
        "        Обрабатывает все книги из входного файла и сохраняет результат.\n",
        "        \"\"\"\n",
        "        books_data = self.load_data()\n",
        "        if not books_data:\n",
        "            logging.error(\"Нет данных для обработки.\")\n",
        "            return\n",
        "\n",
        "        for book in books_data:\n",
        "            processed_book = self.process_book(book)\n",
        "            if processed_book:\n",
        "                self.processed_books.append(processed_book)\n",
        "                logging.info(f\"Обработана книга: {processed_book['title']}\")\n",
        "\n",
        "        # Сохранение в JSON\n",
        "        with open(f\"{self.output_base}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(self.processed_books, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Сохранение в XML\n",
        "        root = ET.Element(\"books\")\n",
        "        for book in self.processed_books:\n",
        "            book_elem = ET.SubElement(root, \"book\")\n",
        "            ET.SubElement(book_elem, \"title\").text = book[\"title\"]\n",
        "            ET.SubElement(book_elem, \"author\").text = book[\"author\"]\n",
        "            ET.SubElement(book_elem, \"language\").text = book[\"language\"]\n",
        "            ET.SubElement(book_elem, \"text\").text = book[\"text\"]\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(f\"{self.output_base}.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "        logging.info(f\"Все книги успешно обработаны и сохранены в {self.output_base}.json и {self.output_base}.xml.\")\n",
        "        logging.info(f\"Обработано книг: {len(self.processed_books)}.\")\n",
        "\n",
        "# Пример использования класса\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = input(\"Введите путь к файлу (JSON, TXT или XML): \").strip()\n",
        "    processor = CleanCorpusProcessor(input_file, output_base=\"clean_corpus\")\n",
        "    processor.process_all_books()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZMcNzm4tiO4",
        "outputId": "cbe7010b-402c-4eb1-f6c3-d7da6a1220ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите путь к файлу (JSON, TXT или XML): /content/tajik_books.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web"
      ],
      "metadata": {
        "id": "_MifEFelx8aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube tweepy readability-lxml lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp57PsAka3c2",
        "outputId": "ab0af1f1-9fc6-4984-aa3f-25d214c79dad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: readability-lxml in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (5.3.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from typing import List, Dict, Optional  # Добавлен импорт Optional\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document  # Для извлечения основного текста\n",
        "import xml.etree.ElementTree as ET  # Для создания XML\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "class WebCorpusProcessor:\n",
        "    def __init__(self, output_base: str = \"news_corpus\"):\n",
        "        \"\"\"\n",
        "        Инициализация класса.\n",
        "        :param output_base: Базовое имя выходных файлов (без расширений).\n",
        "        \"\"\"\n",
        "        self.output_base = output_base\n",
        "        self.processed_items: List[Dict] = []\n",
        "\n",
        "    def clean_text(self, text: str, custom_patterns: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Очищает текст от HTML-тегов и лишних символов.\n",
        "        :param text: Исходный текст.\n",
        "        :param custom_patterns: Список пользовательских регулярных выражений для очистки текста.\n",
        "        :return: Очищенный текст.\n",
        "        \"\"\"\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "        plain_text = soup.get_text(separator=\" \")\n",
        "        patterns = [\n",
        "            r\"^\\s*\\d+\\s*$\",  # Удалить номера страниц\n",
        "            r\"[^\\w\\s\\.,!?;:()«»“”'\\\"\\\\/-]\",  # Удалить специальные символы\n",
        "            r\"\\s+\",  # Заменить множественные пробелы на один пробел\n",
        "            r\"^\\s*$\",  # Удалить пустые строки\n",
        "            r\"\\t+\",  # Удалить табуляции\n",
        "            r\"\\.{2,}\",  # Заменить многоточия на точку\n",
        "        ]\n",
        "        if custom_patterns:\n",
        "            patterns.extend(custom_patterns)\n",
        "        for pattern in patterns:\n",
        "            plain_text = re.sub(pattern, \" \", plain_text, flags=re.MULTILINE)\n",
        "        return plain_text.strip()\n",
        "\n",
        "    def clean_content(self, data: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Очищает содержимое словаря с ключами 'title', 'author' и 'content'.\n",
        "        :param data: Словарь с данными.\n",
        "        :return: Очищенный словарь.\n",
        "        \"\"\"\n",
        "        cleaned_data = {\n",
        "            \"title\": self.clean_text(data.get(\"title\", \"\")),\n",
        "            \"author\": self.clean_text(data.get(\"author\", \"\")),\n",
        "            \"content\": self.clean_text(data.get(\"content\", \"\"))\n",
        "        }\n",
        "        return cleaned_data\n",
        "\n",
        "    def extract_web_content(self, url: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Извлекает заголовок, автора и основной текстовый контент из веб-страницы.\n",
        "        :param url: URL веб-страницы.\n",
        "        :return: Словарь с заголовком, автором и текстом.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            title = soup.title.string.strip() if soup.title else \"No Title\"\n",
        "            author_tag = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
        "            author = author_tag[\"content\"].strip() if author_tag else \"Unknown Author\"\n",
        "            doc = Document(response.text)\n",
        "            raw_text = doc.summary()\n",
        "            raw_data = {\n",
        "                \"title\": title,\n",
        "                \"author\": author,\n",
        "                \"content\": raw_text\n",
        "            }\n",
        "            cleaned_data = self.clean_content(raw_data)\n",
        "            return cleaned_data\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при извлечении контента из {url}: {e}\")\n",
        "            return {\"title\": \"Error\", \"author\": \"Unknown\", \"content\": \"\"}\n",
        "\n",
        "    def process_all_sources(self, sources: List[Dict]):\n",
        "        \"\"\"\n",
        "        Обрабатывает все источники данных (только веб-сайты).\n",
        "        :param sources: Список источников данных.\n",
        "        \"\"\"\n",
        "        all_data = []\n",
        "        for source in sources:\n",
        "            if source[\"type\"] == \"web\":\n",
        "                content = self.extract_web_content(source[\"url\"])\n",
        "                if content:\n",
        "                    all_data.append({\"source\": \"web\", \"url\": source[\"url\"], \"content\": content})\n",
        "\n",
        "        # Сохранение в JSON\n",
        "        self.save_to_json(all_data)\n",
        "        # Сохранение в TXT\n",
        "        self.save_to_txt(all_data)\n",
        "        # Сохранение в XML\n",
        "        self.save_to_xml(all_data)\n",
        "        logging.info(f\"Все данные успешно сохранены в {self.output_base}.json, {self.output_base}.txt и {self.output_base}.xml.\")\n",
        "        logging.info(f\"Обработано источников: {len(all_data)}.\")\n",
        "\n",
        "    def save_to_json(self, data: List[Dict]):\n",
        "        \"\"\"\n",
        "        Сохраняет данные в JSON-файл.\n",
        "        :param data: Список словарей с данными.\n",
        "        \"\"\"\n",
        "        with open(f\"{self.output_base}.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def save_to_txt(self, data: List[Dict]):\n",
        "        \"\"\"\n",
        "        Сохраняет данные в TXT-файл.\n",
        "        :param data: Список словарей с данными.\n",
        "        \"\"\"\n",
        "        with open(f\"{self.output_base}.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
        "            for item in data:\n",
        "                txt_file.write(f\"Title: {item.get('content', {}).get('title', 'N/A')}\\n\")\n",
        "                txt_file.write(f\"Author: {item.get('content', {}).get('author', 'N/A')}\\n\")\n",
        "                txt_file.write(f\"Content: {item.get('content', {}).get('content', '')}\\n\\n\")\n",
        "\n",
        "    def save_to_xml(self, data: List[Dict]):\n",
        "        \"\"\"\n",
        "        Сохраняет данные в XML-файл.\n",
        "        :param data: Список словарей с данными.\n",
        "        \"\"\"\n",
        "        root = ET.Element(\"news_corpus\")\n",
        "        for item in data:\n",
        "            entry = ET.SubElement(root, \"entry\")\n",
        "            content = item.get(\"content\", {})\n",
        "            ET.SubElement(entry, \"title\").text = content.get(\"title\", \"N/A\")\n",
        "            ET.SubElement(entry, \"author\").text = content.get(\"author\", \"N/A\")\n",
        "            ET.SubElement(entry, \"content\").text = content.get(\"content\", \"\")\n",
        "        tree = ET.ElementTree(root)\n",
        "        tree.write(f\"{self.output_base}.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "# Пример использования класса\n",
        "if __name__ == \"__main__\":\n",
        "    # Создание экземпляра класса\n",
        "    processor = WebCorpusProcessor(output_base=\"web_news\")\n",
        "    # Список источников данных (только веб-сайты)\n",
        "    sources = [\n",
        "        {\"type\": \"web\", \"url\": \"https://en.wikipedia.org/wiki/Web_scraping\"},\n",
        "        {\"type\": \"web\", \"url\": \"https://docs.python.org/3/tutorial/index.html\"},\n",
        "        {\"type\": \"web\", \"url\": \"https://habr.com/ru/articles/881502/\"}\n",
        "    ]\n",
        "    # Обработка всех источников\n",
        "    processor.process_all_sources(sources)"
      ],
      "metadata": {
        "id": "acruVlXUXRpH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Соц. сети"
      ],
      "metadata": {
        "id": "7va8mOC1xqcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "from abc import ABC, abstractmethod\n",
        "import time\n",
        "import logging\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK (если они еще не скачаны)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class DataSource(ABC):\n",
        "    \"\"\"\n",
        "    Абстрактный базовый класс для работы с источниками данных.\n",
        "    \"\"\"\n",
        "    def __init__(self, source_id: str):\n",
        "        self.source_id = source_id\n",
        "\n",
        "    @abstractmethod\n",
        "    def fetch_data(self) -> list:\n",
        "        \"\"\"\n",
        "        Метод для получения данных из источника.\n",
        "        :return: Список словарей с данными.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def process_data(self, data: list) -> list:\n",
        "        \"\"\"\n",
        "        Метод для обработки и очистки данных.\n",
        "        :param data: Список словарей с данными.\n",
        "        :return: Обработанный список словарей с данными.\n",
        "        \"\"\"\n",
        "        processed_data = []\n",
        "        for item in data:\n",
        "            processed_item = item.copy()\n",
        "            # Очищаем текст комментария, если он существует\n",
        "            if 'text' in processed_item and isinstance(processed_item['text'], str):\n",
        "                processed_item['text'] = self.clean_text(processed_item['text'])\n",
        "            elif 'text' in processed_item:\n",
        "                # Если текст отсутствует или является NaN, заменяем его на пустую строку\n",
        "                processed_item['text'] = ''\n",
        "            processed_data.append(processed_item)\n",
        "        return processed_data\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Метод для очистки текста.\n",
        "        :param text: Исходный текст.\n",
        "        :return: Очищенный текст.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):  # Проверка, чтобы убедиться, что это строка\n",
        "            return ''\n",
        "\n",
        "        # Удаление HTML тегов\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        # Удаление упоминаний (@username)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        # Удаление URL\n",
        "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "        # Удаление специальных символов и цифр\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        # Приведение к нижнему регистру\n",
        "        text = text.lower()\n",
        "        # Токенизация\n",
        "        tokens = word_tokenize(text)\n",
        "        # Удаление стоп-слов\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        # Лемматизация\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "        # Соединение слов обратно в строку\n",
        "        cleaned_text = ' '.join(tokens)\n",
        "        return cleaned_text\n",
        "\n",
        "    def to_dataframe(self, data: list) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Преобразует данные в DataFrame.\n",
        "        :param data: Список словарей с данными.\n",
        "        :return: DataFrame.\n",
        "        \"\"\"\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def save_to_csv(self, data: list, output_file: str):\n",
        "        \"\"\"\n",
        "        Сохраняет данные в CSV файл.\n",
        "        :param data: Список словарей с данными.\n",
        "        :param output_file: Имя выходного файла.\n",
        "        \"\"\"\n",
        "        df = self.to_dataframe(data)\n",
        "        df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "        logging.info(f\"Data saved to {output_file}\")\n",
        "\n",
        "class YouTubeDataSource(DataSource):\n",
        "    \"\"\"\n",
        "    Класс для работы с комментариями YouTube.\n",
        "    \"\"\"\n",
        "    def __init__(self, video_id: str, api_key: str):\n",
        "        super().__init__(source_id=video_id)\n",
        "        self.api_key = api_key\n",
        "        self.youtube = build('youtube', 'v3', developerKey=self.api_key)\n",
        "\n",
        "    def fetch_data(self) -> list:\n",
        "        \"\"\"\n",
        "        Получает комментарии для указанного видео.\n",
        "        :return: Список словарей с комментариями.\n",
        "        \"\"\"\n",
        "        comments = []\n",
        "        next_page_token = None\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                request = self.youtube.commentThreads().list(\n",
        "                    part=\"snippet\",\n",
        "                    videoId=self.source_id,\n",
        "                    maxResults=100,\n",
        "                    pageToken=next_page_token\n",
        "                )\n",
        "                response = request.execute()\n",
        "\n",
        "                for item in response.get(\"items\", []):\n",
        "                    comment = {\n",
        "                        \"comment_id\": item[\"id\"],\n",
        "                        \"text\": item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"],\n",
        "                        \"author\": item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"],\n",
        "                        \"published_at\": item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"],\n",
        "                        \"like_count\": item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
        "                    }\n",
        "                    comments.append(comment)\n",
        "\n",
        "                next_page_token = response.get(\"nextPageToken\")\n",
        "                if not next_page_token:\n",
        "                    break\n",
        "\n",
        "                # Add a delay to avoid hitting rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"An error occurred: {e}\")\n",
        "                break\n",
        "\n",
        "        logging.info(f\"Collected {len(comments)} comments for video ID: {self.source_id}\")\n",
        "        return comments\n",
        "\n",
        "\n",
        "class CSVDataSource(DataSource):\n",
        "    \"\"\"\n",
        "    Класс для работы с CSV файлами.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path: str):\n",
        "        super().__init__(source_id=file_path)\n",
        "\n",
        "    def fetch_data(self) -> list:\n",
        "        \"\"\"\n",
        "        Читает данные из CSV файла.\n",
        "        :return: Список словарей с данными.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(self.source_id)\n",
        "            data = df.to_dict(orient='records')\n",
        "            logging.info(f\"Loaded {len(data)} records from {self.source_id}\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading CSV file: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Константы для YouTube\n",
        "    API_KEY = ''\n",
        "    VIDEO_ID = 'AVYfyTvc9KY'\n",
        "\n",
        "    # Создаем экземпляр YouTubeDataSource\n",
        "    youtube_source = YouTubeDataSource(video_id=VIDEO_ID, api_key=API_KEY)\n",
        "    youtube_comments = youtube_source.fetch_data()\n",
        "\n",
        "    # Обрабатываем данные\n",
        "    processed_youtube_comments = youtube_source.process_data(youtube_comments)\n",
        "\n",
        "    # Сохраняем данные в CSV\n",
        "    youtube_output_file = f\"youtube_comments_{VIDEO_ID}.csv\"\n",
        "    youtube_source.save_to_csv(processed_youtube_comments, youtube_output_file)\n",
        "\n",
        "    # Создаем экземпляр CSVDataSource\n",
        "    csv_file_path = youtube_output_file  # Можно использовать любой другой CSV файл\n",
        "    csv_source = CSVDataSource(file_path=csv_file_path)\n",
        "    csv_data = csv_source.fetch_data()\n",
        "\n",
        "    # Обрабатываем данные из CSV\n",
        "    processed_csv_data = csv_source.process_data(csv_data)\n",
        "\n",
        "    # Сохраняем данные из CSV в новый файл (для демонстрации)\n",
        "    csv_output_file = f\"processed_data_{VIDEO_ID}.csv\"\n",
        "    csv_source.save_to_csv(processed_csv_data, csv_output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZInYqmYkzkx",
        "outputId": "0d12cb4f-be0d-4610-bf8e-667e53cff85e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}