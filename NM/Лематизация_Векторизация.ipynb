{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf5SjPVnvxvRZxDFFTmmbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NM/%D0%9B%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2 langdetect natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7DXcE7zbuJR",
        "outputId": "82e6e9a4-295d-484f-9793-1e4dd49f51a5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: natasha in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.11/dist-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from navec>=0.9.0->natasha) (1.26.4)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFC2aVFibHzq",
        "outputId": "3247929e-912a-4417-b67e-87f2fb6173c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from transformers import AutoTokenizer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect, LangDetectException\n",
        "import spacy\n",
        "import json\n",
        "import os\n",
        "from natasha import MorphVocab\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, text, language=None):\n",
        "        self.text = text\n",
        "        self.language = language if language else self.detect_language()\n",
        "        self.morph_vocab = MorphVocab() if self.language == 'ru' else None\n",
        "        self.morph_analyzer = MorphAnalyzer() if self.language == 'ru' else None\n",
        "        self.stop_words = set(stopwords.words(self.language)) if self.language in stopwords.fileids() else set()\n",
        "        self.nlp = self._load_spacy_model() if self.language in {'en', 'de'} else None\n",
        "        self.results = []  # Для хранения результатов методов\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        model_map = {\n",
        "            'en': \"en_core_web_sm\",\n",
        "            'de': \"de_core_news_sm\"\n",
        "        }\n",
        "\n",
        "        if self.language in model_map:\n",
        "            model_name = model_map[self.language]\n",
        "            if not spacy.util.is_package(model_name):\n",
        "                raise NotImplementedError(f\"spaCy model '{model_name}' is not installed. Please install it using 'python -m spacy download {model_name}'.\")\n",
        "            try:\n",
        "                return spacy.load(model_name)\n",
        "            except OSError:\n",
        "                raise NotImplementedError(f\"spaCy model for language '{self.language}' is not available.\")\n",
        "        else:\n",
        "            print(f\"spaCy does not support the language '{self.language}'. Using alternative NLP tools.\")\n",
        "            return None\n",
        "\n",
        "    def clean_text(self):\n",
        "        cleaned_text = re.sub(r\"http\\S+\", \"\", self.text)\n",
        "        cleaned_text = re.sub(r\"<.*?>\", \"\", cleaned_text)\n",
        "        cleaned_text = re.sub(r\"[^a-zA-Zа-яА-ЯёЁ0-9\\s]\", \"\", cleaned_text)\n",
        "        return cleaned_text.strip()\n",
        "\n",
        "    def remove_stopwords(self, tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = self.standard_tokenization()\n",
        "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
        "\n",
        "    def simple_tokenization(self):\n",
        "        tokens = self.clean_text().split()\n",
        "        self._save_result(\"Simple Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def standard_tokenization(self):\n",
        "        tokens = word_tokenize(self.clean_text())\n",
        "        self._save_result(\"Standard Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def character_tokenization(self):\n",
        "        tokens = list(self.clean_text())\n",
        "        self._save_result(\"Character Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def byte_level_tokenization(self):\n",
        "        tokens = [byte for byte in self.clean_text().encode('utf-8')]\n",
        "        self._save_result(\"Byte-Level Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def zipf_law_analysis(self, save_path=\"zipf_law.png\"):\n",
        "        \"\"\"\n",
        "        Выполняет анализ закона Ципфа и сохраняет график в файл.\n",
        "        \"\"\"\n",
        "        tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "        freq_dist = Counter(tokens)\n",
        "        frequencies = sorted(freq_dist.values(), reverse=True)\n",
        "        ranks = range(1, len(frequencies) + 1)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(ranks, frequencies, marker='o')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Rank')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(\"Zipf's Law Analysis\")\n",
        "\n",
        "        # Сохраняем график в файл\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "\n",
        "        # Сохраняем результат\n",
        "        self._save_result(\"Zipf's Law Analysis\", {\"image\": save_path})\n",
        "        return save_path\n",
        "\n",
        "    def visualize_word_frequencies(self, save_path=\"word_frequencies.png\"):\n",
        "        \"\"\"\n",
        "        Визуализирует частоты слов в виде облака слов и сохраняет изображение в файл.\n",
        "        \"\"\"\n",
        "        tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "        freq_dist = Counter(tokens)\n",
        "        wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(freq_dist)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Word Frequencies Visualization\")\n",
        "\n",
        "        # Сохраняем график в файл\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "\n",
        "        # Сохраняем результат\n",
        "        self._save_result(\"Word Frequencies Visualization\", {\"image\": save_path})\n",
        "        return save_path\n",
        "\n",
        "    def rule_based_tokenization(self, pattern=r'\\w+'):\n",
        "        tokens = regexp_tokenize(self.clean_text(), pattern)\n",
        "        self._save_result(\"Rule-Based Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def stemming(self):\n",
        "        \"\"\"\n",
        "        Выполняет стемминг текста с использованием SnowballStemmer.\n",
        "        Для русского языка используется лемматизация через pymorphy2,\n",
        "        так как стемминг для русского языка менее эффективен.\n",
        "        \"\"\"\n",
        "        if self.language == 'ru':\n",
        "            # Для русского языка используем лемматизацию вместо стемминга\n",
        "            print(\"Stemming is not recommended for Russian. Using lemmatization instead.\")\n",
        "            tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "            lemmas = [self.morph_analyzer.parse(token)[0].normal_form for token in tokens]\n",
        "            self._save_result(\"Lemmatization\", lemmas)\n",
        "            return lemmas\n",
        "\n",
        "        elif self.language in SnowballStemmer.languages:\n",
        "            # Для остальных языков, поддерживаемых SnowballStemmer, выполняем стемминг\n",
        "            stemmer = SnowballStemmer(self.language)\n",
        "            tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "            stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "            self._save_result(\"Stemming\", stemmed_tokens)\n",
        "            return stemmed_tokens\n",
        "\n",
        "        #else:\n",
        "            # Если язык не поддерживается, выбрасываем исключение\n",
        "            #raise ValueError(f\"Stemming is not supported for language '{self.language}'.\")\n",
        "\n",
        "    def lemmatize(self, cleaned_text):\n",
        "        if self.language == 'ru':\n",
        "            words = word_tokenize(cleaned_text)\n",
        "            lemmas = [self.morph_analyzer.parse(word)[0].normal_form for word in words]\n",
        "            self._save_result(\"Lemmatization\", lemmas)\n",
        "            return lemmas\n",
        "        elif self.nlp:\n",
        "            doc = self.nlp(cleaned_text)\n",
        "            lemmas = [token.lemma_ for token in doc]\n",
        "            self._save_result(\"Lemmatization\", lemmas)\n",
        "            return lemmas\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Лемматизация для языка '{self.language}' не поддерживается.\")\n",
        "\n",
        "    def universal_lemmatization(self):\n",
        "        \"\"\"\n",
        "        Универсальный метод лемматизации, который выбирает подходящий инструмент в зависимости от языка.\n",
        "        \"\"\"\n",
        "        if self.language == 'ru':\n",
        "            tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "            lemmatized_tokens = [(token, self.morph_analyzer.parse(token)[0].normal_form) for token in tokens]\n",
        "        elif self.language in {'en', 'de'}:\n",
        "            doc = self.nlp(self.clean_text())\n",
        "            lemmatized_tokens = [(token.text, token.lemma_) for token in doc if token.text.lower() not in self.stop_words]\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Lemmatization is not implemented for language '{self.language}'.\")\n",
        "\n",
        "        self._save_result(\"Universal Lemmatization\", lemmatized_tokens)\n",
        "        return lemmatized_tokens\n",
        "\n",
        "    def morphological_analysis(self):\n",
        "        if self.language == 'ru':\n",
        "            tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "            morph_tokens = [(token, self.morph_analyzer.parse(token)[0].normal_form) for token in tokens]\n",
        "        elif self.language in {'en', 'de'}:\n",
        "            doc = self.nlp(self.clean_text())\n",
        "            morph_tokens = [(token.text, token.morph) for token in doc if token.text.lower() not in self.stop_words]\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Morphological analysis is not implemented for language '{self.language}'.\")\n",
        "\n",
        "        self._save_result(\"Morphological Analysis\", morph_tokens)\n",
        "        return morph_tokens\n",
        "\n",
        "    def subword_tokenization(self, model_name=\"bert-base-uncased\"):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokens = tokenizer.tokenize(self.clean_text())\n",
        "        self._save_result(\"Subword Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def neural_tokenization(self, model_name=\"bert-base-uncased\"):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        token_ids = tokenizer.encode(self.clean_text(), add_special_tokens=True)\n",
        "        self._save_result(\"Neural Tokenization\", token_ids)\n",
        "        return token_ids\n",
        "\n",
        "    def hybrid_tokenization(self):\n",
        "        rule_tokens = self.rule_based_tokenization()\n",
        "        if self.language in SnowballStemmer.languages:\n",
        "            stemmer = SnowballStemmer(self.language)\n",
        "            tokens = [stemmer.stem(token) for token in rule_tokens]\n",
        "        else:\n",
        "            tokens = rule_tokens\n",
        "        self._save_result(\"Hybrid Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def language_specific_tokenization(self):\n",
        "        if self.language == 'en':\n",
        "            tokens = self.standard_tokenization()\n",
        "        elif self.language == 'ru':\n",
        "            tokens = self.morphological_analysis()\n",
        "        elif self.language == 'de':\n",
        "            if not self.nlp:\n",
        "                raise NotImplementedError(f\"Language-specific tokenization is not implemented for language '{self.language}'.\")\n",
        "            doc = self.nlp(self.clean_text())\n",
        "            tokens = [token.text for token in doc]\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Language '{self.language}' not supported yet.\")\n",
        "\n",
        "        self._save_result(\"Language-Specific Tokenization\", tokens)\n",
        "        return tokens\n",
        "\n",
        "    def detect_language(self):\n",
        "        try:\n",
        "            return detect(self.text)\n",
        "        except LangDetectException:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def generate_ngrams(self, n=2):\n",
        "        tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "        ngrams = [\" \".join(ngram) for ngram in ngrams]\n",
        "        self._save_result(f\"{n}-grams\", ngrams)\n",
        "        return ngrams\n",
        "\n",
        "    def save_tokens_to_file(self, filename=\"tokens.json\"):\n",
        "        tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(tokens, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def load_tokens_from_file(self, filename=\"tokens.json\"):\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def evaluate_tokenization(self, gold_standard_tokens):\n",
        "        predicted_tokens = self.remove_stopwords(self.standard_tokenization())\n",
        "        correct = len(set(predicted_tokens) & set(gold_standard_tokens))\n",
        "        precision = correct / len(predicted_tokens) if predicted_tokens else 0\n",
        "        recall = correct / len(gold_standard_tokens) if gold_standard_tokens else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        result = {\"precision\": precision, \"recall\": recall, \"f1_score\": f1_score}\n",
        "        self._save_result(\"Tokenization Evaluation\", result)\n",
        "        return result\n",
        "\n",
        "    def _save_result(self, method_name, result):\n",
        "        \"\"\"\n",
        "        Сохраняет результат выполнения метода в список результатов.\n",
        "        \"\"\"\n",
        "        self.results.append({\n",
        "            \"method\": method_name,\n",
        "            \"text\": self.text,\n",
        "            \"result\": result\n",
        "        })\n",
        "\n",
        "    def execute_all_methods(self):\n",
        "        \"\"\"\n",
        "        Выполняет все методы класса TextProcessor и сохраняет их результаты.\n",
        "        \"\"\"\n",
        "        methods_to_execute = [\n",
        "            (\"Simple Tokenization\", self.simple_tokenization),\n",
        "            (\"Standard Tokenization\", self.standard_tokenization),\n",
        "            (\"Character Tokenization\", self.character_tokenization),\n",
        "            (\"Byte-Level Tokenization\", self.byte_level_tokenization),\n",
        "            (\"Rule-Based Tokenization\", lambda: self.rule_based_tokenization(pattern=r'\\w+')),\n",
        "            (\"Stemming\", self.stemming),\n",
        "            (\"Lemmatization\", lambda: self.lemmatize(self.clean_text())),\n",
        "            (\"Universal Lemmatization\", self.universal_lemmatization),\n",
        "            (\"Morphological Analysis\", self.morphological_analysis),\n",
        "            (\"Subword Tokenization\", lambda: self.subword_tokenization(model_name=\"bert-base-uncased\")),\n",
        "            (\"Neural Tokenization\", lambda: self.neural_tokenization(model_name=\"bert-base-uncased\")),\n",
        "            (\"Hybrid Tokenization\", self.hybrid_tokenization),\n",
        "            (\"Language-Specific Tokenization\", self.language_specific_tokenization),\n",
        "            (\"2-grams\", lambda: self.generate_ngrams(n=2)),\n",
        "            (\"3-grams\", lambda: self.generate_ngrams(n=3)),\n",
        "            (\"Zipf's Law Analysis\", lambda: self.zipf_law_analysis(save_path=f\"{self.language}_zipf_law.png\")),\n",
        "            (\"Word Frequencies Visualization\", lambda: self.visualize_word_frequencies(save_path=f\"{self.language}_word_frequencies.png\"))\n",
        "        ]\n",
        "\n",
        "        for method_name, method in methods_to_execute:\n",
        "            try:\n",
        "                if method_name in {\"Zipf's Law Analysis\", \"Word Frequencies Visualization\"}:\n",
        "                    # Для методов, которые только рисуют графики, не сохраняем результат\n",
        "                    method()\n",
        "                else:\n",
        "                    result = method()\n",
        "                    self._save_result(method_name, result)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при выполнении метода {method_name}: {e}\")\n",
        "\n",
        "\n",
        "    def _save_result(self, method_name, result):\n",
        "            \"\"\"\n",
        "            Сохраняет результат выполнения метода в список результатов.\n",
        "            \"\"\"\n",
        "            self.results.append({\n",
        "                \"method\": method_name,\n",
        "                \"text\": self.text,\n",
        "                \"result\": result,\n",
        "                \"language\": self.language  # Добавляем информацию о языке\n",
        "            })\n",
        "\n",
        "    def get_results(self):\n",
        "        \"\"\"\n",
        "        Возвращает все сохраненные результаты.\n",
        "        \"\"\"\n",
        "        return self.results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HTMLReportGenerator:\n",
        "    def __init__(self, results_list):\n",
        "        \"\"\"\n",
        "        Инициализирует генератор HTML-отчетов.\n",
        "\n",
        "        :param results_list: Список списков результатов из разных TextProcessor'ов.\n",
        "        \"\"\"\n",
        "        self.results = []\n",
        "        seen_results = set()  # Для отслеживания уникальности записей\n",
        "\n",
        "        for results in results_list:\n",
        "            for result in results:\n",
        "                # Добавляем столбец Language\n",
        "                result_with_language = {\n",
        "                    \"language\": result.get(\"language\", \"Unknown\"),\n",
        "                    \"method\": result[\"method\"],\n",
        "                    \"text\": result[\"text\"],\n",
        "                    \"result\": result[\"result\"]\n",
        "                }\n",
        "\n",
        "                # Проверяем уникальность записи\n",
        "                result_key = (result_with_language[\"language\"], result_with_language[\"method\"], result_with_language[\"text\"])\n",
        "                if result_key not in seen_results:\n",
        "                    self.results.append(result_with_language)\n",
        "                    seen_results.add(result_key)\n",
        "\n",
        "    def _create_html_content(self):\n",
        "        \"\"\"\n",
        "        Создает содержимое HTML-документа на основе результатов.\n",
        "        \"\"\"\n",
        "        html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Combined Text Processing Report</title>\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            margin: 20px;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #333;\n",
        "        }\n",
        "        table {\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "        th, td {\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 8px;\n",
        "            text-align: left;\n",
        "        }\n",
        "        th {\n",
        "            background-color: #f4f4f4;\n",
        "        }\n",
        "        img {\n",
        "            max-width: 100%;\n",
        "            height: auto;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Combined Text Processing Report</h1>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Language</th>\n",
        "            <th>Method</th>\n",
        "            <th>Text</th>\n",
        "            <th>Result</th>\n",
        "        </tr>\n",
        "\"\"\"\n",
        "        for result in self.results:\n",
        "            language = result[\"language\"]\n",
        "            method = result[\"method\"]\n",
        "            text = result[\"text\"].replace(\"\\n\", \"<br>\")  # Обрабатываем переносы строк\n",
        "            res = result[\"result\"]\n",
        "\n",
        "            if isinstance(res, dict) and \"image\" in res:  # Если результат содержит изображение\n",
        "                image_path = res[\"image\"]\n",
        "                res_html = f'<img src=\"{os.path.basename(image_path)}\" alt=\"{method}\">'\n",
        "            else:  # Иначе просто выводим текстовый результат\n",
        "                res_html = str(res).replace(\"\\n\", \"<br>\")\n",
        "\n",
        "            html_content += f\"\"\"\n",
        "        <tr>\n",
        "            <td>{language}</td>\n",
        "            <td>{method}</td>\n",
        "            <td>{text}</td>\n",
        "            <td>{res_html}</td>\n",
        "        </tr>\n",
        "\"\"\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "    </table>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "        return html_content\n",
        "\n",
        "    def generate_report(self, output_filename=\"combined_report.html\"):\n",
        "        \"\"\"\n",
        "        Генерирует и сохраняет HTML-отчет в файл.\n",
        "\n",
        "        :param output_filename: Имя файла для сохранения отчета.\n",
        "        \"\"\"\n",
        "        html_content = self._create_html_content()\n",
        "\n",
        "        # Определяем папку для сохранения изображений\n",
        "        images_dir = \"images\"\n",
        "        os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "        # Копируем все используемые изображения в папку images\n",
        "        for result in self.results:\n",
        "            if isinstance(result[\"result\"], dict) and \"image\" in result[\"result\"]:\n",
        "                image_path = result[\"result\"][\"image\"]\n",
        "                new_image_path = os.path.join(images_dir, os.path.basename(image_path))\n",
        "                os.replace(image_path, new_image_path)  # Перемещаем файл\n",
        "\n",
        "        # Записываем HTML-контент в файл\n",
        "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        print(f\"Общий отчет успешно сгенерирован и сохранен в файле {output_filename}.\")"
      ],
      "metadata": {
        "id": "tHVhqD0Q8mAm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    text_en = \"Hello, world! This is an example of text processing in English.\"\n",
        "    text_ru = \"Привет, мир! Это пример обработки текста на русском языке.\"\n",
        "    text_de = \"Hallo Welt! Dies ist ein Beispiel für Textverarbeitung auf Deutsch.\"\n",
        "\n",
        "    processor_en = TextProcessor(text_en, language='en')\n",
        "    processor_ru = TextProcessor(text_ru, language='ru')\n",
        "    processor_de = TextProcessor(text_de, language='de')\n",
        "\n",
        "    # Выполнение всех методов для каждого процессора\n",
        "    processor_en.execute_all_methods()\n",
        "    processor_ru.execute_all_methods()\n",
        "    processor_de.execute_all_methods()\n",
        "\n",
        "    # Получение результатов для каждого процессора\n",
        "    results_en = processor_en.get_results()\n",
        "    results_ru = processor_ru.get_results()\n",
        "    results_de = processor_de.get_results()\n",
        "\n",
        "    # Объединение результатов\n",
        "    all_results = [results_en, results_ru, results_de]\n",
        "\n",
        "    # Создание и генерация общего HTML-отчета\n",
        "    report_generator = HTMLReportGenerator(all_results)\n",
        "    report_generator.generate_report(\"combined_text_processing_report.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqkLJXdT9648",
        "outputId": "5140a6a9-8fa2-46e8-b5f6-f1e20b11a838"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming is not recommended for Russian. Using lemmatization instead.\n",
            "Общий отчет успешно сгенерирован и сохранен в файле combined_text_processing_report.html.\n"
          ]
        }
      ]
    }
  ]
}