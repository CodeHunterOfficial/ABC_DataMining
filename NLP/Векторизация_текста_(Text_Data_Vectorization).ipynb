{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/ZO7LBDNyFRRN+jax+Xsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0_(Text_Data_Vectorization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Введение в векторизацию текста**\n",
        "\n",
        "## **1.1. Почему машины не могут напрямую работать с текстом?**\n",
        "\n",
        "Машинное обучение и искусственный интеллект основываются на математических моделях, которые требуют числовых данных для выполнения вычислений. Текстовые данные, будучи последовательностями символов, букв или слов, изначально не подходят для непосредственного использования в алгоритмах машинного обучения. Это связано с несколькими ключевыми причинами:\n",
        "\n",
        "1. **Несовместимость с числовыми операциями**: Алгоритмы машинного обучения выполняют арифметические операции, такие как сложение, умножение и скалярные произведения. Эти операции невозможны для строковых данных.\n",
        "\n",
        "2. **Отсутствие структуры**: Тексты имеют переменную длину (например, предложение может состоять из 5 или 50 слов), что затрудняет их использование в алгоритмах, требующих фиксированной размерности входных данных.\n",
        "\n",
        "3. **Амбивалентность семантики**: Одно и то же слово может иметь разные значения в зависимости от контекста. Например, слово \"банк\" может означать финансовое учреждение или берег реки. Машины не способны интерпретировать такую многозначность без дополнительных механизмов.\n",
        "\n",
        "4. **Необходимость метрик близости**: Для многих задач, таких как классификация или кластеризация, важно уметь измерять схожесть между объектами. Строковые данные не позволяют это делать напрямую, поскольку нет естественной метрики расстояния между словами или предложениями.\n",
        "\n",
        "В результате, перед тем как использовать текстовые данные в алгоритмах машинного обучения, необходимо преобразовать их в числовые представления, которые сохраняют важную информацию о содержании текста и позволяют выполнять необходимые операции.\n",
        "\n",
        "\n",
        "\n",
        "## **1.2. Что такое векторизация?**\n",
        "\n",
        "Векторизация — это процесс преобразования текстовых данных в числовые векторы фиксированной длины. Каждый текстовый элемент (слово, предложение или документ) представляется в виде набора чисел, который можно использовать в качестве входных данных для алгоритмов машинного обучения. Векторы обычно содержат информацию о частоте встречаемости слов, их контексте или семантическом значении.\n",
        "\n",
        "Основная цель векторизации — обеспечить возможность работы с текстом в числовом пространстве, где можно применять математические операции, такие как вычисление расстояний между векторами, поиск подобных элементов или определение наиболее важных признаков.\n",
        "\n",
        "Пример:  \n",
        "Пусть имеется два слова: \"кошка\" и \"собака\". Без векторизации мы не можем сравнить их напрямую. Однако после векторизации они могут быть представлены числами:\n",
        "- \"кошка\" → [0.8, -0.5, 0.3]\n",
        "- \"собака\" → [0.7, -0.6, 0.2]\n",
        "\n",
        "Теперь мы можем вычислить расстояние между этими векторами, чтобы определить степень их схожести.\n",
        "\n",
        "\n",
        "\n",
        "## **1.3. Основные требования к векторам**\n",
        "\n",
        "Чтобы векторы были полезными для алгоритмов машинного обучения, они должны соответствовать следующим требованиям:\n",
        "\n",
        "1. **Фиксированная длина**: Все векторы должны иметь одинаковое количество компонентов. Это необходимо для совместимости с алгоритмами, которые ожидают однородный формат входных данных.\n",
        "\n",
        "2. **Численное представление**: Каждая компонента вектора должна быть числом, что позволяет выполнять математические операции.\n",
        "\n",
        "3. **Семантическая информативность**: Вектор должен отражать смысл исходного текстового элемента. Например, векторы похожих слов (\"кошка\" и \"кот\") должны быть близкими, а векторы различных слов (\"кошка\" и \"автомобиль\") — далекими.\n",
        "\n",
        "4. **Эффективность хранения и обработки**: Векторы должны быть компактными и эффективными для хранения и вычислений, особенно при работе с большими объемами данных.\n",
        "\n",
        "\n",
        "\n",
        "## **1.4. Примеры преобразования текста в числовой формат**\n",
        "\n",
        "Для лучшего понимания рассмотрим пример преобразования нескольких слов и коротких предложений в числовой формат вручную.\n",
        "\n",
        "### **Пример 1: Преобразование слов**\n",
        "\n",
        "Пусть имеется словарь из трех слов: {\"кошка\", \"собака\", \"дерево\"}. Мы можем создать числовой код для каждого слова, используя простой порядковый номер:\n",
        "- \"кошка\" → 1\n",
        "- \"собака\" → 2\n",
        "- \"дерево\" → 3\n",
        "\n",
        "Однако такой подход имеет ограничения, так как он не учитывает контекст или схожесть слов.\n",
        "\n",
        "### **Пример 2: Преобразование предложений**\n",
        "\n",
        "Рассмотрим два коротких предложения:\n",
        "1. \"Кошка спит на столе.\"\n",
        "2. \"Собака играет во дворе.\"\n",
        "\n",
        "Мы можем преобразовать каждое предложение в числовой формат, используя простой подсчет частоты слов. Предположим, что наш словарь состоит из пяти уникальных слов: {\"кошка\", \"спит\", \"на\", \"собака\", \"играет\"}. Каждому слову присваивается индекс:\n",
        "- \"кошка\" → 1\n",
        "- \"спит\" → 2\n",
        "- \"на\" → 3\n",
        "- \"собака\" → 4\n",
        "- \"играет\" → 5\n",
        "\n",
        "Теперь каждое предложение можно представить в виде вектора, где каждый элемент соответствует частоте встречаемости слова в предложении:\n",
        "- \"Кошка спит на столе.\" → [1, 1, 1, 0, 0] (слово \"столе\" отсутствует в словаре)\n",
        "- \"Собака играет во дворе.\" → [0, 0, 0, 1, 1] (слова \"во\" и \"дворе\" также отсутствуют)\n",
        "\n",
        "\n",
        "\n",
        "## **1.5. Практическое задание**\n",
        "\n",
        "### **Задание 1: Преобразование слов**\n",
        "Преобразуйте следующие слова в числовой формат, используя простой порядковый номер:\n",
        "- \"яблоко\"\n",
        "- \"груша\"\n",
        "- \"апельсин\"\n",
        "\n",
        "**Решение**:\n",
        "- \"яблоко\" → 1\n",
        "- \"груша\" → 2\n",
        "- \"апельсин\" → 3\n",
        "\n",
        "\n",
        "\n",
        "## **Заключение**\n",
        "\n",
        "Векторизация текста является ключевым этапом подготовки данных для машинного обучения. Она позволяет преобразовать текстовые данные в числовые представления, которые можно использовать в алгоритмах классификации, кластеризации, поиска и других задачах. В дальнейших разделах мы рассмотрим различные методы векторизации, начиная от простых (One-Hot Encoding, Bag of Words) до более сложных (Word Embeddings, BERT).\n",
        "\n",
        "\n",
        "\n",
        "# **2. One-Hot Encoding: Математические основы**\n",
        "\n",
        "## **2.1. Что такое One-Hot Encoding?**\n",
        "\n",
        "One-Hot Encoding — это метод преобразования категориальных данных (например, слов или меток классов) в числовые векторы фиксированной длины. Каждый уникальный элемент из исходного набора данных кодируется в виде вектора, где только один элемент равен 1, а остальные равны 0. Это позволяет представить дискретные категории в форме, подходящей для использования в алгоритмах машинного обучения.\n",
        "\n",
        "Основная идея One-Hot Encoding заключается в том, чтобы создать отдельную бинарную переменную (или \"размерность\") для каждого уникального значения в наборе данных. Например, если у нас есть три уникальных слова: \"кошка\", \"собака\" и \"дерево\", то каждое слово будет представлено вектором размерности 3, где только одна компонента равна 1, а остальные равны 0.\n",
        "\n",
        "\n",
        "\n",
        "## **2.2. Математическое описание процесса**\n",
        "\n",
        "Пусть имеется множество уникальных категорий $ C = \\{c_1, c_2, \\dots, c_n\\} $, где $ n $ — общее количество уникальных значений. Для каждого элемента $ c_i \\in C $ мы создаем вектор $ \\mathbf{v}_i $ длины $ n $, такой что:\n",
        "\n",
        "$$\n",
        "v_{ij} =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } j = i \\\\\n",
        "0, & \\text{в противном случае}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ v_{ij} $ — значение $ j $-го элемента вектора $ \\mathbf{v}_i $,\n",
        "- $ i $ — индекс категории $ c_i $,\n",
        "- $ j $ — позиция в векторе.\n",
        "\n",
        "Таким образом, каждый элемент множества $ C $ получает свой уникальный вектор, состоящий из нулей и одной единицы.\n",
        "\n",
        "### **Пример математического представления**\n",
        "Для множества $ C = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}\\} $:\n",
        "- \"кошка\" → $ \\mathbf{v}_1 = [1, 0, 0] $,\n",
        "- \"собака\" → $ \\mathbf{v}_2 = [0, 1, 0] $,\n",
        "- \"дерево\" → $ \\mathbf{v}_3 = [0, 0, 1] $.\n",
        "\n",
        "\n",
        "\n",
        "## **2.3. Формализация процесса One-Hot Encoding**\n",
        "\n",
        "Процесс One-Hot Encoding можно формализовать как функцию $ f : C \\to \\mathbb{R}^n $, где:\n",
        "- $ C $ — множество уникальных категорий,\n",
        "- $ n = |C| $ — количество уникальных категорий,\n",
        "- $ \\mathbb{R}^n $ — пространство векторов размерности $ n $.\n",
        "\n",
        "Функция $ f(c_i) $ определяется следующим образом:\n",
        "$$\n",
        "f(c_i) = \\mathbf{v}_i = [v_{i1}, v_{i2}, \\dots, v_{in}]\n",
        "$$\n",
        "где:\n",
        "$$\n",
        "v_{ij} =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } j = i \\\\\n",
        "0, & \\text{в противном случае}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Эта функция является инъекцией (однозначным отображением), так как каждому уникальному элементу $ c_i $ соответствует уникальный вектор $ \\mathbf{v}_i $.\n",
        "\n",
        "\n",
        "\n",
        "## **2.4. Преобразование текста с помощью One-Hot Encoding**\n",
        "\n",
        "При применении One-Hot Encoding к текстовым данным каждое уникальное слово из корпуса текста рассматривается как категория. Таким образом, весь корпус текста преобразуется в матрицу, где:\n",
        "- Каждая строка соответствует одному текстовому элементу (например, слову или предложению),\n",
        "- Каждый столбец соответствует уникальному слову в словаре.\n",
        "\n",
        "Если словарь содержит $ n $ уникальных слов, то каждое слово будет представлено вектором размерности $ n $.\n",
        "\n",
        "### **Математическая интерпретация текстового преобразования**\n",
        "Пусть имеется текст $ T = \\{w_1, w_2, \\dots, w_m\\} $, где $ m $ — количество слов в тексте. Для каждого слова $ w_k \\in T $ строится вектор $ \\mathbf{v}_k $ по правилу One-Hot Encoding. В результате текст $ T $ преобразуется в матрицу $ M \\in \\mathbb{R}^{m \\times n} $, где:\n",
        "- $ m $ — количество слов в тексте,\n",
        "- $ n $ — количество уникальных слов в словаре.\n",
        "\n",
        "Каждая строка матрицы $ M $ представляет одно слово из текста.\n",
        "\n",
        "\n",
        "\n",
        "## **2.5. Проблемы One-Hot Encoding**\n",
        "\n",
        "\n",
        "### **2.5.1. Разреженность векторов**\n",
        "One-Hot Encoding приводит к созданию разреженных векторов, где большинство компонент равны 0. Это связано с тем, что каждый вектор содержит только одну единицу, а остальные элементы заполнены нулями. Разреженность может вызывать проблемы при работе с большими объемами данных, так как требуется много памяти для хранения этих векторов.\n",
        "\n",
        "\n",
        "Предположим, что у нас есть словарь из 10 уникальных слов:  \n",
        "$$ \\text{словарь} = \\{\\text{\"яблоко\", \"банан\", \"груша\", \"арбуз\", \"виноград\", \"апельсин\", \"мандарин\", \"киви\", \"персик\", \"абрикос\"}\\} $$\n",
        "\n",
        "Если мы хотим закодировать слово \"груша\" с помощью One-Hot Encoding, получим следующий вектор размерности 10:\n",
        "$$\n",
        "\\text{вектор(\"груша\")} = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "$$\n",
        "\n",
        "Здесь только один элемент равен 1 (третий элемент), а остальные — нули. Если словарь будет содержать 1000 слов, то вектор станет размером 1000, но все равно будет иметь только одну единицу и 999 нулей.\n",
        "\n",
        "**Пример разреженности:**  \n",
        "- Размер словаря $ n = 1000 $\n",
        "- Количество ненулевых элементов в каждом векторе = 1\n",
        "- Процент заполнения: $ \\frac{1}{1000} \\times 100\\% = 0.1\\% $\n",
        "\n",
        "Такая разреженность приводит к неэффективному использованию памяти и может замедлить вычисления.\n",
        "\n",
        "\n",
        "\n",
        "### **2.5.2. Высокая размерность**\n",
        "\n",
        "Если словарь содержит большое количество уникальных слов ($ n $ велико), то размерность векторов также становится большой. Это может затруднять работу с данными, так как многие алгоритмы плохо масштабируются с увеличением размерности.\n",
        "\n",
        "\n",
        "Рассмотрим другой пример, где словарь содержит уже 10000 уникальных слов. Если мы кодируем каждое слово через One-Hot Encoding, то каждый вектор будет размерности 10000.\n",
        "\n",
        "**Пример высокой размерности:**  \n",
        "- Размер словаря $ n = 10000 $\n",
        "- Размерность каждого вектора = 10000\n",
        "- Если набор данных содержит 1000 слов, то общее количество элементов в матрице признаков:  \n",
        "$$ 1000 \\times 10000 = 10^7 $$\n",
        "\n",
        "Такое большое количество элементов затрудняет работу алгоритмов машинного обучения, особенно тех, которые плохо масштабируются с ростом размерности данных.\n",
        "\n",
        "\n",
        "\n",
        "### **2.5.3. Отсутствие семантической информации**\n",
        "\n",
        "One-Hot Encoding не учитывает семантическую связь между словами. Например, векторы для слов \"кошка\" и \"собака\" будут совершенно различными, хотя эти слова имеют схожее значение. Это ограничивает возможности метода в задачах, требующих учета контекста или смысловых отношений.\n",
        "\n",
        "\n",
        "\n",
        "Пусть словарь содержит слова \"кошка\", \"собака\", \"стол\", \"стул\". Мы кодируем эти слова через One-Hot Encoding:\n",
        "\n",
        "$$\n",
        "\\text{вектор(\"кошка\")} = [1, 0, 0, 0]\n",
        "$$\n",
        "$$\n",
        "\\text{вектор(\"собака\")} = [0, 1, 0, 0]\n",
        "$$\n",
        "$$\n",
        "\\text{вектор(\"стол\")} = [0, 0, 1, 0]\n",
        "$$\n",
        "$$\n",
        "\\text{вектор(\"стул\")} = [0, 0, 0, 1]\n",
        "$$\n",
        "\n",
        "Видно, что векторы для \"кошки\" и \"собаки\" полностью различны, хотя эти слова имеют схожее значение (оба относятся к животным). Аналогично, векторы для \"стола\" и \"стула\" также различны, хотя они оба относятся к мебели.\n",
        "\n",
        "**Метрика расстояния между векторами:**  \n",
        "Если посчитать Евклидово расстояние между векторами \"кошки\" и \"собаки\":\n",
        "$$\n",
        "d(\\text{\"кошка\"}, \\text{\"собака\"}) = \\sqrt{(1-0)^2 + (0-1)^2 + (0-0)^2 + (0-0)^2} = \\sqrt{2}\n",
        "$$\n",
        "\n",
        "Аналогично, расстояние между \"столом\" и \"стулом\" также будет $ \\sqrt{2} $. Таким образом, One-Hot Encoding не отражает реальных смысловых связей между словами.\n",
        "\n",
        "\n",
        "\n",
        "### **Итоговый пример всех недостатков**\n",
        "\n",
        "Пусть словарь содержит 5000 уникальных слов. Мы хотим закодировать предложение из 50 слов.  \n",
        "\n",
        "1. **Разреженность:**  \n",
        "   Каждый вектор будет размерности 5000, но только 1 элемент будет равен 1, а остальные — 0. Процент заполнения:  \n",
        "   $$\n",
        "   \\frac{1}{5000} \\times 100\\% = 0.02\\%\n",
        "   $$\n",
        "\n",
        "2. **Высокая размерность:**  \n",
        "   Для всего предложения потребуется матрица размера $ 50 \\times 5000 = 250000 $ элементов.\n",
        "\n",
        "3. **Отсутствие семантики:**  \n",
        "   Если в предложении встречаются слова \"кошка\" и \"собака\", их векторы будут совершенно различными, хотя эти слова близки по смыслу.\n",
        "\n",
        "\n",
        "# **2. One-Hot Encoding: Конкретные числовые примеры**\n",
        "\n",
        "В предыдущем разделе мы рассмотрели математические основы One-Hot Encoding. Теперь давайте разберем конкретные числовые примеры, чтобы лучше понять процесс преобразования категориальных данных в числовые векторы.\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 1: Преобразование уникальных слов**\n",
        "\n",
        "Пусть у нас есть следующее множество уникальных слов:\n",
        "$$\n",
        "C = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}\\}.\n",
        "$$\n",
        "\n",
        "### **Шаг 1: Определение размерности**\n",
        "Множество содержит $ n = 3 $ уникальных элемента. Следовательно, каждый вектор будет иметь размерность 3.\n",
        "\n",
        "### **Шаг 2: Создание векторов**\n",
        "Каждому уникальному слову соответствует вектор длины 3, где только одна компонента равна 1, а остальные — 0:\n",
        "- \"кошка\" → $ [1, 0, 0] $,\n",
        "- \"собака\" → $ [0, 1, 0] $,\n",
        "- \"дерево\" → $ [0, 0, 1] $.\n",
        "\n",
        "### **Матричное представление**\n",
        "Если объединить все векторы в одну матрицу, получим следующую таблицу:\n",
        "\n",
        "| Слово    | Вектор          |\n",
        "|----------|-----------------|\n",
        "| кошка    | $ [1, 0, 0] $  |\n",
        "| собака   | $ [0, 1, 0] $  |\n",
        "| дерево   | $ [0, 0, 1] $  |\n",
        "\n",
        "\n",
        "## **Пример 2: Преобразование текста**\n",
        "\n",
        "Теперь рассмотрим более сложный случай — преобразование целого текста. Пусть исходный текст состоит из нескольких слов:\n",
        "$$\n",
        "T = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}, \\text{\"кошка\"}, \\text{\"дерево\"}\\}.\n",
        "$$\n",
        "\n",
        "### **Шаг 1: Создание словаря**\n",
        "Сначала создаем словарь уникальных слов:\n",
        "$$\n",
        "C = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}\\}.\n",
        "$$\n",
        "\n",
        "Размерность векторов будет равна $ n = 3 $.\n",
        "\n",
        "### **Шаг 2: Преобразование каждого слова**\n",
        "Для каждого слова из текста $ T $ строим соответствующий вектор:\n",
        "- \"кошка\" → $ [1, 0, 0] $,\n",
        "- \"собака\" → $ [0, 1, 0] $,\n",
        "- \"дерево\" → $ [0, 0, 1] $,\n",
        "- \"кошка\" → $ [1, 0, 0] $,\n",
        "- \"дерево\" → $ [0, 0, 1] $.\n",
        "\n",
        "### **Шаг 3: Матричное представление текста**\n",
        "Объединяем все векторы в матрицу $ M $, где каждая строка соответствует одному слову из текста:\n",
        "\n",
        "$$\n",
        "M =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1 \\\\\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- Количество строк ($ m = 5 $) равно количеству слов в тексте,\n",
        "- Количество столбцов ($ n = 3 $) равно количеству уникальных слов в словаре.\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 3: Преобразование категорий**\n",
        "\n",
        "One-Hot Encoding также можно применять к категориальным данным, например, меткам классов. Пусть имеется набор данных с тремя категориями:\n",
        "$$\n",
        "D = \\{\\text{\"красный\"}, \\text{\"зеленый\"}, \\text{\"синий\"}, \\text{\"зеленый\"}, \\text{\"красный\"}\\}.\n",
        "$$\n",
        "\n",
        "### **Шаг 1: Создание словаря**\n",
        "Создаем словарь уникальных категорий:\n",
        "$$\n",
        "C = \\{\\text{\"красный\"}, \\text{\"зеленый\"}, \\text{\"синий\"}\\}.\n",
        "$$\n",
        "\n",
        "Размерность векторов будет равна $ n = 3 $.\n",
        "\n",
        "### **Шаг 2: Преобразование каждой категории**\n",
        "Для каждой категории строим соответствующий вектор:\n",
        "- \"красный\" → $ [1, 0, 0] $,\n",
        "- \"зеленый\" → $ [0, 1, 0] $,\n",
        "- \"синий\" → $ [0, 0, 1] $,\n",
        "- \"зеленый\" → $ [0, 1, 0] $,\n",
        "- \"красный\" → $ [1, 0, 0] $.\n",
        "\n",
        "### **Шаг 3: Матричное представление**\n",
        "Объединяем все векторы в матрицу $ M $:\n",
        "\n",
        "$$\n",
        "M =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "1 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 4: Разреженность и высокая размерность**\n",
        "\n",
        "Рассмотрим случай, когда словарь содержит много уникальных слов. Пусть:\n",
        "$$\n",
        "C = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}, \\text{\"стол\"}, \\text{\"книга\"}, \\dots, \\text{\"автомобиль\"}\\},\n",
        "$$\n",
        "где $ n = 100 $ (100 уникальных слов).\n",
        "\n",
        "Если текст состоит из одного слова, например \"кошка\", то его One-Hot вектор будет выглядеть так:\n",
        "$$\n",
        "[1, 0, 0, 0, 0, \\dots, 0],\n",
        "$$\n",
        "где всего 100 компонент, и только первая равна 1.\n",
        "\n",
        "Этот пример демонстрирует проблему разреженности: большинство компонент вектора равны 0, что требует дополнительной памяти для хранения данных.\n",
        "\n",
        "\n",
        "\n",
        "## **Заключение**\n",
        "\n",
        "One-Hot Encoding позволяет эффективно преобразовать категориальные данные в числовые векторы. Однако при работе с большими словарями возникают проблемы разреженности и высокой размерности. Эти недостатки становятся особенно заметными при обработке текстовых данных, где количество уникальных слов может быть очень велико. В следующих разделах мы рассмотрим более продвинутые методы векторизации, которые решают эти проблемы.\n",
        "\n",
        "**Числовые примеры:**\n",
        "1. Для множества $ C = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}\\} $:\n",
        "   - \"кошка\" → $ [1, 0, 0] $,\n",
        "   - \"собака\" → $ [0, 1, 0] $,\n",
        "   - \"дерево\" → $ [0, 0, 1] $.\n",
        "2. Для текста $ T = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}, \\text{\"кошка\"}, \\text{\"дерево\"}\\} $:\n",
        "   $$\n",
        "   M =\n",
        "   \\begin{bmatrix}\n",
        "   1 & 0 & 0 \\\\\n",
        "   0 & 1 & 0 \\\\\n",
        "   0 & 0 & 1 \\\\\n",
        "   1 & 0 & 0 \\\\\n",
        "   0 & 0 & 1\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "   \n",
        " # **Реализация One-Hot Encoding на Python**\n",
        "\n",
        "Мы можем реализовать One-Hot Encoding в Python несколькими способами. Рассмотрим два основных подхода:\n",
        "\n",
        "1. **С использованием библиотеки `pandas`**.\n",
        "2. **С использованием библиотеки `scikit-learn`**.\n",
        "\n",
        "Для примера будем работать с набором данных, состоящим из слов: `[\"кошка\", \"собака\", \"дерево\", \"кошка\", \"дерево\"]`.\n",
        "\n",
        "\n",
        "\n",
        "## **1. Реализация One-Hot Encoding с помощью `pandas`**\n",
        "\n",
        "Библиотека `pandas` предоставляет простой метод `get_dummies`, который автоматически создает One-Hot векторы для категориальных данных.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Исходные данные\n",
        "data = [\"кошка\", \"собака\", \"дерево\", \"кошка\", \"дерево\"]\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"слово\"])\n",
        "\n",
        "# Применяем One-Hot Encoding\n",
        "one_hot_encoded = pd.get_dummies(df[\"слово\"])\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"One-Hot Encoding с помощью pandas:\")\n",
        "print(one_hot_encoded)\n",
        "```\n",
        "\n",
        "\n",
        "## **2. Реализация One-Hot Encoding с помощью `scikit-learn`**\n",
        "\n",
        "Библиотека `scikit-learn` предлагает класс `OneHotEncoder`, который более гибкий и часто используется в задачах машинного обучения.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "data = np.array([\"кошка\", \"собака\", \"дерево\", \"кошка\", \"дерево\"]).reshape(-1, 1)\n",
        "\n",
        "# Создаем объект OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Устанавливаем sparse_output=False для получения массива NumPy\n",
        "\n",
        "# Применяем One-Hot Encoding\n",
        "one_hot_encoded = encoder.fit_transform(data)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"One-Hot Encoding с помощью scikit-learn:\")\n",
        "print(one_hot_encoded)\n",
        "```\n",
        "\n",
        "\n",
        "## **3. Сравнение двух подходов**\n",
        "\n",
        "| **Параметр**         | **Pandas**                              | **Scikit-learn**                       |\n",
        "|-----------------------|-----------------------------------------|----------------------------------------|\n",
        "| **Простота использования** | Очень простой, один вызов `get_dummies` | Требует создания объекта `OneHotEncoder` |\n",
        "| **Гибкость**          | Ограниченная                            | Более гибкая (например, можно контролировать порядок категорий) |\n",
        "| **Формат выходных данных** | DataFrame                              | NumPy массив                          |\n",
        "\n",
        "\n",
        "\n",
        "## **4. Дополнительный пример: Обработка нескольких категорий**\n",
        "\n",
        "Если у нас есть несколько категориальных признаков, то можно применить One-Hot Encoding к каждому из них.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Исходные данные с несколькими категориями\n",
        "data = pd.DataFrame({\n",
        "    \"животное\": [\"кошка\", \"собака\", \"дерево\", \"кошка\", \"дерево\"],\n",
        "    \"цвет\": [\"красный\", \"зеленый\", \"синий\", \"красный\", \"синий\"]\n",
        "})\n",
        "\n",
        "# Метод 1: Использование pandas\n",
        "one_hot_encoded_pandas = pd.get_dummies(data)\n",
        "print(\"One-Hot Encoding с помощью pandas для нескольких категорий:\")\n",
        "print(one_hot_encoded_pandas)\n",
        "\n",
        "# Метод 2: Использование scikit-learn\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_encoded_sklearn = encoder.fit_transform(data)\n",
        "print(\"\\nOne-Hot Encoding с помощью scikit-learn для нескольких категорий:\")\n",
        "print(one_hot_encoded_sklearn)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **3. Bag of Words (BoW): Математические основы**\n",
        "\n",
        "## **3.1. Что такое Bag of Words (BoW)?**\n",
        "\n",
        "Bag of Words (BoW) — это метод векторизации текста, который представляет документ как мешок слов без учета порядка их следования. Каждый документ кодируется в виде вектора, где каждая компонента соответствует частоте встречаемости определенного слова из заранее созданного словаря.\n",
        "\n",
        "Основная идея BoW заключается в том, что семантика документа может быть выражена через частоту использования различных слов. Этот подход игнорирует грамматическую структуру и синтаксис, фокусируясь только на содержании.\n",
        "\n",
        "\n",
        "\n",
        "## **3.2. Создание словаря слов**\n",
        "\n",
        "Словарь — это множество уникальных слов, которые встречаются во всем корпусе текстов. Процесс создания словаря можно формализовать следующим образом:\n",
        "\n",
        "Пусть $ D = \\{d_1, d_2, \\dots, d_m\\} $ — набор документов, где каждый документ $ d_i $ является последовательностью слов:\n",
        "$$\n",
        "d_i = \\{w_{i1}, w_{i2}, \\dots, w_{in_i}\\},\n",
        "$$\n",
        "где $ n_i $ — количество слов в документе $ d_i $.\n",
        "\n",
        "Словарь $ V $ строится как объединение всех уникальных слов из всех документов:\n",
        "$$\n",
        "V = \\bigcup_{i=1}^m \\{w_{i1}, w_{i2}, \\dots, w_{in_i}\\}.\n",
        "$$\n",
        "\n",
        "Таким образом, размер словаря $ |V| = n $ равен количеству уникальных слов во всем корпусе.\n",
        "\n",
        "\n",
        "\n",
        "## **3.3. Построение вектора на основе частоты встречаемости слов**\n",
        "\n",
        "Каждый документ $ d_i $ преобразуется в вектор $ \\mathbf{v}_i \\in \\mathbb{R}^n $, где $ n = |V| $ — размер словаря. Компоненты вектора $ \\mathbf{v}_i $ соответствуют частотам встречаемости слов из словаря $ V $ в документе $ d_i $. Формально, если $ f(w, d_i) $ обозначает частоту слова $ w $ в документе $ d_i $, то вектор $ \\mathbf{v}_i $ определяется как:\n",
        "$$\n",
        "\\mathbf{v}_i = [f(w_1, d_i), f(w_2, d_i), \\dots, f(w_n, d_i)],\n",
        "$$\n",
        "где $ w_1, w_2, \\dots, w_n $ — слова из словаря $ V $.\n",
        "\n",
        "### **Пример математического представления**\n",
        "Пусть словарь содержит три уникальных слова: $ V = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}\\} $. Если документ $ d_1 = \\{\\text{\"кошка\"}, \\text{\"собака\"}, \\text{\"дерево\"}, \\text{\"кошка\"}\\} $, то его вектор будет выглядеть так:\n",
        "- Частота \"кошка\" = 2,\n",
        "- Частота \"собака\" = 1,\n",
        "- Частота \"дерево\" = 1.\n",
        "\n",
        "Следовательно:\n",
        "$$\n",
        "\\mathbf{v}_1 = [2, 1, 1].\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## **3.4. Математическая интерпретация корпуса текстов**\n",
        "\n",
        "Если corpus состоит из $ m $ документов, то весь корпус можно представить в виде матрицы $ M \\in \\mathbb{R}^{m \\times n} $, где:\n",
        "- $ m $ — количество документов,\n",
        "- $ n $ — размер словаря.\n",
        "\n",
        "Каждая строка матрицы $ M $ соответствует одному документу, а каждый столбец — одной уникальной категории (слову).\n",
        "\n",
        "Матрица $ M $ называется **матрицей терм-документ** (Term-Document Matrix). Ее элементы $ M_{ij} $ представляют собой частоту слова $ w_j $ в документе $ d_i $:\n",
        "$$\n",
        "M_{ij} = f(w_j, d_i).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## **3.5. Преобразование текста с помощью BoW**\n",
        "\n",
        "Процесс преобразования текста с помощью BoW можно разбить на несколько этапов:\n",
        "\n",
        "1. **Токенизация**: Разбиение текста на отдельные слова (токены).\n",
        "2. **Лемматизация/стемминг**: Приведение слов к нормальной форме для уменьшения размерности словаря.\n",
        "3. **Удаление стоп-слов**: Исключение часто встречающихся слов (например, \"и\", \"в\", \"на\"), которые не несут смысловой нагрузки.\n",
        "4. **Подсчет частот**: Подсчет количества вхождений каждого слова из словаря в документ.\n",
        "5. **Нормализация (необязательно)**: Приведение векторов к единой шкале (например, деление на общее количество слов в документе).\n",
        "\n",
        "\n",
        "\n",
        "## **3.6. Преимущества BoW**\n",
        "\n",
        "1. **Простота реализации**: BoW легко понять и реализовать.\n",
        "2. **Эффективность для многих задач**: Хорошо работает для задач классификации текстов, таких как спам-фильтрация или анализ тональности.\n",
        "3. **Фиксированная размерность**: Все документы преобразуются в векторы одинаковой длины, что удобно для алгоритмов машинного обучения.\n",
        "\n",
        "\n",
        "\n",
        "## **3.7. Недостатки BoW**\n",
        "\n",
        "1. **Игнорирование порядка слов**: BoW не учитывает грамматическую структуру и синтаксис текста, что может привести к потере важной информации.\n",
        "2. **Высокая размерность**: Размер словаря может быть очень большим, особенно для больших корпусов текстов, что увеличивает объем данных и замедляет вычисления.\n",
        "3. **Разреженность векторов**: Большинство компонент вектора часто равны нулю, так как большинство слов из словаря могут не встречаться в конкретном документе.\n",
        "4. **Отсутствие учета контекста**: BoW не учитывает контекст использования слов, что ограничивает его способность отражать семантические отношения между словами.\n",
        "\n",
        "\n",
        "\n",
        "# **3. Bag of Words (BoW): Конкретные числовые примеры**\n",
        "\n",
        "В предыдущем разделе мы рассмотрели математические основы метода Bag of Words (BoW). Теперь давайте разберем конкретные числовые примеры, чтобы лучше понять процесс преобразования текстов в числовые векторы.\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 1: Создание словаря и векторизация одного документа**\n",
        "\n",
        "Пусть у нас есть следующий документ:\n",
        "$$\n",
        "d_1 = \\{\\text{\"кошка\"}, \\text{\"спит\"}, \\text{\"на\"}, \\text{\"столе\"}, \\text{\"кошка\"}\\}.\n",
        "$$\n",
        "\n",
        "### **Шаг 1: Создание словаря**\n",
        "Сначала создаем словарь уникальных слов из документа $ d_1 $. Уникальные слова:\n",
        "$$\n",
        "V = \\{\\text{\"кошка\"}, \\text{\"спит\"}, \\text{\"на\"}, \\text{\"столе\"}\\}.\n",
        "$$\n",
        "\n",
        "Размер словаря $ |V| = 4 $.\n",
        "\n",
        "### **Шаг 2: Подсчет частот встречаемости**\n",
        "Для каждого слова из словаря подсчитываем, сколько раз оно встречается в документе $ d_1 $:\n",
        "- \"кошка\" → 2,\n",
        "- \"спит\" → 1,\n",
        "- \"на\" → 1,\n",
        "- \"столе\" → 1.\n",
        "\n",
        "Таким образом, вектор для документа $ d_1 $ будет выглядеть так:\n",
        "$$\n",
        "\\mathbf{v}_1 = [2, 1, 1, 1].\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 2: Векторизация нескольких документов**\n",
        "\n",
        "Теперь рассмотрим корпус из двух документов:\n",
        "$$\n",
        "D = \\{d_1, d_2\\},\n",
        "$$\n",
        "где:\n",
        "$$\n",
        "d_1 = \\{\\text{\"кошка\"}, \\text{\"спит\"}, \\text{\"на\"}, \\text{\"столе\"}, \\text{\"кошка\"}\\},\n",
        "$$\n",
        "$$\n",
        "d_2 = \\{\\text{\"собака\"}, \\text{\"играет\"}, \\text{\"во\"}, \\text{\"дворе\"}\\}.\n",
        "$$\n",
        "\n",
        "### **Шаг 1: Создание общего словаря**\n",
        "Общий словарь $ V $ создается как объединение всех уникальных слов из обоих документов:\n",
        "$$\n",
        "V = \\{\\text{\"кошка\"}, \\text{\"спит\"}, \\text{\"на\"}, \\text{\"столе\"}, \\text{\"собака\"}, \\text{\"играет\"}, \\text{\"во\"}, \\text{\"дворе\"}\\}.\n",
        "$$\n",
        "\n",
        "Размер словаря $ |V| = 8 $.\n",
        "\n",
        "### **Шаг 2: Подсчет частот встречаемости для каждого документа**\n",
        "\n",
        "#### Для документа $ d_1 $:\n",
        "- \"кошка\" → 2,\n",
        "- \"спит\" → 1,\n",
        "- \"на\" → 1,\n",
        "- \"столе\" → 1,\n",
        "- \"собака\" → 0,\n",
        "- \"играет\" → 0,\n",
        "- \"во\" → 0,\n",
        "- \"дворе\" → 0.\n",
        "\n",
        "Вектор для $ d_1 $:\n",
        "$$\n",
        "\\mathbf{v}_1 = [2, 1, 1, 1, 0, 0, 0, 0].\n",
        "$$\n",
        "\n",
        "#### Для документа $ d_2 $:\n",
        "- \"кошка\" → 0,\n",
        "- \"спит\" → 0,\n",
        "- \"на\" → 0,\n",
        "- \"столе\" → 0,\n",
        "- \"собака\" → 1,\n",
        "- \"играет\" → 1,\n",
        "- \"во\" → 1,\n",
        "- \"дворе\" → 1.\n",
        "\n",
        "Вектор для $ d_2 $:\n",
        "$$\n",
        "\\mathbf{v}_2 = [0, 0, 0, 0, 1, 1, 1, 1].\n",
        "$$\n",
        "\n",
        "### **Шаг 3: Матрица терм-документ**\n",
        "Объединяем все векторы в матрицу $ M $, где каждая строка соответствует одному документу, а каждый столбец — одному слову из словаря:\n",
        "\n",
        "$$\n",
        "M =\n",
        "\\begin{bmatrix}\n",
        "2 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 1 & 1 & 1 & 1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- Количество строк ($ m = 2 $) равно количеству документов,\n",
        "- Количество столбцов ($ n = 8 $) равно размеру словаря.\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 3: Нормализация векторов**\n",
        "\n",
        "Нормализация используется для приведения векторов к единой шкале. Один из популярных способов нормализации — деление каждой компоненты на общее количество слов в документе.\n",
        "\n",
        "#### Для документа $ d_1 $:\n",
        "Общее количество слов в $ d_1 $: $ N_1 = 5 $.\n",
        "Нормализованный вектор:\n",
        "$$\n",
        "\\mathbf{v}_1^{\\text{norm}} = \\left[\\frac{2}{5}, \\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5}, 0, 0, 0, 0\\right] = [0.4, 0.2, 0.2, 0.2, 0, 0, 0, 0].\n",
        "$$\n",
        "\n",
        "#### Для документа $ d_2 $:\n",
        "Общее количество слов в $ d_2 $: $ N_2 = 4 $.\n",
        "Нормализованный вектор:\n",
        "$$\n",
        "\\mathbf{v}_2^{\\text{norm}} = \\left[0, 0, 0, 0, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}\\right] = [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25].\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## **Пример 4: Разреженность векторов**\n",
        "\n",
        "Разреженность возникает, когда большинство компонент вектора равны нулю. Рассмотрим более крупный словарь:\n",
        "$$\n",
        "V = \\{\\text{\"кошка\"}, \\text{\"спит\"}, \\text{\"на\"}, \\text{\"столе\"}, \\text{\"собака\"}, \\text{\"играет\"}, \\text{\"во\"}, \\text{\"дворе\"}, \\text{\"дерево\"}, \\text{\"лист\"}\\}.\n",
        "$$\n",
        "\n",
        "Теперь размер словаря $ |V| = 10 $. Для документа $ d_1 $:\n",
        "$$\n",
        "\\mathbf{v}_1 = [2, 1, 1, 1, 0, 0, 0, 0, 0, 0].\n",
        "$$\n",
        "\n",
        "Здесь только 4 из 10 компонент отличаются от нуля, что демонстрирует проблему разреженности.\n",
        "\n",
        "\n",
        "\n",
        "# **Реализация Bag of Words (BoW) на Python**\n",
        "\n",
        "Мы можем реализовать метод Bag of Words (BoW) с использованием библиотеки `scikit-learn`. Библиотека предоставляет класс `CountVectorizer`, который автоматически создает словарь и преобразует текстовые данные в числовые векторы.\n",
        "\n",
        "\n",
        "\n",
        "## **Пример: Реализация BoW для корпуса текстов**\n",
        "\n",
        "Пусть у нас есть следующий набор документов:\n",
        "```python\n",
        "corpus = [\n",
        "    \"кошка спит на столе кошка\",\n",
        "    \"собака играет во дворе\",\n",
        "    \"дерево растет возле дома\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# Импортируем необходимые библиотеки\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Для векторизации текста\n",
        "import pandas as pd  # Для удобного представления результатов в виде таблицы\n",
        "\n",
        "# Определяем корпус текстов (набор документов)\n",
        "corpus = [\n",
        "    \"кошка спит на столе кошка\",  # Документ 1\n",
        "    \"собака играет во дворе\",     # Документ 2\n",
        "    \"дерево растет возле дома\"    # Документ 3\n",
        "]\n",
        "\n",
        "# Создаем объект CountVectorizer\n",
        "# Этот класс автоматически создает словарь и преобразует текстовые данные в числовые векторы\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Применяем fit_transform для создания словаря и преобразования текстов в матрицу терм-документ\n",
        "# Метод fit_transform выполняет два действия:\n",
        "# 1. fit: создает словарь уникальных слов из всего корпуса\n",
        "# 2. transform: преобразует каждый документ в числовой вектор на основе частот встречаемости слов\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Получаем список слов из словаря\n",
        "# Метод get_feature_names_out() возвращает массив уникальных слов, которые встречаются в корпусе\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"Словарь:\", feature_names)  # Выводим словарь\n",
        "\n",
        "# Преобразуем результат в DataFrame для лучшего представления\n",
        "# X.toarray() преобразует разреженную матрицу в обычную numpy-матрицу\n",
        "df_bow = pd.DataFrame(X.toarray(), columns=feature_names)\n",
        "print(\"\\nМатрица терм-документ:\")  # Выводим заголовок\n",
        "print(df_bow)  # Выводим матрицу терм-документ\n",
        "\n",
        "# Дополнительные опции (необязательно)\n",
        "# 1. Удаление стоп-слов\n",
        "# Можно исключить часто встречающиеся слова (например, \"и\", \"в\", \"на\") с помощью параметра stop_words\n",
        "vectorizer_with_stop_words = CountVectorizer(stop_words=[\"на\", \"во\", \"возле\"])\n",
        "X_without_stop_words = vectorizer_with_stop_words.fit_transform(corpus)\n",
        "df_bow_without_stop_words = pd.DataFrame(\n",
        "    X_without_stop_words.toarray(),\n",
        "    columns=vectorizer_with_stop_words.get_feature_names_out()\n",
        ")\n",
        "print(\"\\nМатрица терм-документ без стоп-слов:\")\n",
        "print(df_bow_without_stop_words)\n",
        "\n",
        "# 2. Нормализация\n",
        "# Нормализацию можно выполнить после получения матрицы, например, путем деления каждой строки на сумму ее элементов\n",
        "normalized_X = X.toarray() / X.sum(axis=1).reshape(-1, 1)  # Нормализуем каждую строку\n",
        "df_normalized = pd.DataFrame(normalized_X, columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nНормализованная матрица терм-документ:\")\n",
        "print(df_normalized)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CaUJCH5Oy5e"
      }
    }
  ]
}