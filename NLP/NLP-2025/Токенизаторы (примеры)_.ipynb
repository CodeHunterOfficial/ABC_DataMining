{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMaFlhUTqX8nbXiVctZX/Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80%D1%8B%20(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE"
      ],
      "metadata": {
        "id": "2u3Ph5v4gLaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class BPE:\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        Инициализация BPE токенизатора.\n",
        "        :param vocab_size: Желаемый размер словаря.\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}  # Словарь подслов\n",
        "        self.merges = [] # Список выполненных слияний\n",
        "        self.token_to_id = {} # Отображение токенов на их ID\n",
        "        self.id_to_token = {} # Отображение ID на токены\n",
        "\n",
        "    def _get_pairs(self, word_tokens):\n",
        "        \"\"\"\n",
        "        Находит все смежные пары токенов в списке токенов слова.\n",
        "        :param word_tokens: Список токенов слова (например, ['l', 'o', 'w']).\n",
        "        :return: Словарь, где ключ - это пара токенов, значение - ее частота.\n",
        "        \"\"\"\n",
        "        pairs = collections.defaultdict(int)\n",
        "        for i in range(len(word_tokens) - 1):\n",
        "            pairs[(word_tokens[i], word_tokens[i+1])] += 1\n",
        "        return pairs\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        Обучает BPE токенизатор на заданном корпусе.\n",
        "        :param corpus: Список строк (предложений) для обучения.\n",
        "        \"\"\"\n",
        "        # Шаг 1: Инициализация - каждое слово разбивается на символы\n",
        "        # Добавляем символ '_' для обозначения пробелов, как в SentencePiece\n",
        "        word_freqs = collections.defaultdict(int)\n",
        "        for text in corpus:\n",
        "            # Заменяем пробелы на специальный символ и разбиваем на слова\n",
        "            # Затем каждое \"слово\" разбиваем на символы\n",
        "            processed_text = text.replace(' ', '_')\n",
        "            words = processed_text.split('_') # Временное разбиение для обработки\n",
        "\n",
        "            for word in words:\n",
        "                if word: # Убедимся, что слово не пустое\n",
        "                    word_freqs[tuple(list(word))] += 1 # Словарь: (символы слова) -> частота\n",
        "\n",
        "        # Инициализируем словарь всеми уникальными символами\n",
        "        initial_vocab = set()\n",
        "        for word_tokens in word_freqs.keys():\n",
        "            initial_vocab.update(word_tokens)\n",
        "\n",
        "        # Добавляем символ пробела в начальный словарь\n",
        "        initial_vocab.add('_')\n",
        "\n",
        "        # Преобразуем начальный словарь в список для присвоения ID\n",
        "        self.vocab = {token: idx for idx, token in enumerate(sorted(list(initial_vocab)))}\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(sorted(list(initial_vocab)))}\n",
        "        self.id_to_token = {idx: token for idx, token in enumerate(sorted(list(initial_vocab)))}\n",
        "\n",
        "        print(f\"Начальный размер словаря: {len(self.vocab)}\")\n",
        "        print(f\"Начальные токены: {list(self.vocab.keys())[:20]}...\") # Показываем первые 20\n",
        "\n",
        "        # Шаг 2: Итеративное слияние пар\n",
        "        while len(self.vocab) < self.vocab_size:\n",
        "            all_pairs = collections.defaultdict(int)\n",
        "            # Для каждого слова в корпусе, представленного его текущими токенами\n",
        "            for word_tokens, freq in word_freqs.items():\n",
        "                pairs = self._get_pairs(list(word_tokens))\n",
        "                for pair, count in pairs.items():\n",
        "                    all_pairs[pair] += count * freq # Учитываем частоту слова\n",
        "\n",
        "            if not all_pairs:\n",
        "                break # Если нет больше пар для слияния, выходим\n",
        "\n",
        "            # Находим наиболее частую пару\n",
        "            best_pair = max(all_pairs, key=all_pairs.get)\n",
        "\n",
        "            # Если частота лучшей пары равна 1, и мы не можем достичь vocab_size,\n",
        "            # или если мы уже достигли максимального количества слияний,\n",
        "            # или если нет больше уникальных пар для добавления,\n",
        "            # можно выйти из цикла.\n",
        "            if all_pairs[best_pair] == 1 and len(self.vocab) >= self.vocab_size:\n",
        "                break\n",
        "\n",
        "            new_token = \"\".join(best_pair)\n",
        "\n",
        "            # Если новый токен уже в словаре, пропускаем\n",
        "            if new_token in self.vocab:\n",
        "                continue\n",
        "\n",
        "            # Добавляем новый токен в словарь\n",
        "            new_id = len(self.vocab)\n",
        "            self.vocab[new_token] = new_id\n",
        "            self.token_to_id[new_token] = new_id\n",
        "            self.id_to_token[new_id] = new_token\n",
        "            self.merges.append(best_pair) # Записываем выполненное слияние\n",
        "\n",
        "            # Обновляем токены в word_freqs\n",
        "            new_word_freqs = collections.defaultdict(int)\n",
        "            for word_tokens, freq in word_freqs.items():\n",
        "                # Заменяем все вхождения best_pair на new_token в word_tokens\n",
        "                merged_tokens = self._merge_pair_in_list(list(word_tokens), best_pair, new_token)\n",
        "                new_word_freqs[tuple(merged_tokens)] += freq\n",
        "            word_freqs = new_word_freqs\n",
        "\n",
        "            # print(f\"Слияние: {best_pair} -> {new_token}. Новый размер словаря: {len(self.vocab)}\")\n",
        "\n",
        "        print(f\"\\nОбучение завершено. Итоговый размер словаря: {len(self.vocab)}\")\n",
        "        print(f\"Примеры токенов после обучения: {list(self.vocab.keys())[len(initial_vocab):len(initial_vocab)+20]}...\")\n",
        "\n",
        "\n",
        "    def _merge_pair_in_list(self, tokens, pair, new_token):\n",
        "        \"\"\"\n",
        "        Вспомогательная функция для слияния пары в списке токенов.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == pair:\n",
        "                result.append(new_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                result.append(tokens[i])\n",
        "                i += 1\n",
        "        return result\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Токенизирует входной текст, используя обученную модель BPE.\n",
        "        :param text: Входная строка.\n",
        "        :return: Список подслов (строк).\n",
        "        \"\"\"\n",
        "        # Предварительная обработка текста: замена пробелов на '_'\n",
        "        processed_text = text.replace(' ', '_')\n",
        "\n",
        "        # Начальная токенизация на символы\n",
        "        tokens = list(processed_text)\n",
        "\n",
        "        # Применяем все выполненные слияния в том же порядке\n",
        "        for pair, new_token in self.merges:\n",
        "            tokens = self._merge_pair_in_list(tokens, pair, new_token)\n",
        "\n",
        "        # Финальная проверка: если какой-то токен не был объединен,\n",
        "        # убедимся, что он есть в словаре. Если нет, это OOV символ,\n",
        "        # но в нашей реализации BPE все символы из обучающего корпуса\n",
        "        # должны быть в начальном словаре.\n",
        "\n",
        "        # Для удобства вывода, если токен начинается с '_',\n",
        "        # это означает начало слова.\n",
        "        final_pieces = []\n",
        "        current_piece = \"\"\n",
        "        for token in tokens:\n",
        "            if token.startswith('_') and current_piece:\n",
        "                final_pieces.append(current_piece)\n",
        "                current_piece = token\n",
        "            else:\n",
        "                current_piece += token\n",
        "        if current_piece:\n",
        "            final_pieces.append(current_piece)\n",
        "\n",
        "        return final_pieces\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"\n",
        "        Декодирует список подслов обратно в исходный текст.\n",
        "        :param tokens: Список подслов (строк).\n",
        "        :return: Восстановленная строка.\n",
        "        \"\"\"\n",
        "        # Объединяем токены, заменяя '_ ' на ' '\n",
        "        # Сначала объединяем все токены\n",
        "        merged_text = \"\".join(tokens)\n",
        "        # Затем заменяем специальный символ пробела на обычный\n",
        "        decoded_text = merged_text.replace('_', ' ').strip() # strip() для удаления начального/конечного пробела\n",
        "        return decoded_text\n",
        "\n",
        "# --- Пример использования ---\n",
        "print(\"\\n--- Демонстрация работы BPE с нуля ---\")\n",
        "\n",
        "# Обучающий корпус\n",
        "corpus = [\n",
        "    \"Учитель говорит: «Откройте учебники».\",\n",
        "    \"Ученики открывают учебники.\",\n",
        "    \"Откройте страницу 15.\",\n",
        "    \"Страница сложная.\",\n",
        "    \"Страницы учебника содержат много информации.\"\n",
        "]\n",
        "\n",
        "# Создаем и обучаем BPE токенизатор\n",
        "# Зададим небольшой размер словаря для наглядности процесса слияния.\n",
        "# В реальных задачах vocab_size может быть 8000, 16000, 32000 и т.д.\n",
        "bpe_tokenizer = BPE(vocab_size=50) # Попробуйте изменить vocab_size\n",
        "bpe_tokenizer.train(corpus)\n",
        "\n",
        "# --- Токенизация ---\n",
        "print(\"\\n--- Токенизация нового текста ---\")\n",
        "text_to_tokenize = \"Учитель говорит: «Откройте учебники». Ученики открывают учебники. Откройте страницу 15. Страница сложная. Страницы учебника содержат много информации.\"\n",
        "\n",
        "# Токенизируем текст\n",
        "tokenized_pieces = bpe_tokenizer.tokenize(text_to_tokenize)\n",
        "print(f\"Исходный текст: '{text_to_tokenize}'\")\n",
        "print(f\"Токенизированные подслова: {tokenized_pieces}\")\n",
        "\n",
        "# --- Детокенизация ---\n",
        "print(\"\\n--- Детокенизация ---\")\n",
        "decoded_text = bpe_tokenizer.decode(tokenized_pieces)\n",
        "print(f\"Детокенизированный текст: '{decoded_text}'\")\n",
        "\n",
        "# Проверка обратимости\n",
        "if text_to_tokenize.replace(' ', '') == decoded_text.replace(' ', ''): # Сравниваем без пробелов для упрощения\n",
        "    print(\"\\nПроцесс токенизации и детокенизации успешно обратим (без учета оригинальных пробелов).\")\n",
        "else:\n",
        "    print(\"\\nВнимание: Детокенизированный текст не полностью совпадает с исходным.\")\n",
        "    print(f\"Ожидалось (без пробелов): '{text_to_tokenize.replace(' ', '')}'\")\n",
        "    print(f\"Получено (без пробелов): '{decoded_text.replace(' ', '')}'\")\n",
        "\n",
        "print(\"\\n--- Демонстрация работы BPE завершена ---\")\n"
      ],
      "metadata": {
        "id": "_3Sr07lngi4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordPiece"
      ],
      "metadata": {
        "id": "SBOZ-fJJgJmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkUXyyRH_xKi"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Предварительная обработка текста: приведение к нижнему регистру и разбиение на слова.\n",
        "    \"\"\"\n",
        "    # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "    processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "    # Разбиваем на слова по пробелам\n",
        "    words = processed_text.split(' ')\n",
        "    # Добавляем пробелы как отдельные токены между словами\n",
        "    initial_tokens = []\n",
        "    for i, word in enumerate(words):\n",
        "        if word: # Убедимся, что слово не пустое\n",
        "            initial_tokens.extend(list(word))\n",
        "        if i < len(words) - 1:\n",
        "            initial_tokens.append(' ') # Добавляем пробел между словами\n",
        "    return initial_tokens, words # Возвращаем начальные токены и список слов для удобства\n",
        "\n",
        "def calculate_frequencies(tokens):\n",
        "    \"\"\"\n",
        "    Подсчет частот отдельных токенов (униграмм) и биграмм в текущем корпусе.\n",
        "    \"\"\"\n",
        "    unigram_freq = collections.defaultdict(int)\n",
        "    bigram_freq = collections.defaultdict(int)\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        unigram_freq[tokens[i]] += 1\n",
        "        if i < len(tokens) - 1:\n",
        "            bigram_freq[(tokens[i], tokens[i+1])] += 1\n",
        "    return unigram_freq, bigram_freq\n",
        "\n",
        "def calculate_score(unigram_freq, bigram_freq):\n",
        "    \"\"\"\n",
        "    Вычисление оценки слияния для всех возможных биграмм.\n",
        "    Score(A, B) = frequency(AB) / (frequency(A) * frequency(B))\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    for (token_a, token_b), freq_ab in bigram_freq.items():\n",
        "        freq_a = unigram_freq[token_a]\n",
        "        freq_b = unigram_freq[token_b]\n",
        "        if freq_a > 0 and freq_b > 0: # Избегаем деления на ноль\n",
        "            score = freq_ab / (freq_a * freq_b)\n",
        "            scores[(token_a, token_b)] = score\n",
        "    return scores\n",
        "\n",
        "def merge_tokens(tokens, best_bigram):\n",
        "    \"\"\"\n",
        "    Слияние лучшей биграммы в корпусе.\n",
        "    \"\"\"\n",
        "    merged_token = \"\".join(best_bigram)\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == best_bigram:\n",
        "            new_tokens.append(merged_token)\n",
        "            i += 2 # Пропускаем оба слитых токена\n",
        "        else:\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "    return new_tokens\n",
        "\n",
        "def train_wordpiece(text, target_vocab_size=None, num_merges=None):\n",
        "    \"\"\"\n",
        "    Обучение WordPiece токенизатора.\n",
        "    Итеративно сливает токены до достижения целевого размера словаря\n",
        "    или заданного количества слияний.\n",
        "    \"\"\"\n",
        "    print(f\"Исходный текст: '{text}'\")\n",
        "    current_tokens, _ = preprocess_text(text)\n",
        "\n",
        "    # Начальный словарь состоит из всех уникальных символов\n",
        "    vocab = set(current_tokens)\n",
        "    print(f\"\\nШаг 1: Начальный словарь (V0) содержит {len(vocab)} токенов.\")\n",
        "    print(f\"Начальный корпус: {current_tokens}\")\n",
        "\n",
        "    merges_count = 0\n",
        "    while True:\n",
        "        unigram_freq, bigram_freq = calculate_frequencies(current_tokens)\n",
        "        scores = calculate_score(unigram_freq, bigram_freq)\n",
        "\n",
        "        if not scores:\n",
        "            print(\"\\nНет больше биграмм для слияния. Остановка.\")\n",
        "            break\n",
        "\n",
        "        # Находим биграмму с наивысшей оценкой\n",
        "        best_bigram = max(scores, key=scores.get)\n",
        "        best_score = scores[best_bigram]\n",
        "\n",
        "        # Критерий остановки: если оценка слишком низкая (можно задать порог)\n",
        "        # Для простоты примера, мы пока не используем порог, а полагаемся на num_merges или target_vocab_size\n",
        "        # if best_score < MIN_SCORE_THRESHOLD:\n",
        "        #     print(f\"\\nОценка лучшей биграммы ({best_score}) ниже порога. Остановка.\")\n",
        "        #     break\n",
        "\n",
        "        merged_token = \"\".join(best_bigram)\n",
        "\n",
        "        # Критерий остановки: если новый токен уже в словаре (такое может быть, если он был добавлен ранее)\n",
        "        if merged_token in vocab:\n",
        "            # Если лучший токен уже существует, удалим его из scores, чтобы найти следующий лучший\n",
        "            del scores[best_bigram]\n",
        "            continue # Продолжаем поиск\n",
        "\n",
        "        # Выводим информацию о текущем слиянии\n",
        "        print(f\"\\nИтерация {merges_count + 1}:\")\n",
        "        print(f\"  Лучшая биграмма для слияния: '{best_bigram[0]}' + '{best_bigram[1]}' -> '{merged_token}' (Score: {best_score:.4f})\")\n",
        "\n",
        "        # Выполняем слияние в корпусе\n",
        "        current_tokens = merge_tokens(current_tokens, best_bigram)\n",
        "\n",
        "        # Добавляем новый токен в словарь\n",
        "        vocab.add(merged_token)\n",
        "        merges_count += 1\n",
        "\n",
        "        print(f\"  Корпус после слияния: {current_tokens}\")\n",
        "        print(f\"  Текущий размер словаря: {len(vocab)}\")\n",
        "\n",
        "        # Критерии остановки\n",
        "        if target_vocab_size is not None and len(vocab) >= target_vocab_size:\n",
        "            print(f\"\\nДостигнут целевой размер словаря ({target_vocab_size}). Остановка.\")\n",
        "            break\n",
        "        if num_merges is not None and merges_count >= num_merges:\n",
        "            print(f\"\\nДостигнуто заданное количество слияний ({num_merges}). Остановка.\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nОбучение завершено. Итоговый словарь содержит {len(vocab)} токенов.\")\n",
        "    return sorted(list(vocab)) # Возвращаем отсортированный список токенов\n",
        "\n",
        "def tokenize_with_wordpiece(text, vocab):\n",
        "    \"\"\"\n",
        "    Токенизация нового текста с использованием обученного словаря WordPiece.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Токенизация нового текста ---\")\n",
        "    print(f\"Текст для токенизации: '{text}'\")\n",
        "\n",
        "    # Предварительная обработка текста для токенизации\n",
        "    processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "    words = processed_text.split(' ')\n",
        "\n",
        "    final_tokens = []\n",
        "    for word in words:\n",
        "        if not word: # Пропускаем пустые строки, если они возникли из-за множественных пробелов\n",
        "            continue\n",
        "\n",
        "        word_tokens = []\n",
        "        remaining_word = word\n",
        "\n",
        "        while remaining_word:\n",
        "            found_match = False\n",
        "            # Ищем самый длинный подтокен из словаря, который является префиксом оставшейся части слова\n",
        "            for i in range(len(remaining_word), 0, -1):\n",
        "                subword = remaining_word[:i]\n",
        "                # Для подслов, которые не являются началом слова, в реальных WordPiece токенизаторах\n",
        "                # используется префикс '##'. Здесь мы упрощаем, но можно добавить:\n",
        "                # if word_tokens and i < len(remaining_word):\n",
        "                #     subword_with_prefix = '##' + subword\n",
        "                #     if subword_with_prefix in vocab:\n",
        "                #         word_tokens.append(subword_with_prefix)\n",
        "                #         remaining_word = remaining_word[i:]\n",
        "                #         found_match = True\n",
        "                #         break\n",
        "\n",
        "                if subword in vocab:\n",
        "                    word_tokens.append(subword)\n",
        "                    remaining_word = remaining_word[i:]\n",
        "                    found_match = True\n",
        "                    break\n",
        "\n",
        "            if not found_match:\n",
        "                # Если не удалось найти соответствующий токен, разбиваем на символы\n",
        "                # или используем токен [UNK]\n",
        "                if remaining_word[0] in vocab: # Если символ есть в словаре\n",
        "                    word_tokens.append(remaining_word[0])\n",
        "                else: # Если символ даже не в начальном словаре (очень редкий случай)\n",
        "                    word_tokens.append('[UNK]')\n",
        "                remaining_word = remaining_word[1:]\n",
        "\n",
        "\n",
        "        final_tokens.extend(word_tokens)\n",
        "        final_tokens.append(' ') # Добавляем пробел между токенизированными словами\n",
        "\n",
        "    # Удаляем последний пробел, если он есть\n",
        "    if final_tokens and final_tokens[-1] == ' ':\n",
        "        final_tokens.pop()\n",
        "\n",
        "    print(f\"Токенизированный текст: {final_tokens}\")\n",
        "    return final_tokens\n",
        "\n",
        "# --- Пример использования ---\n",
        "text_to_train = \"В училище учитель учит ученикам по новому учебнику\"\n",
        "\n",
        "# Обучаем токенизатор, задав максимальное количество слияний для демонстрации\n",
        "# В реальных сценариях задают target_vocab_size (например, 30000)\n",
        "final_vocab = train_wordpiece(text_to_train, num_merges=50) # Ограничимся 10 слияниями для примера\n",
        "\n",
        "print(f\"\\nИтоговый словарь WordPiece: {final_vocab}\")\n",
        "\n",
        "# Токенизируем тот же текст с использованием обученного словаря\n",
        "tokenize_with_wordpiece(text_to_train, final_vocab)\n",
        "\n",
        "# Пример токенизации нового слова, которое может быть разбито на подслова\n",
        "new_text = \"учительница\"\n",
        "# Добавим '##ница' в словарь для демонстрации, если оно не было сформировано\n",
        "if 'ница' not in final_vocab:\n",
        "    final_vocab.append('ница')\n",
        "    final_vocab.append('##ница') # В реальных WordPiece так обозначаются подслова\n",
        "    final_vocab.sort() # Пересортируем, чтобы было красиво\n",
        "\n",
        "print(f\"\\nИтоговый словарь WordPiece (с 'ница' для демонстрации): {final_vocab}\")\n",
        "tokenize_with_wordpiece(new_text, final_vocab)\n",
        "\n",
        "# Пример слова, которое может быть разбито на части, если целиком его нет в словаре\n",
        "# Для этого нам нужно, чтобы 'учитель' был в словаре, а 'учительница' - нет,\n",
        "# но 'ница' или '##ница' были.\n",
        "\n",
        "# Давайте создадим упрощенный словарь для демонстрации токенизации подслов\n",
        "simplified_vocab = set(['у', 'ч', 'и', 'т', 'е', 'л', 'ь', 'н', 'ц', 'а', 'учитель', '##ница', '##тель', '##ница', '##а'])\n",
        "print(f\"\\n--- Демонстрация токенизации подслов с упрощенным словарем ---\")\n",
        "tokenize_with_wordpiece(\"учительница\", sorted(list(simplified_vocab)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Униграмной языковой модели"
      ],
      "metadata": {
        "id": "dZw5ZOm5gE_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Токенизирует текст, приводя его к нижнему регистру и удаляя знаки препинания.\n",
        "    Args:\n",
        "        text (str): Входной текст.\n",
        "    Returns:\n",
        "        list: Список токенов (слов).\n",
        "    \"\"\"\n",
        "    # Приводим текст к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаляем знаки препинания и разбиваем на слова\n",
        "    # Используем re.findall для извлечения только буквенных последовательностей\n",
        "    tokens = re.findall(r'\\b[а-яёa-z]+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "def train_unigram_model(corpus_text):\n",
        "    \"\"\"\n",
        "    Обучает униграмную языковую модель на заданном корпусе текста.\n",
        "    Рассчитывает вероятности слов методом максимального правдоподобия (MLE)\n",
        "    и со сглаживанием по Лапласу.\n",
        "    Args:\n",
        "        corpus_text (str): Текст корпуса для обучения модели.\n",
        "    Returns:\n",
        "        tuple: Кортеж, содержащий:\n",
        "            - dict: Вероятности слов по MLE.\n",
        "            - dict: Вероятности слов со сглаживанием по Лапласу.\n",
        "            - int: Общее количество слов в корпусе (N).\n",
        "            - int: Размер словаря (количество уникальных слов).\n",
        "    \"\"\"\n",
        "    # Шаг 1: Токенизация корпуса\n",
        "    tokens = tokenize(corpus_text)\n",
        "\n",
        "    # Общее количество слов в корпусе (N)\n",
        "    N = len(tokens)\n",
        "    if N == 0:\n",
        "        print(\"Ошибка: Корпус пуст после токенизации.\")\n",
        "        return {}, {}, 0, 0\n",
        "\n",
        "    # Шаг 2: Подсчет частот слов\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Шаг 3: Определение словаря и его размера\n",
        "    vocabulary = list(word_counts.keys())\n",
        "    V = len(vocabulary)\n",
        "\n",
        "    # Шаг 4: Расчет вероятностей слов (MLE)\n",
        "    mle_probabilities = {}\n",
        "    for word, count in word_counts.items():\n",
        "        mle_probabilities[word] = count / N\n",
        "\n",
        "    # Расчет вероятностей слов со сглаживанием по Лапласу\n",
        "    laplace_probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        laplace_probabilities[word] = (word_counts[word] + 1) / (N + V)\n",
        "    # Для слов, которых нет в словаре, но могут встретиться в новом тексте\n",
        "    # Их вероятность со сглаживанием будет (0 + 1) / (N + V)\n",
        "    # Мы не добавляем их сюда напрямую, но это учитывается при использовании модели.\n",
        "\n",
        "    print(f\"Общее количество слов в корпусе (N): {N}\")\n",
        "    print(f\"Размер словаря (|V|): {V}\")\n",
        "    print(\"\\nЧастоты слов:\")\n",
        "    for word, count in word_counts.items():\n",
        "        print(f\"  '{word}': {count}\")\n",
        "    print(\"\\nВероятности слов (MLE):\")\n",
        "    for word, prob in mle_probabilities.items():\n",
        "        print(f\"  P('{word}') = {prob:.4f}\")\n",
        "    print(\"\\nВероятности слов (Laplace Smoothing):\")\n",
        "    for word, prob in laplace_probabilities.items():\n",
        "        print(f\"  P_Laplace('{word}') = {prob:.4f}\")\n",
        "\n",
        "    return mle_probabilities, laplace_probabilities, N, V\n",
        "\n",
        "def calculate_sentence_probability(sentence, model_probabilities, N, V, smoothing_type='laplace'):\n",
        "    \"\"\"\n",
        "    Рассчитывает вероятность предложения, используя обученную униграмную модель.\n",
        "    Args:\n",
        "        sentence (str): Предложение для оценки.\n",
        "        model_probabilities (dict): Словарь вероятностей слов (MLE или Laplace).\n",
        "        N (int): Общее количество слов в обучающем корпусе.\n",
        "        V (int): Размер словаря обучающего корпуса.\n",
        "        smoothing_type (str): Тип сглаживания ('none' для MLE, 'laplace' для сглаживания по Лапласу).\n",
        "    Returns:\n",
        "        float: Вероятность предложения.\n",
        "    \"\"\"\n",
        "    sentence_tokens = tokenize(sentence)\n",
        "    probability = 1.0\n",
        "\n",
        "    print(f\"\\nОценка вероятности предложения: '{sentence}'\")\n",
        "    print(f\"Токены предложения: {sentence_tokens}\")\n",
        "\n",
        "    for token in sentence_tokens:\n",
        "        if smoothing_type == 'laplace':\n",
        "            # Используем формулу Лапласа для каждого токена\n",
        "            # Если токена нет в model_probabilities (т.е. Count(token) = 0),\n",
        "            # то его вероятность будет (0 + 1) / (N + V)\n",
        "            word_count_in_model = model_probabilities.get(token, 0) * N # Восстанавливаем Count(token)\n",
        "            # Если слово не было в словаре, то word_count_in_model будет 0.0,\n",
        "            # но для Лапласа нам нужен его реальный счетчик (0).\n",
        "            # Проще использовать прямой расчет:\n",
        "            if token in model_probabilities:\n",
        "                p_token = (word_count_in_model + 1) / (N + V)\n",
        "            else:\n",
        "                p_token = 1 / (N + V) # Для неизвестных слов\n",
        "            print(f\"  P_Laplace('{token}') = {p_token:.8f}\")\n",
        "        else: # Без сглаживания (MLE)\n",
        "            p_token = model_probabilities.get(token, 0.0) # Если слово не найдено, вероятность 0\n",
        "            print(f\"  P_MLE('{token}') = {p_token:.8f}\")\n",
        "\n",
        "        if p_token == 0:\n",
        "            # Если вероятность слова 0, то вероятность всего предложения 0\n",
        "            probability = 0.0\n",
        "            print(f\"  Обнаружено неизвестное слово '{token}' без сглаживания. Вероятность предложения = 0.\")\n",
        "            break\n",
        "        probability *= p_token\n",
        "\n",
        "    return probability\n",
        "\n",
        "# --- Пример использования ---\n",
        "corpus_text = \"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"\n",
        "\n",
        "print(\"--- Обучение Униграмной модели ---\")\n",
        "mle_probs, laplace_probs, N_corpus, V_corpus = train_unigram_model(corpus_text)\n",
        "\n",
        "# Оценка вероятности предложения из корпуса (без сглаживания)\n",
        "sentence1 = \"Ученики пишут диктант\"\n",
        "prob_sentence1_mle = calculate_sentence_probability(sentence1, mle_probs, N_corpus, V_corpus, smoothing_type='none')\n",
        "print(f\"\\nВероятность предложения '{sentence1}' (MLE): {prob_sentence1_mle:.15f}\")\n",
        "\n",
        "# Оценка вероятности предложения из корпуса (со сглаживанием по Лапласу)\n",
        "prob_sentence1_laplace = calculate_sentence_probability(sentence1, laplace_probs, N_corpus, V_corpus, smoothing_type='laplace')\n",
        "print(f\"\\nВероятность предложения '{sentence1}' (Laplace Smoothing): {prob_sentence1_laplace:.15f}\")\n",
        "\n",
        "# Оценка вероятности нового предложения с неизвестным словом (без сглаживания)\n",
        "sentence2 = \"Учитель читает книгу\"\n",
        "prob_sentence2_mle = calculate_sentence_probability(sentence2, mle_probs, N_corpus, V_corpus, smoothing_type='none')\n",
        "print(f\"\\nВероятность предложения '{sentence2}' (MLE): {prob_sentence2_mle:.15f}\")\n",
        "\n",
        "# Оценка вероятности нового предложения с неизвестным словом (со сглаживанием по Лапласу)\n",
        "prob_sentence2_laplace = calculate_sentence_probability(sentence2, laplace_probs, N_corpus, V_corpus, smoothing_type='laplace')\n",
        "print(f\"\\nВероятность предложения '{sentence2}' (Laplace Smoothing): {prob_sentence2_laplace:.15f}\")\n",
        "\n",
        "# Пример с одним словом, которого нет в корпусе\n",
        "unknown_word_sentence = \"школа\"\n",
        "prob_unknown_laplace = calculate_sentence_probability(unknown_word_sentence, laplace_probs, N_corpus, V_corpus, smoothing_type='laplace')\n",
        "print(f\"\\nВероятность слова '{unknown_word_sentence}' (Laplace Smoothing): {prob_unknown_laplace:.15f}\")\n"
      ],
      "metadata": {
        "id": "Smok_jSifd1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece"
      ],
      "metadata": {
        "id": "aSRNpUAbgAuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем библиотеку SentencePiece\n",
        "import sentencepiece as spm\n",
        "import os # Для работы с файловой системой\n",
        "\n",
        "print(\"--- Полный пример использования SentencePiece ---\")\n",
        "\n",
        "# Шаг 1: Установка SentencePiece (инструкция)\n",
        "# Если у вас еще не установлен SentencePiece, вы можете установить его с помощью pip:\n",
        "# pip install sentencepiece\n",
        "# (Раскомментируйте строку выше и запустите в терминале, если необходимо)\n",
        "\n",
        "# Шаг 2: Подготовка данных для обучения\n",
        "# Для обучения SentencePiece требуется текстовый файл.\n",
        "# Создадим пример файла с текстом, который мы будем использовать.\n",
        "# Используем тот же текст, что и в лекции.\n",
        "text_data = \"\"\"Учитель говорит: «Откройте учебники». Ученики открывают учебники. Откройте страницу 15. Страница сложная. Страницы учебника содержат много информации.\"\"\"\n",
        "\n",
        "# Определяем имя файла для обучения\n",
        "train_file_name = \"train_text_for_sp.txt\"\n",
        "\n",
        "# Создаем файл для обучения и записываем в него текст\n",
        "try:\n",
        "    with open(train_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_data)\n",
        "    print(f\"\\nСоздан файл для обучения: '{train_file_name}'\")\n",
        "except IOError as e:\n",
        "    print(f\"Ошибка при создании файла для обучения: {e}\")\n",
        "    exit() # Выходим, если не удалось создать файл\n",
        "\n",
        "# Шаг 3: Обучение модели SentencePiece\n",
        "# Мы будем обучать модель с использованием алгоритма Unigram (по умолчанию).\n",
        "# model_prefix: префикс для файлов модели (model.model и model.vocab)\n",
        "# vocab_size: желаемый размер словаря подслов\n",
        "# character_coverage: процент символов, которые должны быть покрыты моделью\n",
        "#                  (1.0 означает, что все символы из входных данных должны быть представлены)\n",
        "# model_type: 'unigram' (по умолчанию) или 'bpe'\n",
        "# input: путь к файлу(ам) для обучения\n",
        "\n",
        "model_prefix = \"my_custom_sentencepiece_model\"\n",
        "vocab_size = 50 # Увеличим размер словаря для лучшей демонстрации\n",
        "\n",
        "print(f\"\\nНачинается обучение модели SentencePiece с vocab_size={vocab_size}...\")\n",
        "try:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=train_file_name,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1.0,\n",
        "        model_type='unigram' # Можно изменить на 'bpe'\n",
        "    )\n",
        "    print(f\"Обучение завершено. Созданы файлы: '{model_prefix}.model' и '{model_prefix}.vocab'\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при обучении модели SentencePiece: {e}\")\n",
        "    # Попытаемся удалить созданный файл, если обучение не удалось\n",
        "    if os.path.exists(train_file_name):\n",
        "        os.remove(train_file_name)\n",
        "    exit()\n",
        "\n",
        "# Шаг 4: Загрузка обученной модели\n",
        "# После обучения мы можем загрузить модель для использования.\n",
        "sp = spm.SentencePieceProcessor()\n",
        "model_path = f\"{model_prefix}.model\"\n",
        "\n",
        "try:\n",
        "    sp.load(model_path)\n",
        "    print(f\"\\nМодель SentencePiece загружена: '{model_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели SentencePiece: {e}\")\n",
        "    # Попытаемся удалить созданные файлы, если загрузка не удалась\n",
        "    if os.path.exists(train_file_name): os.remove(train_file_name)\n",
        "    if os.path.exists(f\"{model_prefix}.model\"): os.remove(f\"{model_prefix}.model\")\n",
        "    if os.path.exists(f\"{model_prefix}.vocab\"): os.remove(f\"{model_prefix}.vocab\")\n",
        "    exit()\n",
        "\n",
        "# Шаг 5: Токенизация текста (кодирование)\n",
        "# Теперь используем загруженную модель для токенизации нашего текста.\n",
        "\n",
        "input_text = \"Учитель говорит: «Откройте учебники». Ученики открывают учебники. Откройте страницу 15. Страница сложная. Страницы учебника содержат много информации.\"\n",
        "\n",
        "print(f\"\\n--- Токенизация ---\")\n",
        "print(f\"Исходный текст: '{input_text}'\")\n",
        "\n",
        "# Кодирование в ID токенов\n",
        "encoded_ids = sp.encode_as_ids(input_text)\n",
        "print(f\"Токенизация в ID: {encoded_ids}\")\n",
        "\n",
        "# Кодирование в строковые токены (подслова)\n",
        "encoded_pieces = sp.encode_as_pieces(input_text)\n",
        "print(f\"Токенизация в подслова: {encoded_pieces}\")\n",
        "\n",
        "# Шаг 6: Детокенизация текста (декодирование)\n",
        "# Восстановим исходный текст из токенизированных ID или подслов.\n",
        "\n",
        "print(f\"\\n--- Детокенизация ---\")\n",
        "\n",
        "decoded_text_from_ids = sp.decode_ids(encoded_ids)\n",
        "print(f\"Детокенизация из ID: '{decoded_text_from_ids}'\")\n",
        "\n",
        "decoded_text_from_pieces = sp.decode_pieces(encoded_pieces)\n",
        "print(f\"Детокенизация из подслов: '{decoded_text_from_pieces}'\")\n",
        "\n",
        "# Проверка обратимости\n",
        "if input_text == decoded_text_from_ids and input_text == decoded_text_from_pieces:\n",
        "    print(\"\\nПроцесс токенизации и детокенизации полностью обратим!\")\n",
        "else:\n",
        "    print(\"\\nВнимание: Детокенизированный текст не полностью совпадает с исходным.\")\n",
        "    print(f\"Ожидалось: '{input_text}'\")\n",
        "    print(f\"Получено из ID: '{decoded_text_from_ids}'\")\n",
        "    print(f\"Получено из подслов: '{decoded_text_from_pieces}'\")\n",
        "\n",
        "\n",
        "# Шаг 7: Очистка (удаление созданных файлов)\n",
        "# Удалим файлы, созданные в процессе обучения, чтобы не засорять рабочую директорию.\n",
        "print(f\"\\n--- Очистка временных файлов ---\")\n",
        "try:\n",
        "    if os.path.exists(train_file_name):\n",
        "        os.remove(train_file_name)\n",
        "        print(f\"Удален файл: '{train_file_name}'\")\n",
        "    if os.path.exists(f\"{model_prefix}.model\"):\n",
        "        os.remove(f\"{model_prefix}.model\")\n",
        "        print(f\"Удален файл: '{model_prefix}.model'\")\n",
        "    if os.path.exists(f\"{model_prefix}.vocab\"):\n",
        "        os.remove(f\"{model_prefix}.vocab\")\n",
        "        print(f\"Удален файл: '{model_prefix}.vocab'\")\n",
        "except OSError as e:\n",
        "    print(f\"Ошибка при удалении файлов: {e}\")\n",
        "\n",
        "print(\"\\n--- Использование SentencePiece завершено ---\")\n"
      ],
      "metadata": {
        "id": "CgSI01X3bSV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}