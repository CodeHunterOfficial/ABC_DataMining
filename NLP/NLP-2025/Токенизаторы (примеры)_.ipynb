{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNXergGSKpSUg+zX5DyF/+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80%D1%8B%20(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE"
      ],
      "metadata": {
        "id": "2u3Ph5v4gLaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class BPE:\n",
        "    \"\"\"\n",
        "    Реализация алгоритма Byte-Pair Encoding (BPE) для токенизации текста.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Инициализирует BPE токенизатор.\n",
        "        :param vocab_size: Желаемый максимальный размер словаря субтокенов.\n",
        "                           Если указан, num_merges будет игнорироваться.\n",
        "        :param num_merges: Количество итераций объединения пар.\n",
        "                           Если vocab_size не указан, будет использоваться это значение.\n",
        "        \"\"\"\n",
        "        if vocab_size is None and num_merges is None:\n",
        "            raise ValueError(\"Необходимо указать либо vocab_size, либо num_merges.\")\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = {}  # Словарь для хранения выполненных объединений {pair: new_token}\n",
        "        self.vocabulary = set() # Текущий словарь субтокенов\n",
        "        self.token_to_id = {} # Словарь для преобразования токенов в ID\n",
        "        self.id_to_token = [] # Список для преобразования ID в токены\n",
        "\n",
        "    def _get_initial_tokens(self, text):\n",
        "        \"\"\"\n",
        "        Разбивает текст на начальные символьные токены и добавляет символ конца слова.\n",
        "        :param text: Входная строка.\n",
        "        :return: Список списков символьных токенов для каждого слова.\n",
        "        \"\"\"\n",
        "        # Заменяем подчеркивания на пробелы для разделения слов, затем разбиваем по пробелам\n",
        "        words = text.replace('_', ' ').split()\n",
        "        initial_tokens = []\n",
        "        for word in words:\n",
        "            # Добавляем символ конца слова к каждому слову\n",
        "            initial_tokens.append(list(word) + ['</w>'])\n",
        "        return initial_tokens\n",
        "\n",
        "    def _get_pair_counts(self, tokenized_corpus):\n",
        "        \"\"\"\n",
        "        Подсчитывает частоту встречаемости всех смежных пар токенов в корпусе.\n",
        "        :param tokenized_corpus: Список списков токенов (представляющих слова).\n",
        "        :return: Словарь с частотами пар {('token1', 'token2'): count}.\n",
        "        \"\"\"\n",
        "        pair_counts = collections.defaultdict(int)\n",
        "        for word_tokens in tokenized_corpus:\n",
        "            # Итерируем по парам токенов в каждом слове\n",
        "            for i in range(len(word_tokens) - 1):\n",
        "                pair_counts[(word_tokens[i], word_tokens[i+1])] += 1\n",
        "        return pair_counts\n",
        "\n",
        "    def _merge_pair(self, tokenized_corpus, pair, new_token):\n",
        "        \"\"\"\n",
        "        Объединяет все вхождения заданной пары в новый токен в корпусе.\n",
        "        :param tokenized_corpus: Текущий корпус токенов.\n",
        "        :param pair: Пара токенов для объединения (tuple).\n",
        "        :param new_token: Новый токен, который заменит пару.\n",
        "        :return: Обновленный корпус токенов.\n",
        "        \"\"\"\n",
        "        updated_corpus = []\n",
        "        for word_tokens in tokenized_corpus:\n",
        "            new_word_tokens = []\n",
        "            i = 0\n",
        "            while i < len(word_tokens):\n",
        "                # Проверяем, является ли текущая позиция началом искомой пары\n",
        "                if i + 1 < len(word_tokens) and (word_tokens[i], word_tokens[i+1]) == pair:\n",
        "                    new_word_tokens.append(new_token)\n",
        "                    i += 2 # Пропускаем оба токена, которые были объединены\n",
        "                else:\n",
        "                    new_word_tokens.append(word_tokens[i])\n",
        "                    i += 1\n",
        "            updated_corpus.append(new_word_tokens)\n",
        "        return updated_corpus\n",
        "\n",
        "    def fit(self, corpus_text):\n",
        "        \"\"\"\n",
        "        Обучает BPE модель на заданном текстовом корпусе.\n",
        "        :param corpus_text: Строка, представляющая обучающий корпус.\n",
        "        \"\"\"\n",
        "        # Шаг 1: Инициализация\n",
        "        # Начальное разбиение на символы и добавление символа конца слова\n",
        "        current_corpus_tokens = self._get_initial_tokens(corpus_text)\n",
        "\n",
        "        # Формирование начального словаря из всех уникальных символов\n",
        "        for word_tokens in current_corpus_tokens:\n",
        "            for token in word_tokens:\n",
        "                self.vocabulary.add(token)\n",
        "\n",
        "        # Определяем количество итераций объединения\n",
        "        if self.vocab_size is not None:\n",
        "            # Если задан размер словаря, вычисляем необходимое количество объединений\n",
        "            # (целевой размер словаря - текущий размер словаря)\n",
        "            num_iterations = self.vocab_size - len(self.vocabulary)\n",
        "        elif self.num_merges is not None:\n",
        "            num_iterations = self.num_merges\n",
        "        else:\n",
        "            # Это условие не должно быть достигнуто благодаря проверке в __init__\n",
        "            num_iterations = 0\n",
        "\n",
        "        # Шаг 2: Итеративное объединение\n",
        "        for i in range(num_iterations):\n",
        "            pair_counts = self._get_pair_counts(current_corpus_tokens)\n",
        "\n",
        "            if not pair_counts: # Если нет пар для объединения, останавливаемся\n",
        "                break\n",
        "\n",
        "            # Находим наиболее частую пару\n",
        "            # Сортируем по частоте (убывание), затем по лексикографическому порядку (возрастание)\n",
        "            best_pair = max(pair_counts, key=lambda p: (pair_counts[p], -len(p[0]) - len(p[1]), p))\n",
        "\n",
        "            # Если частота лучшей пары меньше 1, или нет пар для объединения, останавливаемся\n",
        "            if pair_counts[best_pair] < 1:\n",
        "                break\n",
        "\n",
        "            # Формируем новый токен из объединенной пары\n",
        "            new_token = \"\".join(best_pair)\n",
        "\n",
        "            # Сохраняем объединение\n",
        "            self.merges[best_pair] = new_token\n",
        "\n",
        "            # Обновляем корпус, заменяя все вхождения пары новым токеном\n",
        "            current_corpus_tokens = self._merge_pair(current_corpus_tokens, best_pair, new_token)\n",
        "\n",
        "            # Добавляем новый токен в словарь\n",
        "            self.vocabulary.add(new_token)\n",
        "\n",
        "            # Вывод результатов каждого шага\n",
        "            print(f\"Итерация {i+1}: Объединение '{best_pair[0]}{best_pair[1]}' в '{new_token}'. Частота: {pair_counts[best_pair]}\")\n",
        "            print(f\"Обновленный корпус: {current_corpus_tokens}\")\n",
        "            print(f\"Текущий словарь: {sorted(list(self.vocabulary))}\\n\")\n",
        "\n",
        "        # После обучения создаем отображения токенов в ID и обратно\n",
        "        self.id_to_token = sorted(list(self.vocabulary))\n",
        "        self.token_to_id = {token: i for i, token in enumerate(self.id_to_token)}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Токенизирует новую строку, используя обученные объединения.\n",
        "        :param text: Строка для токенизации.\n",
        "        :return: Список ID токенов.\n",
        "        \"\"\"\n",
        "        # Начальное разбиение текста на символы с добавлением </w>\n",
        "        words_tokens = self._get_initial_tokens(text)\n",
        "        encoded_tokens_ids = []\n",
        "\n",
        "        for word_tokens in words_tokens:\n",
        "            # Для каждого слова применяем объединения в порядке их обучения\n",
        "            # Создаем копию списка токенов слова для модификации\n",
        "            current_word_tokens = list(word_tokens)\n",
        "\n",
        "            # Применяем объединения итеративно, пока не сможем найти более длинный токен\n",
        "            # или пока не закончатся объединения\n",
        "            while True:\n",
        "                merged_once = False\n",
        "                new_word_tokens = []\n",
        "                i = 0\n",
        "                while i < len(current_word_tokens):\n",
        "                    found_merge = False\n",
        "                    # Ищем самое длинное возможное объединение, которое начинается с текущего токена\n",
        "                    # Проходим по всем известным объединениям в порядке их создания (от коротких к длинным)\n",
        "                    # или просто ищем самое длинное совпадение\n",
        "                    best_match_len = 0\n",
        "                    best_match_token = None\n",
        "\n",
        "                    # Для простоты, будем применять объединения из self.merges\n",
        "                    # в порядке их создания (от коротких к длинным)\n",
        "                    # Это не совсем оптимально, но для демонстрации подходит\n",
        "                    # В реальных реализациях используют более сложные структуры данных\n",
        "                    # для эффективного поиска наиболее длинного совпадения\n",
        "                    for (p1, p2), merged_token in self.merges.items():\n",
        "                        if i + 1 < len(current_word_tokens) and \\\n",
        "                           current_word_tokens[i] == p1 and \\\n",
        "                           current_word_tokens[i+1] == p2:\n",
        "                            # Проверяем, является ли это объединение частью более длинного\n",
        "                            # уже существующего в словаре токена\n",
        "                            if len(merged_token) > best_match_len:\n",
        "                                best_match_len = len(merged_token)\n",
        "                                best_match_token = merged_token\n",
        "                                found_merge = True\n",
        "\n",
        "                    if found_merge:\n",
        "                        new_word_tokens.append(best_match_token)\n",
        "                        i += 2\n",
        "                        merged_once = True\n",
        "                    else:\n",
        "                        new_word_tokens.append(current_word_tokens[i])\n",
        "                        i += 1\n",
        "\n",
        "                if not merged_once:\n",
        "                    break # Больше нет объединений для этого слова\n",
        "\n",
        "                current_word_tokens = new_word_tokens\n",
        "\n",
        "            # Преобразуем токены слова в их ID\n",
        "            for token in current_word_tokens:\n",
        "                if token in self.token_to_id:\n",
        "                    encoded_tokens_ids.append(self.token_to_id[token])\n",
        "                else:\n",
        "                    # Если токен не найден (например, из-за OOV или неполного обучения),\n",
        "                    # разбиваем его на символы и добавляем их ID\n",
        "                    for char in token:\n",
        "                        if char in self.token_to_id:\n",
        "                            encoded_tokens_ids.append(self.token_to_id[char])\n",
        "                        else:\n",
        "                            # Этого не должно произойти, если начальный словарь содержит все символы\n",
        "                            print(f\"Предупреждение: Символ '{char}' не найден в словаре.\")\n",
        "                            # Можно добавить токен для неизвестных символов, например, <unk>\n",
        "        return encoded_tokens_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Декодирует последовательность ID токенов обратно в строку.\n",
        "        :param token_ids: Список ID токенов.\n",
        "        :return: Декодированная строка.\n",
        "        \"\"\"\n",
        "        decoded_tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id < len(self.id_to_token):\n",
        "                decoded_tokens.append(self.id_to_token[token_id])\n",
        "            else:\n",
        "                print(f\"Предупреждение: ID токена {token_id} вне диапазона словаря.\")\n",
        "                decoded_tokens.append('') # Или можно использовать <unk>\n",
        "\n",
        "        # Объединяем токены, убираем символ конца слова и заменяем пробелы\n",
        "        decoded_text = \"\".join(decoded_tokens).replace('</w>', ' ').strip()\n",
        "        return decoded_text.replace(' ', '_') # Возвращаем исходный формат с подчеркиваниями\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus = \"Ученик_учится_в_школе,_а_учитель_учит_ученика\"\n",
        "\n",
        "    # Создаем экземпляр BPE, указывая количество объединений\n",
        "    # Можно также указать vocab_size=X для желаемого размера словаря\n",
        "    bpe_model = BPE(num_merges=10) # Выполним 10 итераций объединения\n",
        "\n",
        "    print(\"--- Обучение BPE модели ---\")\n",
        "    bpe_model.fit(corpus)\n",
        "    print(\"\\n--- Обучение завершено ---\")\n",
        "    print(f\"Финальный словарь BPE (отсортированный): {sorted(list(bpe_model.vocabulary))}\")\n",
        "    print(f\"Выполненные объединения: {bpe_model.merges}\")\n",
        "\n",
        "    # Тестирование токенизации\n",
        "    print(\"\\n--- Тестирование токенизации ---\")\n",
        "    text_to_encode = \"Ученик_учится_в_школе,_а_учитель_учит_ученика\"\n",
        "    encoded_ids = bpe_model.encode(text_to_encode)\n",
        "    print(f\"Исходный текст: '{text_to_encode}'\")\n",
        "    print(f\"Закодированные ID: {encoded_ids}\")\n",
        "\n",
        "    # Декодирование\n",
        "    decoded_text = bpe_model.decode(encoded_ids)\n",
        "    print(f\"Декодированный текст: '{decoded_text}'\")\n",
        "\n",
        "    # Проверка на новое слово (OOV)\n",
        "    print(\"\\n--- Тестирование OOV слова ---\")\n",
        "    oov_text = \"Учительница_учит\"\n",
        "    encoded_oov_ids = bpe_model.encode(oov_text)\n",
        "    print(f\"OOV текст: '{oov_text}'\")\n",
        "    print(f\"Закодированные ID OOV: {encoded_oov_ids}\")\n",
        "    decoded_oov_text = bpe_model.decode(encoded_oov_ids)\n",
        "    print(f\"Декодированный OOV текст: '{decoded_oov_text}'\")\n",
        "\n",
        "    # Пример токенов по ID\n",
        "    print(\"\\n--- Токены по ID ---\")\n",
        "    for i, token in enumerate(bpe_model.id_to_token):\n",
        "        print(f\"ID {i}: '{token}'\")"
      ],
      "metadata": {
        "id": "_3Sr07lngi4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordPiece"
      ],
      "metadata": {
        "id": "SBOZ-fJJgJmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkUXyyRH_xKi"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.merges = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Предварительная обработка текста: приведение к нижнему регистру и разбиение на слова.\n",
        "        \"\"\"\n",
        "        # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        # Разбиваем на слова по пробелам\n",
        "        words = processed_text.split(' ')\n",
        "        # Добавляем пробелы как отдельные токены между словами\n",
        "        initial_tokens = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word: # Убедимся, что слово не пустое\n",
        "                initial_tokens.extend(list(word))\n",
        "            if i < len(words) - 1:\n",
        "                initial_tokens.append(' ') # Добавляем пробел между словами\n",
        "        return initial_tokens, words # Возвращаем начальные токены и список слов для удобства\n",
        "\n",
        "    def calculate_frequencies(self, tokens):\n",
        "        \"\"\"\n",
        "        Подсчет частот отдельных токенов (униграмм) и биграмм в текущем корпусе.\n",
        "        \"\"\"\n",
        "        unigram_freq = collections.defaultdict(int)\n",
        "        bigram_freq = collections.defaultdict(int)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            unigram_freq[tokens[i]] += 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram_freq[(tokens[i], tokens[i+1])] += 1\n",
        "        return unigram_freq, bigram_freq\n",
        "\n",
        "    def calculate_score(self, unigram_freq, bigram_freq):\n",
        "        \"\"\"\n",
        "        Вычисление оценки слияния для всех возможных биграмм.\n",
        "        Score(A, B) = frequency(AB) / (frequency(A) * frequency(B))\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for (token_a, token_b), freq_ab in bigram_freq.items():\n",
        "            freq_a = unigram_freq[token_a]\n",
        "            freq_b = unigram_freq[token_b]\n",
        "            if freq_a > 0 and freq_b > 0: # Избегаем деления на ноль\n",
        "                score = freq_ab / (freq_a * freq_b)\n",
        "                scores[(token_a, token_b)] = score\n",
        "        return scores\n",
        "\n",
        "    def merge_tokens(self, tokens, best_bigram):\n",
        "        \"\"\"\n",
        "        Слияние лучшей биграммы в корпусе.\n",
        "        \"\"\"\n",
        "        merged_token = \"\".join(best_bigram)\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == best_bigram:\n",
        "                new_tokens.append(merged_token)\n",
        "                i += 2 # Пропускаем оба слитых токена\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        return new_tokens\n",
        "\n",
        "    def train(self, text, target_vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Обучение WordPiece токенизатора.\n",
        "        Итеративно сливает токены до достижения целевого размера словаря\n",
        "        или заданного количества слияний.\n",
        "        \"\"\"\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        current_tokens, _ = self.preprocess_text(text)\n",
        "\n",
        "        # Начальный словарь состоит из всех уникальных символов\n",
        "        self.vocab = set(current_tokens)\n",
        "        print(f\"\\nШаг 1: Начальный словарь (V0) содержит {len(self.vocab)} токенов.\")\n",
        "        print(f\"Начальный корпус: {current_tokens}\")\n",
        "\n",
        "        merges_count = 0\n",
        "        while True:\n",
        "            unigram_freq, bigram_freq = self.calculate_frequencies(current_tokens)\n",
        "            scores = self.calculate_score(unigram_freq, bigram_freq)\n",
        "\n",
        "            if not scores:\n",
        "                print(\"\\nНет больше биграмм для слияния. Остановка.\")\n",
        "                break\n",
        "\n",
        "            # Находим биграмму с наивысшей оценкой\n",
        "            best_bigram = max(scores, key=scores.get)\n",
        "            best_score = scores[best_bigram]\n",
        "\n",
        "            merged_token = \"\".join(best_bigram)\n",
        "\n",
        "            # Критерий остановки: если новый токен уже в словаре\n",
        "            if merged_token in self.vocab:\n",
        "                # Если лучший токен уже существует, удалим его из scores, чтобы найти следующий лучший\n",
        "                del scores[best_bigram]\n",
        "                continue # Продолжаем поиск\n",
        "\n",
        "            # Выводим информацию о текущем слиянии\n",
        "            print(f\"\\nИтерация {merges_count + 1}:\")\n",
        "            print(f\"  Лучшая биграмма для слияния: '{best_bigram[0]}' + '{best_bigram[1]}' -> '{merged_token}' (Score: {best_score:.4f})\")\n",
        "\n",
        "            # Выполняем слияние в корпусе\n",
        "            current_tokens = self.merge_tokens(current_tokens, best_bigram)\n",
        "\n",
        "            # Добавляем новый токен в словарь и сохраняем слияние\n",
        "            self.vocab.add(merged_token)\n",
        "            self.merges.append(best_bigram)\n",
        "            merges_count += 1\n",
        "\n",
        "            print(f\"  Корпус после слияния: {current_tokens}\")\n",
        "            print(f\"  Текущий размер словаря: {len(self.vocab)}\")\n",
        "\n",
        "            # Критерии остановки\n",
        "            if target_vocab_size is not None and len(self.vocab) >= target_vocab_size:\n",
        "                print(f\"\\nДостигнут целевой размер словаря ({target_vocab_size}). Остановка.\")\n",
        "                break\n",
        "            if num_merges is not None and merges_count >= num_merges:\n",
        "                print(f\"\\nДостигнуто заданное количество слияний ({num_merges}). Остановка.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nОбучение завершено. Итоговый словарь содержит {len(self.vocab)} токенов.\")\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Токенизация нового текста с использованием обученного словаря WordPiece.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'vocab') or len(self.vocab) == 0:\n",
        "            raise ValueError(\"Токенизатор не обучен. Сначала вызовите метод train().\")\n",
        "\n",
        "        print(f\"\\n--- Токенизация нового текста ---\")\n",
        "        print(f\"Текст для токенизации: '{text}'\")\n",
        "\n",
        "        # Предварительная обработка текста для токенизации\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        words = processed_text.split(' ')\n",
        "\n",
        "        final_tokens = []\n",
        "        for word in words:\n",
        "            if not word: # Пропускаем пустые строки, если они возникли из-за множественных пробелов\n",
        "                continue\n",
        "\n",
        "            word_tokens = []\n",
        "            remaining_word = word\n",
        "\n",
        "            while remaining_word:\n",
        "                found_match = False\n",
        "                # Ищем самый длинный подтокен из словаря, который является префиксом оставшейся части слова\n",
        "                for i in range(len(remaining_word), 0, -1):\n",
        "                    subword = remaining_word[:i]\n",
        "\n",
        "                    if subword in self.vocab:\n",
        "                        word_tokens.append(subword)\n",
        "                        remaining_word = remaining_word[i:]\n",
        "                        found_match = True\n",
        "                        break\n",
        "\n",
        "                if not found_match:\n",
        "                    # Если не удалось найти соответствующий токен, разбиваем на символы\n",
        "                    # или используем токен [UNK]\n",
        "                    if remaining_word[0] in self.vocab: # Если символ есть в словаре\n",
        "                        word_tokens.append(remaining_word[0])\n",
        "                    else: # Если символ даже не в начальном словаре\n",
        "                        word_tokens.append('[UNK]')\n",
        "                    remaining_word = remaining_word[1:]\n",
        "\n",
        "            final_tokens.extend(word_tokens)\n",
        "            final_tokens.append(' ') # Добавляем пробел между токенизированными словами\n",
        "\n",
        "        # Удаляем последний пробел, если он есть\n",
        "        if final_tokens and final_tokens[-1] == ' ':\n",
        "            final_tokens.pop()\n",
        "\n",
        "        print(f\"Токенизированный текст: {final_tokens}\")\n",
        "        return final_tokens\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Возвращает текущий словарь токенов.\"\"\"\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def get_merges(self):\n",
        "        \"\"\"Возвращает список выполненных слияний.\"\"\"\n",
        "        return self.merges\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    text_to_train = \"В училище учитель учит ученикам по новому учебнику\"\n",
        "\n",
        "    # Создаем экземпляр токенизатора\n",
        "    tokenizer = WordPieceTokenizer()\n",
        "\n",
        "    # Обучаем токенизатор\n",
        "    final_vocab = tokenizer.train(text_to_train, num_merges=50)\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece: {final_vocab}\")\n",
        "\n",
        "    # Токенизируем тот же текст\n",
        "    tokenizer.tokenize(text_to_train)\n",
        "\n",
        "    # Пример токенизации нового слова\n",
        "    new_text = \"учительница\"\n",
        "    # Добавим '##ница' в словарь для демонстрации\n",
        "    if 'ница' not in final_vocab:\n",
        "        tokenizer.vocab.add('ница')\n",
        "        tokenizer.vocab.add('##ница')\n",
        "        final_vocab = tokenizer.get_vocab()\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece (с 'ница' для демонстрации): {final_vocab}\")\n",
        "    tokenizer.tokenize(new_text)\n",
        "\n",
        "    # Демонстрация с упрощенным словарем\n",
        "    simplified_tokenizer = WordPieceTokenizer()\n",
        "    simplified_tokenizer.vocab = set(['у', 'ч', 'и', 'т', 'е', 'л', 'ь', 'н', 'ц', 'а', 'учитель', '##ница', '##тель', '##а'])\n",
        "    print(f\"\\n--- Демонстрация токенизации подслов с упрощенным словарем ---\")\n",
        "    simplified_tokenizer.tokenize(\"учительница\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Униграмной языковой модели"
      ],
      "metadata": {
        "id": "dZw5ZOm5gE_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "import math\n",
        "\n",
        "class UnigramLanguageModel:\n",
        "    \"\"\"\n",
        "    Реализация Униграмной Языковой Модели.\n",
        "    Эта модель предполагает, что вероятность каждого слова в последовательности\n",
        "    не зависит от других слов.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.word_counts = collections.defaultdict(int) # Счетчик вхождений каждого слова\n",
        "        self.total_words = 0 # Общее количество слов в корпусе\n",
        "        self.vocabulary = set() # Множество уникальных слов (словарь)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Приватный метод для токенизации текста.\n",
        "        Приводит текст к нижнему регистру и разбивает на слова, удаляя знаки препинания.\n",
        "        :param text: Входная строка текста.\n",
        "        :return: Список токенов (слов).\n",
        "        \"\"\"\n",
        "        # Приводим к нижнему регистру\n",
        "        text = text.lower()\n",
        "        # Удаляем знаки препинания (кроме тех, что могут быть частью слова, но для простоты здесь удаляем все)\n",
        "        # Использование регулярного выражения для извлечения только буквенных последовательностей\n",
        "        tokens = re.findall(r'\\b[а-яё]+\\b', text)\n",
        "        return tokens\n",
        "\n",
        "    def train(self, corpus_text):\n",
        "        \"\"\"\n",
        "        Обучает униграмную языковую модель на заданном текстовом корпусе.\n",
        "        Подсчитывает частоты слов и формирует словарь.\n",
        "        :param corpus_text: Строка, представляющая обучающий корпус.\n",
        "        \"\"\"\n",
        "        print(\"--- Начало обучения Униграмной Модели ---\")\n",
        "        tokens = self._tokenize(corpus_text)\n",
        "\n",
        "        if not tokens:\n",
        "            print(\"Предупреждение: Корпус пуст или не содержит слов после токенизации.\")\n",
        "            return\n",
        "\n",
        "        self.total_words = len(tokens)\n",
        "\n",
        "        for word in tokens:\n",
        "            self.word_counts[word] += 1\n",
        "            self.vocabulary.add(word)\n",
        "\n",
        "        print(f\"Обучение завершено. Общее количество слов (N): {self.total_words}\")\n",
        "        print(f\"Размер словаря (|V|): {len(self.vocabulary)}\")\n",
        "        print(f\"Частоты слов: {dict(self.word_counts)}\")\n",
        "        print(\"--- Обучение завершено ---\")\n",
        "\n",
        "    def get_word_probability(self, word, smoothing=None):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность слова P(w).\n",
        "        :param word: Слово, для которого нужно рассчитать вероятность.\n",
        "        :param smoothing: Метод сглаживания ('laplace' для сглаживания по Лапласу, None для MLE).\n",
        "        :return: Вероятность слова.\n",
        "        \"\"\"\n",
        "        # Если модель не обучена или корпус пуст\n",
        "        if self.total_words == 0:\n",
        "            print(\"Ошибка: Модель не обучена или корпус пуст. Невозможно рассчитать вероятность.\")\n",
        "            return 0.0\n",
        "\n",
        "        count_w = self.word_counts[word] # Count(w)\n",
        "\n",
        "        if smoothing == 'laplace':\n",
        "            # Сглаживание по Лапласу: (Count(w) + 1) / (N + |V|)\n",
        "            probability = (count_w + 1) / (self.total_words + len(self.vocabulary))\n",
        "            # print(f\"P_Laplace('{word}') = ({count_w} + 1) / ({self.total_words} + {len(self.vocabulary)}) = {probability:.4f}\")\n",
        "        else:\n",
        "            # Метод максимального правдоподобия (MLE): Count(w) / N\n",
        "            if count_w == 0:\n",
        "                # print(f\"P_MLE('{word}') = 0 (слово не найдено в корпусе)\")\n",
        "                return 0.0 # Если слово не найдено, вероятность 0 без сглаживания\n",
        "            probability = count_w / self.total_words\n",
        "            # print(f\"P_MLE('{word}') = {count_w} / {self.total_words} = {probability:.4f}\")\n",
        "\n",
        "        return probability\n",
        "\n",
        "    def get_sequence_probability(self, sequence_text, smoothing=None):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность последовательности слов P(W).\n",
        "        :param sequence_text: Строка, представляющая последовательность слов.\n",
        "        :param smoothing: Метод сглаживания ('laplace' для сглаживания по Лапласу, None для MLE).\n",
        "        :return: Вероятность последовательности.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Расчет вероятности последовательности: '{sequence_text}' ---\")\n",
        "        tokens = self._tokenize(sequence_text)\n",
        "\n",
        "        if not tokens:\n",
        "            print(\"Предупреждение: Последовательность пуста или не содержит слов после токенизации.\")\n",
        "            return 0.0\n",
        "\n",
        "        sequence_probability = 1.0\n",
        "        probabilities_list = []\n",
        "\n",
        "        for word in tokens:\n",
        "            p_word = self.get_word_probability(word, smoothing)\n",
        "            probabilities_list.append(p_word)\n",
        "            sequence_probability *= p_word\n",
        "\n",
        "            # Если хоть одно слово имеет нулевую вероятность без сглаживания,\n",
        "            # вся последовательность будет иметь нулевую вероятность.\n",
        "            if smoothing != 'laplace' and p_word == 0:\n",
        "                print(f\"Слово '{word}' имеет нулевую вероятность. Вероятность всей последовательности = 0.\")\n",
        "                return 0.0 # Оптимизация: если P(w)=0, то произведение будет 0\n",
        "\n",
        "        print(f\"Токены последовательности: {tokens}\")\n",
        "        print(f\"Вероятности отдельных слов: {probabilities_list}\")\n",
        "        print(f\"Итоговая вероятность последовательности: {sequence_probability}\")\n",
        "        print(\"--- Расчет завершен ---\")\n",
        "        return sequence_probability\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_example = \"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"\n",
        "\n",
        "    # Создаем экземпляр модели\n",
        "    unigram_model = UnigramLanguageModel()\n",
        "\n",
        "    # Обучаем модель на корпусе\n",
        "    unigram_model.train(corpus_example)\n",
        "\n",
        "    print(\"\\n--- Расчет вероятностей отдельных слов (MLE) ---\")\n",
        "    words_to_check = ['пишут', 'учитель', 'школа', 'мы']\n",
        "    for word in words_to_check:\n",
        "        prob = unigram_model.get_word_probability(word, smoothing=None)\n",
        "        print(f\"P_MLE('{word}') = {prob:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Расчет вероятностей отдельных слов (Лаплас) ---\")\n",
        "    for word in words_to_check:\n",
        "        prob = unigram_model.get_word_probability(word, smoothing='laplace')\n",
        "        print(f\"P_Laplace('{word}') = {prob:.4f}\")\n",
        "\n",
        "    # Расчет вероятности предложения (без сглаживания)\n",
        "    sentence_mle = \"Ученики пишут диктант\"\n",
        "    prob_mle = unigram_model.get_sequence_probability(sentence_mle, smoothing=None)\n",
        "    print(f\"Вероятность предложения (MLE): {prob_mle:.10f}\")\n",
        "\n",
        "    sentence_mle_oov = \"Ученики пишут школа\" # Содержит OOV слово 'школа'\n",
        "    prob_mle_oov = unigram_model.get_sequence_probability(sentence_mle_oov, smoothing=None)\n",
        "    print(f\"Вероятность предложения с OOV (MLE): {prob_mle_oov:.10f}\")\n",
        "\n",
        "    # Расчет вероятности предложения (со сглаживанием по Лапласу)\n",
        "    sentence_laplace = \"Ученики пишут диктант\"\n",
        "    prob_laplace = unigram_model.get_sequence_probability(sentence_laplace, smoothing='laplace')\n",
        "    print(f\"Вероятность предложения (Лаплас): {prob_laplace:.10f}\")\n",
        "\n",
        "    sentence_laplace_oov = \"Ученики пишут школа\" # Содержит OOV слово 'школа'\n",
        "    prob_laplace_oov = unigram_model.get_sequence_probability(sentence_laplace_oov, smoothing='laplace')\n",
        "    print(f\"Вероятность предложения с OOV (Лаплас): {prob_laplace_oov:.10f}\")\n",
        "\n",
        "    # Пример из документации: \"Ученики пишут диктант Учитель диктует Мы пишем он пишет они пишут Пишите внимательно\"\n",
        "    full_sentence = \"Ученики пишут диктант Учитель диктует Мы пишем он пишет они пишут Пишите внимательно\"\n",
        "    prob_full_mle = unigram_model.get_sequence_probability(full_sentence, smoothing=None)\n",
        "    print(f\"Вероятность полного предложения (MLE): {prob_full_mle:.15f}\")\n",
        "\n",
        "    prob_full_laplace = unigram_model.get_sequence_probability(full_sentence, smoothing='laplace')\n",
        "    print(f\"Вероятность полного предложения (Лаплас): {prob_full_laplace:.15f}\")\n"
      ],
      "metadata": {
        "id": "Smok_jSifd1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece"
      ],
      "metadata": {
        "id": "aSRNpUAbgAuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "class SentencePieceWrapper:\n",
        "    \"\"\"\n",
        "    Класс-обертка для демонстрации использования библиотеки SentencePiece.\n",
        "    Позволяет обучать модель, токенизировать и детокенизировать текст.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.sp_processor = spm.SentencePieceProcessor()\n",
        "        self.model_prefix = None\n",
        "        self.model_path = None\n",
        "        self.vocab_path = None\n",
        "        self._is_model_loaded = False # Добавляем флаг для отслеживания состояния загрузки модели\n",
        "\n",
        "    def train(self, text_data, model_prefix=\"my_sp_model\", vocab_size=8000, model_type='unigram', character_coverage=1.0):\n",
        "        \"\"\"\n",
        "        Обучает модель SentencePiece на заданном текстовом корпусе.\n",
        "        Создает временный файл для обучения и сохраняет модель.\n",
        "\n",
        "        :param text_data: Строка, содержащая обучающий текст.\n",
        "        :param model_prefix: Префикс для имен файлов модели (model.model и model.vocab).\n",
        "        :param vocab_size: Желаемый размер словаря подслов.\n",
        "        :param model_type: Алгоритм обучения ('unigram' или 'bpe').\n",
        "        :param character_coverage: Процент символов, которые должны быть покрыты моделью.\n",
        "        \"\"\"\n",
        "        print(f\"--- Начинается обучение модели SentencePiece ---\")\n",
        "        print(f\"Текст для обучения (фрагмент): '{text_data[:100]}...'\")\n",
        "\n",
        "        # Создаем временный файл для обучения\n",
        "        # NamedTemporaryFile гарантирует уникальное имя и автоматическое удаление при закрытии\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False) as temp_file:\n",
        "            temp_file.write(text_data)\n",
        "            train_file_name = temp_file.name\n",
        "\n",
        "        self.model_prefix = model_prefix\n",
        "        self.model_path = f\"{model_prefix}.model\"\n",
        "        self.vocab_path = f\"{model_prefix}.vocab\"\n",
        "\n",
        "        try:\n",
        "            print(f\"Обучение с параметрами: vocab_size={vocab_size}, model_type='{model_type}'\")\n",
        "            spm.SentencePieceTrainer.train(\n",
        "                input=train_file_name,\n",
        "                model_prefix=self.model_prefix,\n",
        "                vocab_size=vocab_size,\n",
        "                character_coverage=character_coverage,\n",
        "                model_type=model_type\n",
        "            )\n",
        "            print(f\"Обучение завершено. Созданы файлы: '{self.model_path}' и '{self.vocab_path}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обучении модели SentencePiece: {e}\")\n",
        "            # Попытаемся удалить созданные файлы, если обучение не удалось\n",
        "            self._cleanup_files()\n",
        "            raise # Перевыбрасываем исключение\n",
        "        finally:\n",
        "            # Удаляем временный файл после обучения\n",
        "            if os.path.exists(train_file_name):\n",
        "                os.remove(train_file_name)\n",
        "                print(f\"Временный файл обучения удален: '{train_file_name}'\")\n",
        "\n",
        "        # Загружаем обученную модель\n",
        "        self.load_model(self.model_path)\n",
        "        print(\"--- Обучение и загрузка модели завершены ---\")\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Загружает обученную модель SentencePiece из файла.\n",
        "        :param model_path: Путь к файлу модели (.model).\n",
        "        \"\"\"\n",
        "        print(f\"--- Загрузка модели SentencePiece ---\")\n",
        "        try:\n",
        "            self.sp_processor.load(model_path)\n",
        "            self._is_model_loaded = True # Устанавливаем флаг после успешной загрузки\n",
        "            print(f\"Модель SentencePiece успешно загружена из: '{model_path}'\")\n",
        "        except Exception as e:\n",
        "            self._is_model_loaded = False # Сбрасываем флаг в случае ошибки\n",
        "            print(f\"Ошибка при загрузке модели SentencePiece: {e}\")\n",
        "            raise # Перевыбрасываем исключение\n",
        "        print(\"--- Загрузка модели завершена ---\")\n",
        "\n",
        "    def encode_as_ids(self, text):\n",
        "        \"\"\"\n",
        "        Кодирует текст в последовательность числовых ID токенов.\n",
        "        :param text: Входная строка текста.\n",
        "        :return: Список числовых ID токенов.\n",
        "        \"\"\"\n",
        "        if not self._is_model_loaded: # Используем наш флаг\n",
        "            raise ValueError(\"Модель SentencePiece не загружена. Сначала обучите или загрузите модель.\")\n",
        "\n",
        "        print(f\"\\n--- Кодирование текста в ID ---\")\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        encoded_ids = self.sp_processor.encode_as_ids(text)\n",
        "        print(f\"Закодированные ID: {encoded_ids}\")\n",
        "        return encoded_ids\n",
        "\n",
        "    def encode_as_pieces(self, text):\n",
        "        \"\"\"\n",
        "        Кодирует текст в последовательность строковых токенов (подслов).\n",
        "        :param text: Входная строка текста.\n",
        "        :return: Список строковых токенов.\n",
        "        \"\"\"\n",
        "        if not self._is_model_loaded: # Используем наш флаг\n",
        "            raise ValueError(\"Модель SentencePiece не загружена. Сначала обучите или загрузите модель.\")\n",
        "\n",
        "        print(f\"\\n--- Кодирование текста в подслова ---\")\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        encoded_pieces = self.sp_processor.encode_as_pieces(text)\n",
        "        print(f\"Закодированные подслова: {encoded_pieces}\")\n",
        "        return encoded_pieces\n",
        "\n",
        "    def decode_ids(self, ids):\n",
        "        \"\"\"\n",
        "        Декодирует последовательность числовых ID токенов обратно в строку.\n",
        "        :param ids: Список числовых ID токенов.\n",
        "        :return: Декодированная строка.\n",
        "        \"\"\"\n",
        "        if not self._is_model_loaded: # Используем наш флаг\n",
        "            raise ValueError(\"Модель SentencePiece не загружена. Сначала обучите или загрузите модель.\")\n",
        "\n",
        "        print(f\"\\n--- Декодирование ID в текст ---\")\n",
        "        print(f\"ID для декодирования: {ids}\")\n",
        "        decoded_text = self.sp_processor.decode_ids(ids)\n",
        "        print(f\"Декодированный текст: '{decoded_text}'\")\n",
        "        return decoded_text\n",
        "\n",
        "    def decode_pieces(self, pieces):\n",
        "        \"\"\"\n",
        "        Декодирует последовательность строковых токенов (подслов) обратно в строку.\n",
        "        :param pieces: Список строковых токенов.\n",
        "        :return: Декодированная строка.\n",
        "        \"\"\"\n",
        "        if not self._is_model_loaded: # Используем наш флаг\n",
        "            raise ValueError(\"Модель SentencePiece не загружена. Сначала обучите или загрузите модель.\")\n",
        "\n",
        "        print(f\"\\n--- Декодирование подслов в текст ---\")\n",
        "        print(f\"Подслова для декодирования: {pieces}\")\n",
        "        decoded_text = self.sp_processor.decode_pieces(pieces)\n",
        "        print(f\"Декодированный текст: '{decoded_text}'\")\n",
        "        return decoded_text\n",
        "\n",
        "    def _cleanup_files(self):\n",
        "        \"\"\"\n",
        "        Удаляет файлы модели SentencePiece (.model и .vocab), если они существуют.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Очистка временных файлов модели ---\")\n",
        "        if self.model_path and os.path.exists(self.model_path):\n",
        "            os.remove(self.model_path)\n",
        "            print(f\"Удален файл модели: '{self.model_path}'\")\n",
        "        if self.vocab_path and os.path.exists(self.vocab_path):\n",
        "            os.remove(self.vocab_path)\n",
        "            print(f\"Удален файл словаря: '{self.vocab_path}'\")\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Пример текста для обучения\n",
        "    corpus_text = \"\"\"Учитель говорит: «Откройте учебники». Ученики открывают учебники. Откройте страницу 15. Страница сложная. Страницы учебника содержат много информации.\"\"\"\n",
        "\n",
        "    # Создаем экземпляр класса SentencePieceWrapper\n",
        "    sp_wrapper = SentencePieceWrapper()\n",
        "\n",
        "    try:\n",
        "        # Обучаем модель с заданным размером словаря (для демонстрации)\n",
        "        sp_wrapper.train(corpus_text, vocab_size=50, model_type='unigram')\n",
        "\n",
        "        # Тестируем кодирование и декодирование\n",
        "        input_text_1 = \"Ученики открывают учебники.\"\n",
        "        encoded_ids_1 = sp_wrapper.encode_as_ids(input_text_1)\n",
        "        decoded_text_1 = sp_wrapper.decode_ids(encoded_ids_1)\n",
        "        print(f\"\\nПроверка обратимости 1: Исходный: '{input_text_1}', Декодированный: '{decoded_text_1}'\")\n",
        "        assert input_text_1 == decoded_text_1, \"Ошибка обратимости для input_text_1\"\n",
        "\n",
        "        input_text_2 = \"Страница сложная.\"\n",
        "        encoded_pieces_2 = sp_wrapper.encode_as_pieces(input_text_2)\n",
        "        decoded_text_2 = sp_wrapper.decode_pieces(encoded_pieces_2)\n",
        "        print(f\"\\nПроверка обратимости 2: Исходный: '{input_text_2}', Декодированный: '{decoded_text_2}'\")\n",
        "        assert input_text_2 == decoded_text_2, \"Ошибка обратимости для input_text_2\"\n",
        "\n",
        "        # Пример с OOV-словом (если оно не было полностью обучено)\n",
        "        oov_text = \"Преподавательница читает.\"\n",
        "        print(f\"\\n--- Тестирование OOV-слова ---\")\n",
        "        encoded_oov_ids = sp_wrapper.encode_as_ids(oov_text)\n",
        "        decoded_oov_text = sp_wrapper.decode_ids(encoded_oov_ids)\n",
        "        print(f\"Исходный OOV: '{oov_text}', Закодированные ID: {encoded_oov_ids}, Декодированный: '{decoded_oov_text}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка в процессе выполнения: {e}\")\n",
        "    finally:\n",
        "        # Очищаем созданные файлы модели\n",
        "        sp_wrapper._cleanup_files()\n"
      ],
      "metadata": {
        "id": "CgSI01X3bSV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Класс для работы с Hugging Face Tokenizers\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N68TN7saxryB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, normalizers, processors, trainers\n",
        "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents\n",
        "from typing import List, Optional, Dict, Any, Union\n",
        "\n",
        "class HuggingFaceTokenizerWrapper:\n",
        "    \"\"\"\n",
        "    Класс-обертка для работы с библиотекой Hugging Face Tokenizers.\n",
        "\n",
        "    Предоставляет функциональность для обучения, сохранения, загрузки,\n",
        "    кодирования и декодирования текста с использованием различных\n",
        "    алгоритмов субтокенизации (BPE, WordPiece, Unigram).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Инициализирует обертку токенизатора.\n",
        "        \"\"\"\n",
        "        self.tokenizer: Optional[Tokenizer] = None\n",
        "        self.model_config: Dict[str, Any] = {}\n",
        "        self.trained_model_path: Optional[str] = None\n",
        "        print(\"HuggingFaceTokenizerWrapper: Инициализация.\")\n",
        "\n",
        "    def _create_temp_train_file(self, text_data: Union[str, List[str]]) -> str:\n",
        "        \"\"\"\n",
        "        Создает временный файл(ы) для обучения токенизатора.\n",
        "\n",
        "        :param text_data: Строка или список строк с обучающими данными.\n",
        "        :return: Путь к временному файлу обучения.\n",
        "        \"\"\"\n",
        "        # Используем NamedTemporaryFile для безопасного создания временных файлов\n",
        "        # и их автоматического удаления при закрытии\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False) as temp_file:\n",
        "            if isinstance(text_data, list):\n",
        "                temp_file.write(\"\\n\".join(text_data))\n",
        "            else:\n",
        "                temp_file.write(text_data)\n",
        "            temp_file_path = temp_file.name\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Создан временный файл для обучения: '{temp_file_path}'\")\n",
        "        return temp_file_path\n",
        "\n",
        "    def train(self,\n",
        "              corpus_text: Union[str, List[str]],\n",
        "              model_type: str = 'bpe',\n",
        "              vocab_size: int = 30000,\n",
        "              min_frequency: int = 2,\n",
        "              special_tokens: Optional[List[str]] = None,\n",
        "              output_path: str = \"my_hf_tokenizer.json\",\n",
        "              normalizer_config: Optional[Dict[str, Any]] = None,\n",
        "              pretokenizer_config: Optional[Dict[str, Any]] = None,\n",
        "              postprocessor_config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Обучает новый токенизатор на заданном корпусе.\n",
        "\n",
        "        :param corpus_text: Текст или список текстов для обучения.\n",
        "        :param model_type: Тип модели токенизации ('bpe', 'wordpiece', 'unigram', 'wordlevel', 'charlevel').\n",
        "                           По умолчанию 'bpe'.\n",
        "        :param vocab_size: Желаемый размер словаря.\n",
        "        :param min_frequency: Минимальная частота токена для включения в словарь.\n",
        "        :param special_tokens: Список специальных токенов (например, [UNK], [CLS], [SEP]).\n",
        "                               По умолчанию: [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"].\n",
        "        :param output_path: Путь для сохранения обученного токенизатора в формате JSON.\n",
        "        :param normalizer_config: Словарь для настройки нормализатора.\n",
        "                                  Пример: {'type': 'Sequence', 'sub_normalizers': [{'type': 'NFD'}, {'type': 'Lowercase'}]}\n",
        "        :param pretokenizer_config: Словарь для настройки предварительного токенизатора.\n",
        "                                   Пример: {'type': 'Whitespace'}\n",
        "        :param postprocessor_config: Словарь для настройки постобработчика.\n",
        "                                    Пример: {'type': 'BertProcessing'}\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Начинается обучение токенизатора типа '{model_type}' ---\")\n",
        "\n",
        "        if special_tokens is None:\n",
        "            special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "\n",
        "        # 1. Инициализация модели токенизации\n",
        "        if model_type == 'bpe':\n",
        "            self.tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "            trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=special_tokens)\n",
        "        elif model_type == 'wordpiece':\n",
        "            self.tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "            trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=special_tokens)\n",
        "        elif model_type == 'unigram':\n",
        "            self.tokenizer = Tokenizer(models.Unigram())\n",
        "            # UnigramTrainer не имеет min_frequency, но имеет limit_alphabet\n",
        "            trainer = trainers.UnigramTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "        elif model_type == 'wordlevel':\n",
        "            self.tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
        "            trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=special_tokens)\n",
        "        elif model_type == 'charlevel':\n",
        "            self.tokenizer = Tokenizer(models.CharLevel(unk_token=\"[UNK]\"))\n",
        "            trainer = trainers.CharLevelTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=special_tokens)\n",
        "        else:\n",
        "            raise ValueError(f\"Неизвестный тип модели: {model_type}. Поддерживаются: bpe, wordpiece, unigram, wordlevel, charlevel.\")\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Инициализирована модель: {model_type}\")\n",
        "\n",
        "        # 2. Настройка нормализатора\n",
        "        if normalizer_config:\n",
        "            self._set_normalizer(normalizer_config)\n",
        "        else:\n",
        "            # Нормализатор по умолчанию: NFD, Lowercase, StripAccents\n",
        "            self.tokenizer.normalizer = Sequence([\n",
        "                NFD(),\n",
        "                Lowercase(),\n",
        "                StripAccents()\n",
        "            ])\n",
        "            print(\"HuggingFaceTokenizerWrapper: Настроен нормализатор по умолчанию (NFD, Lowercase, StripAccents).\")\n",
        "\n",
        "        # 3. Настройка предварительного токенизатора\n",
        "        if pretokenizer_config:\n",
        "            self._set_pretokenizer(pretokenizer_config)\n",
        "        else:\n",
        "            # Предварительный токенизатор по умолчанию: Whitespace\n",
        "            self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "            print(\"HuggingFaceTokenizerWrapper: Настроен предварительный токенизатор по умолчанию (Whitespace).\")\n",
        "\n",
        "        # 4. Обучение токенизатора\n",
        "        temp_file_path = self._create_temp_train_file(corpus_text)\n",
        "        try:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Начинается обучение модели...\")\n",
        "            # Передаем объект trainer в метод train\n",
        "            self.tokenizer.train([temp_file_path], trainer=trainer)\n",
        "            print(\"HuggingFaceTokenizerWrapper: Обучение завершено.\")\n",
        "        except Exception as e:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Ошибка при обучении токенизатора: {e}\")\n",
        "            raise\n",
        "        finally:\n",
        "            if os.path.exists(temp_file_path):\n",
        "                os.remove(temp_file_path)\n",
        "                print(f\"HuggingFaceTokenizerWrapper: Временный файл обучения удален: '{temp_file_path}'\")\n",
        "\n",
        "        # 5. Настройка постобработчика\n",
        "        if postprocessor_config:\n",
        "            self._set_postprocessor(postprocessor_config)\n",
        "        else:\n",
        "            # Постобработчик по умолчанию для BERT-подобных моделей\n",
        "            if \"[CLS]\" in special_tokens and \"[SEP]\" in special_tokens:\n",
        "                self.tokenizer.post_processor = processors.TemplateProcessing(\n",
        "                    single=\"[CLS] $A [SEP]\",\n",
        "                    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "                    special_tokens=[\n",
        "                        (\"[CLS]\", self.tokenizer.token_to_id(\"[CLS]\")),\n",
        "                        (\"[SEP]\", self.tokenizer.token_to_id(\"[SEP]\")),\n",
        "                    ],\n",
        "                )\n",
        "                print(\"HuggingFaceTokenizerWrapper: Настроен постобработчик по умолчанию (BERT-совместимый).\")\n",
        "            else:\n",
        "                print(\"HuggingFaceTokenizerWrapper: Постобработчик не настроен (специальные токены [CLS]/[SEP] отсутствуют).\")\n",
        "\n",
        "\n",
        "        # 6. Сохранение обученного токенизатора\n",
        "        self.save_tokenizer(output_path)\n",
        "        print(f\"--- Обучение и сохранение токенизатора завершены ---\")\n",
        "\n",
        "    def _set_normalizer(self, config: Dict[str, Any]):\n",
        "        \"\"\"Настраивает нормализатор токенизатора.\"\"\"\n",
        "        norm_type = config.get('type')\n",
        "        if norm_type == 'Sequence':\n",
        "            sub_normalizers = []\n",
        "            for sub_norm_cfg in config.get('sub_normalizers', []):\n",
        "                sub_norm_type = sub_norm_cfg.get('type')\n",
        "                if sub_norm_type == 'NFD': sub_normalizers.append(NFD())\n",
        "                elif sub_norm_type == 'Lowercase': sub_normalizers.append(Lowercase())\n",
        "                elif sub_norm_type == 'StripAccents': sub_normalizers.append(StripAccents())\n",
        "                # Добавьте другие нормализаторы по мере необходимости\n",
        "                else: print(f\"HuggingFaceTokenizerWrapper: Неизвестный суб-нормализатор: {sub_norm_type}\")\n",
        "            self.tokenizer.normalizer = Sequence(sub_normalizers)\n",
        "        elif norm_type == 'Lowercase':\n",
        "            self.tokenizer.normalizer = Lowercase()\n",
        "        # Добавьте другие типы нормализаторов по мере необходимости\n",
        "        else:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Неизвестный тип нормализатора: {norm_type}. Используется по умолчанию.\")\n",
        "            self.tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Настроен нормализатор: {norm_type}\")\n",
        "\n",
        "    def _set_pretokenizer(self, config: Dict[str, Any]):\n",
        "        \"\"\"Настраивает предварительный токенизатор.\"\"\"\n",
        "        pre_tok_type = config.get('type')\n",
        "        if pre_tok_type == 'Whitespace':\n",
        "            self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        elif pre_tok_type == 'WhitespaceSplit':\n",
        "            self.tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "        elif pre_tok_type == 'Punctuation':\n",
        "            self.tokenizer.pre_tokenizer = pre_tokenizers.Punctuation()\n",
        "        # Добавьте другие типы предварительных токенизаторов по мере необходимости\n",
        "        else:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Неизвестный тип предварительного токенизатора: {pre_tok_type}. Используется по умолчанию.\")\n",
        "            self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Настроен предварительный токенизатор: {pre_tok_type}\")\n",
        "\n",
        "    def _set_postprocessor(self, config: Dict[str, Any]):\n",
        "        \"\"\"Настраивает постобработчик.\"\"\"\n",
        "        post_proc_type = config.get('type')\n",
        "        if post_proc_type == 'TemplateProcessing':\n",
        "            special_tokens_map = []\n",
        "            for token_name, token_id in config.get('special_tokens', []):\n",
        "                special_tokens_map.append((token_name, self.tokenizer.token_to_id(token_name)))\n",
        "            self.tokenizer.post_processor = processors.TemplateProcessing(\n",
        "                single=config.get('single'),\n",
        "                pair=config.get('pair'),\n",
        "                special_tokens=special_tokens_map\n",
        "            )\n",
        "        elif post_proc_type == 'BertProcessing':\n",
        "            # Предполагаем, что CLS и SEP токены уже в словаре\n",
        "            cls_id = self.tokenizer.token_to_id(\"[CLS]\")\n",
        "            sep_id = self.tokenizer.token_to_id(\"[SEP]\")\n",
        "            if cls_id is not None and sep_id is not None:\n",
        "                self.tokenizer.post_processor = processors.BertProcessing(\n",
        "                    sep=(f\"[SEP]\", sep_id),\n",
        "                    cls=(f\"[CLS]\", cls_id)\n",
        "                )\n",
        "            else:\n",
        "                print(\"HuggingFaceTokenizerWrapper: Не удалось настроить BertProcessing: отсутствуют [CLS] или [SEP] ID.\")\n",
        "        # Добавьте другие типы постобработчиков по мере необходимости\n",
        "        else:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Неизвестный тип постобработчика: {post_proc_type}. Постобработчик не настроен.\")\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Настроен постобработчик: {post_proc_type}\")\n",
        "\n",
        "\n",
        "    def save_tokenizer(self, path: str):\n",
        "        \"\"\"\n",
        "        Сохраняет обученный токенизатор в файл JSON.\n",
        "\n",
        "        :param path: Путь к файлу для сохранения.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Токенизатор не обучен. Сначала вызовите метод 'train'.\")\n",
        "        try:\n",
        "            self.tokenizer.save(path)\n",
        "            self.trained_model_path = path\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Токенизатор успешно сохранен в '{path}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Ошибка при сохранении токенизатора: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_tokenizer(self, path: str):\n",
        "        \"\"\"\n",
        "        Загружает токенизатор из файла JSON.\n",
        "\n",
        "        :param path: Путь к файлу JSON с токенизатором.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Загрузка токенизатора из '{path}' ---\")\n",
        "        try:\n",
        "            self.tokenizer = Tokenizer.from_file(path)\n",
        "            self.trained_model_path = path\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Токенизатор успешно загружен из '{path}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"HuggingFaceTokenizerWrapper: Ошибка при загрузке токенизатора: {e}\")\n",
        "            raise\n",
        "        print(f\"--- Загрузка токенизатора завершена ---\")\n",
        "\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Кодирует входной текст.\n",
        "\n",
        "        :param text: Текст для кодирования.\n",
        "        :param add_special_tokens: Добавлять ли специальные токены (например, [CLS], [SEP]).\n",
        "        :return: Словарь с закодированными данными (ids, tokens, attention_mask, type_ids, offsets).\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Токенизатор не загружен или не обучен. Сначала обучите или загрузите модель.\")\n",
        "\n",
        "        print(f\"\\n--- Кодирование текста: '{text}' ---\")\n",
        "        # Метод encode возвращает объект Encoding, который содержит все необходимые данные\n",
        "        encoding = self.tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
        "\n",
        "        result = {\n",
        "            # 'normalized_str' не является прямым атрибутом Encoding объекта.\n",
        "            # Если нужна нормализованная строка, ее нужно получить до кодирования.\n",
        "            # Для простоты, мы будем использовать оригинальную строку, если она нужна для вывода.\n",
        "            # \"normalized_str\": encoding.normalized_str, # Эту строку убираем\n",
        "            \"tokens\": encoding.tokens,\n",
        "            \"ids\": encoding.ids,\n",
        "            \"attention_mask\": encoding.attention_mask,\n",
        "            \"type_ids\": encoding.type_ids,\n",
        "            \"offsets\": encoding.offsets\n",
        "        }\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Закодированные токены: {result['tokens']}\")\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Закодированные ID: {result['ids']}\")\n",
        "        return result\n",
        "\n",
        "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Декодирует последовательность ID токенов обратно в строку.\n",
        "\n",
        "        :param ids: Список числовых ID токенов.\n",
        "        :param skip_special_tokens: Пропускать ли специальные токены при декодировании.\n",
        "        :return: Декодированная строка.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Токенизатор не загружен или не обучен. Сначала обучите или загрузите модель.\")\n",
        "\n",
        "        print(f\"\\n--- Декодирование ID: {ids} ---\")\n",
        "        decoded_text = self.tokenizer.decode(ids, skip_special_tokens=skip_special_tokens)\n",
        "        print(f\"HuggingFaceTokenizerWrapper: Декодированный текст: '{decoded_text}'\")\n",
        "        return decoded_text\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Возвращает словарь токенов (токен -> ID).\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Токенизатор не загружен или не обучен.\")\n",
        "        return self.tokenizer.get_vocab()\n",
        "\n",
        "    def get_vocab_size(self) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает размер словаря токенизатора.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Токенизатор не загружен или не обучен.\")\n",
        "        return self.tokenizer.get_vocab_size()\n",
        "\n",
        "    def _cleanup_files(self):\n",
        "        \"\"\"\n",
        "        Удаляет файлы обученной модели токенизатора, если они существуют.\n",
        "        \"\"\"\n",
        "        if self.trained_model_path and os.path.exists(self.trained_model_path):\n",
        "            try:\n",
        "                os.remove(self.trained_model_path)\n",
        "                print(f\"HuggingFaceTokenizerWrapper: Удален файл модели: '{self.trained_model_path}'\")\n",
        "            except OSError as e:\n",
        "                print(f\"HuggingFaceTokenizerWrapper: Ошибка при удалении файла '{self.trained_model_path}': {e}\")\n",
        "        # Также удаляем файл .vocab, если он создается (для BPE и WordPiece)\n",
        "        vocab_path = self.trained_model_path.replace(\".json\", \"-vocab.txt\") if self.trained_model_path else None\n",
        "        if vocab_path and os.path.exists(vocab_path):\n",
        "            try:\n",
        "                os.remove(vocab_path)\n",
        "                print(f\"HuggingFaceTokenizerWrapper: Удален файл словаря: '{vocab_path}'\")\n",
        "            except OSError as e:\n",
        "                print(f\"HuggingFaceTokenizerWrapper: Ошибка при удалении файла '{vocab_path}': {e}\")\n",
        "\n",
        "\n",
        "# --- Пример использования класса ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_for_training = \"\"\"\n",
        "    В современном мире обработки естественного языка (НЛП) эффективная и быстрая токенизация\n",
        "    является краеугольным камнем для работы с большими языковыми моделями.\n",
        "    Библиотека Hugging Face Tokenizers — это высокопроизводительная реализация самых популярных\n",
        "    алгоритмов токенизации, разработанная для обеспечения скорости, гибкости и совместимости\n",
        "    с экосистемой Hugging Face Transformers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Инициализация обертки\n",
        "    hf_tokenizer = HuggingFaceTokenizerWrapper()\n",
        "\n",
        "    # Путь для сохранения модели\n",
        "    output_model_file = \"my_custom_hf_tokenizer.json\"\n",
        "\n",
        "    try:\n",
        "        # Обучение BPE токенизатора\n",
        "        hf_tokenizer.train(\n",
        "            corpus_for_training,\n",
        "            model_type='bpe',\n",
        "            vocab_size=100, # Небольшой словарь для демонстрации\n",
        "            min_frequency=1,\n",
        "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "            output_path=output_model_file\n",
        "        )\n",
        "\n",
        "        # Загрузка обученного токенизатора\n",
        "        loaded_hf_tokenizer = HuggingFaceTokenizerWrapper()\n",
        "        loaded_hf_tokenizer.load_tokenizer(output_model_file)\n",
        "\n",
        "        # Кодирование текста\n",
        "        text_to_process = \"Hugging Face Tokenizers - это мощный инструмент для НЛП.\"\n",
        "        encoded_output = loaded_hf_tokenizer.encode(text_to_process)\n",
        "\n",
        "        print(\"\\n--- Детали закодированного вывода ---\")\n",
        "        # print(f\"Нормализованная строка: {encoded_output['normalized_str']}\") # Эта строка удалена\n",
        "        print(f\"Токены: {encoded_output['tokens']}\")\n",
        "        print(f\"ID токенов: {encoded_output['ids']}\")\n",
        "        print(f\"Маска внимания: {encoded_output['attention_mask']}\")\n",
        "        print(f\"Типы токенов: {encoded_output['type_ids']}\")\n",
        "        print(f\"Офсеты: {encoded_output['offsets']}\")\n",
        "\n",
        "        # Декодирование текста\n",
        "        decoded_text = loaded_hf_tokenizer.decode(encoded_output['ids'])\n",
        "        print(f\"\\nПроверка обратимости: Исходный: '{text_to_process}', Декодированный: '{decoded_text}'\")\n",
        "        # Примечание: Декодированный текст может отличаться от исходного из-за нормализации и токенизации.\n",
        "        # Например, пунктуация может быть отделена, регистр изменен.\n",
        "\n",
        "        # Пример с другим типом модели (WordPiece)\n",
        "        print(\"\\n--- Демонстрация WordPiece токенизатора ---\")\n",
        "        wordpiece_tokenizer = HuggingFaceTokenizerWrapper()\n",
        "        wordpiece_tokenizer.train(\n",
        "            corpus_for_training,\n",
        "            model_type='wordpiece',\n",
        "            vocab_size=100,\n",
        "            min_frequency=1,\n",
        "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "            output_path=\"my_wordpiece_tokenizer.json\"\n",
        "        )\n",
        "        wp_encoded = wordpiece_tokenizer.encode(\"Hugging Face Tokenizers - это мощный инструмент для НЛП.\")\n",
        "        wp_decoded = wordpiece_tokenizer.decode(wp_encoded['ids'])\n",
        "        print(f\"WordPiece Декодированный: '{wp_decoded}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка в процессе выполнения: {e}\")\n",
        "    finally:\n",
        "        # Очистка созданных файлов\n",
        "        hf_tokenizer._cleanup_files()\n",
        "        if os.path.exists(\"my_wordpiece_tokenizer.json\"):\n",
        "            os.remove(\"my_wordpiece_tokenizer.json\")\n",
        "            print(\"HuggingFaceTokenizerWrapper: Удален файл модели: 'my_wordpiece_tokenizer.json'\")\n",
        "        # Также удаляем файл .vocab, если он создается для WordPiece\n",
        "        if os.path.exists(\"my_wordpiece_tokenizer-vocab.txt\"):\n",
        "            os.remove(\"my_wordpiece_tokenizer-vocab.txt\")\n",
        "            print(\"HuggingFaceTokenizerWrapper: Удален файл словаря: 'my_wordpiece_tokenizer-vocab.txt'\")\n"
      ],
      "metadata": {
        "id": "5rP5e2-Nxv0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}