{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMDwEGr9D3taQpoDBd30fit",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80%D1%8B%20(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D1%8B)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE"
      ],
      "metadata": {
        "id": "2u3Ph5v4gLaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class BPE:\n",
        "    \"\"\"\n",
        "    Реализация алгоритма Byte-Pair Encoding (BPE) для токенизации текста.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Инициализирует BPE токенизатор.\n",
        "        :param vocab_size: Желаемый максимальный размер словаря субтокенов.\n",
        "                           Если указан, num_merges будет игнорироваться.\n",
        "        :param num_merges: Количество итераций объединения пар.\n",
        "                           Если vocab_size не указан, будет использоваться это значение.\n",
        "        \"\"\"\n",
        "        if vocab_size is None and num_merges is None:\n",
        "            raise ValueError(\"Необходимо указать либо vocab_size, либо num_merges.\")\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = {}  # Словарь для хранения выполненных объединений {pair: new_token}\n",
        "        self.vocabulary = set() # Текущий словарь субтокенов\n",
        "        self.token_to_id = {} # Словарь для преобразования токенов в ID\n",
        "        self.id_to_token = [] # Список для преобразования ID в токены\n",
        "\n",
        "    def _get_initial_tokens(self, text):\n",
        "        \"\"\"\n",
        "        Разбивает текст на начальные символьные токены и добавляет символ конца слова.\n",
        "        :param text: Входная строка.\n",
        "        :return: Список списков символьных токенов для каждого слова.\n",
        "        \"\"\"\n",
        "        # Заменяем подчеркивания на пробелы для разделения слов, затем разбиваем по пробелам\n",
        "        words = text.replace('_', ' ').split()\n",
        "        initial_tokens = []\n",
        "        for word in words:\n",
        "            # Добавляем символ конца слова к каждому слову\n",
        "            initial_tokens.append(list(word) + ['</w>'])\n",
        "        return initial_tokens\n",
        "\n",
        "    def _get_pair_counts(self, tokenized_corpus):\n",
        "        \"\"\"\n",
        "        Подсчитывает частоту встречаемости всех смежных пар токенов в корпусе.\n",
        "        :param tokenized_corpus: Список списков токенов (представляющих слова).\n",
        "        :return: Словарь с частотами пар {('token1', 'token2'): count}.\n",
        "        \"\"\"\n",
        "        pair_counts = collections.defaultdict(int)\n",
        "        for word_tokens in tokenized_corpus:\n",
        "            # Итерируем по парам токенов в каждом слове\n",
        "            for i in range(len(word_tokens) - 1):\n",
        "                pair_counts[(word_tokens[i], word_tokens[i+1])] += 1\n",
        "        return pair_counts\n",
        "\n",
        "    def _merge_pair(self, tokenized_corpus, pair, new_token):\n",
        "        \"\"\"\n",
        "        Объединяет все вхождения заданной пары в новый токен в корпусе.\n",
        "        :param tokenized_corpus: Текущий корпус токенов.\n",
        "        :param pair: Пара токенов для объединения (tuple).\n",
        "        :param new_token: Новый токен, который заменит пару.\n",
        "        :return: Обновленный корпус токенов.\n",
        "        \"\"\"\n",
        "        updated_corpus = []\n",
        "        for word_tokens in tokenized_corpus:\n",
        "            new_word_tokens = []\n",
        "            i = 0\n",
        "            while i < len(word_tokens):\n",
        "                # Проверяем, является ли текущая позиция началом искомой пары\n",
        "                if i + 1 < len(word_tokens) and (word_tokens[i], word_tokens[i+1]) == pair:\n",
        "                    new_word_tokens.append(new_token)\n",
        "                    i += 2 # Пропускаем оба токена, которые были объединены\n",
        "                else:\n",
        "                    new_word_tokens.append(word_tokens[i])\n",
        "                    i += 1\n",
        "            updated_corpus.append(new_word_tokens)\n",
        "        return updated_corpus\n",
        "\n",
        "    def fit(self, corpus_text):\n",
        "        \"\"\"\n",
        "        Обучает BPE модель на заданном текстовом корпусе.\n",
        "        :param corpus_text: Строка, представляющая обучающий корпус.\n",
        "        \"\"\"\n",
        "        # Шаг 1: Инициализация\n",
        "        # Начальное разбиение на символы и добавление символа конца слова\n",
        "        current_corpus_tokens = self._get_initial_tokens(corpus_text)\n",
        "\n",
        "        # Формирование начального словаря из всех уникальных символов\n",
        "        for word_tokens in current_corpus_tokens:\n",
        "            for token in word_tokens:\n",
        "                self.vocabulary.add(token)\n",
        "\n",
        "        # Определяем количество итераций объединения\n",
        "        if self.vocab_size is not None:\n",
        "            # Если задан размер словаря, вычисляем необходимое количество объединений\n",
        "            # (целевой размер словаря - текущий размер словаря)\n",
        "            num_iterations = self.vocab_size - len(self.vocabulary)\n",
        "        elif self.num_merges is not None:\n",
        "            num_iterations = self.num_merges\n",
        "        else:\n",
        "            # Это условие не должно быть достигнуто благодаря проверке в __init__\n",
        "            num_iterations = 0\n",
        "\n",
        "        # Шаг 2: Итеративное объединение\n",
        "        for i in range(num_iterations):\n",
        "            pair_counts = self._get_pair_counts(current_corpus_tokens)\n",
        "\n",
        "            if not pair_counts: # Если нет пар для объединения, останавливаемся\n",
        "                break\n",
        "\n",
        "            # Находим наиболее частую пару\n",
        "            # Сортируем по частоте (убывание), затем по лексикографическому порядку (возрастание)\n",
        "            best_pair = max(pair_counts, key=lambda p: (pair_counts[p], -len(p[0]) - len(p[1]), p))\n",
        "\n",
        "            # Если частота лучшей пары меньше 1, или нет пар для объединения, останавливаемся\n",
        "            if pair_counts[best_pair] < 1:\n",
        "                break\n",
        "\n",
        "            # Формируем новый токен из объединенной пары\n",
        "            new_token = \"\".join(best_pair)\n",
        "\n",
        "            # Сохраняем объединение\n",
        "            self.merges[best_pair] = new_token\n",
        "\n",
        "            # Обновляем корпус, заменяя все вхождения пары новым токеном\n",
        "            current_corpus_tokens = self._merge_pair(current_corpus_tokens, best_pair, new_token)\n",
        "\n",
        "            # Добавляем новый токен в словарь\n",
        "            self.vocabulary.add(new_token)\n",
        "\n",
        "            # Вывод результатов каждого шага\n",
        "            print(f\"Итерация {i+1}: Объединение '{best_pair[0]}{best_pair[1]}' в '{new_token}'. Частота: {pair_counts[best_pair]}\")\n",
        "            print(f\"Обновленный корпус: {current_corpus_tokens}\")\n",
        "            print(f\"Текущий словарь: {sorted(list(self.vocabulary))}\\n\")\n",
        "\n",
        "        # После обучения создаем отображения токенов в ID и обратно\n",
        "        self.id_to_token = sorted(list(self.vocabulary))\n",
        "        self.token_to_id = {token: i for i, token in enumerate(self.id_to_token)}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Токенизирует новую строку, используя обученные объединения.\n",
        "        :param text: Строка для токенизации.\n",
        "        :return: Список ID токенов.\n",
        "        \"\"\"\n",
        "        # Начальное разбиение текста на символы с добавлением </w>\n",
        "        words_tokens = self._get_initial_tokens(text)\n",
        "        encoded_tokens_ids = []\n",
        "\n",
        "        for word_tokens in words_tokens:\n",
        "            # Для каждого слова применяем объединения в порядке их обучения\n",
        "            # Создаем копию списка токенов слова для модификации\n",
        "            current_word_tokens = list(word_tokens)\n",
        "\n",
        "            # Применяем объединения итеративно, пока не сможем найти более длинный токен\n",
        "            # или пока не закончатся объединения\n",
        "            while True:\n",
        "                merged_once = False\n",
        "                new_word_tokens = []\n",
        "                i = 0\n",
        "                while i < len(current_word_tokens):\n",
        "                    found_merge = False\n",
        "                    # Ищем самое длинное возможное объединение, которое начинается с текущего токена\n",
        "                    # Проходим по всем известным объединениям в порядке их создания (от коротких к длинным)\n",
        "                    # или просто ищем самое длинное совпадение\n",
        "                    best_match_len = 0\n",
        "                    best_match_token = None\n",
        "\n",
        "                    # Для простоты, будем применять объединения из self.merges\n",
        "                    # в порядке их создания (от коротких к длинным)\n",
        "                    # Это не совсем оптимально, но для демонстрации подходит\n",
        "                    # В реальных реализациях используют более сложные структуры данных\n",
        "                    # для эффективного поиска наиболее длинного совпадения\n",
        "                    for (p1, p2), merged_token in self.merges.items():\n",
        "                        if i + 1 < len(current_word_tokens) and \\\n",
        "                           current_word_tokens[i] == p1 and \\\n",
        "                           current_word_tokens[i+1] == p2:\n",
        "                            # Проверяем, является ли это объединение частью более длинного\n",
        "                            # уже существующего в словаре токена\n",
        "                            if len(merged_token) > best_match_len:\n",
        "                                best_match_len = len(merged_token)\n",
        "                                best_match_token = merged_token\n",
        "                                found_merge = True\n",
        "\n",
        "                    if found_merge:\n",
        "                        new_word_tokens.append(best_match_token)\n",
        "                        i += 2\n",
        "                        merged_once = True\n",
        "                    else:\n",
        "                        new_word_tokens.append(current_word_tokens[i])\n",
        "                        i += 1\n",
        "\n",
        "                if not merged_once:\n",
        "                    break # Больше нет объединений для этого слова\n",
        "\n",
        "                current_word_tokens = new_word_tokens\n",
        "\n",
        "            # Преобразуем токены слова в их ID\n",
        "            for token in current_word_tokens:\n",
        "                if token in self.token_to_id:\n",
        "                    encoded_tokens_ids.append(self.token_to_id[token])\n",
        "                else:\n",
        "                    # Если токен не найден (например, из-за OOV или неполного обучения),\n",
        "                    # разбиваем его на символы и добавляем их ID\n",
        "                    for char in token:\n",
        "                        if char in self.token_to_id:\n",
        "                            encoded_tokens_ids.append(self.token_to_id[char])\n",
        "                        else:\n",
        "                            # Этого не должно произойти, если начальный словарь содержит все символы\n",
        "                            print(f\"Предупреждение: Символ '{char}' не найден в словаре.\")\n",
        "                            # Можно добавить токен для неизвестных символов, например, <unk>\n",
        "        return encoded_tokens_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Декодирует последовательность ID токенов обратно в строку.\n",
        "        :param token_ids: Список ID токенов.\n",
        "        :return: Декодированная строка.\n",
        "        \"\"\"\n",
        "        decoded_tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id < len(self.id_to_token):\n",
        "                decoded_tokens.append(self.id_to_token[token_id])\n",
        "            else:\n",
        "                print(f\"Предупреждение: ID токена {token_id} вне диапазона словаря.\")\n",
        "                decoded_tokens.append('') # Или можно использовать <unk>\n",
        "\n",
        "        # Объединяем токены, убираем символ конца слова и заменяем пробелы\n",
        "        decoded_text = \"\".join(decoded_tokens).replace('</w>', ' ').strip()\n",
        "        return decoded_text.replace(' ', '_') # Возвращаем исходный формат с подчеркиваниями\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus = \"Ученик_учится_в_школе,_а_учитель_учит_ученика\"\n",
        "\n",
        "    # Создаем экземпляр BPE, указывая количество объединений\n",
        "    # Можно также указать vocab_size=X для желаемого размера словаря\n",
        "    bpe_model = BPE(num_merges=10) # Выполним 10 итераций объединения\n",
        "\n",
        "    print(\"--- Обучение BPE модели ---\")\n",
        "    bpe_model.fit(corpus)\n",
        "    print(\"\\n--- Обучение завершено ---\")\n",
        "    print(f\"Финальный словарь BPE (отсортированный): {sorted(list(bpe_model.vocabulary))}\")\n",
        "    print(f\"Выполненные объединения: {bpe_model.merges}\")\n",
        "\n",
        "    # Тестирование токенизации\n",
        "    print(\"\\n--- Тестирование токенизации ---\")\n",
        "    text_to_encode = \"Ученик_учится_в_школе,_а_учитель_учит_ученика\"\n",
        "    encoded_ids = bpe_model.encode(text_to_encode)\n",
        "    print(f\"Исходный текст: '{text_to_encode}'\")\n",
        "    print(f\"Закодированные ID: {encoded_ids}\")\n",
        "\n",
        "    # Декодирование\n",
        "    decoded_text = bpe_model.decode(encoded_ids)\n",
        "    print(f\"Декодированный текст: '{decoded_text}'\")\n",
        "\n",
        "    # Проверка на новое слово (OOV)\n",
        "    print(\"\\n--- Тестирование OOV слова ---\")\n",
        "    oov_text = \"Учительница_учит\"\n",
        "    encoded_oov_ids = bpe_model.encode(oov_text)\n",
        "    print(f\"OOV текст: '{oov_text}'\")\n",
        "    print(f\"Закодированные ID OOV: {encoded_oov_ids}\")\n",
        "    decoded_oov_text = bpe_model.decode(encoded_oov_ids)\n",
        "    print(f\"Декодированный OOV текст: '{decoded_oov_text}'\")\n",
        "\n",
        "    # Пример токенов по ID\n",
        "    print(\"\\n--- Токены по ID ---\")\n",
        "    for i, token in enumerate(bpe_model.id_to_token):\n",
        "        print(f\"ID {i}: '{token}'\")"
      ],
      "metadata": {
        "id": "_3Sr07lngi4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordPiece"
      ],
      "metadata": {
        "id": "SBOZ-fJJgJmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkUXyyRH_xKi"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.merges = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Предварительная обработка текста: приведение к нижнему регистру и разбиение на слова.\n",
        "        \"\"\"\n",
        "        # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        # Разбиваем на слова по пробелам\n",
        "        words = processed_text.split(' ')\n",
        "        # Добавляем пробелы как отдельные токены между словами\n",
        "        initial_tokens = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word: # Убедимся, что слово не пустое\n",
        "                initial_tokens.extend(list(word))\n",
        "            if i < len(words) - 1:\n",
        "                initial_tokens.append(' ') # Добавляем пробел между словами\n",
        "        return initial_tokens, words # Возвращаем начальные токены и список слов для удобства\n",
        "\n",
        "    def calculate_frequencies(self, tokens):\n",
        "        \"\"\"\n",
        "        Подсчет частот отдельных токенов (униграмм) и биграмм в текущем корпусе.\n",
        "        \"\"\"\n",
        "        unigram_freq = collections.defaultdict(int)\n",
        "        bigram_freq = collections.defaultdict(int)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            unigram_freq[tokens[i]] += 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram_freq[(tokens[i], tokens[i+1])] += 1\n",
        "        return unigram_freq, bigram_freq\n",
        "\n",
        "    def calculate_score(self, unigram_freq, bigram_freq):\n",
        "        \"\"\"\n",
        "        Вычисление оценки слияния для всех возможных биграмм.\n",
        "        Score(A, B) = frequency(AB) / (frequency(A) * frequency(B))\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for (token_a, token_b), freq_ab in bigram_freq.items():\n",
        "            freq_a = unigram_freq[token_a]\n",
        "            freq_b = unigram_freq[token_b]\n",
        "            if freq_a > 0 and freq_b > 0: # Избегаем деления на ноль\n",
        "                score = freq_ab / (freq_a * freq_b)\n",
        "                scores[(token_a, token_b)] = score\n",
        "        return scores\n",
        "\n",
        "    def merge_tokens(self, tokens, best_bigram):\n",
        "        \"\"\"\n",
        "        Слияние лучшей биграммы в корпусе.\n",
        "        \"\"\"\n",
        "        merged_token = \"\".join(best_bigram)\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == best_bigram:\n",
        "                new_tokens.append(merged_token)\n",
        "                i += 2 # Пропускаем оба слитых токена\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        return new_tokens\n",
        "\n",
        "    def train(self, text, target_vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Обучение WordPiece токенизатора.\n",
        "        Итеративно сливает токены до достижения целевого размера словаря\n",
        "        или заданного количества слияний.\n",
        "        \"\"\"\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        current_tokens, _ = self.preprocess_text(text)\n",
        "\n",
        "        # Начальный словарь состоит из всех уникальных символов\n",
        "        self.vocab = set(current_tokens)\n",
        "        print(f\"\\nШаг 1: Начальный словарь (V0) содержит {len(self.vocab)} токенов.\")\n",
        "        print(f\"Начальный корпус: {current_tokens}\")\n",
        "\n",
        "        merges_count = 0\n",
        "        while True:\n",
        "            unigram_freq, bigram_freq = self.calculate_frequencies(current_tokens)\n",
        "            scores = self.calculate_score(unigram_freq, bigram_freq)\n",
        "\n",
        "            if not scores:\n",
        "                print(\"\\nНет больше биграмм для слияния. Остановка.\")\n",
        "                break\n",
        "\n",
        "            # Находим биграмму с наивысшей оценкой\n",
        "            best_bigram = max(scores, key=scores.get)\n",
        "            best_score = scores[best_bigram]\n",
        "\n",
        "            merged_token = \"\".join(best_bigram)\n",
        "\n",
        "            # Критерий остановки: если новый токен уже в словаре\n",
        "            if merged_token in self.vocab:\n",
        "                # Если лучший токен уже существует, удалим его из scores, чтобы найти следующий лучший\n",
        "                del scores[best_bigram]\n",
        "                continue # Продолжаем поиск\n",
        "\n",
        "            # Выводим информацию о текущем слиянии\n",
        "            print(f\"\\nИтерация {merges_count + 1}:\")\n",
        "            print(f\"  Лучшая биграмма для слияния: '{best_bigram[0]}' + '{best_bigram[1]}' -> '{merged_token}' (Score: {best_score:.4f})\")\n",
        "\n",
        "            # Выполняем слияние в корпусе\n",
        "            current_tokens = self.merge_tokens(current_tokens, best_bigram)\n",
        "\n",
        "            # Добавляем новый токен в словарь и сохраняем слияние\n",
        "            self.vocab.add(merged_token)\n",
        "            self.merges.append(best_bigram)\n",
        "            merges_count += 1\n",
        "\n",
        "            print(f\"  Корпус после слияния: {current_tokens}\")\n",
        "            print(f\"  Текущий размер словаря: {len(self.vocab)}\")\n",
        "\n",
        "            # Критерии остановки\n",
        "            if target_vocab_size is not None and len(self.vocab) >= target_vocab_size:\n",
        "                print(f\"\\nДостигнут целевой размер словаря ({target_vocab_size}). Остановка.\")\n",
        "                break\n",
        "            if num_merges is not None and merges_count >= num_merges:\n",
        "                print(f\"\\nДостигнуто заданное количество слияний ({num_merges}). Остановка.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nОбучение завершено. Итоговый словарь содержит {len(self.vocab)} токенов.\")\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Токенизация нового текста с использованием обученного словаря WordPiece.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'vocab') or len(self.vocab) == 0:\n",
        "            raise ValueError(\"Токенизатор не обучен. Сначала вызовите метод train().\")\n",
        "\n",
        "        print(f\"\\n--- Токенизация нового текста ---\")\n",
        "        print(f\"Текст для токенизации: '{text}'\")\n",
        "\n",
        "        # Предварительная обработка текста для токенизации\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        words = processed_text.split(' ')\n",
        "\n",
        "        final_tokens = []\n",
        "        for word in words:\n",
        "            if not word: # Пропускаем пустые строки, если они возникли из-за множественных пробелов\n",
        "                continue\n",
        "\n",
        "            word_tokens = []\n",
        "            remaining_word = word\n",
        "\n",
        "            while remaining_word:\n",
        "                found_match = False\n",
        "                # Ищем самый длинный подтокен из словаря, который является префиксом оставшейся части слова\n",
        "                for i in range(len(remaining_word), 0, -1):\n",
        "                    subword = remaining_word[:i]\n",
        "\n",
        "                    if subword in self.vocab:\n",
        "                        word_tokens.append(subword)\n",
        "                        remaining_word = remaining_word[i:]\n",
        "                        found_match = True\n",
        "                        break\n",
        "\n",
        "                if not found_match:\n",
        "                    # Если не удалось найти соответствующий токен, разбиваем на символы\n",
        "                    # или используем токен [UNK]\n",
        "                    if remaining_word[0] in self.vocab: # Если символ есть в словаре\n",
        "                        word_tokens.append(remaining_word[0])\n",
        "                    else: # Если символ даже не в начальном словаре\n",
        "                        word_tokens.append('[UNK]')\n",
        "                    remaining_word = remaining_word[1:]\n",
        "\n",
        "            final_tokens.extend(word_tokens)\n",
        "            final_tokens.append(' ') # Добавляем пробел между токенизированными словами\n",
        "\n",
        "        # Удаляем последний пробел, если он есть\n",
        "        if final_tokens and final_tokens[-1] == ' ':\n",
        "            final_tokens.pop()\n",
        "\n",
        "        print(f\"Токенизированный текст: {final_tokens}\")\n",
        "        return final_tokens\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Возвращает текущий словарь токенов.\"\"\"\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def get_merges(self):\n",
        "        \"\"\"Возвращает список выполненных слияний.\"\"\"\n",
        "        return self.merges\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    text_to_train = \"В училище учитель учит ученикам по новому учебнику\"\n",
        "\n",
        "    # Создаем экземпляр токенизатора\n",
        "    tokenizer = WordPieceTokenizer()\n",
        "\n",
        "    # Обучаем токенизатор\n",
        "    final_vocab = tokenizer.train(text_to_train, num_merges=50)\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece: {final_vocab}\")\n",
        "\n",
        "    # Токенизируем тот же текст\n",
        "    tokenizer.tokenize(text_to_train)\n",
        "\n",
        "    # Пример токенизации нового слова\n",
        "    new_text = \"учительница\"\n",
        "    # Добавим '##ница' в словарь для демонстрации\n",
        "    if 'ница' not in final_vocab:\n",
        "        tokenizer.vocab.add('ница')\n",
        "        tokenizer.vocab.add('##ница')\n",
        "        final_vocab = tokenizer.get_vocab()\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece (с 'ница' для демонстрации): {final_vocab}\")\n",
        "    tokenizer.tokenize(new_text)\n",
        "\n",
        "    # Демонстрация с упрощенным словарем\n",
        "    simplified_tokenizer = WordPieceTokenizer()\n",
        "    simplified_tokenizer.vocab = set(['у', 'ч', 'и', 'т', 'е', 'л', 'ь', 'н', 'ц', 'а', 'учитель', '##ница', '##тель', '##а'])\n",
        "    print(f\"\\n--- Демонстрация токенизации подслов с упрощенным словарем ---\")\n",
        "    simplified_tokenizer.tokenize(\"учительница\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Униграмной языковой модели"
      ],
      "metadata": {
        "id": "dZw5ZOm5gE_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Токенизирует текст, приводя его к нижнему регистру и удаляя знаки препинания.\n",
        "    Args:\n",
        "        text (str): Входной текст.\n",
        "    Returns:\n",
        "        list: Список токенов (слов).\n",
        "    \"\"\"\n",
        "    # Приводим текст к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаляем знаки препинания и разбиваем на слова\n",
        "    # Используем re.findall для извлечения только буквенных последовательностей\n",
        "    tokens = re.findall(r'\\b[а-яёa-z]+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "def train_unigram_model(corpus_text):\n",
        "    \"\"\"\n",
        "    Обучает униграмную языковую модель на заданном корпусе текста.\n",
        "    Рассчитывает вероятности слов методом максимального правдоподобия (MLE)\n",
        "    и со сглаживанием по Лапласу.\n",
        "    Args:\n",
        "        corpus_text (str): Текст корпуса для обучения модели.\n",
        "    Returns:\n",
        "        tuple: Кортеж, содержащий:\n",
        "            - dict: Вероятности слов по MLE.\n",
        "            - dict: Вероятности слов со сглаживанием по Лапласу.\n",
        "            - int: Общее количество слов в корпусе (N).\n",
        "            - int: Размер словаря (количество уникальных слов).\n",
        "    \"\"\"\n",
        "    # Шаг 1: Токенизация корпуса\n",
        "    tokens = tokenize(corpus_text)\n",
        "\n",
        "    # Общее количество слов в корпусе (N)\n",
        "    N = len(tokens)\n",
        "    if N == 0:\n",
        "        print(\"Ошибка: Корпус пуст после токенизации.\")\n",
        "        return {}, {}, 0, 0\n",
        "\n",
        "    # Шаг 2: Подсчет частот слов\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Шаг 3: Определение словаря и его размера\n",
        "    vocabulary = list(word_counts.keys())\n",
        "    V = len(vocabulary)\n",
        "\n",
        "    # Шаг 4: Расчет вероятностей слов (MLE)\n",
        "    mle_probabilities = {}\n",
        "    for word, count in word_counts.items():\n",
        "        mle_probabilities[word] = count / N\n",
        "\n",
        "    # Расчет вероятностей слов со сглаживанием по Лапласу\n",
        "    laplace_probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        laplace_probabilities[word] = (word_counts[word] + 1) / (N + V)\n",
        "    # Для слов, которых нет в словаре, но могут встретиться в новом тексте\n",
        "    # Их вероятность со сглаживанием будет (0 + 1) / (N + V)\n",
        "    # Мы не добавляем их сюда напрямую, но это учитывается при использовании модели.\n",
        "\n",
        "    print(f\"Общее количество слов в корпусе (N): {N}\")\n",
        "    print(f\"Размер словаря (|V|): {V}\")\n",
        "    print(\"\\nЧастоты слов:\")\n",
        "    for word, count in word_counts.items():\n",
        "        print(f\"  '{word}': {count}\")\n",
        "    print(\"\\nВероятности слов (MLE):\")\n",
        "    for word, prob in mle_probabilities.items():\n",
        "        print(f\"  P('{word}') = {prob:.4f}\")\n",
        "    print(\"\\nВероятности слов (Laplace Smoothing):\")\n",
        "    for word, prob in laplace_probabilities.items():\n",
        "        print(f\"  P_Laplace('{word}') = {prob:.4f}\")\n",
        "\n",
        "    return mle_probabilities, laplace_probabilities, N, V\n",
        "\n",
        "def calculate_sentence_probability(sentence, model_probabilities, N, V, smoothing_type='laplace'):\n",
        "    \"\"\"\n",
        "    Рассчитывает вероятность предложения, используя обученную униграмную модель.\n",
        "    Args:\n",
        "        sentence (str): Предложение для оценки.\n",
        "        model_probabilities (dict): Словарь вероятностей слов (MLE или Laplace).\n",
        "        N (int): Общее количество слов в обучающем корпусе.\n",
        "        V (int): Размер словаря обучающего корпуса.\n",
        "        smoothing_type (str): Тип сглаживания ('none' для MLE, 'laplace' для сглаживания по Лапласу).\n",
        "    Returns:\n",
        "        float: Вероятность предложения.\n",
        "    \"\"\"\n",
        "    sentence_tokens = tokenize(sentence)\n",
        "    probability = 1.0\n",
        "\n",
        "    print(f\"\\nОценка вероятности предложения: '{sentence}'\")\n",
        "    print(f\"Токены предложения: {sentence_tokens}\")\n",
        "\n",
        "    for token in sentence_tokens:\n",
        "        if smoothing_type == 'laplace':\n",
        "            # Используем формулу Лапласа для каждого токена\n",
        "            # Если токена нет в model_probabilities (т.е. Count(token) = 0),\n",
        "            # то его вероятность будет (0 + 1) / (N + V)\n",
        "            word_count_in_model = model_probabilities.get(token, 0) * N # Восстанавливаем Count(token)\n",
        "            # Если слово не было в словаре, то word_count_in_model будет 0.0,\n",
        "            # но для Лапласа нам нужен его реальный счетчик (0).\n",
        "            # Проще использовать прямой расчет:\n",
        "            if token in model_probabilities:\n",
        "                p_token = (word_count_in_model + 1) / (N + V)\n",
        "            else:\n",
        "                p_token = 1 / (N + V) # Для неизвестных слов\n",
        "            print(f\"  P_Laplace('{token}') = {p_token:.8f}\")\n",
        "        else: # Без сглаживания (MLE)\n",
        "            p_token = model_probabilities.get(token, 0.0) # Если слово не найдено, вероятность 0\n",
        "            print(f\"  P_MLE('{token}') = {p_token:.8f}\")\n",
        "\n",
        "        if p_token == 0:\n",
        "            # Если вероятность слова 0, то вероятность всего предложения 0\n",
        "            probability = 0.0\n",
        "            print(f\"  Обнаружено неизвестное слово '{token}' без сглаживания. Вероятность предложения = 0.\")\n",
        "            break\n",
        "        probability *= p_token\n",
        "\n",
        "    return probability\n",
        "\n",
        "# --- Пример использования ---\n",
        "corpus_text = \"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"\n",
        "\n",
        "print(\"--- Обучение Униграмной модели ---\")\n",
        "mle_probs, laplace_probs, N_corpus, V_corpus = train_unigram_model(corpus_text)\n",
        "\n",
        "# Оценка вероятности предложения из корпуса (без сглаживания)\n",
        "sentence1 = \"Ученики пишут диктант\"\n",
        "prob_sentence1_mle = calculate_sentence_probability(sentence1, mle_probs, N_corpus, V_corpus, smoothing_type='none')\n",
        "print(f\"\\nВероятность предложения '{sentence1}' (MLE): {prob_sentence1_mle:.15f}\")\n",
        "\n",
        "# Оценка вероятности предложения из корпуса (со сглаживанием по Лапласу)\n",
        "prob_sentence1_laplace = calculate_sentence_probability(sentence1, laplace_probs, N_corpus, V_corpus, smoothing_type='laplace')\n",
        "print(f\"\\nВероятность предложения '{sentence1}' (Laplace Smoothing): {prob_sentence1_laplace:.15f}\")\n",
        "\n",
        "# Оценка вероятности нового предложения с неизвестным словом (без сглаживания)\n",
        "sentence2 = \"Учитель читает книгу\"\n",
        "prob_sentence2_mle = calculate_sentence_probability(sentence2, mle_probs, N_corpus, V_corpus, smoothing_type='none')\n",
        "print(f\"\\nВероятность предложения '{sentence2}' (MLE): {prob_sentence2_mle:.15f}\")\n",
        "\n",
        "# Оценка вероятности нового предложения с неизвестным словом (со сглаживанием по Лапласу)\n",
        "prob_sentence2_laplace = calculate_sentence_probability(sentence2, laplace_probs, N_corpus, V_corpus, smoothing_type='laplace')\n",
        "print(f\"\\nВероятность предложения '{sentence2}' (Laplace Smoothing): {prob_sentence2_laplace:.15f}\")\n",
        "\n",
        "# Пример с одним словом, которого нет в корпусе\n",
        "unknown_word_sentence = \"школа\"\n",
        "prob_unknown_laplace = calculate_sentence_probability(unknown_word_sentence, laplace_probs, N_corpus, V_corpus, smoothing_type='laplace')\n",
        "print(f\"\\nВероятность слова '{unknown_word_sentence}' (Laplace Smoothing): {prob_unknown_laplace:.15f}\")\n"
      ],
      "metadata": {
        "id": "Smok_jSifd1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece"
      ],
      "metadata": {
        "id": "aSRNpUAbgAuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем библиотеку SentencePiece\n",
        "import sentencepiece as spm\n",
        "import os # Для работы с файловой системой\n",
        "\n",
        "print(\"--- Полный пример использования SentencePiece ---\")\n",
        "\n",
        "# Шаг 1: Установка SentencePiece (инструкция)\n",
        "# Если у вас еще не установлен SentencePiece, вы можете установить его с помощью pip:\n",
        "# pip install sentencepiece\n",
        "# (Раскомментируйте строку выше и запустите в терминале, если необходимо)\n",
        "\n",
        "# Шаг 2: Подготовка данных для обучения\n",
        "# Для обучения SentencePiece требуется текстовый файл.\n",
        "# Создадим пример файла с текстом, который мы будем использовать.\n",
        "# Используем тот же текст, что и в лекции.\n",
        "text_data = \"\"\"Учитель говорит: «Откройте учебники». Ученики открывают учебники. Откройте страницу 15. Страница сложная. Страницы учебника содержат много информации.\"\"\"\n",
        "\n",
        "# Определяем имя файла для обучения\n",
        "train_file_name = \"train_text_for_sp.txt\"\n",
        "\n",
        "# Создаем файл для обучения и записываем в него текст\n",
        "try:\n",
        "    with open(train_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_data)\n",
        "    print(f\"\\nСоздан файл для обучения: '{train_file_name}'\")\n",
        "except IOError as e:\n",
        "    print(f\"Ошибка при создании файла для обучения: {e}\")\n",
        "    exit() # Выходим, если не удалось создать файл\n",
        "\n",
        "# Шаг 3: Обучение модели SentencePiece\n",
        "# Мы будем обучать модель с использованием алгоритма Unigram (по умолчанию).\n",
        "# model_prefix: префикс для файлов модели (model.model и model.vocab)\n",
        "# vocab_size: желаемый размер словаря подслов\n",
        "# character_coverage: процент символов, которые должны быть покрыты моделью\n",
        "#                  (1.0 означает, что все символы из входных данных должны быть представлены)\n",
        "# model_type: 'unigram' (по умолчанию) или 'bpe'\n",
        "# input: путь к файлу(ам) для обучения\n",
        "\n",
        "model_prefix = \"my_custom_sentencepiece_model\"\n",
        "vocab_size = 50 # Увеличим размер словаря для лучшей демонстрации\n",
        "\n",
        "print(f\"\\nНачинается обучение модели SentencePiece с vocab_size={vocab_size}...\")\n",
        "try:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=train_file_name,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1.0,\n",
        "        model_type='unigram' # Можно изменить на 'bpe'\n",
        "    )\n",
        "    print(f\"Обучение завершено. Созданы файлы: '{model_prefix}.model' и '{model_prefix}.vocab'\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при обучении модели SentencePiece: {e}\")\n",
        "    # Попытаемся удалить созданный файл, если обучение не удалось\n",
        "    if os.path.exists(train_file_name):\n",
        "        os.remove(train_file_name)\n",
        "    exit()\n",
        "\n",
        "# Шаг 4: Загрузка обученной модели\n",
        "# После обучения мы можем загрузить модель для использования.\n",
        "sp = spm.SentencePieceProcessor()\n",
        "model_path = f\"{model_prefix}.model\"\n",
        "\n",
        "try:\n",
        "    sp.load(model_path)\n",
        "    print(f\"\\nМодель SentencePiece загружена: '{model_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели SentencePiece: {e}\")\n",
        "    # Попытаемся удалить созданные файлы, если загрузка не удалась\n",
        "    if os.path.exists(train_file_name): os.remove(train_file_name)\n",
        "    if os.path.exists(f\"{model_prefix}.model\"): os.remove(f\"{model_prefix}.model\")\n",
        "    if os.path.exists(f\"{model_prefix}.vocab\"): os.remove(f\"{model_prefix}.vocab\")\n",
        "    exit()\n",
        "\n",
        "# Шаг 5: Токенизация текста (кодирование)\n",
        "# Теперь используем загруженную модель для токенизации нашего текста.\n",
        "\n",
        "input_text = \"Учитель говорит: «Откройте учебники». Ученики открывают учебники. Откройте страницу 15. Страница сложная. Страницы учебника содержат много информации.\"\n",
        "\n",
        "print(f\"\\n--- Токенизация ---\")\n",
        "print(f\"Исходный текст: '{input_text}'\")\n",
        "\n",
        "# Кодирование в ID токенов\n",
        "encoded_ids = sp.encode_as_ids(input_text)\n",
        "print(f\"Токенизация в ID: {encoded_ids}\")\n",
        "\n",
        "# Кодирование в строковые токены (подслова)\n",
        "encoded_pieces = sp.encode_as_pieces(input_text)\n",
        "print(f\"Токенизация в подслова: {encoded_pieces}\")\n",
        "\n",
        "# Шаг 6: Детокенизация текста (декодирование)\n",
        "# Восстановим исходный текст из токенизированных ID или подслов.\n",
        "\n",
        "print(f\"\\n--- Детокенизация ---\")\n",
        "\n",
        "decoded_text_from_ids = sp.decode_ids(encoded_ids)\n",
        "print(f\"Детокенизация из ID: '{decoded_text_from_ids}'\")\n",
        "\n",
        "decoded_text_from_pieces = sp.decode_pieces(encoded_pieces)\n",
        "print(f\"Детокенизация из подслов: '{decoded_text_from_pieces}'\")\n",
        "\n",
        "# Проверка обратимости\n",
        "if input_text == decoded_text_from_ids and input_text == decoded_text_from_pieces:\n",
        "    print(\"\\nПроцесс токенизации и детокенизации полностью обратим!\")\n",
        "else:\n",
        "    print(\"\\nВнимание: Детокенизированный текст не полностью совпадает с исходным.\")\n",
        "    print(f\"Ожидалось: '{input_text}'\")\n",
        "    print(f\"Получено из ID: '{decoded_text_from_ids}'\")\n",
        "    print(f\"Получено из подслов: '{decoded_text_from_pieces}'\")\n",
        "\n",
        "\n",
        "# Шаг 7: Очистка (удаление созданных файлов)\n",
        "# Удалим файлы, созданные в процессе обучения, чтобы не засорять рабочую директорию.\n",
        "print(f\"\\n--- Очистка временных файлов ---\")\n",
        "try:\n",
        "    if os.path.exists(train_file_name):\n",
        "        os.remove(train_file_name)\n",
        "        print(f\"Удален файл: '{train_file_name}'\")\n",
        "    if os.path.exists(f\"{model_prefix}.model\"):\n",
        "        os.remove(f\"{model_prefix}.model\")\n",
        "        print(f\"Удален файл: '{model_prefix}.model'\")\n",
        "    if os.path.exists(f\"{model_prefix}.vocab\"):\n",
        "        os.remove(f\"{model_prefix}.vocab\")\n",
        "        print(f\"Удален файл: '{model_prefix}.vocab'\")\n",
        "except OSError as e:\n",
        "    print(f\"Ошибка при удалении файлов: {e}\")\n",
        "\n",
        "print(\"\\n--- Использование SentencePiece завершено ---\")\n"
      ],
      "metadata": {
        "id": "CgSI01X3bSV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}