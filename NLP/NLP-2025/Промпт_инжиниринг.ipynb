{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO0YB541AaYht/GEOAhOGjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%9F%D1%80%D0%BE%D0%BC%D0%BF%D1%82_%D0%B8%D0%BD%D0%B6%D0%B8%D0%BD%D0%B8%D1%80%D0%B8%D0%BD%D0%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Лекция: Промпт-инжиниринг: Методологии, Применение и Оценка\n",
        "\n",
        "Целевая аудитория: магистранты направления «Анализ данных», изучающие курс по обработке естественного языка (NLP).  \n",
        "Ключевой навык: умение проектировать, тестировать и оптимизировать входные инструкции (промпты) для больших языковых моделей (LLM) так, чтобы получать **предсказуемые, точные и структурированные** результаты, пригодные для дальнейшего анализа или интеграции в автоматизированные системы.\n",
        "\n",
        "Промпт-инжиниринг сегодня — не просто «умение задавать вопросы ИИ». Это инженерная дисциплина, сочетающая элементы лингвистики, когнитивного дизайна и системного мышления. Особенно в контексте анализа данных, где важна не только правдоподобность ответа, но и его воспроизводимость, точность и соответствие формату.\n",
        "\n",
        "## 1. Введение: PE как ключевой навык аналитика данных\n",
        "\n",
        "Промпт-инжиниринг (Prompt Engineering, PE) — это практика целенаправленного конструирования входных запросов для LLM с целью управления содержанием, стилем и структурой их ответов. В отличие от традиционного программирования, где вы пишете явные инструкции для машины, в PE вы «настраиваете» модель через язык, контекст и формулировки.\n",
        "\n",
        "Для современного аналитика данных PE становится таким же базовым инструментом, как SQL или pandas. Вот три ключевые области применения:\n",
        "\n",
        "- **Автоматизация рутинных задач**: генерация SQL-запросов по описанию, написание скриптов на Python для очистки данных, построение ETL-пайплайнов по текстовому ТЗ.  \n",
        "- **Извлечение инсайтов из неструктурированных источников**: анализ пользовательских отзывов, логов поддержки, внутренних документов — с классификацией, суммаризацией или извлечением сущностей.  \n",
        "- **Обоснование решений через RAG (Retrieval-Augmented Generation)**: привязка ответов модели к актуальным корпоративным данным (например, справочникам, отчётам, базам знаний), что снижает риск «галлюцинаций» и повышает доверие к выводам.\n",
        "\n",
        "Важно понимать: эффективный промпт — это не удачно сформулированная фраза, а **исполняемая спецификация**, которую модель может интерпретировать однозначно.\n",
        "\n",
        "## 2. Мета-анализ: Разбор образовательного фреймворка как примера PE\n",
        "\n",
        "Чтобы продемонстрировать силу структурированного подхода к промптам, рассмотрим реальный пример — фреймворк для генерации персонализированных учебных планов. Этот промпт сам по себе является образцом продвинутого PE: он не просто запрашивает информацию, а задаёт полную схему взаимодействия.\n",
        "\n",
        "### Мета-Фреймворк для Проектирования Обучения (Исходный Промпт)\n",
        "\n",
        "```text\n",
        "[SUBJECT]=Тема или навык для изучения\n",
        "[CURRENT_LEVEL]=Начальный уровень знаний (начальный/средний/продвинутый)\n",
        "[TIME_AVAILABLE]=Сколько часов в неделю готовы уделять обучению\n",
        "[LEARNING_STYLE]=Предпочтительный метод обучения (визуальный/слуховой/практический/чтение)\n",
        "[GOAL]=Конкретная цель обучения или целевой уровень навыка\n",
        "\n",
        "Создай недельный учебный план по теме [SUBJECT] для человека с уровнем [CURRENT_LEVEL],\n",
        "который может тратить [TIME_AVAILABLE] часов в неделю и предпочитает [LEARNING_STYLE] стиль обучения.\n",
        "Цель: [GOAL]. План должен включать:\n",
        "1. Темы для изучения на неделю\n",
        "2. Рекомендуемые ресурсы (ссылки, видео, статьи)\n",
        "3. Практическое задание\n",
        "4. Критерии успешного выполнения\n",
        "```\n",
        "\n",
        "Этот пример показывает, что хороший промпт:\n",
        "\n",
        "- Чётко разделяет **параметры** (в квадратных скобках) и **инструкцию**.  \n",
        "- Задаёт **роль** (неявно — «ты — методист»).  \n",
        "- Указывает **формат вывода** (четыре пункта).  \n",
        "- Включает **контекст** (время, стиль, цель).  \n",
        "\n",
        "> **Вывод**: промпт здесь работает как **исполняемый план** (*Executable Plan*). Он не оставляет места для интерпретации — модель знает, *кто она*, *что делать*, *на чём основываться* и *в каком виде отдавать результат*.\n",
        "\n",
        "Такой подход напрямую переносится на задачи анализа данных: вместо «проанализируй данные» вы задаёте структуру, которая гарантирует нужный выход.\n",
        "\n",
        "## 3. Фундаментальные концепции (Zero-shot / Few-shot / Chain-of-Thought)\n",
        "\n",
        "Прежде чем переходить к продвинутым техникам (например, few-shot или CoT), важно освоить базовый «строительный блок» — компонентную структуру промпта. Даже в zero-shot режиме (без примеров) качество ответа напрямую зависит от того, насколько полно вы описали задачу.\n",
        "\n",
        "### 3.1. Разложение промпта на ключевые компоненты\n",
        "\n",
        "Любой эффективный промпт можно разложить на четыре обязательных компонента. Их наличие превращает расплывчатый запрос в точную инструкцию.\n",
        "\n",
        "```text\n",
        "1. Персона/Роль (Persona)\n",
        "   Назначение: Задаёт экспертизу, тон и стиль ответа. Без роли модель отвечает «нейтрально», что часто означает — обобщённо и поверхностно.\n",
        "   Пример: \"Ты — дата-сайентист с 10-летним опытом в банковской аналитике.\"\n",
        "   Дополнительно: Роль может включать ограничения — \"не используй библиотеку scikit-learn\", \"объясняй так, будто я студент-первокурсник\".\n",
        "\n",
        "2. Инструкция (Instruction)\n",
        "   Назначение: Ядро задачи — что именно нужно сделать. Должна быть конкретной, атомарной и избегать двусмысленности.\n",
        "   Пример плохой: \"Разбери эти данные.\"\n",
        "   Пример хороший: \"Проведи одновыборочный t-тест, чтобы проверить, отличается ли среднее значение метрики 'время_сеанса' от 5 минут.\"\n",
        "   Совет: Используйте глаголы действия — \"рассчитай\", \"классифицируй\", \"визуализируй\", \"сравни\".\n",
        "\n",
        "3. Контекст/Данные (Context)\n",
        "   Назначение: Предоставляет модели информацию, которой нет в её предобученных знаниях. Это может быть фрагмент таблицы, бизнес-правило, описание схемы БД или выдержка из документа.\n",
        "   Пример: \"Вот фрагмент логов за 1 час:\n",
        "   user_id,event,timestamp\n",
        "   101,login,2025-10-30T10:00:00\n",
        "   101,click_button,2025-10-30T10:02:15\n",
        "   ...\"\n",
        "   Важно: чем больше релевантного контекста — тем точнее и актуальнее ответ.\n",
        "\n",
        "4. Формат вывода (Output Format)\n",
        "   Назначение: Гарантирует, что результат можно сразу использовать в коде или отчёте без постобработки.\n",
        "   Пример: \"Ответ должен быть в формате JSON с полями: {'hypothesis': str, 'p_value': float, 'decision': str}.\"\n",
        "   Распространённые форматы: JSON, CSV, Markdown-таблица, Python-словарь, SQL-запрос.\n",
        "```\n",
        "\n",
        "Рассмотрим полный пример, объединяющий все компоненты:\n",
        "\n",
        "> «Ты — аналитик в мобильном приложении для фитнеса. Ниже приведены данные о ежедневной активности 5 пользователей за неделю (user_id, steps, active_minutes).  \n",
        "> Рассчитай для каждого пользователя среднее количество шагов и определи, кто из них попал в топ-20% по активности.  \n",
        "> Ответ представь в виде JSON-массива объектов с полями: `user_id`, `avg_steps`, `is_top_20_percent` (булево значение).»\n",
        "\n",
        "Такой промпт минимизирует риск ошибки и делает результат пригодным для автоматической загрузки в дашборд или БД.\n",
        "\n",
        "В следующих разделах мы покажем, как усиливать такие промпты с помощью **few-shot learning** (примеров в самом запросе) и **chain-of-thought** (пошаговых рассуждений), чтобы решать ещё более сложные аналитические задачи."
      ],
      "metadata": {
        "id": "9Drlt2l14WLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 3.2. Основополагающие концепции промптинга и примеры\n",
        "\n",
        "Существует несколько фундаментальных подходов к взаимодействию с LLM, которые определяют, насколько точно и надёжно модель выполнит задачу. Эти подходы — не просто «разные способы задавать вопросы», а стратегии управления внутренними механизмами рассуждения модели. Выбор метода зависит от сложности задачи, требуемой точности и доступности примеров.\n",
        "\n",
        "Ниже представлены три ключевые парадигмы: **Zero-Shot**, **Few-Shot** и **Chain-of-Thought (CoT)**. Каждая из них имеет свои сценарии применения, ограничения и уровень эффективности в контексте анализа данных.\n",
        "\n",
        "```text\n",
        "Концепция                | Описание                                                                 | Пример промпта (Практика)                                                                                     | Эффективность\n",
        "------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|------------------\n",
        "Zero-Shot               | Модель отвечает, опираясь только на общие знания и явную инструкцию.     | \"Создай регулярное выражение для извлечения адресов электронной почты из произвольного текста.\"               | Низкая для узкоспециализированных или нетривиальных задач. Подходит для базовых, общеизвестных шаблонов.\n",
        "Few-Shot                | В промпт включаются несколько примеров \"ввод → вывод\" для демонстрации паттерна. | \"Пример 1: 'Отличный сервис!' → Positive\\nПример 2: 'Очень разочарован.' → Negative\\nКлассифицируй: 'Товар пришёл с опозданием.' → ?\" | Высокая для задач классификации, извлечения сущностей, форматирования и парсинга. Особенно полезна при работе с доменными или нестандартными форматами.\n",
        "Chain-of-Thought (CoT)  | Модели даётся инструкция рассуждать пошагово перед выдачей окончательного ответа. | \"Рассчитай медиану следующих чисел: 10, 5, 20, 8, 15. Рассуждай пошагово, прежде чем дать финальный ответ.\"   | Очень высокая для арифметических, логических, статистических и многоэтапных аналитических задач. Снижает количество ошибок в рассуждениях.\n",
        "```\n",
        "\n",
        "#### Пояснения и рекомендации\n",
        "\n",
        "- **Zero-Shot** — самый простой способ, но часто недостаточный в профессиональной аналитике. Например, запрос «напиши SQL-запрос для нахождения активных пользователей» может дать разные результаты в зависимости от того, как модель интерпретирует «активных». Без контекста или примеров риск неточности высок.\n",
        "\n",
        "- **Few-Shot** особенно мощен, когда у вас есть контроль над форматом входа и выхода. Даже 2–3 качественных примера могут «настроить» модель на нужную логику. Например, при извлечении дат из неструктурированных логов:\n",
        "  ```\n",
        "  \"23 марта 2024 года\" → \"2024-03-23\"\n",
        "  \"вчера\" → \"2025-10-29\"\n",
        "  Извлеки дату: \"Событие произошло 30 октября.\" → ?\n",
        "  ```\n",
        "  Такой подход часто используется в production-пайплайнах для предобработки текста.\n",
        "\n",
        "- **Chain-of-Thought (CoT)** — не просто «добавь фразу „рассуждай пошагово“», а способ заставить модель имитировать человеческое логическое мышление. Это критически важно в задачах, где промежуточные шаги влияют на финальный результат. Например, при расчёте метрик в A/B-тесте:\n",
        "  > «Сначала проверь нормальность распределения, затем выбери подходящий тест (t-test или Mann-Whitney), рассчитай p-value и сделай вывод.»\n",
        "\n",
        "  Без CoT модель может пропустить этап проверки предпосылок и сразу применить t-test, что приведёт к неверному выводу.\n",
        "\n",
        "> **Практический совет**: в реальных проектах эти методы часто комбинируются. Например, **Few-Shot + CoT** — вы даёте примеры, в которых каждый включает пошаговое рассуждение. Это особенно эффективно для сложных доменных задач, таких как финансовый анализ или медицинская документация.\n",
        "\n",
        "В следующем разделе мы рассмотрим, как оценивать качество промптов и выбирать оптимальную стратегию на основе метрик, а не интуиции."
      ],
      "metadata": {
        "id": "WC9TSSdOFBa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 4. Продвинутые методики и Практикум (RAG, Self-Correction, Case Studies)\n",
        "\n",
        "На базовом уровне промпт-инжиниринг помогает получать предсказуемые ответы. Однако в реальных аналитических и production-сценариях этого недостаточно. Модель может «галлюцинировать», опираться на устаревшие знания или выдавать технически некорректные решения (например, битый SQL или неверную статистику).  \n",
        "\n",
        "Для решения этих проблем применяются **продвинутые методики**, которые либо расширяют контекст модели внешними источниками, либо вводят механизмы самопроверки. Две ключевые стратегии — **RAG** и **Self-Correction** — позволяют превратить LLM из «умного ассистента» в надёжный компонент аналитического пайплайна.\n",
        "\n",
        "### 4.1. Продвинутые техники\n",
        "\n",
        "#### Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "**Суть**: RAG решает две фундаментальные проблемы LLM:  \n",
        "1. **Knowledge cutoff** — модель не знает о событиях и данных после даты её обучения.  \n",
        "2. **Галлюцинации** — склонность генерировать правдоподобные, но ложные утверждения.  \n",
        "\n",
        "RAG обходит эти ограничения, подключая модель к **актуальной, верифицированной внешней базе знаний** (например, внутренним отчётам, документации, базе знаний компании). Ответ генерируется **только на основе извлечённого контекста**, а не из внутренней памяти модели.\n",
        "\n",
        "**Как это работает**:  \n",
        "1. Пользовательский запрос поступает в систему.  \n",
        "2. Система ищет наиболее релевантные фрагменты в векторной базе данных (например, с помощью embedding-поиска).  \n",
        "3. Эти фрагменты встраиваются в промпт как контекст.  \n",
        "4. LLM генерирует ответ, строго опираясь на предоставленный контекст.\n",
        "\n",
        "```text\n",
        "Пример RAG-промпта:\n",
        "\n",
        "Используй ТОЛЬКО следующий контекст для ответа на вопрос. Если ответ не содержится в контексте, напиши: \"Информация отсутствует\".\n",
        "\n",
        "Контекст:\n",
        "---\n",
        "В отчёте за Q3 2025 указано: средний чек вырос на 12% по сравнению с Q2 и составил 2 450 руб. Основной рост обеспечили категории \"Электроника\" и \"Красота\".\n",
        "---\n",
        "\n",
        "Вопрос: Каков был средний чек в Q3 2025?\n",
        "\n",
        "Ответ:\n",
        "```\n",
        "\n",
        "> **Практическое применение в анализе данных**:  \n",
        "> - Ответы на вопросы по внутренним метрикам без доступа к BI-системе.  \n",
        "> - Автоматическая генерация пояснений к отчётам на основе свежих данных.  \n",
        "> - Поддержка аналитиков через семантический поиск по архиву Jupyter-ноутбуков или SQL-запросов.\n",
        "\n",
        "#### Self-Correction / Self-Refinement (Самокоррекция)\n",
        "\n",
        "**Суть**: Эта техника имитирует процесс ревью кода или экспертной проверки. Модель сначала генерирует черновой ответ, а затем **сама же его анализирует и исправляет**, используя дополнительные инструкции или контекст (например, схему БД, бизнес-правила, статистические допущения).\n",
        "\n",
        "Это особенно полезно в задачах, где ошибка в синтаксисе или логике делает результат бесполезным (например, SQL, Python, статистические выводы).\n",
        "\n",
        "```text\n",
        "Пример: самокоррекция SQL-запроса\n",
        "\n",
        "Шаг 1 (генерация):\n",
        "Напиши SQL-запрос для расчёта общей выручки по месяцам за 2024 год из таблицы orders (customer_id, revenue, date).\n",
        "\n",
        "Шаг 2 (самокоррекция):\n",
        "Проверь предыдущий SQL-запрос на наличие ошибок:\n",
        "- Убедись, что поле date имеет тип DATE.\n",
        "- Агрегация должна быть по месяцу, а не по дню.\n",
        "- Используй корректное имя таблицы и столбцов.\n",
        "\n",
        "Исправь запрос, если найдены ошибки.\n",
        "```\n",
        "\n",
        "> **Почему это работает**:  \n",
        "> Модель часто «торопится» дать ответ, пропуская важные детали. Самокоррекция заставляет её «вторично обдумать» решение, что значительно повышает точность. В экспериментах такая двухэтапная стратегия снижает количество синтаксических и логических ошибок на 30–50%.\n",
        "\n",
        "> **Совет**: для максимального эффекта давайте модели **чёткие критерии проверки** («проверь, что GROUP BY соответствует SELECT», «убедись, что нет деления на ноль» и т.д.).\n",
        "\n",
        "---\n",
        "\n",
        "Эти методики редко используются по отдельности. На практике их **комбинируют**:  \n",
        "- Сначала применяется **RAG**, чтобы получить актуальный контекст.  \n",
        "- Затем генерируется черновой ответ.  \n",
        "- И, наконец, запускается **Self-Correction**, чтобы проверить соответствие бизнес-логике или техническим требованиям.\n",
        "\n",
        "В следующем разделе мы разберём **реальные кейсы** из практики анализа данных: автоматизация отчётности, извлечение метрик из PDF-документов и генерация ETL-скриптов с самопроверкой."
      ],
      "metadata": {
        "id": "s4gvbvLDFEky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 4.2. Примеры практических заданий (Промпты)\n",
        "\n",
        "На лекции недостаточно просто знать теорию — важно уметь применять методы промпт-инжиниринга в реальных аналитических сценариях. Ниже приведены два типовых задания, отражающих повседневные задачи аналитика данных:  \n",
        "1. **Анализ неструктурированного текста с агрегацией метрик** (NLP + бизнес-аналитика).  \n",
        "2. **Генерация и верификация кода** (инженерия данных + самокоррекция).  \n",
        "\n",
        "Оба примера демонстрируют, как комбинировать компоненты промпта (роль, инструкцию, контекст, формат) с продвинутыми техниками (CoT, Self-Correction, JSON-вывод).\n",
        "\n",
        "---\n",
        "\n",
        "#### Пример 1: Анализ тональности и расчёт NPS для ритейла\n",
        "\n",
        "**Цель**: Преобразовать «сырые» отзывы в структурированный отчёт с ключевыми метриками и инсайтами, пригодный для презентации руководству.\n",
        "\n",
        "**Методы**:  \n",
        "- Zero/Few-shot классификация настроений  \n",
        "- Агрегация по бизнес-метрике (NPS)  \n",
        "- Строгий JSON-вывод для автоматической загрузки в BI-систему\n",
        "\n",
        "```text\n",
        "Промпт:\n",
        "\n",
        "Ты — аналитик sentiment analysis в крупной e-commerce компании.  \n",
        "Проанализируй следующие отзывы клиентов и выполни следующие действия:\n",
        "\n",
        "1. Для каждого отзыва определи числовую оценку (если явно указана) или оцени косвенно по тональности.\n",
        "2. Классифицируй каждый отзыв как:\n",
        "   - 'Detractor' (оценка 0–6)\n",
        "   - 'Passive' (7–8)\n",
        "   - 'Promoter' (9–10)\n",
        "3. Рассчитай NPS по формуле: % Promoters – % Detractors.\n",
        "4. Выдели топ-3 позитивные темы и топ-3 проблемы на основе содержания отзывов.\n",
        "5. Определи общий тренд: восходящий (улучшение) или нисходящий (ухудшение), если отзывы содержат временные маркеры.\n",
        "\n",
        "Отзывы:\n",
        "- \"Ужасная доставка, опоздали на 5 дней. Больше не закажу.\"  \n",
        "- \"Всё пришло вовремя, упаковка отличная, спасибо!\"  \n",
        "- \"Цены выросли, но качество на уровне. Ставлю 8.\"  \n",
        "- \"Лучший магазин! Рекомендую всем. 10 из 10!\"  \n",
        "\n",
        "Предоставь результат строго в формате JSON без дополнительного текста:\n",
        "{\n",
        "  \"nps_score\": <число от -100 до 100>,\n",
        "  \"key_positive_themes\": [\"тема1\", \"тема2\", \"тема3\"],\n",
        "  \"key_issues\": [\"проблема1\", \"проблема2\", \"проблема3\"],\n",
        "  \"sentiment_trend\": \"восходящий\" или \"нисходящий\"\n",
        "}\n",
        "```\n",
        "\n",
        "> **Почему это работает**:  \n",
        "> Чёткое разделение шагов + строгий формат вывода позволяют интегрировать такой промпт в ежедневный пайплайн обработки отзывов. JSON можно сразу загрузить в Power BI, Tableau или внутренний дашборд.\n",
        "\n",
        "---\n",
        "\n",
        "#### Пример 2: Генерация и отладка Python-скрипта с самокоррекцией\n",
        "\n",
        "**Цель**: Получить не просто рабочий код, а **надёжный, проверенный скрипт**, учитывающий тонкости обработки временных рядов и пропусков.\n",
        "\n",
        "**Методы**:  \n",
        "- Chain-of-Thought (пошаговое планирование)  \n",
        "- Self-Correction (проверка и исправление)  \n",
        "- Явное указание бизнес-логики (обработка NaN)\n",
        "\n",
        "```text\n",
        "Промпт:\n",
        "\n",
        "Ты — старший инженер по обработке данных (Data Engineer) в аналитической команде.\n",
        "\n",
        "Задача: Напиши Python-скрипт для предобработки файла 'sales_data.csv', содержащего столбцы: 'date' (в формате 'YYYY-MM-DD'), 'revenue'.\n",
        "\n",
        "Следуй инструкции пошагово (Chain-of-Thought):\n",
        "1. Загрузи CSV-файл в pandas DataFrame.\n",
        "2. Преобразуй столбец 'date' в тип datetime.\n",
        "3. Убедись, что данные отсортированы по дате.\n",
        "4. Рассчитай скользящее среднее выручки с окном 7 дней (center=False).\n",
        "5. Замени все оставшиеся NaN значения на 0.\n",
        "\n",
        "После генерации кода проведи этап Self-Correction:\n",
        "— Проверь: после расчёта rolling mean в начале ряда появятся NaN (первые 6 значений).  \n",
        "— Убедись, что шаг 5 действительно заменяет их на 0.  \n",
        "— Если в коде есть ошибка (например, fillna применён до rolling), исправь её и кратко объясни, почему это важно для корректности временного ряда.\n",
        "\n",
        "Выведи сначала исправленный код, затем — пояснение (1–2 предложения).\n",
        "```\n",
        "\n",
        "> **Ожидаемый результат**:  \n",
        "> Модель сначала может сгенерировать код, где `fillna(0)` применён до `rolling()`, что некорректно. На этапе самокоррекции она это замечает и перемещает `fillna()` **после** расчёта скользящего среднего — что соответствует правильной практике обработки временных рядов.\n",
        "\n",
        "> **Практическая ценность**: Такой подход снижает количество ошибок при автоматической генерации ETL-скриптов и повышает доверие к LLM как к инструменту разработки.\n",
        "\n",
        "---\n",
        "\n",
        "Эти задания можно использовать как основу для лабораторных работ или внутренних хакатонов по промпт-инжинирингу. В следующем разделе мы обсудим, как **оценивать качество промптов** количественно — с помощью метрик точности, полноты и устойчивости к переформулировкам."
      ],
      "metadata": {
        "id": "DRvwoU8CFMeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 4.3. Кейс-стади: Автоматизация еженедельного аналитического отчёта\n",
        "\n",
        "#### Проблема\n",
        "\n",
        "Аналитик в крупной digital-компании еженедельно тратил **8 часов** на подготовку стандартного отчёта, включающего:  \n",
        "- Сравнение KPI (конверсия, CAC, retention) с предыдущим периодом,  \n",
        "- Анализ аномалий в логах пользовательского поведения,  \n",
        "- Интерпретацию изменений в контексте маркетинговых кампаний и технических инцидентов.  \n",
        "\n",
        "Источники данных были разнородными:  \n",
        "- Внутренняя BI-система (агрегированные метрики),  \n",
        "- Логи событий из Kafka,  \n",
        "- Отчёты по рекламе из Google Ads и Meta,  \n",
        "- Архив прошлых аналитических записок (в формате PDF и Markdown),  \n",
        "- Тикеты из Jira по инцидентам.\n",
        "\n",
        "Ручная сверка, интерпретация и оформление занимали большую часть времени, при этом риск арифметической ошибки или упущенного инсайта оставался значительным.\n",
        "\n",
        "#### Решение: RAG + CoT + Few-Shot + Self-Correction пайплайн\n",
        "\n",
        "Команда разработала автоматизированный промпт-пайплайн, интегрированный в существующую аналитическую инфраструктуру. Он состоит из четырёх последовательных этапов:\n",
        "\n",
        "```text\n",
        "1. Data Retrieval (RAG)\n",
        "   — Система автоматически извлекает:\n",
        "     • Агрегированные метрики за текущую и предыдущую неделю,\n",
        "     • Релевантные фрагменты из архива отчётов (например, \"аномалия в конверсии 15 октября 2024\"),\n",
        "     • Описания активных маркетинговых кампаний,\n",
        "     • Инциденты из Jira за период.\n",
        "   — Все данные эмбедятся и хранятся в векторной БД (например, Chroma или FAISS).\n",
        "   — При генерации отчёта выполняется семантический поиск по запросу: \"Еженедельный отчёт за 2025-W44\".\n",
        "\n",
        "2. Analysis Generation (Chain-of-Thought)\n",
        "   Промпт:\n",
        "   \"Сравни метрики текущей недели с медианным значением предыдущего квартала.\n",
        "    Объясни три ключевых отклонения, рассуждая пошагово:\n",
        "    1. Какое изменение зафиксировано?\n",
        "    2. Какие внешние факторы (кампании, инциденты) могли повлиять?\n",
        "    3. Является ли отклонение статистически значимым (используй правило 2-х сигм)?\"\n",
        "\n",
        "3. Formatting (Few-Shot)\n",
        "   В промпт добавлены 2–3 примера структуры прошлых отчётов:\n",
        "   \"Пример 1:\n",
        "    ## Ключевые метрики\n",
        "    - Конверсия: +12% (vs Q3 median)\n",
        "    ## Аномалии\n",
        "    - Резкий спад в сегменте iOS 18.0 (см. инцидент INC-2025-1029)\n",
        "    ## Рекомендации\n",
        "    - Провести A/B-тест нового онбординга...\"\n",
        "\n",
        "   Это гарантирует единообразие стиля и структуры.\n",
        "\n",
        "4. Validation (Self-Correction)\n",
        "   Финальный этап:\n",
        "   \"Проверь сгенерированный отчёт на соответствие исходным данным:\n",
        "    — Совпадают ли указанные процентные изменения с расчётами из контекста?\n",
        "    — Упомянуты ли все значимые инциденты?\n",
        "    — Нет ли противоречий между разделами?\n",
        "    Исправь неточности и выведи только исправленную версию.\"\n",
        "```\n",
        "\n",
        "#### Результат\n",
        "\n",
        "- **Время подготовки отчёта** сократилось с **8 часов до 45 минут** (включая ручную проверку аналитиком).  \n",
        "- **Точность числовых расчётов** — **99.2%** (по сравнению с 94% при ручной обработке, где встречались ошибки в формулах Excel).  \n",
        "- Аналитик перешёл от рутинной сверки к **интерпретации и стратегическим рекомендациям** — его роль стала более экспертной.  \n",
        "- Пайплайн был масштабирован на 5 других команд (маркетинг, поддержка, продукт).\n",
        "\n",
        "> **Вывод**: Совмещение RAG (актуальный контекст), CoT (логика), Few-Shot (формат) и Self-Correction (надёжность) превращает LLM из «помощника» в **автономный аналитический модуль**, способный выполнять сложные, многоэтапные задачи с минимальным контролем человека.\n",
        "\n",
        "В следующем разделе мы рассмотрим, как количественно **оценивать эффективность промптов** — с помощью метрик, A/B-тестов и автоматизированной верификации."
      ],
      "metadata": {
        "id": "VIlBgzhrFQVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Инструментарий, Оценка и метрики\n",
        "\n",
        "Эффективный промпт-инжиниринг невозможен без правильного инструментария и системы оценки. На практике это означает:  \n",
        "- использование сред для быстрого прототипирования,  \n",
        "- применение фреймворков для сборки надёжных пайплайнов,  \n",
        "- внедрение метрик для объективного сравнения версий промптов.  \n",
        "\n",
        "Без этого промптинг остаётся «искусством проб и ошибок», а не инженерной дисциплиной.\n",
        "\n",
        "### 5.1. Инструменты и Среды разработки\n",
        "\n",
        "#### API Playgrounds (Rapid Prototyping)\n",
        "\n",
        "Для начальной итерации промптов идеально подходят **интерактивные playground’и**, предоставляемые провайдерами LLM:\n",
        "\n",
        "- **OpenAI Playground**  \n",
        "- **Anthropic Console**  \n",
        "- **Google Vertex AI Studio**  \n",
        "- **Hugging Face Chat UI** (для open-source моделей)\n",
        "\n",
        "Они позволяют в реальном времени:\n",
        "- менять **температуру** (0.0–1.0): низкая → детерминированный ответ, высокая → креативность,  \n",
        "- настраивать **top_p** (ядерное семплирование): контролирует разнообразие токенов,  \n",
        "- экспериментировать с **максимальной длиной ответа**, **частотным штрафом** и другими параметрами.\n",
        "\n",
        "> **Совет**: всегда сохраняйте успешные комбинации параметров вместе с промптом — они часть спецификации!\n",
        "\n",
        "#### Оркестрационные фреймворки (Production-уровень)\n",
        "\n",
        "Для перехода от прототипа к production-решению используются специализированные библиотеки:\n",
        "\n",
        "- **LangChain** — гибкий фреймворк для построения цепочек (chains), агентов и RAG-систем.  \n",
        "- **LlamaIndex** — оптимизирован для эффективного извлечения и индексации контекста (идеален для RAG с документами).  \n",
        "\n",
        "Эти инструменты позволяют:\n",
        "- интегрировать внешние источники данных (API, БД, файлы),  \n",
        "- кэшировать промежуточные результаты,  \n",
        "- логировать запросы и ответы для аудита,  \n",
        "- шаблонизировать промпты и управлять версиями.\n",
        "\n",
        "---\n",
        "\n",
        "#### Практическое упражнение: Шаблонизация промптов с LangChain\n",
        "\n",
        "**Зачем шаблонизировать?**  \n",
        "Жёстко закодированные промпты в виде строк (`f\"Анализируй {data}...\"`) быстро становятся неуправляемыми. Шаблоны обеспечивают:\n",
        "- **Воспроизводимость** (reproducibility),  \n",
        "- **Соблюдение схемы вывода** (schema compliance),  \n",
        "- **Безопасность** (защита от prompt injection через валидацию переменных).\n",
        "\n",
        "Пример: создание параметризованного промпта для анализа метрик.\n",
        "\n",
        "```python\n",
        "# Файл: prompt_template.py\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Шаблон промпта с чёткими плейсхолдерами\n",
        "ANALYTICS_PROMPT = PromptTemplate.from_template(\n",
        "\"\"\"\n",
        "Ты — старший аналитик данных.\n",
        "Проанализируй следующие метрики за период {period}:\n",
        "\n",
        "{metrics_data}\n",
        "\n",
        "Выполни:\n",
        "1. Сравни с базовым периодом ({baseline_period}).\n",
        "2. Выдели топ-3 аномалии.\n",
        "3. Предоставь рекомендации.\n",
        "\n",
        "Ответ строго в формате JSON с полями:\n",
        "- \"anomalies\": список строк,\n",
        "- \"recommendations\": список строк,\n",
        "- \"summary\": краткое резюме (1 предложение).\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    formatted_prompt = ANALYTICS_PROMPT.format(\n",
        "        period=\"2025-W44\",\n",
        "        baseline_period=\"Q3 2025 median\",\n",
        "        metrics_data=\"conversion: 4.2% (+12%), CAC: $28 (-5%), retention_D7: 31% (±0%)\"\n",
        "    )\n",
        "    print(formatted_prompt)\n",
        "```\n",
        "\n",
        "> **Результат**: такой подход позволяет легко интегрировать промпт в автоматизированный пайплайн, тестировать его с разными входами и гарантировать, что структура вывода всегда соответствует ожиданиям парсера.\n",
        "\n",
        "---\n",
        "\n",
        "В следующем подразделе мы рассмотрим, **как измерять качество промптов**: от точности и полноты до устойчивости к переформулировкам и вычислительной эффективности."
      ],
      "metadata": {
        "id": "TqMJv6srFTcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 5.2. Система отслеживания прогресса (Progress Tracking System)\n",
        "\n",
        "Промпт-инжиниринг перестаёт быть «магией», когда вы начинаете **измерять** его эффективность. Без количественных индикаторов невозможно понять, улучшается ли ваш промпт или просто «звучит умнее».  \n",
        "\n",
        "Для этого внедряется **система отслеживания прогресса**, основанная на **измеримых индикаторах (Measurable Progress Indicators, MPI)**. Эти метрики охватывают как техническую, так и когнитивную стороны работы с LLM.\n",
        "\n",
        "```text\n",
        "Измеримые индикаторы прогресса (MPI):\n",
        "\n",
        "| Метрика                          | Описание                                                                 | Как измеряется                                                                 | Целевое значение |\n",
        "|----------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------|------------------|\n",
        "| Точность извлечения/генерации (Fidelity) | Насколько вывод соответствует эталонному (ground truth) ответу.           | % совпадения по ключевым полям (для JSON), BLEU/ROUGE (для текста), точность классификации. | ≥ 95%            |\n",
        "| Эффективность промпта (Token Efficiency) | Экономичность использования контекста и генерации.                        | Соотношение: output_tokens / input_tokens. Чем ниже — тем лучше (меньше «воды»). | Минимизация      |\n",
        "| Устойчивость (Robustness Score)  | Способность давать корректный ответ при наличии шума или переформулировок. | % успешных ответов при: (а) добавлении случайного текста в контекст, (б) синонимичной замене слов в инструкции. | ≥ 90%            |\n",
        "| Психологическая устойчивость     | Субъективное состояние аналитика при работе с LLM.                        | Самооценка по шкале 1–5: «Насколько вы чувствуете контроль над результатом?» (еженедельно). | ≥ 4              |\n",
        "```\n",
        "\n",
        "#### Пояснения по ключевым метрикам\n",
        "\n",
        "- **Fidelity (Точность)** — главная метрика для production-задач. Например, если промпт должен извлечь `revenue` из отчёта, а эталон = 12500, то ответ «12 500» = ✅, «12.5 тыс.» = ❌ (если не указано иное). Для структурированных задач (JSON, SQL) используется **строгая валидация схемы**.\n",
        "\n",
        "- **Token Efficiency** — критически важна при работе с платными API. Длинные, многословные промпты увеличивают стоимость и задержку. Оптимизация может включать:\n",
        "  - удаление избыточных формулировок,\n",
        "  - использование кратких инструкций (`\"Выведи JSON\"` вместо `\"Пожалуйста, предоставь результат в формате JSON...\"`),\n",
        "  - сжатие контекста через RAG (извлекать только релевантные фрагменты).\n",
        "\n",
        "- **Robustness Score** — показывает, насколько промпт «ломается» при малейших изменениях. Хороший промпт должен работать даже если:\n",
        "  - в отзыве написано «не понравилось» вместо «негативный»,\n",
        "  - дата указана как «30/10/2025» вместо «2025-10-30»,\n",
        "  - в контекст случайно попал фрагмент лога.\n",
        "\n",
        "- **Психологическая устойчивость** — часто игнорируемый, но важный аспект. Если аналитик тратит больше энергии на «борьбу» с моделью, чем на анализ, это снижает продуктивность и мотивацию. Регулярная самооценка помогает вовремя пересмотреть стратегию (например, перейти от zero-shot к few-shot).\n",
        "\n",
        "> **Практический совет**: автоматизируйте сбор первых трёх метрик в CI/CD-пайплайне. Например, при каждом коммите в репозиторий с промптами запускайте тестовый набор (test suite) из 50–100 примеров и стройте дашборд динамики MPI.\n",
        "\n",
        "---\n",
        "\n",
        "Эта система превращает промпт-инжиниринг из субъективного процесса в **инженерную дисциплину с обратной связью**, где каждое изменение можно оценить объективно.  \n",
        "\n",
        "В заключительном разделе мы обсудим этические риски, ограничения LLM и принципы ответственного использования промптинга в образовательной и корпоративной среде."
      ],
      "metadata": {
        "id": "n-jTqfATFbIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 5.3. Бенчмаркинг промптов для анализа данных (Ключевые Метрики)\n",
        "\n",
        "В аналитике данных промпт — это не просто инструмент генерации текста, а **компонент вычислительной системы**. Поэтому его качество должно оцениваться по тем же строгим критериям, что и код или SQL-запрос: точность, воспроизводимость, соответствие спецификации.\n",
        "\n",
        "Для этого используется **бенчмаркинг промптов** — регулярное тестирование на фиксированном наборе примеров с измерением ключевых метрик. Ниже приведены метрики, наиболее релевантные для задач анализа данных.\n",
        "\n",
        "```text\n",
        "Ключевые метрики бенчмаркинга:\n",
        "\n",
        "| Метрика               | Описание                                                                 | Целевое значение | Примечание                                                                 |\n",
        "|-----------------------|--------------------------------------------------------------------------|------------------|----------------------------------------------------------------------------|\n",
        "| Data Accuracy         | Доля правильных числовых расчётов, статистических выводов и логических заключений. | >95%             | Критично: ошибка в p-value или NPS делает отчёт бесполезным или вредным. |\n",
        "| Schema Compliance     | Доля ответов, строго соответствующих заданной структуре (JSON, CSV, XML). | 100%             | Нарушение формата ломает автоматизацию. Используйте JSON Schema валидацию. |\n",
        "| Context Relevance     | Доля утверждений в ответе, подтверждённых RAG-контекстом (без галлюцинаций). | >90%             | Проверяется путём сравнения с исходным контекстом: каждое утверждение должно иметь источник. |\n",
        "| Token Efficiency      | Соотношение: количество сгенерированных токенов / количество входных переменных (или ключевых параметров). | <1.5             | Низкое значение = лаконичность. Высокое = «вода», что увеличивает стоимость и задержку. |\n",
        "```\n",
        "\n",
        "#### Criteria Check: Когда промпт считается «готовым»?\n",
        "\n",
        "Простого улучшения недостаточно — нужен **чёткий критерий завершения итерации**. Например:\n",
        "\n",
        "> **Промпт проходит бенчмарк, если одновременно выполнены условия**:  \n",
        "> - **Data Accuracy ≥ 95%**,  \n",
        "> - **Schema Compliance = 100%**,  \n",
        "> - **Token Efficiency снизился как минимум на 10%** по сравнению с предыдущей версией.\n",
        "\n",
        "Такой подход исключает «переоптимизацию» под одну метрику в ущерб другим (например, сверхточный, но многословный и неструктурированный ответ).\n",
        "\n",
        "---\n",
        "\n",
        "#### Практический пример: валидация промпта для расчёта NPS\n",
        "\n",
        "Допустим, у вас есть тестовый набор из 100 отзывов с размеченными оценками. Промпт генерирует JSON с полем `\"nps_score\"`.  \n",
        "\n",
        "- **Data Accuracy**: из 100 ответов — 97 совпадают с эталонным NPS → **97% ✅**  \n",
        "- **Schema Compliance**: 100 из 100 ответов — валидный JSON с нужными полями → **100% ✅**  \n",
        "- **Context Relevance**: в 94 случаях все утверждения («рост из-за новой доставки») подтверждены контекстом → **94% ✅**  \n",
        "- **Token Efficiency**: 180 output tokens / 120 input tokens = **1.5 → на грани**  \n",
        "\n",
        "→ Промпт почти готов, но требует небольшой оптимизации длины.\n",
        "\n",
        "---\n",
        "\n",
        "Такая система позволяет **объективно сравнивать версии промптов**, документировать прогресс и обеспечивать надёжность в production-среде.  \n",
        "\n",
        "В следующем (заключительном) разделе мы обсудим **этические и методологические ограничения LLM**, а также принципы ответственного использования промпт-инжиниринга в академической и профессиональной практике."
      ],
      "metadata": {
        "id": "WDrZxawVFcRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 6. Адаптация и Итерация (Adaptation & Iteration)\n",
        "\n",
        "Промпт-инжиниринг как дисциплина характеризуется высокой степенью зависимости от контекста, динамичности задач и нестационарности поведения больших языковых моделей (Large Language Models, LLM). Вследствие этого, эффективное освоение методологии промптинга требует не только статического усвоения шаблонов, но и внедрения **циклического механизма адаптации**, основанного на количественной оценке, рефлексии и постепенном усложнении когнитивных и технических задач. Данный подход согласуется с принципами **конструктивистской педагогики** и **теории пошагового формирования умственных действий** (П. Я. Гальперин), а также с практиками **непрерывного улучшения (continuous improvement)** в инженерных системах.\n",
        "\n",
        "### 6.1. Механизм итерации и оценки эффективности\n",
        "\n",
        "#### 6.1.1. Цикл итеративного улучшения промптов\n",
        "\n",
        "Эффективная итерация строится на четырёх последовательных этапах, образующих замкнутый цикл обратной связи:\n",
        "\n",
        "1. **Экспериментальное применение** — генерация ответов LLM на контролируемом наборе тестовых примеров (test suite), включающем как типовые, так и пограничные случаи (edge cases).  \n",
        "2. **Количественная оценка** — вычисление измеримых индикаторов прогресса (Measurable Progress Indicators, MPI), таких как Fidelity, Schema Compliance и Token Efficiency (см. раздел 5.2).  \n",
        "3. **Диагностический анализ** — выявление систематических ошибок (например, игнорирование временных меток, неверная агрегация по группам, нарушение JSON-схемы).  \n",
        "4. **Адаптивная коррекция** — модификация промпта с применением продвинутых техник (CoT, Few-Shot, RAG, Self-Correction) и/или повышение сложности задачи.\n",
        "\n",
        "Формально, цикл может быть представлен как отображение:\n",
        "\n",
        "$$\n",
        "P_{t+1} = \\mathcal{A}\\big(P_t, \\mathcal{E}(P_t, D_{\\text{test}})\\big),\n",
        "$$\n",
        "\n",
        "где:  \n",
        "- $P_t$ — версия промпта на итерации $t$,  \n",
        "- $D_{\\text{test}}$ — тестовый датасет,  \n",
        "- $\\mathcal{E}$ — функция оценки (возвращает вектор MPI),  \n",
        "- $\\mathcal{A}$ — адаптивный оператор коррекции.\n",
        "\n",
        "> **Пример 1 (практический)**.  \n",
        "> На итерации $t$ промпт для расчёта NPS демонстрирует Fidelity = 92%. Анализ ошибок показывает, что модель неверно интерпретирует отзывы без явной оценки (например, «всё нормально» → классифицируется как Promoter).  \n",
        "> На итерации $t+1$ в промпт добавляется Few-Shot блок:  \n",
        "> ```\n",
        "> Пример: \"всё нормально\" → Passive  \n",
        "> Пример: \"ничего особенного\" → Passive  \n",
        "> ```  \n",
        "> В результате Fidelity возрастает до 96.5%.\n",
        "\n",
        "#### 6.1.2. Критерии перехода на следующий уровень сложности\n",
        "\n",
        "Переход к более сложным задачам должен быть **условным**, а не временным. Рекомендуется использовать следующее правило:\n",
        "\n",
        "> **Правило адаптации**:  \n",
        "> Повышение сложности допустимо, если в течение **двух последовательных итераций** выполняются все условия:  \n",
        "> - $ \\text{Fidelity} \\geq 0.95 $,  \n",
        "> - $ \\text{Schema Compliance} = 1.0 $,  \n",
        "> - $ \\text{Robustness Score} \\geq 0.90 $.\n",
        "\n",
        "Под «повышением сложности» понимается:  \n",
        "- увеличение числа источников данных (от одного CSV к интеграции SQL + PDF + логи),  \n",
        "- введение неопределённости («данные частично противоречивы — опиши расхождения»),  \n",
        "- требование генерации не только вывода, но и **обоснования методологии** («почему выбран t-тест, а не Mann–Whitney?»).\n",
        "\n",
        "> **Пример 2 (методологический)**.  \n",
        "> После освоения генерации SQL-запросов с CoT, следующая задача:  \n",
        "> *«Сгенерируй два альтернативных запроса для расчёта retention: (а) по cohort-анализу, (б) по rolling window. Сравни их вычислительную сложность и применимость в условиях sparse data.»*  \n",
        "> Такая формулировка требует не только применения, но и **оценки** и **синтеза** — уровней 5–6 по таксономии Блума.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Оценка эффективности обучения: когнитивные и метакогнитивные метрики\n",
        "\n",
        "#### 6.2.1. Коэффициент удержания знаний (Retention Rate)\n",
        "\n",
        "Для объективной оценки долгосрочного усвоения навыков промпт-инжиниринга рекомендуется применять метод **отсроченного ретестирования**. Процедура включает:\n",
        "\n",
        "- Проведение базового теста $T_0$ (например, разработка промпта для извлечения метрик из текста).  \n",
        "- Повторное выполнение аналогичного, но не идентичного задания $T_{14}$ через 14 дней.  \n",
        "- Расчёт коэффициента удержания:\n",
        "\n",
        "$$\n",
        "R = \\frac{\\text{Score}(T_{14})}{\\text{Score}(T_0)}.\n",
        "$$\n",
        "\n",
        "Целевое значение: $ R \\geq 0.90 $. Значение $ R < 0.80 $ указывает на необходимость возврата к фундаментальным техникам (Zero-Shot, структурирование компонентов промпта).\n",
        "\n",
        "> **Пример 3 (эмпирический)**.  \n",
        "> Студент на $T_0$ получил Fidelity = 94% при генерации JSON-отчёта. Через две недели, при работе с новым датасетом (продажи вместо логов), Fidelity = 85%.  \n",
        "> Диагноз: недостаточная **трансферабельность** навыка.  \n",
        "> Интервенция: тренировка на мультидоменных данных (e-commerce, финансы, логистика) с акцентом на **абстрактную структуру промпта**, а не на предметную область.\n",
        "\n",
        "#### 6.2.2. Согласование с таксономией Блума\n",
        "\n",
        "Таксономия Блума (Bloom’s Taxonomy) предоставляет иерархическую модель когнитивных процессов, что позволяет оценивать глубину освоения дисциплины. В контексте промпт-инжиниринга уровни могут быть операционализированы следующим образом:\n",
        "\n",
        "| Уровень таксономии | Операциональное определение в PE | Пример задания |\n",
        "|--------------------|----------------------------------|----------------|\n",
        "| **Запоминание**    | Воспроизведение шаблонов и терминов | «Напишите определение Chain-of-Thought». |\n",
        "| **Понимание**      | Объяснение принципов работы техник | «Почему Few-Shot повышает точность классификации?» |\n",
        "| **Применение**     | Использование техник в знакомом контексте | «Создайте CoT-промпт для расчёта медианы». |\n",
        "| **Анализ**         | Сравнение, декомпозиция, выявление структуры | «Сравните Token Efficiency трёх версий промпта для одной задачи». |\n",
        "| **Оценка**         | Критическое суждение о пригодности метода | «Когда RAG избыточен? Приведите аргументы». |\n",
        "| **Создание**       | Синтез новых пайплайнов или гибридных подходов | «Спроектируйте промпт-пайплайн для автоматического аудита финансовых отчётов с Self-Correction и JSON Schema». |\n",
        "\n",
        "> **Рекомендация**: к завершению курса не менее **70% практических заданий** должны соответствовать уровням «Анализ», «Оценка» и «Создание». Это гарантирует переход от репродуктивного к продуктивному мышлению.\n",
        "\n",
        "Таким образом, адаптация и итерация в промпт-инжиниринге — это не вспомогательный, а **центральный процесс**, обеспечивающий как техническую надёжность решений, так и когнитивное развитие специалиста. Внедрение формализованных метрик, условных правил перехода и когнитивной оценки позволяет трансформировать обучение из эвристического опыта в **воспроизводимую, измеримую и масштабируемую инженерную практику**. Такой подход особенно актуален в условиях быстрой эволюции архитектур LLM и растущих требований к прозрачности и воспроизводимости аналитических выводов."
      ],
      "metadata": {
        "id": "Hv1HgB9pFnTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Методология количественной оценки эффективности промптов в задачах анализа данных\n",
        "\n",
        "Оценка качества промптов, используемых для взаимодействия с большими языковыми моделями (Large Language Models, LLM), требует систематического подхода, основанного на воспроизводимых метриках и контролируемых экспериментальных условиях. В отличие от субъективной оценки «качества текста», применяемой в гуманитарных дисциплинах, в прикладной аналитике данных эффективность промпта должна измеряться через **точность, структурную корректность, вычислительную эффективность и устойчивость к возмущениям входных данных**. Ниже излагается формализованная методология верификации ключевых метрик.\n",
        "\n",
        "### 1. Формирование эталонного тестового набора\n",
        "\n",
        "Первым этапом оценки является конструирование **контролируемого тестового набора** $\\mathcal{D}_{\\text{test}} = \\{ (x_i, y_i^{\\text{gt}}) \\}_{i=1}^N$, где:  \n",
        "- $x_i$ — входной промпт или набор переменных для параметризованного шаблона,  \n",
        "- $y_i^{\\text{gt}}$ — эталонный (ground truth) ответ, верифицированный экспертом или полученный из достоверного источника.\n",
        "\n",
        "Размер набора $N$ рекомендуется выбирать в диапазоне 20–100 наблюдений для баланса между статистической надёжностью и вычислительной целесообразностью. Набор должен включать как типовые, так и пограничные случаи (edge cases), моделирующие реальные условия эксплуатации.\n",
        "\n",
        "### 2. Генерация ответов LLM\n",
        "\n",
        "Для каждого элемента $x_i \\in \\mathcal{D}_{\\text{test}}$ выполняется запрос к LLM с фиксированными гиперпараметрами (в частности, температура $T = 0.0$ для детерминированности). Полученный ответ обозначается как $y_i^{\\text{pred}}$.\n",
        "\n",
        "### 3. Количественная оценка по ключевым метрикам\n",
        "\n",
        "#### 3.1. Точность данных (Data Accuracy)\n",
        "\n",
        "Метрика оценивает степень соответствия числовых и логических выводов модели эталонным значениям. Для скалярных величин применяется относительная погрешность:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_i =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } \\left| \\dfrac{y_i^{\\text{pred}} - y_i^{\\text{gt}}}{y_i^{\\text{gt}}} \\right| \\leq \\varepsilon, \\\\\n",
        "0, & \\text{иначе},\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "где $\\varepsilon$ — допустимая относительная погрешность (обычно $\\varepsilon = 0.01$). Для категориальных и логических переменных используется точное совпадение.\n",
        "\n",
        "Итоговая метрика:\n",
        "$$\n",
        "\\text{Data Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Accuracy}_i.\n",
        "$$\n",
        "\n",
        "#### 3.2. Соответствие схеме (Schema Compliance)\n",
        "\n",
        "Данная метрика проверяет строгое соответствие структуры вывода заранее определённой схеме (например, JSON Schema). Используется валидация на основе формальных грамматик:\n",
        "\n",
        "$$\n",
        "\\text{Schema Compliance}_i =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } y_i^{\\text{pred}} \\models \\mathcal{S}, \\\\\n",
        "0, & \\text{иначе},\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "где $\\mathcal{S}$ — формальная схема вывода, а $\\models$ обозначает удовлетворение синтаксическим и семантическим ограничениям.  \n",
        "Целевое значение: **100%**, поскольку нарушение схемы делает автоматическую обработку невозможной.\n",
        "\n",
        "#### 3.3. Релевантность контексту (Context Relevance)\n",
        "\n",
        "Метрика направлена на выявление галлюцинаций — генерации утверждений, не подтверждённых предоставленным контекстом $c_i$. Операционализируется через семантическое покрытие:\n",
        "\n",
        "$$\n",
        "\\text{Relevance}_i = \\frac{|\\{ s \\in \\text{Sent}(y_i^{\\text{pred}}) : \\exists f \\in \\text{Frag}(c_i), \\ \\text{sim}(s, f) \\geq \\tau \\}|}{|\\text{Sent}(y_i^{\\text{pred}})|},\n",
        "$$\n",
        "\n",
        "где:  \n",
        "- $\\text{Sent}(\\cdot)$ — множество предложений,  \n",
        "- $\\text{Frag}(\\cdot)$ — фрагменты контекста,  \n",
        "- $\\text{sim}(\\cdot, \\cdot)$ — косинусное сходство эмбеддингов (например, на основе `all-MiniLM-L6-v2`),  \n",
        "- $\\tau$ — порог сходства (обычно $\\tau = 0.7$).\n",
        "\n",
        "#### 3.4. Эффективность по токенам (Token Efficiency)\n",
        "\n",
        "Оценивает вычислительную и экономическую рациональность промпта:\n",
        "\n",
        "$$\n",
        "\\text{Token Efficiency} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{len}_{\\text{tok}}(y_i^{\\text{pred}})}{\\text{len}_{\\text{tok}}(x_i)},\n",
        "$$\n",
        "\n",
        "где $\\text{len}_{\\text{tok}}(\\cdot)$ — количество токенов, подсчитанное с использованием токенизатора, соответствующего архитектуре LLM (например, `tiktoken` для моделей OpenAI). Целевое значение: $< 1.5$ для структурированных задач.\n",
        "\n",
        "#### 3.5. Устойчивость к возмущениям (Robustness Score)\n",
        "\n",
        "Оценивается на расширенном наборе $\\mathcal{D}_{\\text{test}}^{\\text{noisy}}$, содержащем вариации входных данных:  \n",
        "- синонимическая замена слов в инструкции,  \n",
        "- добавление неинформативного шума в контекст,  \n",
        "- изменение порядка элементов.\n",
        "\n",
        "$$\n",
        "\\text{Robustness} = \\frac{1}{M} \\sum_{j=1}^{M} \\mathbb{I}\\big( y_j^{\\text{pred, noisy}} \\text{ корректен} \\big),\n",
        "$$\n",
        "\n",
        "где $M$ — число зашумлённых примеров, $\\mathbb{I}$ — индикаторная функция. Целевое значение: $\\geq 0.90$.\n",
        "\n",
        "### 4. Агрегация и интерпретация результатов\n",
        "\n",
        "Итоговые метрики агрегируются в отчёт, который может быть интегрирован в системы непрерывной интеграции (CI/CD). Рекомендуется визуализировать динамику метрик по версиям промптов (например, с использованием `MLflow` или `Weights & Biases`).\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Предложенная методология обеспечивает **объективную, воспроизводимую и масштабируемую** оценку качества промптов. Её применение позволяет трансформировать промпт-инжиниринг из эвристической практики в инженерную дисциплину, соответствующую стандартам надёжности, предъявляемым к компонентам аналитических систем. Внедрение подобного подхода особенно актуально в условиях растущей интеграции LLM в критически важные бизнес-процессы, где ошибки могут иметь значимые финансовые или репутационные последствия."
      ],
      "metadata": {
        "id": "jzQPYey1HkIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 7. Этические аспекты, Troubleshooting и Будущее PE\n",
        "\n",
        "Промпт-инжиниринг — это не только технический навык, но и **ответственность**. Особенно в аналитике данных, где решения на основе LLM могут влиять на бизнес-стратегии, распределение ресурсов или даже оценку персонала. Поэтому важно сочетать инженерную эффективность с этической осознанностью и умением быстро диагностировать и исправлять сбои.\n",
        "\n",
        "### 7.1. Детализированное расписание (Study Schedule Generation)\n",
        "\n",
        "Для магистрантов с ограниченной неделей (5 часов на PE), важно **максимально эффективно распределить время** между теорией, практикой и интеграцией. Ниже — пример сбалансированного недельного плана, сочетающего изучение новых методов и hands-on работу.\n",
        "\n",
        "```text\n",
        "Недельное расписание для изучения промпт-инжиниринга (5 часов/неделя)\n",
        "\n",
        "| День        | Время   | Фокус                          | Активность                                                                 |\n",
        "|-------------|---------|--------------------------------|----------------------------------------------------------------------------|\n",
        "| Понедельник | 1 час   | Теория / Чтение                | Изучение продвинутой техники: Tree-of-Thought (ToT) — как разбивать задачу на поддеревья решений. |\n",
        "| Среда       | 2 часа  | Практика CoT и Форматирование  | Разработка промпта для генерации сложного SQL-запроса (например, с оконными функциями).<br>Проведите 5 итераций: от zero-shot до CoT + Few-Shot + Schema Enforcement. |\n",
        "| Пятница     | 2 часа  | RAG и Интеграция               | Создание мини-RAG-прототипа: загрузка PDF-отчётов → эмбеддинги → извлечение → генерация.<br>Оцените Fidelity: насколько ответы соответствуют исходным документам. |\n",
        "```\n",
        "\n",
        "> **Совет**: после каждой сессии фиксируйте **один ключевой инсайт** и **одну ошибку**, которую вы больше не повторите. Это ускоряет обучение в 2–3 раза.\n",
        "\n",
        "---\n",
        "\n",
        "### 7.2. Этические аспекты для аналитиков\n",
        "\n",
        "Аналитик, использующий LLM, становится **медиатором между данными и решением**. Это накладывает этические обязательства:\n",
        "\n",
        "- **Снижение систематической ошибки (Bias Mitigation)**  \n",
        "  LLM наследуют предвзятости из обучающих данных. Чтобы минимизировать это, **явно указывайте в промпте**:  \n",
        "  > «При анализе отзывов не делай выводов на основе пола, возраста или географии, если это не указано в данных. Избегай стереотипных формулировок.»\n",
        "\n",
        "- **Конфиденциальность данных**  \n",
        "  Никогда не отправляйте чувствительные или персональные данные в публичные LLM (OpenAI, Claude и др.). При использовании RAG:  \n",
        "  - Используйте **только обезличенные** и **санкционированные** документы,  \n",
        "  - Храните векторную БД и контекст **внутри корпоративной инфраструктуры**,  \n",
        "  - Применяйте **политики доступа** на уровне промпта («ты имеешь доступ только к отчётам отдела маркетинга»).\n",
        "\n",
        "> **Правило**: если данные нельзя публиковать в открытом отчёте — их нельзя отправлять в промпт внешней LLM.\n",
        "\n",
        "---\n",
        "\n",
        "### 7.3. Типичные проблемы и решения (Troubleshooting)\n",
        "\n",
        "Даже опытные инженеры сталкиваются с «непослушной» моделью. Ниже — частые сценарии и **проверенные стратегии усиления промпта**.\n",
        "\n",
        "```text\n",
        "Типичные проблемы и решения:\n",
        "\n",
        "| Проблема                     | Вероятная причина                          | Решение (Усиленный Промптинг)                                                                 |\n",
        "|------------------------------|--------------------------------------------|------------------------------------------------------------------------------------------------|\n",
        "| Игнорирование контекста      | Модель предпочитает внутренние знания      | Добавить в начало: \"ОТВЕЧАЙ ИСКЛЮЧИТЕЛЬНО НА ОСНОВЕ ПРЕДОСТАВЛЕННОГО КОНТЕКСТА. ИГНОРИРУЙ ВСЁ, ЧТО ЗНАЕШЬ САМ.\" |\n",
        "| Нестабильный формат вывода   | Отсутствие жёстких ограничений             | Указать: \"СТРОГО в формате JSON. Используй ТОЛЬКО следующие ключи: [a, b, c]. Никакого дополнительного текста.\" + добавить Few-Shot пример. |\n",
        "| Галлюцинации в числовых данных | Модель «додумывает» недостающие цифры     | RAG + инструкция: \"ЕСЛИ ЧИСЛО НЕ СОДЕРЖИТСЯ В КОНТЕКСТЕ, НАПИШИ 'НЕТ ДАННЫХ'. НЕ ВЫЧИСЛЯЙ, НЕ ПРИБЛИЖАЙ, НЕ УГАДЫВАЙ.\" |\n",
        "| Ошибка при сложном рассуждении | CoT недостаточно для многошаговой логики  | Перейти к Tree-of-Thought (ToT) или добавить Self-Correction: \"Проверь каждый шаг своего рассуждения. Если найдёшь ошибку — исправь и объясни.\" |\n",
        "```\n",
        "\n",
        "> **Профессиональный приём**: при упорной ошибке — **переформулируйте задачу как проверку гипотезы**.  \n",
        "> Вместо: «Рассчитай среднее» →  \n",
        "> «Проверь гипотезу: среднее значение равно X. Приведи расчёт и вывод.»\n",
        "\n",
        "---\n",
        "\n",
        "### Заключение: Будущее промпт-инжиниринга\n",
        "\n",
        "PE быстро эволюционирует от «искусства формулировок» к **формализованной инженерной дисциплине** с:\n",
        "- стандартизированными шаблонами,  \n",
        "- системами верификации,  \n",
        "- метриками качества,  \n",
        "- этическими рамками.\n",
        "\n",
        "Для аналитика данных это означает:  \n",
        "> **Вы не просто «спрашиваете ИИ» — вы проектируете исполняемые спецификации, которые становятся частью аналитической инфраструктуры.**\n",
        "\n",
        "Освоив методы, представленные в этой лекции, вы получаете не только инструмент ускорения работы, но и **основу для создания надёжных, прозрачных и этичных AI-ассистентов следующего поколения**."
      ],
      "metadata": {
        "id": "vaOyE3PBFnZZ"
      }
    }
  ]
}