{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPjWzqS1B0pYUHzcZ53E7KC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%BA%D1%83%D1%80%D1%81%D0%B0_%D0%BF%D0%BE_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B5_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0_(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Программа курса по обработке естественного языка (NLP) для магистрантов**\n",
        "\n",
        "### **Тема 1. Введение в NLP и предобработка текста**\n",
        "\n",
        "**Общие сведения о NLP**:  \n",
        "Основные понятия, история развития, области применения, ключевые вызовы и проблемы в обработке естественного языка.\n",
        "\n",
        "**Лингвистические основы**:  \n",
        "Морфология, синтаксис, семантика и прагматика — базовые уровни языкового анализа.\n",
        "\n",
        "**Этапы предобработки текста**:  \n",
        "- Токенизация: подходы по словам, символам и subword-токенизация (WordPiece, BPE, SentencePiece).  \n",
        "- Нормализация текста: приведение к нижнему регистру, удаление пунктуации, чисел, обработка эмодзи, сленга и аббревиатур.  \n",
        "- Стемминг и лемматизация: различия, области применения и ограничения.  \n",
        "- Удаление стоп-слов.  \n",
        "- Закон Ципфа и его значение для анализа частотности слов в NLP.  \n",
        "- Униграммная языковая модель.  \n",
        "\n",
        "**Практические аспекты**:  \n",
        "- Загрузка и предварительная обработка текстовых данных.\n",
        "- Парсинг данных\n",
        "- Работа с кодировками (UTF-8).  \n",
        "- Использование регулярных выражений для обработки текста.  \n",
        "\n",
        "**Инструменты**:  \n",
        "Обзор библиотек Python: NLTK, spaCy, `re`, Gensim, TextBlob.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 2. Векторизация текста**\n",
        "\n",
        "**Необходимость числового представления текста**:  \n",
        "Эволюция методов векторизации.\n",
        "\n",
        "**Методы представления текста в виде векторов**:  \n",
        "- One-Hot Encoding.  \n",
        "- Мешок слов (Bag-of-Words, BoW) и TF-IDF (включая N-граммы).  \n",
        "\n",
        "**Векторные представления слов (Word Embeddings)**:  \n",
        "- Word2Vec: архитектуры CBOW и Skip-gram, интуиция и обучение.  \n",
        "- GloVe: глобальные векторные представления на основе статистики.  \n",
        "- FastText: учёт внутренней структуры слов, обработка неизвестных слов (OOV).  \n",
        "\n",
        "**Ограничения статических эмбеддингов**.\n",
        "\n",
        "**Практические аспекты**:  \n",
        "- Создание текстового корпуса.  \n",
        "- Предварительное обучение (pre-training) векторных моделей.  \n",
        "- Тонкая настройка (fine-tuning) моделей Word2Vec, FastText и GloVe на специализированных корпусах.  \n",
        "\n",
        "**Практикум**:  \n",
        "Сравнение различных методов векторизации и их влияния на качество моделей.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 3. Классические методы машинного обучения в NLP**\n",
        "\n",
        "**Задачи классификации текста**:  \n",
        "Бинарная, многоклассовая и многометковая (multi-label) классификация.\n",
        "\n",
        "**Применение алгоритмов машинного обучения (с использованием scikit-learn)**:  \n",
        "- Наивный байесовский классификатор.  \n",
        "- Метод опорных векторов (SVM).  \n",
        "- Логистическая регрессия.  \n",
        "- Деревья решений и случайный лес.  \n",
        "- Введение в ансамблевые методы: бэггинг и бустинг.\n",
        "\n",
        "**Оценка качества моделей**:  \n",
        "- Метрики: точность, полнота, F1-мера, ROC-AUC, PR-кривая, калибровка.  \n",
        "- Матрица ошибок.  \n",
        "- Гиперпараметрическая оптимизация и кросс-валидация.  \n",
        "\n",
        "**Практикум**:  \n",
        "Реализация текстовых классификаторов с акцентом на выбор признаков и оценку эффективности.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 4. Кластеризация текстов**\n",
        "\n",
        "**Введение в кластеризацию**:  \n",
        "Задачи и применение в NLP: тематическое моделирование, обнаружение дубликатов, группировка документов.\n",
        "\n",
        "**Методы кластеризации**:  \n",
        "- K-Means.  \n",
        "- Иерархическая кластеризация.  \n",
        "- DBSCAN.  \n",
        "\n",
        "**Определение оптимального числа кластеров**:  \n",
        "Метод локтя, коэффициент силуэта.\n",
        "\n",
        "**Оценка качества кластеризации**:  \n",
        "Силуэт, скорректированный индекс Рэнда (Adjusted Rand Index), нормированная взаимная информация (Normalized Mutual Information).\n",
        "\n",
        "**Практикум**:  \n",
        "Применение кластеризации для тематического анализа текстов, визуализация результатов.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 5. Нейронные сети для NLP: MLP и CNN**\n",
        "\n",
        "**Основы нейронных сетей**:  \n",
        "Многослойные перцептроны (MLP): архитектура, функции активации, оптимизаторы, функции потерь.\n",
        "\n",
        "**Свёрточные нейронные сети (CNN) в NLP**:  \n",
        "- Архитектура CNN для текстов: свёрточные фильтры, пулинг, паддинг.  \n",
        "- Применение CNN в задачах классификации и анализа тональности.  \n",
        "\n",
        "**Практикум**:  \n",
        "Построение и обучение простых CNN для классификации текстов с использованием PyTorch или TensorFlow.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 6. Рекуррентные нейронные сети (RNN) в NLP**\n",
        "\n",
        "**Введение в RNN**:  \n",
        "Особенности обработки последовательных данных, проблема затухающих и взрывающихся градиентов.\n",
        "\n",
        "**Архитектуры RNN**:  \n",
        "- LSTM (Long Short-Term Memory).  \n",
        "- GRU (Gated Recurrent Unit).  \n",
        "- Двунаправленные RNN (Bi-RNN, Bi-LSTM, Bi-GRU).\n",
        "\n",
        "**Применение RNN в задачах NLP**:  \n",
        "- Классификация и анализ тональности текста.  \n",
        "- Распознавание именованных сущностей (NER).  \n",
        "- Основы машинного перевода: модели «последовательность-к-последовательности» (Seq2Seq) на базе RNN.\n",
        "\n",
        "**Практикум**:  \n",
        "Реализация RNN-моделей для решения задач с последовательными данными.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 7. Трансформеры и механизм внимания**\n",
        "\n",
        "**Ограничения RNN и появление механизма внимания**:  \n",
        "Интуиция и математическая основа внимания.\n",
        "\n",
        "**Архитектура трансформера**:  \n",
        "- Self-Attention и Multi-Head Attention.  \n",
        "- Позиционное кодирование.  \n",
        "- Encoder-Decoder архитектура.\n",
        "\n",
        "**Предобученные модели на основе трансформеров**:  \n",
        "- BERT, GPT, T5: особенности архитектуры и схемы предобучения.\n",
        "\n",
        "**Работа с предобученными моделями**:  \n",
        "- Библиотека Hugging Face Transformers: загрузка моделей, использование пайплайнов, API для Dataset и Trainer.  \n",
        "\n",
        "**Применение трансформеров**:  \n",
        "Классификация, суммаризация, генерация текста, вопросно-ответные системы.\n",
        "\n",
        "**Практикум**:  \n",
        "Тонкая настройка (fine-tuning) предобученных трансформеров на специфических задачах.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 8. Адаптация и дообучение больших языковых моделей (LLM)**\n",
        "\n",
        "**Введение в LLM**:  \n",
        "Архитектуры, возможности, ограничения, масштабирование и emergent abilities (возникающие способности).\n",
        "\n",
        "**Стратегии адаптации LLM**:  \n",
        "- **Prompt Engineering**: эффективное конструирование промптов (zero-shot, few-shot, chain-of-thought, tree-of-thought).  \n",
        "- **In-Context Learning**: zero-shot и few-shot обучение.  \n",
        "\n",
        "**Методы дообучения (fine-tuning)**:  \n",
        "- Полное дообучение (ограничения по вычислительным ресурсам).  \n",
        "- Parameter-Efficient Fine-Tuning (PEFT):  \n",
        "  - LoRA (Low-Rank Adaptation).  \n",
        "  - QLoRA.  \n",
        "  - Prefix-tuning.  \n",
        "  - Prompt-tuning.  \n",
        "  - Adapter-tuning.  \n",
        "- Сравнение PEFT-методов: выбор подхода в зависимости от задачи и ресурсов.\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)**:  \n",
        "Концепция и реализация: ретривер, генератор, векторные базы данных.\n",
        "\n",
        "**Практикум**:  \n",
        "Эксперименты с промпт-инжинирингом и PEFT-методами на реальных примерах.\n",
        "\n",
        "\n",
        "### **Тема 9. Применение LLM в реальных проектах и продвинутые темы**\n",
        "\n",
        "**Практическое применение LLM**:  \n",
        "- Вопросно-ответные системы (QA).  \n",
        "- Суммаризация, машинный перевод, генерация кода.  \n",
        "\n",
        "**Фреймворки для разработки LLM-приложений**:  \n",
        "- LangChain, LlamaIndex: агенты, цепочки, инструменты.  \n",
        "\n",
        "**Примеры реальных проектов**:  \n",
        "Чат-боты, виртуальные ассистенты, системы рекомендаций, автоматизация бизнес-процессов.\n",
        "\n",
        "**Развертывание LLM-приложений**:  \n",
        "Инференс, оптимизация производительности, управление затратами.\n",
        "\n",
        "**Этические и технические вызовы**:  \n",
        "- Предвзятость моделей.  \n",
        "- Галлюцинации.  \n",
        "- Безопасность и контроль генерации.\n",
        "\n",
        "**Продвинутые темы**:  \n",
        "- Основы мультимодальных моделей (текст + изображение, текст + аудио/видео).  \n",
        "- Агенты на основе LLM и автономные системы.\n",
        "\n"
      ],
      "metadata": {
        "id": "hvZ2XauQmc7j"
      }
    }
  ]
}