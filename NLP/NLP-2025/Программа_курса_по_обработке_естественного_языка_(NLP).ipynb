{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM9PcH/JieMk37y7ogtJAaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%BA%D1%83%D1%80%D1%81%D0%B0_%D0%BF%D0%BE_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B5_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0_(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Программа курса по обработке естественного языка (NLP) для магистрантов**\n",
        "\n",
        "### **Тема 1. Введение в NLP и предобработка текста**\n",
        "\n",
        "**Основы обработки естественного языка (NLP)**:  \n",
        "Ключевые понятия, история развития, области применения, основные вызовы и фундаментальные проблемы NLP.\n",
        "\n",
        "**Лингвистические основы языка**:  \n",
        "Морфология, синтаксис, семантика и прагматика — уровни языкового анализа, значимые для построения NLP-систем.\n",
        "\n",
        "**Этапы предобработки текстовых данных**:  \n",
        "- **Токенизация**: подходы по словам, символам и subword-токенизация (WordPiece, BPE, SentencePiece).  \n",
        "- **Нормализация текста**: приведение к нижнему регистру, удаление пунктуации и чисел, обработка эмодзи, сленга и аббревиатур.  \n",
        "- **Стемминг и лемматизация**: различия, практическое применение и ограничения.  \n",
        "- **Удаление стоп-слов**.  \n",
        "- **Закон Ципфа** и его значение для анализа частотности слов в текстах.  \n",
        "- **Униграммная языковая модель** как базовая модель вероятности слов.\n",
        "\n",
        "**Практические аспекты работы с текстом**:  \n",
        "- Загрузка и предварительная обработка текстовых данных.  \n",
        "- Работа с кодировками (UTF-8).  \n",
        "- Использование регулярных выражений для обработки и очистки текста.\n",
        "\n",
        "**Инструменты Python для анализа текста**:  \n",
        "Обзор библиотек: NLTK, spaCy, `re`, Gensim, TextBlob.\n",
        "\n",
        "**Практикум**:  \n",
        "Парсинг новостных статей с веб-сайтов или других источников, сбор текстовых данных и применение базовых методов предобработки.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 2. Управление данными и векторизация в NLP**\n",
        "\n",
        "**Сбор текстовых данных**:  \n",
        "Источники данных, методы сбора (веб-скрейпинг, использование API), форматы хранения (JSON, CSV, TXT).\n",
        "\n",
        "**Разметка (аннотация) данных**:  \n",
        "Принципы ручной и полуавтоматической разметки, инструменты для аннотации (например, Label Studio), оценка качества разметки (интераннотаторская согласованность, Cohen’s Kappa).\n",
        "\n",
        "**Аугментация текстовых данных**:  \n",
        "Методы: замена синонимами, перефразирование, back-translation. Цели и ограничения аугментации.\n",
        "\n",
        "**Работа с несбалансированными выборками**:  \n",
        "Стратегии: oversampling, undersampling, использование синтетических данных (например, SMOTE). Влияние дисбаланса на обучение моделей.\n",
        "\n",
        "**Проблема data leakage в NLP**:  \n",
        "Определение, примеры (например, утечка информации из тестовой выборки), методы предотвращения.\n",
        "\n",
        "**Числовое представление текста**:  \n",
        "Необходимость векторизации и эволюция подходов.\n",
        "\n",
        "**Методы векторизации**:  \n",
        "- One-Hot Encoding.  \n",
        "- Bag-of-Words (BoW) и TF-IDF (с учётом униграмм и N-грамм).  \n",
        "\n",
        "**Векторные представления слов (Word Embeddings)**:  \n",
        "- Word2Vec (CBOW, Skip-gram): архитектура и интуиция.  \n",
        "- GloVe: глобальные векторные представления на основе статистики.  \n",
        "- FastText: учёт морфологической структуры слов и обработка неизвестных (OOV) слов.  \n",
        "\n",
        "**Ограничения статических эмбеддингов**.\n",
        "\n",
        "**Практические аспекты**:  \n",
        "- Создание корпуса и предварительное обучение (pre-training) эмбеддингов.  \n",
        "- Тонкая настройка (fine-tuning) моделей Word2Vec, FastText, GloVe на специализированных корпусах.\n",
        "\n",
        "**Практикум**:  \n",
        "Сбор и базовая разметка небольшого датасета, применение методов аугментации, сравнение различных подходов к векторизации и их влияния на качество моделей.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 3. Классические методы машинного обучения в NLP: классификация и кластеризация**\n",
        "\n",
        "**Задачи классификации текста**:  \n",
        "Бинарная, многоклассовая и многометковая (multi-label) классификация.\n",
        "\n",
        "**Применение классических алгоритмов (scikit-learn)**:  \n",
        "- Наивный байесовский классификатор.  \n",
        "- Метод опорных векторов (SVM).  \n",
        "- Логистическая регрессия.  \n",
        "- Деревья решений и случайный лес.  \n",
        "- Ансамблевые методы: бэггинг и бустинг.\n",
        "\n",
        "**Оценка качества классификации**:  \n",
        "- Метрики: точность, полнота, F1-мера, ROC-AUC, PR-кривая, калибровка.  \n",
        "- Матрица ошибок.  \n",
        "- Гиперпараметрическая оптимизация и кросс-валидация.\n",
        "\n",
        "**Введение в кластеризацию текстов**:  \n",
        "Задачи и применение: тематическое моделирование, обнаружение дубликатов, группировка документов.\n",
        "\n",
        "**Методы кластеризации**:  \n",
        "- K-Means.  \n",
        "- Иерархическая кластеризация (включая агломеративную).  \n",
        "- DBSCAN.  \n",
        "- Гауссовы смеси (Gaussian Mixture Models, GMM).  \n",
        "- Спектральная кластеризация (Spectral Clustering).\n",
        "\n",
        "**Определение оптимального числа кластеров**:  \n",
        "Метод локтя, коэффициент силуэта.\n",
        "\n",
        "**Оценка качества кластеризации**:  \n",
        "Силуэт, Adjusted Rand Index (ARI), Normalized Mutual Information (NMI).\n",
        "\n",
        "**Визуализация кластеров**:  \n",
        "Методы снижения размерности: PCA, t-SNE, UMAP.\n",
        "\n",
        "**Практикум**:  \n",
        "Реализация классификаторов и кластеризации на реальных данных с акцентом на выбор признаков, оценку качества и визуализацию кластеров.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 4. Нейронные сети для NLP: основы, CNN и RNN**\n",
        "\n",
        "**Основы нейронных сетей**:  \n",
        "Многослойные перцептроны (MLP): архитектура, функции активации, оптимизаторы, функции потерь.\n",
        "\n",
        "**Свёрточные нейронные сети (CNN) в NLP**:  \n",
        "- Архитектура CNN для текстов: свёрточные фильтры, пулинг, паддинг.  \n",
        "- Применение CNN в задачах классификации и анализа тональности.\n",
        "\n",
        "**Рекуррентные нейронные сети (RNN)**:  \n",
        "- Особенности обработки последовательных данных.  \n",
        "- Проблема затухающих и взрывающихся градиентов.\n",
        "\n",
        "**Архитектуры RNN**:  \n",
        "- LSTM (Long Short-Term Memory).  \n",
        "- GRU (Gated Recurrent Unit).  \n",
        "- Двунаправленные RNN (Bi-RNN, Bi-LSTM, Bi-GRU).\n",
        "\n",
        "**Применение RNN в задачах NLP**:  \n",
        "- Классификация и анализ тональности текста.  \n",
        "- Распознавание именованных сущностей (NER).  \n",
        "- Основы машинного перевода: модели «последовательность-к-последовательности» (Seq2Seq) на базе RNN.\n",
        "\n",
        "**Практикум**:  \n",
        "Построение и обучение простых CNN и RNN для классификации текстов и решения последовательных задач с использованием PyTorch или TensorFlow.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 5. Трансформеры и механизм внимания**\n",
        "\n",
        "**Ограничения RNN и появление механизма внимания**:  \n",
        "Интуиция и математическая основа внимания.\n",
        "\n",
        "**Архитектура трансформера**:  \n",
        "- Self-Attention и Multi-Head Attention.  \n",
        "- Позиционное кодирование.  \n",
        "- Encoder-Decoder архитектура.\n",
        "\n",
        "**Предобученные модели на основе трансформеров**:  \n",
        "- BERT, GPT, T5: особенности архитектуры и схем предобучения.\n",
        "\n",
        "**Работа с предобученными моделями**:  \n",
        "- Библиотека Hugging Face Transformers: загрузка моделей, использование пайплайнов, API для Dataset и Trainer.\n",
        "\n",
        "**Применение трансформеров**:  \n",
        "Классификация, суммаризация, генерация текста, вопросно-ответные системы.\n",
        "\n",
        "**Практикум**:  \n",
        "Тонкая настройка (fine-tuning) предобученных трансформеров на специализированных задачах.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 6. Большие языковые модели (LLM): основы и адаптация**\n",
        "\n",
        "**Введение в LLM**:  \n",
        "Архитектуры, возможности и ограничения, масштабирование, emergent abilities (возникающие способности).\n",
        "\n",
        "**Стратегии адаптации LLM**:  \n",
        "- **Prompt Engineering**: эффективное конструирование промптов (zero-shot, few-shot, chain-of-thought, tree-of-thought).  \n",
        "- **In-Context Learning**: zero-shot и few-shot обучение.\n",
        "\n",
        "**Методы дообучения (fine-tuning)**:  \n",
        "- Полное дообучение: ограничения по вычислительным ресурсам.  \n",
        "- Parameter-Efficient Fine-Tuning (PEFT):  \n",
        "  - LoRA (Low-Rank Adaptation).  \n",
        "  - QLoRA.  \n",
        "  - Prefix-tuning.  \n",
        "  - Prompt-tuning.  \n",
        "  - Adapter-tuning.  \n",
        "- Сравнение PEFT-методов: выбор в зависимости от задачи и доступных ресурсов.\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)**:  \n",
        "Концепция и реализация: ретривер, генератор, векторные базы данных.\n",
        "\n",
        "**Практикум**:  \n",
        "Эксперименты с промпт-инжинирингом, PEFT-методами и разработка простого RAG-приложения.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 7. Большие языковые модели (LLM): применение и оценка**\n",
        "\n",
        "**Применение LLM в задачах NLP**:  \n",
        "- Вопросно-ответные системы (QA).  \n",
        "- Суммаризация текста.  \n",
        "- Машинный перевод.  \n",
        "- Генерация кода.\n",
        "\n",
        "**Фреймворки для разработки LLM-приложений**:  \n",
        "LangChain, LlamaIndex: агенты, цепочки, инструменты.\n",
        "\n",
        "**Реальные кейсы применения LLM**:  \n",
        "Чат-боты, виртуальные ассистенты, системы рекомендаций, автоматизация бизнес-процессов.\n",
        "\n",
        "**Оценка качества LLM**:  \n",
        "- Perplexity — метрика языковой модели.  \n",
        "- BLEU, ROUGE, BERTScore — метрики для оценки генерации.  \n",
        "- Человеческая оценка (human evaluation): связность, релевантность, оригинальность.\n",
        "\n",
        "**Практикум**:  \n",
        "Разработка приложения на основе LLM и комплексная оценка его производительности.\n",
        "\n",
        "\n",
        "\n",
        "### **Тема 8. Продвинутые аспекты и развертывание NLP-моделей**\n",
        "\n",
        "**Развертывание NLP-моделей**:  \n",
        "Инференс, оптимизация (ONNX, vLLM, TensorRT), управление затратами и задержками.\n",
        "\n",
        "**Интерпретируемость моделей**:  \n",
        "Методы объяснения предсказаний: LIME, SHAP.\n",
        "\n",
        "**Этические и технические вызовы**:  \n",
        "- Предвзятость и дискриминация в LLM.  \n",
        "- Галлюцинации.  \n",
        "- Безопасность: prompt injection, защита данных.\n",
        "\n",
        "**Продвинутые темы**:  \n",
        "- Основы мультимодальных моделей (текст + изображение, текст + аудио/видео).  \n",
        "- Агенты на основе LLM и автономные системы.\n",
        "\n",
        "\n",
        "### **Тема 9. Финальный проект и обзор реальных кейсов**\n",
        "\n",
        "**Обсуждение идей для финальных проектов**:  \n",
        "Выбор тем, формирование групп, планирование.\n",
        "\n",
        "**Реализация проектов**:  \n",
        "Разработка индивидуальных или групповых проектов, объединяющих знания по всему курсу (например: чат-бот с RAG, система анализа тональности, генератор контента).\n",
        "\n",
        "**Презентации проектов**:  \n",
        "Демонстрация решений, обсуждение результатов и обратная связь.\n",
        "\n",
        "**Обзор реальных кейсов в индустрии**:  \n",
        "Примеры использования NLP и LLM в технологических компаниях, медицине, финансах, образовании.\n",
        "\n",
        "**Перспективы развития NLP и LLM**:  \n",
        "Тренды, вызовы и будущее обработки естественного языка.\n",
        "\n",
        "\n",
        "\n",
        "✅ **Примечание**:  \n",
        "Курс сочетает теоретические лекции, практические занятия и проектную работу. По завершении студенты будут способны разрабатывать, адаптировать и внедрять современные NLP-решения, включая использование больших языковых моделей и современных инструментов.\n"
      ],
      "metadata": {
        "id": "hvZ2XauQmc7j"
      }
    }
  ]
}