{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOpkrDo9AG4ZSrpLgJKyqmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_(%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BE%D0%B4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SoIW2jUyXhT"
      },
      "outputs": [],
      "source": [
        "text = \"Привет, мир! Это простой пример токенизации.\"\n",
        "tokens = text.split()\n",
        "print(f\"Исходный текст: '{text}'\")\n",
        "print(f\"Токены (Whitespace Tokenization): {tokens}\")\n",
        "\n",
        "text_with_punctuation = \"Он сказал: 'Привет!' Как дела?\"\n",
        "tokens_punct = text_with_punctuation.split()\n",
        "print(f\"\\nИсходный текст с пунктуацией: '{text_with_punctuation}'\")\n",
        "print(f\"Токены (Whitespace Tokenization): {tokens_punct}\")\n",
        "\n",
        "text_with_contraction = \"I don't know. It's complicated.\"\n",
        "tokens_contraction = text_with_contraction.split()\n",
        "print(f\"\\nИсходный текст с сокращением: '{text_with_contraction}'\")\n",
        "print(f\"Токены (Whitespace Tokenization): {tokens_contraction}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Регулярное выражение для извлечения слов и чисел, игнорируя пунктуацию\n",
        "# \\b - граница слова\n",
        "# \\w+ - один или более буквенно-цифровых символов (буквы, цифры, подчеркивание)\n",
        "# \\d+ - один или более цифр\n",
        "# [^\\w\\s] - любой символ, который не является буквенно-цифровым и не является пробелом (т.е. пунктуация)\n",
        "# | - ИЛИ\n",
        "text = \"Привет, мир! Это простой пример токенизации. 123.45\"\n",
        "# Вариант 1: Извлекаем слова и числа, пунктуацию отдельно\n",
        "# pattern = r'\\b\\w+\\b|\\d+\\.\\d+|\\S' # Более сложный шаблон для чисел и пунктуации\n",
        "pattern = r\"\\b\\w+\\b|\\d+\\.\\d+|[^\\w\\s]\" # Извлекает слова, числа с десятичной точкой и отдельные знаки пунктуации\n",
        "tokens = re.findall(pattern, text)\n",
        "print(f\"Исходный текст: '{text}'\")\n",
        "print(f\"Токены (Regex Tokenization - Вариант 1): {tokens}\")\n",
        "\n",
        "# Более продвинутый Regex для слов, чисел и пунктуации,\n",
        "# который также пытается сохранить сокращения.\n",
        "# \\w+ - слова, \\d+ - числа, \\s+ - пробелы, [.,!?;:] - пунктуация\n",
        "# (?:[a-zA-Z]+(?:'[a-zA-Z]+)?|\\d+(?:\\.\\d+)?|[.,!?;:])\n",
        "# Это регулярное выражение пытается захватить:\n",
        "# 1. Слова, возможно, с апострофом (например, don't, It's)\n",
        "# 2. Числа, возможно, с десятичной точкой (например, 123, 123.45)\n",
        "# 3. Отдельные знаки пунктуации\n",
        "pattern_advanced = r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|\\d+(?:\\.\\d+)?|[.,!?;:]\"\n",
        "text_combined = \"Привет, мир! I don't know. Это 123.45.\"\n",
        "tokens_advanced = re.findall(pattern_advanced, text_combined, re.IGNORECASE | re.UNICODE)\n",
        "# re.IGNORECASE для игнорирования регистра, re.UNICODE для работы с юникодом (русские буквы)\n",
        "print(f\"\\nИсходный текст (комбинированный): '{text_combined}'\")\n",
        "print(f\"Токены (Regex Tokenization - Продвинутый): {tokens_advanced}\")\n",
        "\n",
        "# Для русского текста лучше использовать более общий шаблон для слов\n",
        "# или специализированные библиотеки.\n",
        "# \\b - граница слова, \\w+ - буквенно-цифровые символы (включая русские буквы при re.UNICODE)\n",
        "pattern_russian = r'\\b\\w+\\b|[.,!?;:]'\n",
        "text_russian = \"Привет, мир! Как дела? Это русский текст.\"\n",
        "tokens_russian = re.findall(pattern_russian, text_russian, re.UNICODE)\n",
        "print(f\"\\nИсходный текст (русский): '{text_russian}'\")\n",
        "print(f\"Токены (Regex Tokenization - Русский): {tokens_russian}\")\n"
      ],
      "metadata": {
        "id": "mJfwGXDiy45q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка NLTK (если еще не установлено)\n",
        "# pip install nltk\n",
        "\n",
        "# Загрузка необходимых данных NLTK\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "hiZ9SWNDzeCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text_en = \"Hello, Mr. Smith! How are you doing today? I don't know.\"\n",
        "tokens_en = word_tokenize(text_en)\n",
        "print(f\"Исходный текст (английский): '{text_en}'\")\n",
        "print(f\"Токены (nltk.tokenize.word_tokenize): {tokens_en}\")\n",
        "\n",
        "text_ru = \"Привет, мир! Как у вас дела сегодня? Я не знаю.\"\n",
        "tokens_ru = word_tokenize(text_ru)\n",
        "print(f\"\\nИсходный текст (русский): '{text_ru}'\")\n",
        "print(f\"Токены (nltk.tokenize.word_tokenize): {tokens_ru}\")\n",
        "\n",
        "text_complex = \"Dr. Smith said, 'It's 12.30 p.m. now.'\"\n",
        "tokens_complex = word_tokenize(text_complex)\n",
        "print(f\"\\nИсходный текст (сложный): '{text_complex}'\")\n",
        "print(f\"Токены (nltk.tokenize.word_tokenize): {tokens_complex}\")\n"
      ],
      "metadata": {
        "id": "Xc-Jp-oy0J2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text_multi_sentence_en = \"Hello world! How are you? This is a test. Mr. Smith arrived.\"\n",
        "sentences_en = sent_tokenize(text_multi_sentence_en)\n",
        "print(f\"Исходный текст (английский): '{text_multi_sentence_en}'\")\n",
        "print(f\"Предложения (nltk.tokenize.sent_tokenize): {sentences_en}\")\n",
        "\n",
        "text_multi_sentence_ru = \"Привет, мир! Как дела? Это тестовый текст. Доктор Смирнов приехал.\"\n",
        "sentences_ru = sent_tokenize(text_multi_sentence_ru)\n",
        "print(f\"\\nИсходный текст (русский): '{text_multi_sentence_ru}'\")\n",
        "print(f\"Предложения (nltk.tokenize.sent_tokenize): {sentences_ru}\")\n",
        "\n",
        "text_with_abbreviation = \"The U.S. government implemented new policies. Dr. Jones agreed.\"\n",
        "sentences_abbrev = sent_tokenize(text_with_abbreviation)\n",
        "print(f\"\\nИсходный текст с аббревиатурой: '{text_with_abbreviation}'\")\n",
        "print(f\"Предложения (nltk.tokenize.sent_tokenize): {sentences_abbrev}\")"
      ],
      "metadata": {
        "id": "_rScyqpY2tcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "ZRyQLS_64buV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка английской модели (если еще не загружена)\n",
        "# python -m spacy download en_core_web_sm\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Пример 4.1: Токенизация с использованием spaCy (английский)\n",
        "text_en = \"Hello, Mr. Smith! How are you doing today? I don't know. It's 12.30 p.m. now.\"\n",
        "doc_en = nlp_en(text_en)\n",
        "tokens_en = [token.text for token in doc_en]\n",
        "sentences_en = [sent.text for sent in doc_en.sents] # spaCy также умеет токенизировать предложения\n",
        "\n",
        "print(f\"Исходный текст (английский): '{text_en}'\")\n",
        "print(f\"Токены (spaCy): {tokens_en}\")\n",
        "print(f\"Предложения (spaCy): {sentences_en}\")\n",
        "\n",
        "# Загрузка русской модели (если еще не загружена)\n",
        "# python -m spacy download ru_core_news_sm\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Пример 4.2: Токенизация с использованием spaCy (русский)\n",
        "text_ru = \"Привет, мир! Как у вас дела сегодня? Я не знаю. Доктор Смирнов приехал в 12:30.\"\n",
        "doc_ru = nlp_ru(text_ru)\n",
        "tokens_ru = [token.text for token in doc_ru]\n",
        "sentences_ru = [sent.text for sent in doc_ru.sents]\n",
        "\n",
        "print(f\"\\nИсходный текст (русский): '{text_ru}'\")\n",
        "print(f\"Токены (spaCy): {tokens_ru}\")\n",
        "print(f\"Предложения (spaCy): {sentences_ru}\")"
      ],
      "metadata": {
        "id": "oDi_N_9p3GVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTQhvSKW8MCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xK5cMCH8ME4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "<p>Привет, мир! Это <strong>пример</strong> текста с <a href=\"http://example.com\">ссылкой</a>.\n",
        "Мой email: user@domain.com. Телефон: +7 (999) 123-45-67.\n",
        "Дата: 23.07.2025. Цена: $123.45.\n",
        "Много   лишних    пробелов.\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Оригинальный текст:\\n\", text)\n",
        "\n",
        "# 1. Удаление HTML-тегов\n",
        "# <.*?> - нежадный поиск любого текста между < и >\n",
        "cleaned_text = re.sub(r'<.*?>', '', text)\n",
        "print(\"\\n1. Без HTML-тегов:\\n\", cleaned_text)\n",
        "\n",
        "# 2. Удаление URL-адресов\n",
        "# http\\S+ - http, за которым следуют один или более не-пробельных символов\n",
        "# www\\S+ - www, за которым следуют один или более не-пробельных символов\n",
        "cleaned_text = re.sub(r'http\\S+|www\\S+', '', cleaned_text)\n",
        "print(\"\\n2. Без URL-адресов:\\n\", cleaned_text)\n",
        "\n",
        "# 3. Удаление email-адресов\n",
        "# \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b - общий паттерн для email\n",
        "cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', cleaned_text)\n",
        "print(\"\\n3. Без email-адресов:\\n\", cleaned_text)\n",
        "\n",
        "# 4. Удаление чисел и символов, кроме букв и пробелов\n",
        "# [^a-zа-я\\s] - любой символ, который НЕ является латинской буквой, кириллической буквой или пробелом\n",
        "cleaned_text = re.sub(r'[^a-zа-я\\s]', '', cleaned_text.lower()) # Приводим к нижнему регистру\n",
        "print(\"\\n4. Только буквы и пробелы (нижний регистр):\\n\", cleaned_text)\n",
        "\n",
        "# 5. Удаление лишних пробелов и обрезка по краям\n",
        "# \\s+ - один или более пробельных символов\n",
        "final_cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(\"\\n5. Финальный очищенный текст:\\n\", final_cleaned_text)\n",
        "\n",
        "# Дополнительный пример: Извлечение телефонных номеров\n",
        "phone_text = \"Мой номер: +1 (555) 123-4567, или 8-800-555-35-35. А также 123 456 7890.\"\n",
        "# Паттерн для различных форматов телефонных номеров\n",
        "phone_numbers = re.findall(r'\\+?\\d{1,3}[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{2}[-.\\s]?\\d{2}|\\d{1,3}[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', phone_text)\n",
        "print(f\"\\nНайденные телефонные номера: {phone_numbers}\")"
      ],
      "metadata": {
        "id": "nyPm96V78MM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class BPE:\n",
        "    \"\"\"\n",
        "    Реализация алгоритма Byte-Pair Encoding (BPE) для токенизации текста.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Инициализирует BPE токенизатор.\n",
        "        :param vocab_size: Желаемый максимальный размер словаря субтокенов.\n",
        "                           Если указан, num_merges будет игнорироваться.\n",
        "        :param num_merges: Количество итераций объединения пар.\n",
        "                           Если vocab_size не указан, будет использоваться это значение.\n",
        "        \"\"\"\n",
        "        if vocab_size is None and num_merges is None:\n",
        "            raise ValueError(\"Необходимо указать либо vocab_size, либо num_merges.\")\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = {}  # Словарь для хранения выполненных объединений {pair: new_token}\n",
        "        self.vocabulary = set() # Текущий словарь субтокенов\n",
        "        self.token_to_id = {} # Словарь для преобразования токенов в ID\n",
        "        self.id_to_token = [] # Список для преобразования ID в токены\n",
        "\n",
        "    def _get_initial_tokens(self, text):\n",
        "        \"\"\"\n",
        "        Разбивает текст на начальные символьные токены и добавляет символ конца слова.\n",
        "        :param text: Входная строка.\n",
        "        :return: Список списков символьных токенов для каждого слова.\n",
        "        \"\"\"\n",
        "        # Заменяем подчеркивания на пробелы для разделения слов, затем разбиваем по пробелам\n",
        "        words = text.replace('_', ' ').split()\n",
        "        initial_tokens = []\n",
        "        for word in words:\n",
        "            # Добавляем символ конца слова к каждому слову\n",
        "            initial_tokens.append(list(word) + ['</w>'])\n",
        "        return initial_tokens\n",
        "\n",
        "    def _get_pair_counts(self, tokenized_corpus):\n",
        "        \"\"\"\n",
        "        Подсчитывает частоту встречаемости всех смежных пар токенов в корпусе.\n",
        "        :param tokenized_corpus: Список списков токенов (представляющих слова).\n",
        "        :return: Словарь с частотами пар {('token1', 'token2'): count}.\n",
        "        \"\"\"\n",
        "        pair_counts = collections.defaultdict(int)\n",
        "        for word_tokens in tokenized_corpus:\n",
        "            # Итерируем по парам токенов в каждом слове\n",
        "            for i in range(len(word_tokens) - 1):\n",
        "                pair_counts[(word_tokens[i], word_tokens[i+1])] += 1\n",
        "        return pair_counts\n",
        "\n",
        "    def _merge_pair(self, tokenized_corpus, pair, new_token):\n",
        "        \"\"\"\n",
        "        Объединяет все вхождения заданной пары в новый токен в корпусе.\n",
        "        :param tokenized_corpus: Текущий корпус токенов.\n",
        "        :param pair: Пара токенов для объединения (tuple).\n",
        "        :param new_token: Новый токен, который заменит пару.\n",
        "        :return: Обновленный корпус токенов.\n",
        "        \"\"\"\n",
        "        updated_corpus = []\n",
        "        for word_tokens in tokenized_corpus:\n",
        "            new_word_tokens = []\n",
        "            i = 0\n",
        "            while i < len(word_tokens):\n",
        "                # Проверяем, является ли текущая позиция началом искомой пары\n",
        "                if i + 1 < len(word_tokens) and (word_tokens[i], word_tokens[i+1]) == pair:\n",
        "                    new_word_tokens.append(new_token)\n",
        "                    i += 2 # Пропускаем оба токена, которые были объединены\n",
        "                else:\n",
        "                    new_word_tokens.append(word_tokens[i])\n",
        "                    i += 1\n",
        "            updated_corpus.append(new_word_tokens)\n",
        "        return updated_corpus\n",
        "\n",
        "    def fit(self, corpus_text):\n",
        "        \"\"\"\n",
        "        Обучает BPE модель на заданном текстовом корпусе.\n",
        "        :param corpus_text: Строка, представляющая обучающий корпус.\n",
        "        \"\"\"\n",
        "        # Шаг 1: Инициализация\n",
        "        # Начальное разбиение на символы и добавление символа конца слова\n",
        "        current_corpus_tokens = self._get_initial_tokens(corpus_text)\n",
        "\n",
        "        # Формирование начального словаря из всех уникальных символов\n",
        "        for word_tokens in current_corpus_tokens:\n",
        "            for token in word_tokens:\n",
        "                self.vocabulary.add(token)\n",
        "\n",
        "        # Определяем количество итераций объединения\n",
        "        if self.vocab_size is not None:\n",
        "            # Если задан размер словаря, вычисляем необходимое количество объединений\n",
        "            # (целевой размер словаря - текущий размер словаря)\n",
        "            num_iterations = self.vocab_size - len(self.vocabulary)\n",
        "        elif self.num_merges is not None:\n",
        "            num_iterations = self.num_merges\n",
        "        else:\n",
        "            # Это условие не должно быть достигнуто благодаря проверке в __init__\n",
        "            num_iterations = 0\n",
        "\n",
        "        # Шаг 2: Итеративное объединение\n",
        "        for i in range(num_iterations):\n",
        "            pair_counts = self._get_pair_counts(current_corpus_tokens)\n",
        "\n",
        "            if not pair_counts: # Если нет пар для объединения, останавливаемся\n",
        "                break\n",
        "\n",
        "            # Находим наиболее частую пару\n",
        "            # Сортируем по частоте (убывание), затем по лексикографическому порядку (возрастание)\n",
        "            best_pair = max(pair_counts, key=lambda p: (pair_counts[p], -len(p[0]) - len(p[1]), p))\n",
        "\n",
        "            # Если частота лучшей пары меньше 1, или нет пар для объединения, останавливаемся\n",
        "            if pair_counts[best_pair] < 1:\n",
        "                break\n",
        "\n",
        "            # Формируем новый токен из объединенной пары\n",
        "            new_token = \"\".join(best_pair)\n",
        "\n",
        "            # Сохраняем объединение\n",
        "            self.merges[best_pair] = new_token\n",
        "\n",
        "            # Обновляем корпус, заменяя все вхождения пары новым токеном\n",
        "            current_corpus_tokens = self._merge_pair(current_corpus_tokens, best_pair, new_token)\n",
        "\n",
        "            # Добавляем новый токен в словарь\n",
        "            self.vocabulary.add(new_token)\n",
        "\n",
        "            # Вывод результатов каждого шага\n",
        "            print(f\"Итерация {i+1}: Объединение '{best_pair[0]}{best_pair[1]}' в '{new_token}'. Частота: {pair_counts[best_pair]}\")\n",
        "            print(f\"Обновленный корпус: {current_corpus_tokens}\")\n",
        "            print(f\"Текущий словарь: {sorted(list(self.vocabulary))}\\n\")\n",
        "\n",
        "        # После обучения создаем отображения токенов в ID и обратно\n",
        "        self.id_to_token = sorted(list(self.vocabulary))\n",
        "        self.token_to_id = {token: i for i, token in enumerate(self.id_to_token)}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Токенизирует новую строку, используя обученные объединения.\n",
        "        :param text: Строка для токенизации.\n",
        "        :return: Список ID токенов.\n",
        "        \"\"\"\n",
        "        # Начальное разбиение текста на символы с добавлением </w>\n",
        "        words_tokens = self._get_initial_tokens(text)\n",
        "        encoded_tokens_ids = []\n",
        "\n",
        "        for word_tokens in words_tokens:\n",
        "            # Для каждого слова применяем объединения в порядке их обучения\n",
        "            # Создаем копию списка токенов слова для модификации\n",
        "            current_word_tokens = list(word_tokens)\n",
        "\n",
        "            # Применяем объединения итеративно, пока не сможем найти более длинный токен\n",
        "            # или пока не закончатся объединения\n",
        "            while True:\n",
        "                merged_once = False\n",
        "                new_word_tokens = []\n",
        "                i = 0\n",
        "                while i < len(current_word_tokens):\n",
        "                    found_merge = False\n",
        "                    # Ищем самое длинное возможное объединение, которое начинается с текущего токена\n",
        "                    # Проходим по всем известным объединениям в порядке их создания (от коротких к длинным)\n",
        "                    # или просто ищем самое длинное совпадение\n",
        "                    best_match_len = 0\n",
        "                    best_match_token = None\n",
        "\n",
        "                    # Для простоты, будем применять объединения из self.merges\n",
        "                    # в порядке их создания (от коротких к длинным)\n",
        "                    # Это не совсем оптимально, но для демонстрации подходит\n",
        "                    # В реальных реализациях используют более сложные структуры данных\n",
        "                    # для эффективного поиска наиболее длинного совпадения\n",
        "                    for (p1, p2), merged_token in self.merges.items():\n",
        "                        if i + 1 < len(current_word_tokens) and \\\n",
        "                           current_word_tokens[i] == p1 and \\\n",
        "                           current_word_tokens[i+1] == p2:\n",
        "                            # Проверяем, является ли это объединение частью более длинного\n",
        "                            # уже существующего в словаре токена\n",
        "                            if len(merged_token) > best_match_len:\n",
        "                                best_match_len = len(merged_token)\n",
        "                                best_match_token = merged_token\n",
        "                                found_merge = True\n",
        "\n",
        "                    if found_merge:\n",
        "                        new_word_tokens.append(best_match_token)\n",
        "                        i += 2\n",
        "                        merged_once = True\n",
        "                    else:\n",
        "                        new_word_tokens.append(current_word_tokens[i])\n",
        "                        i += 1\n",
        "\n",
        "                if not merged_once:\n",
        "                    break # Больше нет объединений для этого слова\n",
        "\n",
        "                current_word_tokens = new_word_tokens\n",
        "\n",
        "            # Преобразуем токены слова в их ID\n",
        "            for token in current_word_tokens:\n",
        "                if token in self.token_to_id:\n",
        "                    encoded_tokens_ids.append(self.token_to_id[token])\n",
        "                else:\n",
        "                    # Если токен не найден (например, из-за OOV или неполного обучения),\n",
        "                    # разбиваем его на символы и добавляем их ID\n",
        "                    for char in token:\n",
        "                        if char in self.token_to_id:\n",
        "                            encoded_tokens_ids.append(self.token_to_id[char])\n",
        "                        else:\n",
        "                            # Этого не должно произойти, если начальный словарь содержит все символы\n",
        "                            print(f\"Предупреждение: Символ '{char}' не найден в словаре.\")\n",
        "                            # Можно добавить токен для неизвестных символов, например, <unk>\n",
        "        return encoded_tokens_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Декодирует последовательность ID токенов обратно в строку.\n",
        "        :param token_ids: Список ID токенов.\n",
        "        :return: Декодированная строка.\n",
        "        \"\"\"\n",
        "        decoded_tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id < len(self.id_to_token):\n",
        "                decoded_tokens.append(self.id_to_token[token_id])\n",
        "            else:\n",
        "                print(f\"Предупреждение: ID токена {token_id} вне диапазона словаря.\")\n",
        "                decoded_tokens.append('') # Или можно использовать <unk>\n",
        "\n",
        "        # Объединяем токены, убираем символ конца слова и заменяем пробелы\n",
        "        decoded_text = \"\".join(decoded_tokens).replace('</w>', ' ').strip()\n",
        "        return decoded_text.replace(' ', '_') # Возвращаем исходный формат с подчеркиваниями\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus = \"Ученик_учится_в_школе,_а_учитель_учит_ученика\"\n",
        "\n",
        "    # Создаем экземпляр BPE, указывая количество объединений\n",
        "    # Можно также указать vocab_size=X для желаемого размера словаря\n",
        "    bpe_model = BPE(num_merges=10) # Выполним 10 итераций объединения\n",
        "\n",
        "    print(\"--- Обучение BPE модели ---\")\n",
        "    bpe_model.fit(corpus)\n",
        "    print(\"\\n--- Обучение завершено ---\")\n",
        "    print(f\"Финальный словарь BPE (отсортированный): {sorted(list(bpe_model.vocabulary))}\")\n",
        "    print(f\"Выполненные объединения: {bpe_model.merges}\")\n",
        "\n",
        "    # Тестирование токенизации\n",
        "    print(\"\\n--- Тестирование токенизации ---\")\n",
        "    text_to_encode = \"Ученик_учится_в_школе,_а_учитель_учит_ученика\"\n",
        "    encoded_ids = bpe_model.encode(text_to_encode)\n",
        "    print(f\"Исходный текст: '{text_to_encode}'\")\n",
        "    print(f\"Закодированные ID: {encoded_ids}\")\n",
        "\n",
        "    # Декодирование\n",
        "    decoded_text = bpe_model.decode(encoded_ids)\n",
        "    print(f\"Декодированный текст: '{decoded_text}'\")\n",
        "\n",
        "    # Проверка на новое слово (OOV)\n",
        "    print(\"\\n--- Тестирование OOV слова ---\")\n",
        "    oov_text = \"Учительница_учит\"\n",
        "    encoded_oov_ids = bpe_model.encode(oov_text)\n",
        "    print(f\"OOV текст: '{oov_text}'\")\n",
        "    print(f\"Закодированные ID OOV: {encoded_oov_ids}\")\n",
        "    decoded_oov_text = bpe_model.decode(encoded_oov_ids)\n",
        "    print(f\"Декодированный OOV текст: '{decoded_oov_text}'\")\n",
        "\n",
        "    # Пример токенов по ID\n",
        "    print(\"\\n--- Токены по ID ---\")\n",
        "    for i, token in enumerate(bpe_model.id_to_token):\n",
        "        print(f\"ID {i}: '{token}'\")"
      ],
      "metadata": {
        "id": "iRZyHoYAHpal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.merges = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Предварительная обработка текста: приведение к нижнему регистру и разбиение на слова.\n",
        "        \"\"\"\n",
        "        # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        # Разбиваем на слова по пробелам\n",
        "        words = processed_text.split(' ')\n",
        "        # Добавляем пробелы как отдельные токены между словами\n",
        "        initial_tokens = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word: # Убедимся, что слово не пустое\n",
        "                initial_tokens.extend(list(word))\n",
        "            if i < len(words) - 1:\n",
        "                initial_tokens.append(' ') # Добавляем пробел между словами\n",
        "        return initial_tokens, words # Возвращаем начальные токены и список слов для удобства\n",
        "\n",
        "    def calculate_frequencies(self, tokens):\n",
        "        \"\"\"\n",
        "        Подсчет частот отдельных токенов (униграмм) и биграмм в текущем корпусе.\n",
        "        \"\"\"\n",
        "        unigram_freq = collections.defaultdict(int)\n",
        "        bigram_freq = collections.defaultdict(int)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            unigram_freq[tokens[i]] += 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram_freq[(tokens[i], tokens[i+1])] += 1\n",
        "        return unigram_freq, bigram_freq\n",
        "\n",
        "    def calculate_score(self, unigram_freq, bigram_freq):\n",
        "        \"\"\"\n",
        "        Вычисление оценки слияния для всех возможных биграмм.\n",
        "        Score(A, B) = frequency(AB) / (frequency(A) * frequency(B))\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for (token_a, token_b), freq_ab in bigram_freq.items():\n",
        "            freq_a = unigram_freq[token_a]\n",
        "            freq_b = unigram_freq[token_b]\n",
        "            if freq_a > 0 and freq_b > 0: # Избегаем деления на ноль\n",
        "                score = freq_ab / (freq_a * freq_b)\n",
        "                scores[(token_a, token_b)] = score\n",
        "        return scores\n",
        "\n",
        "    def merge_tokens(self, tokens, best_bigram):\n",
        "        \"\"\"\n",
        "        Слияние лучшей биграммы в корпусе.\n",
        "        \"\"\"\n",
        "        merged_token = \"\".join(best_bigram)\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == best_bigram:\n",
        "                new_tokens.append(merged_token)\n",
        "                i += 2 # Пропускаем оба слитых токена\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        return new_tokens\n",
        "\n",
        "    def train(self, text, target_vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Обучение WordPiece токенизатора.\n",
        "        Итеративно сливает токены до достижения целевого размера словаря\n",
        "        или заданного количества слияний.\n",
        "        \"\"\"\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        current_tokens, _ = self.preprocess_text(text)\n",
        "\n",
        "        # Начальный словарь состоит из всех уникальных символов\n",
        "        self.vocab = set(current_tokens)\n",
        "        print(f\"\\nШаг 1: Начальный словарь (V0) содержит {len(self.vocab)} токенов.\")\n",
        "        print(f\"Начальный корпус: {current_tokens}\")\n",
        "\n",
        "        merges_count = 0\n",
        "        while True:\n",
        "            unigram_freq, bigram_freq = self.calculate_frequencies(current_tokens)\n",
        "            scores = self.calculate_score(unigram_freq, bigram_freq)\n",
        "\n",
        "            if not scores:\n",
        "                print(\"\\nНет больше биграмм для слияния. Остановка.\")\n",
        "                break\n",
        "\n",
        "            # Находим биграмму с наивысшей оценкой\n",
        "            best_bigram = max(scores, key=scores.get)\n",
        "            best_score = scores[best_bigram]\n",
        "\n",
        "            merged_token = \"\".join(best_bigram)\n",
        "\n",
        "            # Критерий остановки: если новый токен уже в словаре\n",
        "            if merged_token in self.vocab:\n",
        "                # Если лучший токен уже существует, удалим его из scores, чтобы найти следующий лучший\n",
        "                del scores[best_bigram]\n",
        "                continue # Продолжаем поиск\n",
        "\n",
        "            # Выводим информацию о текущем слиянии\n",
        "            print(f\"\\nИтерация {merges_count + 1}:\")\n",
        "            print(f\"  Лучшая биграмма для слияния: '{best_bigram[0]}' + '{best_bigram[1]}' -> '{merged_token}' (Score: {best_score:.4f})\")\n",
        "\n",
        "            # Выполняем слияние в корпусе\n",
        "            current_tokens = self.merge_tokens(current_tokens, best_bigram)\n",
        "\n",
        "            # Добавляем новый токен в словарь и сохраняем слияние\n",
        "            self.vocab.add(merged_token)\n",
        "            self.merges.append(best_bigram)\n",
        "            merges_count += 1\n",
        "\n",
        "            print(f\"  Корпус после слияния: {current_tokens}\")\n",
        "            print(f\"  Текущий размер словаря: {len(self.vocab)}\")\n",
        "\n",
        "            # Критерии остановки\n",
        "            if target_vocab_size is not None and len(self.vocab) >= target_vocab_size:\n",
        "                print(f\"\\nДостигнут целевой размер словаря ({target_vocab_size}). Остановка.\")\n",
        "                break\n",
        "            if num_merges is not None and merges_count >= num_merges:\n",
        "                print(f\"\\nДостигнуто заданное количество слияний ({num_merges}). Остановка.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nОбучение завершено. Итоговый словарь содержит {len(self.vocab)} токенов.\")\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Токенизация нового текста с использованием обученного словаря WordPiece.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'vocab') or len(self.vocab) == 0:\n",
        "            raise ValueError(\"Токенизатор не обучен. Сначала вызовите метод train().\")\n",
        "\n",
        "        print(f\"\\n--- Токенизация нового текста ---\")\n",
        "        print(f\"Текст для токенизации: '{text}'\")\n",
        "\n",
        "        # Предварительная обработка текста для токенизации\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        words = processed_text.split(' ')\n",
        "\n",
        "        final_tokens = []\n",
        "        for word in words:\n",
        "            if not word: # Пропускаем пустые строки, если они возникли из-за множественных пробелов\n",
        "                continue\n",
        "\n",
        "            word_tokens = []\n",
        "            remaining_word = word\n",
        "\n",
        "            while remaining_word:\n",
        "                found_match = False\n",
        "                # Ищем самый длинный подтокен из словаря, который является префиксом оставшейся части слова\n",
        "                for i in range(len(remaining_word), 0, -1):\n",
        "                    subword = remaining_word[:i]\n",
        "\n",
        "                    if subword in self.vocab:\n",
        "                        word_tokens.append(subword)\n",
        "                        remaining_word = remaining_word[i:]\n",
        "                        found_match = True\n",
        "                        break\n",
        "\n",
        "                if not found_match:\n",
        "                    # Если не удалось найти соответствующий токен, разбиваем на символы\n",
        "                    # или используем токен [UNK]\n",
        "                    if remaining_word[0] in self.vocab: # Если символ есть в словаре\n",
        "                        word_tokens.append(remaining_word[0])\n",
        "                    else: # Если символ даже не в начальном словаре\n",
        "                        word_tokens.append('[UNK]')\n",
        "                    remaining_word = remaining_word[1:]\n",
        "\n",
        "            final_tokens.extend(word_tokens)\n",
        "            final_tokens.append(' ') # Добавляем пробел между токенизированными словами\n",
        "\n",
        "        # Удаляем последний пробел, если он есть\n",
        "        if final_tokens and final_tokens[-1] == ' ':\n",
        "            final_tokens.pop()\n",
        "\n",
        "        print(f\"Токенизированный текст: {final_tokens}\")\n",
        "        return final_tokens\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Возвращает текущий словарь токенов.\"\"\"\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def get_merges(self):\n",
        "        \"\"\"Возвращает список выполненных слияний.\"\"\"\n",
        "        return self.merges\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    text_to_train = \"В училище учитель учит ученикам по новому учебнику\"\n",
        "\n",
        "    # Создаем экземпляр токенизатора\n",
        "    tokenizer = WordPieceTokenizer()\n",
        "\n",
        "    # Обучаем токенизатор\n",
        "    final_vocab = tokenizer.train(text_to_train, num_merges=50)\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece: {final_vocab}\")\n",
        "\n",
        "    # Токенизируем тот же текст\n",
        "    tokenizer.tokenize(text_to_train)\n",
        "\n",
        "    # Пример токенизации нового слова\n",
        "    new_text = \"учительница\"\n",
        "    # Добавим '##ница' в словарь для демонстрации\n",
        "    if 'ница' not in final_vocab:\n",
        "        tokenizer.vocab.add('ница')\n",
        "        tokenizer.vocab.add('##ница')\n",
        "        final_vocab = tokenizer.get_vocab()\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece (с 'ница' для демонстрации): {final_vocab}\")\n",
        "    tokenizer.tokenize(new_text)\n",
        "\n",
        "    # Демонстрация с упрощенным словарем\n",
        "    simplified_tokenizer = WordPieceTokenizer()\n",
        "    simplified_tokenizer.vocab = set(['у', 'ч', 'и', 'т', 'е', 'л', 'ь', 'н', 'ц', 'а', 'учитель', '##ница', '##тель', '##а'])\n",
        "    print(f\"\\n--- Демонстрация токенизации подслов с упрощенным словарем ---\")\n",
        "    simplified_tokenizer.tokenize(\"учительница\")"
      ],
      "metadata": {
        "id": "gdl7FguyJXGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "import math\n",
        "\n",
        "class UnigramLanguageModel:\n",
        "    \"\"\"\n",
        "    Реализация Униграмной Языковой Модели.\n",
        "    Эта модель предполагает, что вероятность каждого слова в последовательности\n",
        "    не зависит от других слов.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.word_counts = collections.defaultdict(int) # Счетчик вхождений каждого слова\n",
        "        self.total_words = 0 # Общее количество слов в корпусе\n",
        "        self.vocabulary = set() # Множество уникальных слов (словарь)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Приватный метод для токенизации текста.\n",
        "        Приводит текст к нижнему регистру и разбивает на слова, удаляя знаки препинания.\n",
        "        :param text: Входная строка текста.\n",
        "        :return: Список токенов (слов).\n",
        "        \"\"\"\n",
        "        # Приводим к нижнему регистру\n",
        "        text = text.lower()\n",
        "        # Удаляем знаки препинания (кроме тех, что могут быть частью слова, но для простоты здесь удаляем все)\n",
        "        # Использование регулярного выражения для извлечения только буквенных последовательностей\n",
        "        tokens = re.findall(r'\\b[а-яё]+\\b', text)\n",
        "        return tokens\n",
        "\n",
        "    def train(self, corpus_text):\n",
        "        \"\"\"\n",
        "        Обучает униграмную языковую модель на заданном текстовом корпусе.\n",
        "        Подсчитывает частоты слов и формирует словарь.\n",
        "        :param corpus_text: Строка, представляющая обучающий корпус.\n",
        "        \"\"\"\n",
        "        print(\"--- Начало обучения Униграмной Модели ---\")\n",
        "        tokens = self._tokenize(corpus_text)\n",
        "\n",
        "        if not tokens:\n",
        "            print(\"Предупреждение: Корпус пуст или не содержит слов после токенизации.\")\n",
        "            return\n",
        "\n",
        "        self.total_words = len(tokens)\n",
        "\n",
        "        for word in tokens:\n",
        "            self.word_counts[word] += 1\n",
        "            self.vocabulary.add(word)\n",
        "\n",
        "        print(f\"Обучение завершено. Общее количество слов (N): {self.total_words}\")\n",
        "        print(f\"Размер словаря (|V|): {len(self.vocabulary)}\")\n",
        "        print(f\"Частоты слов: {dict(self.word_counts)}\")\n",
        "        print(\"--- Обучение завершено ---\")\n",
        "\n",
        "    def get_word_probability(self, word, smoothing=None):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность слова P(w).\n",
        "        :param word: Слово, для которого нужно рассчитать вероятность.\n",
        "        :param smoothing: Метод сглаживания ('laplace' для сглаживания по Лапласу, None для MLE).\n",
        "        :return: Вероятность слова.\n",
        "        \"\"\"\n",
        "        # Если модель не обучена или корпус пуст\n",
        "        if self.total_words == 0:\n",
        "            print(\"Ошибка: Модель не обучена или корпус пуст. Невозможно рассчитать вероятность.\")\n",
        "            return 0.0\n",
        "\n",
        "        count_w = self.word_counts[word] # Count(w)\n",
        "\n",
        "        if smoothing == 'laplace':\n",
        "            # Сглаживание по Лапласу: (Count(w) + 1) / (N + |V|)\n",
        "            probability = (count_w + 1) / (self.total_words + len(self.vocabulary))\n",
        "            # print(f\"P_Laplace('{word}') = ({count_w} + 1) / ({self.total_words} + {len(self.vocabulary)}) = {probability:.4f}\")\n",
        "        else:\n",
        "            # Метод максимального правдоподобия (MLE): Count(w) / N\n",
        "            if count_w == 0:\n",
        "                # print(f\"P_MLE('{word}') = 0 (слово не найдено в корпусе)\")\n",
        "                return 0.0 # Если слово не найдено, вероятность 0 без сглаживания\n",
        "            probability = count_w / self.total_words\n",
        "            # print(f\"P_MLE('{word}') = {count_w} / {self.total_words} = {probability:.4f}\")\n",
        "\n",
        "        return probability\n",
        "\n",
        "    def get_sequence_probability(self, sequence_text, smoothing=None):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность последовательности слов P(W).\n",
        "        :param sequence_text: Строка, представляющая последовательность слов.\n",
        "        :param smoothing: Метод сглаживания ('laplace' для сглаживания по Лапласу, None для MLE).\n",
        "        :return: Вероятность последовательности.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Расчет вероятности последовательности: '{sequence_text}' ---\")\n",
        "        tokens = self._tokenize(sequence_text)\n",
        "\n",
        "        if not tokens:\n",
        "            print(\"Предупреждение: Последовательность пуста или не содержит слов после токенизации.\")\n",
        "            return 0.0\n",
        "\n",
        "        sequence_probability = 1.0\n",
        "        probabilities_list = []\n",
        "\n",
        "        for word in tokens:\n",
        "            p_word = self.get_word_probability(word, smoothing)\n",
        "            probabilities_list.append(p_word)\n",
        "            sequence_probability *= p_word\n",
        "\n",
        "            # Если хоть одно слово имеет нулевую вероятность без сглаживания,\n",
        "            # вся последовательность будет иметь нулевую вероятность.\n",
        "            if smoothing != 'laplace' and p_word == 0:\n",
        "                print(f\"Слово '{word}' имеет нулевую вероятность. Вероятность всей последовательности = 0.\")\n",
        "                return 0.0 # Оптимизация: если P(w)=0, то произведение будет 0\n",
        "\n",
        "        print(f\"Токены последовательности: {tokens}\")\n",
        "        print(f\"Вероятности отдельных слов: {probabilities_list}\")\n",
        "        print(f\"Итоговая вероятность последовательности: {sequence_probability}\")\n",
        "        print(\"--- Расчет завершен ---\")\n",
        "        return sequence_probability\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_example = \"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"\n",
        "\n",
        "    # Создаем экземпляр модели\n",
        "    unigram_model = UnigramLanguageModel()\n",
        "\n",
        "    # Обучаем модель на корпусе\n",
        "    unigram_model.train(corpus_example)\n",
        "\n",
        "    print(\"\\n--- Расчет вероятностей отдельных слов (MLE) ---\")\n",
        "    words_to_check = ['пишут', 'учитель', 'школа', 'мы']\n",
        "    for word in words_to_check:\n",
        "        prob = unigram_model.get_word_probability(word, smoothing=None)\n",
        "        print(f\"P_MLE('{word}') = {prob:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Расчет вероятностей отдельных слов (Лаплас) ---\")\n",
        "    for word in words_to_check:\n",
        "        prob = unigram_model.get_word_probability(word, smoothing='laplace')\n",
        "        print(f\"P_Laplace('{word}') = {prob:.4f}\")\n",
        "\n",
        "    # Расчет вероятности предложения (без сглаживания)\n",
        "    sentence_mle = \"Ученики пишут диктант\"\n",
        "    prob_mle = unigram_model.get_sequence_probability(sentence_mle, smoothing=None)\n",
        "    print(f\"Вероятность предложения (MLE): {prob_mle:.10f}\")\n",
        "\n",
        "    sentence_mle_oov = \"Ученики пишут школа\" # Содержит OOV слово 'школа'\n",
        "    prob_mle_oov = unigram_model.get_sequence_probability(sentence_mle_oov, smoothing=None)\n",
        "    print(f\"Вероятность предложения с OOV (MLE): {prob_mle_oov:.10f}\")\n",
        "\n",
        "    # Расчет вероятности предложения (со сглаживанием по Лапласу)\n",
        "    sentence_laplace = \"Ученики пишут диктант\"\n",
        "    prob_laplace = unigram_model.get_sequence_probability(sentence_laplace, smoothing='laplace')\n",
        "    print(f\"Вероятность предложения (Лаплас): {prob_laplace:.10f}\")\n",
        "\n",
        "    sentence_laplace_oov = \"Ученики пишут школа\" # Содержит OOV слово 'школа'\n",
        "    prob_laplace_oov = unigram_model.get_sequence_probability(sentence_laplace_oov, smoothing='laplace')\n",
        "    print(f\"Вероятность предложения с OOV (Лаплас): {prob_laplace_oov:.10f}\")\n",
        "\n",
        "    # Пример из документации: \"Ученики пишут диктант Учитель диктует Мы пишем он пишет они пишут Пишите внимательно\"\n",
        "    full_sentence = \"Ученики пишут диктант Учитель диктует Мы пишем он пишет они пишут Пишите внимательно\"\n",
        "    prob_full_mle = unigram_model.get_sequence_probability(full_sentence, smoothing=None)\n",
        "    print(f\"Вероятность полного предложения (MLE): {prob_full_mle:.15f}\")\n",
        "\n",
        "    prob_full_laplace = unigram_model.get_sequence_probability(full_sentence, smoothing='laplace')\n",
        "    print(f\"Вероятность полного предложения (Лаплас): {prob_full_laplace:.15f}\")"
      ],
      "metadata": {
        "id": "QRxue645KAt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}