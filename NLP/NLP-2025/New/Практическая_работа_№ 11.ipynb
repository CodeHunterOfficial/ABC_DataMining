{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMPphtj0vQU472kZuDYke4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Практическая работа № 11. Обучение с подкреплением для управления\n",
        "\n",
        "## 1. Цель и задачи работы\n",
        "\n",
        "**Цель работы** — формирование у обучающегося системного представления о методологии применения алгоритмов обучения с подкреплением к задачам управления реальными и симулированными динамическими системами. Работа направлена на освоение практических навыков проектирования сред, формулировки функций вознаграждения, выбора архитектур агентов, а также на проведение строгой оценки качества управления по критериям стабильности, эффективности и обобщающей способности.\n",
        "\n",
        "**Основные задачи работы**:\n",
        "\n",
        "- Использовать **две управляющие среды** различной сложности:\n",
        "  – **Низкоразмерная среда**: классическая задача управления (например, *Inverted Pendulum*, *MountainCar*, *Lunar Lander*) с непрерывным или дискретным пространством действий;\n",
        "  – **Высокоразмерная среда**: прикладная задача управления, основанная на данных из Практической работы № 1 (например, управление температурой в здании, балансировка энергосистемы, оптимизация запасов), где:\n",
        "    • состояние включает временные ряды (≥720 наблюдений) и инженерные признаки (лаги, скользящие средние, циклические кодировки);\n",
        "    • действия — дискретные или непрерывные управляющие сигналы (например, включение/выключение оборудования, регулировка мощности);\n",
        "    • динамика среды — либо симулирована на основе физических моделей, либо аппроксимирована с помощью генеративной модели (например, TimeGAN из Практической работы № 8).\n",
        "\n",
        "- Формализовать задачу управления как **марковский процесс принятия решений (MDP)**:\n",
        "  – определить пространство состояний \\( \\mathcal{S} \\), действий \\( \\mathcal{A} \\), функцию переходов \\( P(s'|s,a) \\) (явную или имитационную);\n",
        "  – разработать **функцию вознаграждения** \\( r(s,a) \\), отражающую цели управления (минимизация энергопотребления, поддержание заданного уровня, избегание нарушений ограничений).\n",
        "\n",
        "- Определить архитектуры и гиперпараметры для всех агентов и провести их обучение **только на тренировочной части данных**, без доступа к тестовой среде.\n",
        "\n",
        "- Реализовать и сравнить следующие категории алгоритмов обучения с подкреплением:\n",
        "  – **Для дискретных действий**:\n",
        "    • DQN (Deep Q-Network) с приоритетным реплейем;\n",
        "    • Double DQN, Dueling DQN;\n",
        "  – **Для непрерывных действий**:\n",
        "    • DDPG (Deep Deterministic Policy Gradient);\n",
        "    • TD3 (Twin Delayed DDPG);\n",
        "    • SAC (Soft Actor-Critic);\n",
        "  – **Современные подходы**:\n",
        "    • PPO (Proximal Policy Optimization) — для обеих сред;\n",
        "    • QR-DQN / Implicit Quantile Networks — для робастности;\n",
        "    • Model-Based RL (например, MBPO или PILCO), если доступна дифференцируемая модель среды.\n",
        "\n",
        "- Обеспечить **безопасность обучения**:\n",
        "  – введение штрафов за нарушение физических или операционных ограничений;\n",
        "  – использование constrained RL (например, Lagrangian relaxation) при наличии жёстких ограничений.\n",
        "\n",
        "- Сгенерировать последовательности управления:\n",
        "  – для каждой среды — не менее 100 эпизодов оценки;\n",
        "  – длина эпизода — сопоставима с исходными временными рядами (≥30 шагов).\n",
        "\n",
        "- Провести углублённую диагностику качества управления по трём критериям:\n",
        "  – **Эффективность**: среднее накопленное вознаграждение, энергозатраты, отклонение от целевого режима;\n",
        "  – **Стабильность**: дисперсия вознаграждения по эпизодам, частота нарушений ограничений;\n",
        "  – **Обобщающая способность**: качество управления на **тестовой части среды** (например, на данных за другой сезон или при изменённой динамике).\n",
        "\n",
        "- Оценить стабильность и эффективность алгоритмов:\n",
        "  – время обучения (в часах);\n",
        "  – время генерации одного эпизода (в секундах);\n",
        "  – устойчивость к осцилляции Q-значений, коллапсу политики, переобучению.\n",
        "\n",
        "- Проанализировать интерпретируемость политики:\n",
        "  – визуализация траекторий состояний и действий;\n",
        "  – анализ attention-карт (если используется трансформерная политика);\n",
        "  – чувствительность действий к изменениям в ключевых признаках (через SHAP или integrated gradients).\n",
        "\n",
        "- Исследовать практические сценарии:\n",
        "  – **Адаптивное управление**: реакция агента на внезапное изменение внешних условий (например, резкое похолодание);\n",
        "  – **Оптимизация по нескольким целям**: баланс между стоимостью и комфортом через multi-objective RL;\n",
        "  – **Имитация эксперта**: комбинирование RL с Imitation Learning (например, DAgger) при наличии демонстраций.\n",
        "\n",
        "- Разработать **модульный RL-фреймворк** с единым интерфейсом:\n",
        "  – `fit(env_train)` — обучение агента;\n",
        "  – `act(state)` — выбор действия;\n",
        "  – `evaluate(env_test)` — оценка на тестовой среде.\n",
        "\n",
        "- Обеспечить воспроизводимость и открытость результатов:\n",
        "  – опубликовать код, обученные политики, логи обучения, сравнительные таблицы и аналитический отчёт в публичном репозитории;\n",
        "  – использовать открытые лицензии: MIT (код), CC BY 4.0 (данные и политики).\n",
        "\n",
        "## 2. Теоретические предпосылки\n",
        "\n",
        "Обучение с подкреплением решает задачу последовательного принятия решений в условиях неопределённости, где агент взаимодействует со средой, чтобы максимизировать ожидаемое дисконтированное вознаграждение. В отличие от контролируемых методов, RL не требует размеченных пар «вход–действие», а учится через пробу и ошибку, что делает его особенно подходящим для задач управления.\n",
        "\n",
        "Ключевая сложность в прикладных задачах — **формулировка функции вознаграждения**. Неправильно спроектированная награда может привести к «хакингу» среды (reward hacking): агент достигает высокого вознаграждения, но не решает практическую задачу (например, выключает датчик вместо поддержания температуры). Поэтому в работе делается акцент на **спецификацию награды через компромисс между целями и ограничениями**.\n",
        "\n",
        "Современные алгоритмы делятся на **model-free** (DQN, SAC, PPO) и **model-based** (MBPO, PILCO). Первые проще в реализации и устойчивы к ошибкам моделирования, но требуют большого числа взаимодействий. Вторые эффективнее по данным, но чувствительны к неточностям модели среды — особенно критичным при работе с реальными временными рядами.\n",
        "\n",
        "Для задач управления с **ограничениями** (например, максимальная мощность, безопасные температуры) применяются методы **constrained RL**, где вознаграждение дополняется штрафами или задача переформулируется как оптимизация при ограничениях на ожидаемые затраты.\n",
        "\n",
        "Важно подчеркнуть: цель RL в управлении — не максимизация вознаграждения «во что бы то ни стало», а **обеспечение надёжного, безопасного и энергоэффективного поведения** в реальных условиях. Агент считается успешным, если демонстрирует стабильное и близкое к оптимальному поведение на **невидимых данных** (например, зимой, если обучался летом).\n",
        "\n",
        "## 3. Порядок выполнения работы\n",
        "\n",
        "### 3.1. Подбор и настройка сред\n",
        "\n",
        "Задача: Обеспечить реалистичные условия управления.  \n",
        "Требования:\n",
        "– Низкоразмерная среда — из Gymnasium/Gym;\n",
        "– Высокоразмерная среда — на основе данных из Практической работы № 1:\n",
        "  • Состояние: [температура, влажность, лаги, время суток, день недели];\n",
        "  • Действие: дискретное (вкл/выкл) или непрерывное (мощность от 0 до 1);\n",
        "  • Динамика: симулятор на основе TVAE или TimeGAN из ПР № 8;\n",
        "– Разделение временного ряда: 80 % — обучение, 20 % — тест (другой сезон/период).\n",
        "\n",
        "### 3.2. Формализация MDP и проектирование награды\n",
        "\n",
        "Задача: Корректно сформулировать задачу RL.  \n",
        "Требования:\n",
        "– Пространство состояний — нормализовано по тренировочной части;\n",
        "– Пространство действий — явно задано (дискретное или ограниченное непрерывное);\n",
        "– Функция вознаграждения:\n",
        "  • Основная компонента: минимизация отклонения от целевой температуры;\n",
        "  • Штрафы: за переключения, нарушение ограничений, энергопотребление;\n",
        "  • Вознаграждение не должно зависеть от внутренней структуры среды (no information leakage).\n",
        "\n",
        "### 3.3. Определение архитектур и обучение агентов\n",
        "\n",
        "Задача: Привести каждый алгоритм к рабочей конфигурации.  \n",
        "Требования:\n",
        "– Использовать реализации из `stable-baselines3`, `RLlib`, или воспроизведение по статьям;\n",
        "– Гиперпараметры по умолчанию или из литературы:\n",
        "  • DQN: lr=1e-4, buffer_size=50000, batch_size=64;\n",
        "  • SAC: entropy_coef=0.2, gamma=0.99;\n",
        "  • PPO: n_steps=2048, gae_lambda=0.95;\n",
        "– Обучение — только на тренировочной части среды;\n",
        "– Фиксация: время обучения, стабильность loss, частота нарушений ограничений.\n",
        "\n",
        "### 3.4. Генерация управляющих траекторий\n",
        "\n",
        "Задача: Получить сопоставимые последовательности управления.  \n",
        "Требования:\n",
        "– Для каждой среды — 100 эпизодов оценки;\n",
        "– Длина эпизода — ≥30 шагов;\n",
        "– Запись: состояния, действия, вознаграждения, флаги нарушений.\n",
        "\n",
        "### 3.5. Построение и настройка агентов\n",
        "\n",
        "Задача: Обеспечить единообразный интерфейс.  \n",
        "Требования:\n",
        "– Все агенты реализуют методы:\n",
        "  • `fit(env_train)`;\n",
        "  • `act(state)`;\n",
        "  • `evaluate(env_test)`;\n",
        "– Для MBPO/Constrained RL — кастомная реализация при отсутствии open-source версии.\n",
        "\n",
        "### 3.6. Диагностика качества управления\n",
        "\n",
        "Задача: Объективно оценить эффективность агентов.  \n",
        "Требования:\n",
        "– **Эффективность**:\n",
        "  • Среднее суммарное вознаграждение;\n",
        "  • Среднее абсолютное отклонение от цели;\n",
        "  • Суммарное энергопотребление;\n",
        "– **Стабильность**:\n",
        "  • Стандартное отклонение вознаграждения по эпизодам;\n",
        "  • Доля эпизодов с нарушением ограничений;\n",
        "– **Обобщающая способность**:\n",
        "  • Сравнение метрик на train vs. test средах;\n",
        "  • Тест при изменённой динамике (например, удвоенное тепловыделение).\n",
        "\n",
        "### 3.7. Оценка качества и сравнительный анализ\n",
        "\n",
        "Задача: Ранжировать агентов по комплексному критерию.  \n",
        "Требования:\n",
        "– Составление сравнительных таблиц:\n",
        "  • Алгоритм × Среда × Вознаграждение × Стабильность × Обобщение × Время обучения;\n",
        "– Ранжирование по взвешенной оценке:\n",
        "  • Эффективность (40 %),\n",
        "  • Стабильность и безопасность (40 %),\n",
        "  • Вычислительная эффективность (20 %).\n",
        "\n",
        "### 3.8. Анализ интерпретируемости политики\n",
        "\n",
        "Задача: Понять, «как» и «почему» агент принимает решения.  \n",
        "Требования:\n",
        "– Визуализация траекторий (состояние–действие–вознаграждение);\n",
        "– Для нейросетевых политик — анализ градиентов/SHAP по ключевым признакам;\n",
        "– Чувствительность: как изменится действие при ±10 % к температуре/лагу?\n",
        "\n",
        "### 3.9. Анализ вычислительной эффективности\n",
        "\n",
        "Задача: Оценить практическую применимость.  \n",
        "Требования:\n",
        "– Фиксация времени:\n",
        "  • Обучение (часы),\n",
        "  • Один шаг управления (мс);\n",
        "– Оценка памяти (GPU/CPU);\n",
        "– Классификация: низкая / средняя / высокая сложность.\n",
        "\n",
        "### 3.10. Практические сценарии применения\n",
        "\n",
        "Задача: Продемонстрировать ценность RL в реальных задачах.  \n",
        "Требования:\n",
        "– **Адаптация к сдвигу**: обучение летом, управление зимой — сравнение с ПИД-регулятором;\n",
        "– **Multi-objective control**: минимизация энергии при поддержании комфорта (через взвешенную награду);\n",
        "– **Hybrid control**: комбинация RL с правилами (например, «если температура > 30°C — включить максимум»).\n",
        "\n",
        "### 3.11. Публикация результатов исследования\n",
        "\n",
        "Задача: Обеспечить открытость и воспроизводимость.  \n",
        "Требования:\n",
        "– В репозитории:\n",
        "  • Код фреймворка;\n",
        "  • Обученные модели (.zip/.pth);\n",
        "  • Логи обучения (TensorBoard или CSV);\n",
        "  • Сравнительные таблицы и визуализации;\n",
        "– Лицензии: MIT (код), CC BY 4.0 (данные и политики);\n",
        "– README с инструкцией по воспроизведению.\n",
        "\n",
        "## 4. Дополнительные исследовательские задания\n",
        "\n",
        "- Сравнение **SAC** и **ПИД-регулятора** по энергоэффективности и устойчивости;\n",
        "- Анализ **опасного поведения**: может ли агент нарушать ограничения в тестовой среде?\n",
        "- Управление с **частичной наблюдаемостью** (POMDP): скрытие части состояния;\n",
        "- **Перенос политики**: обучение на симуляторе → развёртывание на реальных данных;\n",
        "- Исследование зависимости качества от **формы функции вознаграждения** (линейная, квадратичная, sparse);\n",
        "- Оценка **CO₂-следа** обучения: сравнение DDPG и PPO по энергопотреблению.\n",
        "\n",
        "## 5. Требования к отчёту\n",
        "\n",
        "Отчёт оформляется в соответствии с **ГОСТ 7.32–2017** и должен содержать:\n",
        "\n",
        "- **Введение** — постановка задачи управления, актуальность RL, обзор литературы (DQN, SAC, PPO), связь с Практическими работами № 1 и № 8;\n",
        "- **Методология** — описание сред, MDP-формализация, функции вознаграждения, архитектур агентов, метрик качества;\n",
        "- **Результаты экспериментов** — сравнительные таблицы, графики вознаграждения, траектории управления, визуализации политик;\n",
        "- **Обсуждение** — анализ компромиссов между эффективностью, безопасностью и сложностью; сравнение RL и классических контроллеров;\n",
        "- **Заключение** — итоговые выводы, рекомендации по выбору алгоритма, ограничения исследования;\n",
        "- **Список использованных источников** — по ГОСТ Р 7.0.5–2008;\n",
        "- **Приложения** — фрагменты кода, гиперпараметры, скриншоты сред, примеры траекторий.\n",
        "\n",
        "Отчёт сопровождается ссылкой на публичный репозиторий с кодом и результатами.\n",
        "\n",
        "## 6. Критерии оценивания\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|--------|\n",
        "| **Отлично** | Реализованы ≥4 алгоритма RL; оценка по трём критериям (эффективность, стабильность, обобщение); безопасность управления; практические сценарии; модульный код; публикация; отчёт по ГОСТ. |\n",
        "| **Хорошо** | Реализованы 2–3 алгоритма; есть сравнительные таблицы и базовая диагностика; отсутствует лишь отдельный элемент (например, анализ обобщения). |\n",
        "| **Удовлетворительно** | Только DQN или SAC; есть траектории и простые метрики; нет углублённой оценки стабильности или безопасности. |\n",
        "| **Неудовлетворительно** | Управление без оценки качества; нет разделения train/test; отсутствует сравнительный анализ; отчёт не представлен. |\n"
      ],
      "metadata": {
        "id": "fSetszIWCwXQ"
      }
    }
  ]
}