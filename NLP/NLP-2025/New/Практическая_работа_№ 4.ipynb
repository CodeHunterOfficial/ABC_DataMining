{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwtRgaEhSu17pISjT45ZWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Практическая работа № 4  \n",
        "**Тема.** Классификация текста с помощью глубокого обучения и трансформерных моделей\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Цель и задачи работы\n",
        "\n",
        "**Цель работы** — сформировать у обучающегося целостное, системное и практическое понимание современного ландшафта архитектур глубокого обучения, применяемых для решения задач текстовой классификации. Особое внимание уделяется не только реализации моделей, но и **глубокому пониманию их внутреннего устройства**: от простейших многослойных персептронов до сложных трансформерных систем с механизмами внимания. Работа призвана продемонстрировать, как эволюция архитектур — от рекуррентных сетей с энкодер-декодерной структурой до современных трансформеров — позволила решить фундаментальные ограничения ранних подходов, такие как проблема долгосрочной зависимости и неэффективность последовательной обработки.\n",
        "\n",
        "**Основные задачи работы:**\n",
        "\n",
        "1. Использовать единый корпус текстов, собранный в Практической работе № 1 и размеченный в Практической работе № 3, в качестве общей экспериментальной платформы для всех моделей.  \n",
        "2. Реализовать и экспериментально сравнить **полный спектр архитектур глубокого обучения**:  \n",
        "   - **Базовые модели**: многослойный персептрон (MLP), свёрточные нейронные сети (CNN), простые рекуррентные сети (RNN);  \n",
        "   - **Расширенные рекуррентные архитектуры**: LSTM, GRU, двунаправленные (BiLSTM, BiGRU), стекированные (Stacked RNN), а также **энкодер-декодерные схемы** с и без механизма внимания;  \n",
        "   - **Трансформерные модели**: BERT, GPT, а также специализированные русскоязычные версии — **RuBERT** (DeepPavlov), **ruRoBERTa** (Sber AI), **DistilRuBERT**, **rubert-tiny**, **mDeBERTa-v3-base** и другие актуальные модели.  \n",
        "3. Исследовать **роль механизма внимания** — от его первоначального применения в энкодер-декодерных системах до полной замены рекуррентности в архитектуре трансформера.  \n",
        "4. Обеспечить **комплексную визуализацию** процесса обучения и результатов: графики потерь и метрик по эпохам, матрицы ошибок, ROC- и PR-кривые, 2D-проекции эмбеддингов, тепловые карты внимания.  \n",
        "5. Применить современные методы борьбы с дисбалансом классов в условиях глубокого обучения.  \n",
        "6. Проанализировать **интерпретируемость предсказаний** с использованием градиентных методов (saliency maps), визуализации весов внимания и библиотек типа **Captum**.  \n",
        "7. Разработать интерактивный веб-интерфейс, позволяющий сравнивать все реализованные модели на единой платформе.  \n",
        "8. Обеспечить воспроизводимость через публикацию кода, fine-tuned моделей и интерфейса.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Теоретические предпосылки\n",
        "\n",
        "Текстовая классификация с помощью глубокого обучения прошла несколько этапов эволюции, каждый из которых решал определённые ограничения предыдущего поколения.\n",
        "\n",
        "### 2.1. Базовые архитектуры: MLP и CNN\n",
        "\n",
        "- **Многослойный персептрон (MLP)** представляет собой полносвязную нейронную сеть, принимающую на вход фиксированный вектор признаков (например, усреднённый эмбеддинг документа). Несмотря на простоту, MLP служит важным базовым эталоном, позволяющим оценить, насколько сложные архитектуры действительно превосходят «наивный» подход.  \n",
        "- **Свёрточные нейронные сети (CNN)** были адаптированы для текста из компьютерного зрения. Используя 1D-свёртки с различными размерами ядер (например, 3, 4, 5), CNN эффективно выявляют локальные семантические паттерны — своего рода «ключевые n-граммы», определяющие тему или тональность. Архитектура Kim (2014) остаётся классическим примером применения CNN в NLP.\n",
        "\n",
        "### 2.2. Рекуррентные сети и их эволюция\n",
        "\n",
        "Простые RNN страдают от **проблемы исчезающих и взрывающихся градиентов**, что делает их неспособными моделировать долгосрочные зависимости. Эта проблема была решена с появлением:\n",
        "\n",
        "- **LSTM (Long Short-Term Memory)** и **GRU (Gated Recurrent Unit)** — модифицированных RNN с вентильными механизмами, контролирующими поток информации через ячейку памяти.  \n",
        "- **Двунаправленные архитектуры (BiRNN)**, в которых текст обрабатывается в прямом и обратном направлениях, что позволяет каждому токену «видеть» как прошлый, так и будущий контекст.  \n",
        "- **Стекированные RNN (Stacked RNN)**, где выход одного слоя RNN подаётся на вход следующего, что позволяет извлекать иерархические признаки на разных уровнях абстракции.\n",
        "\n",
        "Критическим прорывом стало введение **энкодер-декодерной архитектуры** (Sutskever et al., 2014), первоначально разработанной для машинного перевода. В этой схеме:\n",
        "- **Энкодер** (обычно LSTM или GRU) преобразует входную последовательность в фиксированный **контекстный вектор**;\n",
        "- **Декодер** генерирует выходную последовательность на основе этого вектора.\n",
        "\n",
        "Однако фиксированный контекстный вектор оказывался «узким местом» для длинных текстов. Это привело к появлению **механизма внимания (attention)** (Bahdanau et al., 2015), который позволил декодеру **динамически фокусироваться на разных частях входной последовательности** на каждом шаге генерации. В задачах классификации декодер часто заменяется простым классификационным слоем, но механизм внимания сохраняется для выявления наиболее релевантных слов.\n",
        "\n",
        "### 2.3. Эра трансформеров\n",
        "\n",
        "Архитектура **трансформера** (Vaswani et al., 2017) полностью отказалась от рекуррентности, заменив её **механизмом само-внимания (self-attention)**, который позволяет каждому токену напрямую взаимодействовать со всеми другими токенами в последовательности. Это обеспечивает:\n",
        "- параллельную обработку (ускорение обучения);\n",
        "- эффективное моделирование как локальных, так и глобальных зависимостей.\n",
        "\n",
        "Для русского языка наибольшее распространение получили:\n",
        "- **RuBERT** (DeepPavlov) — BERT, дообученный на Национальном корпусе русского языка и новостях;\n",
        "- **ruRoBERTa** (Sber AI) — более мощная версия, обученная на 180 ГБ текстов с динамической маскировкой;\n",
        "- **DistilRuBERT**, **rubert-tiny** — компактные версии, полученные методами дистилляции знаний, подходящие для развёртывания на устройствах с ограниченными ресурсами;\n",
        "- **mDeBERTa-v3-base** — мультиязычная модель с улучшенным представлением позиций и маски внимания.\n",
        "\n",
        "Все эти модели используют процедуру **fine-tuning**: предобученная модель адаптируется под конкретную задачу классификации путём дообучения небольшого числа слоёв (часто — только головного классификационного слоя).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Методика и порядок выполнения работы\n",
        "\n",
        "### 3.1. Этап 1. Подготовка экспериментального корпуса\n",
        "\n",
        "Используется **единый корпус** из Практической работы № 1, дополненный разметкой из Работы № 3:\n",
        "- **Бинарная классификация**: тональность (позитив/негатив);\n",
        "- **Многоклассовая**: темы (политика, экономика, спорт, культура);\n",
        "- **Многометочная**: несколько тем на документ.\n",
        "\n",
        "**Требования**:  \n",
        "- Объём — не менее 10 000 документов на тип задачи;  \n",
        "- Формат — JSONL с полями `text`, `sentiment`, `category`, `categories`;  \n",
        "- Разделение — 70/15/15 с сохранением стратификации.  \n",
        "> Важно: для трансформеров текст **не подвергается лемматизации или стеммингу**, чтобы сохранить оригинальную орфографию.\n",
        "\n",
        "### 3.2. Этап 2. Подготовка данных\n",
        "\n",
        "- Для **MLP/CNN/RNN**:  \n",
        "  - Токенизация с использованием ваших подсловных моделей (BPE/WordPiece из Работы № 1) или `spaCy`;  \n",
        "  - Преобразование в последовательность ID;  \n",
        "  - Padding/truncation до фиксированной длины (рекомендуется 256 токенов).\n",
        "\n",
        "- Для **трансформеров**:  \n",
        "  - Использование официального токенизатора через `AutoTokenizer.from_pretrained`;  \n",
        "  - Формирование входных тензоров: `input_ids`, `attention_mask`, `token_type_ids` (если требуется).\n",
        "\n",
        "### 3.3. Этап 3. Реализация моделей глубокого обучения\n",
        "\n",
        "Создаётся модуль `deep_classifiers.py` с поддержкой:\n",
        "\n",
        "#### Базовые модели:\n",
        "- **MLP**: 2–3 полносвязных слоя поверх усреднённого эмбеддинга;\n",
        "- **CNN**: 1D-свёртки с ядрами 3, 4, 5 → max-pooling → dropout → классификация.\n",
        "\n",
        "#### Рекуррентные модели:\n",
        "- **Simple RNN**, **LSTM**, **GRU**;\n",
        "- **BiLSTM**, **BiGRU**;\n",
        "- **Stacked LSTM/GRU** (2–3 слоя);\n",
        "- **Энкодер-декодер с attention**:  \n",
        "  - Энкодер: BiLSTM;  \n",
        "  - Механизм внимания: вычисление весов на основе скрытых состояний энкодера и текущего состояния декодера;  \n",
        "  - Декодер (для классификации заменяется на слой внимания + классификатор).\n",
        "\n",
        "#### Трансформерные модели:\n",
        "- `bert-base-multilingual-cased`;  \n",
        "- `DeepPavlov/rubert-base-cased`;  \n",
        "- `sberbank-ai/ruRoberta-large`;  \n",
        "- `cointegrated/rubert-tiny2`;  \n",
        "- `microsoft/mdeberta-v3-base`.\n",
        "\n",
        "Для всех трансформеров добавляется линейный классификационный слой поверх выхода `[CLS]`-токена.\n",
        "\n",
        "### 3.4. Этап 4. Обучение и комплексная визуализация\n",
        "\n",
        "На этом этапе акцент делается не только на метриках, но и на **наглядном представлении процесса и результатов**:\n",
        "\n",
        "- **Графики обучения**: loss и F1 по эпохам для train/validation (для выявления переобучения);  \n",
        "- **Матрицы ошибок (confusion matrices)**: для всех типов задач (нормализованные и абсолютные);  \n",
        "- **ROC- и PR-кривые**: для бинарной и многоклассовой (one-vs-rest);  \n",
        "- **2D-проекции**: UMAP/t-SNE на векторах `[CLS]` или усреднённых эмбеддингах с цветовой разметкой классов;  \n",
        "- **Тепловые карты внимания**: для трансформеров — визуализация весов self-attention между токенами.\n",
        "\n",
        "### 3.5. Этап 5. Борьба с дисбалансом классов\n",
        "\n",
        "- Взвешенная кросс-энтропия (`weight` в PyTorch);  \n",
        "- **Focal Loss** — снижает вклад легко классифицируемых примеров;  \n",
        "- Использование **F1** и **PR-AUC** как основных метрик при дисбалансе > 3:1.\n",
        "\n",
        "### 3.6. Этап 6. Интерпретация предсказаний\n",
        "\n",
        "- Для **трансформеров**:  \n",
        "  - Визуализация attention heads (например, через `bertviz`);  \n",
        "  - **Saliency maps** — вычисление градиента выхода по входным эмбеддингам (Captum);  \n",
        "- Для **RNN/CNN**:  \n",
        "  - Анализ активаций последнего слоя;  \n",
        "  - Для CNN — выделение наиболее активных фильтров и соответствующих n-грамм.\n",
        "\n",
        "### 3.7. Этап 7. Разработка веб-интерфейса\n",
        "\n",
        "Интерфейс (рекомендуется **Gradio**) должен позволять:\n",
        "- ввести текст;\n",
        "- выбрать тип задачи и модель;\n",
        "- увидеть:  \n",
        "  - предсказание и вероятности,  \n",
        "  - график обучения (для выбранной модели),  \n",
        "  - матрицу ошибок (на тестовой выборке),  \n",
        "  - карту внимания или saliency map,  \n",
        "  - 2D-проекцию положения текста в пространстве эмбеддингов.\n",
        "\n",
        "### 3.8. Этап 8. Публикация результатов\n",
        "\n",
        "- Fine-tuned трансформеры — на **Hugging Face Hub** с полным **Model Card**;  \n",
        "- Код — на GitHub/GitLab с `environment.yml`, инструкцией и примерами;  \n",
        "- Веб-приложение — развернуто через **Hugging Face Spaces**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Дополнительные исследовательские задания\n",
        "\n",
        "1. **Эволюция внимания**: Постройте сравнение: энкодер-декодер без внимания → с вниманием → трансформер. Как меняется качество и интерпретируемость?  \n",
        "2. **Глубина vs ширина**: Сравните стекированные LSTM (3 слоя) и широкий MLP (1024 нейрона) — что эффективнее для вашей задачи?  \n",
        "3. **Трансфер от мультиязычных моделей**: Оцените, насколько ruRoBERTa превосходит multilingual BERT и mDeBERTa на русском корпусе.  \n",
        "4. **Анализ ошибок с вниманием**: Найдите случаи, когда модель ошибается, но карта внимания «смотрит» не туда — можно ли это исправить дообучением?  \n",
        "5. **Сравнение скорости**: Измерьте время инференса на CPU для rubert-base, rubert-tiny и BiLSTM — при каком объёме данных выгода от качества трансформера окупает его стоимость?\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Требования к отчёту\n",
        "\n",
        "Отчёт оформляется в соответствии с **ГОСТ 7.32–2017** и должен включать:\n",
        "\n",
        "1. Титульный лист;  \n",
        "2. Введение с обоснованием эволюции архитектур;  \n",
        "3. Подробное описание всех реализованных моделей, включая схемы энкодер-декодера и механизмов внимания;  \n",
        "4. Характеристики корпуса;  \n",
        "5. **Все виды визуализаций** с пояснениями;  \n",
        "6. Сравнение с классическими методами (Работа № 3);  \n",
        "7. Примеры интерпретации;  \n",
        "8. Ссылки на репозиторий, веб-приложение, Hugging Face;  \n",
        "9. Выводы:  \n",
        "   - какая архитектура оптимальна для каждой задачи;  \n",
        "   - какова роль внимания в повышении качества и интерпретируемости;  \n",
        "   - есть ли практический смысл использовать трансформеры при ограниченных ресурсах.\n",
        "\n",
        "Список источников — по **ГОСТ Р 7.0.5–2008**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Критерии оценивания\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|----------|\n",
        "| **Отлично (5)** | Реализованы все типы моделей, включая энкодер-декодер с вниманием и все трансформеры. Есть все виды визуализаций, глубокий анализ внимания, сравнение с классикой. Отчёт — развёрнутый, академичный, с рефлексией. |\n",
        "| **Хорошо (4)** | Реализованы CNN, LSTM/GRU (включая Bi и Stacked), трансформеры. Есть графики, матрицы, внимание, интерфейс. |\n",
        "| **Удовлетворительно (3)** | Реализованы хотя бы три архитектуры (например, CNN, BiLSTM, RuBERT). Есть метрики и базовые визуализации. |\n",
        "| **Неудовлетворительно (2)** | Отсутствует внимание, энкодер-декодер или трансформеры. Нет визуализаций или единого корпуса. |\n"
      ],
      "metadata": {
        "id": "_IvQHKABcKfe"
      }
    }
  ]
}