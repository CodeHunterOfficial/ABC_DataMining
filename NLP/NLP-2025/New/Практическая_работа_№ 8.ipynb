{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNedRd9HYSH2d6eAqPRAk5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Практическая работа № 8\n",
        "**Тема.** Проектирование и реализация предметно-специфичных бенчмарков для оценки крупных языковых моделей\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Цель и задачи работы\n",
        "\n",
        "Современные крупные языковые модели (LLM) демонстрируют впечатляющие результаты на **общих бенчмарках** (MMLU, HELM, Big-Bench), однако их эффективность в **узких предметных областях** — образовании, медицине, юриспруденции, локальных языках — часто остаётся неизученной. Стандартные оценки не отражают способность модели работать с **формализованными правилами**, **отраслевой терминологией**, **локальными нормами** или **специфическими форматами заданий**.\n",
        "\n",
        "**Цель настоящей работы** — сформировать у обучающегося **методологические и практические навыки проектирования, реализации и публикации специализированных бенчмарков**, ориентированных на конкретный домен. Такой бенчмарк позволяет объективно оценить не «общий интеллект» модели, а её **практическую применимость в реальных задачах**.\n",
        "\n",
        "В качестве **иллюстративного кейса** в работе используется **оценка LLM на задачах основного государственного экзамена (ОГЭ)** по русскому языку и математике — не как единственная цель, а как пример домена с чёткими правилами, структурированными критериями и социальной значимостью. Однако разработанный конвейер применим и к другим сценариям:  \n",
        "- медицинская диагностика (на основе клинических протоколов),  \n",
        "- юридическое консультирование (по Гражданскому кодексу РФ),  \n",
        "- обработка запросов на низкоресурсных языках (татарском, башкирском и др.).\n",
        "\n",
        "**Основные задачи работы:**\n",
        "\n",
        "1. Изучить **принципы проектирования бенчмарков**: таксономия задач, выбор метрик, обеспечение воспроизводимости.  \n",
        "2. Выбрать **предметную область** для бенчмарка (ОГЭ — как один из вариантов).  \n",
        "3. Собрать и структурировать **корпус заданий и эталонных ответов** с аннотацией по критериям.  \n",
        "4. Реализовать **конвейер автоматической оценки**, поддерживающий разные типы ответов:  \n",
        "   - краткие (число, слово, выбор),  \n",
        "   - развёрнутые (решение, рассуждение, эссе).  \n",
        "5. Провести **системное тестирование open-weight LLM** (Llama 3, Mistral, Qwen, DeepSeek, ruT5) и, при наличии доступа, коммерческих моделей (GigaChat).  \n",
        "6. Интегрировать **RAG-подход** (из Работы № 6) для предоставления моделям доступа к **доменной базе знаний** (справочники, кодексы, критерии).  \n",
        "7. Оценить модели не только по **точности**, но и по **фактической согласованности**, **объяснимости решений** и **соответствию доменным стандартам**.  \n",
        "8. Проанализировать **разрыв между общими и специализированными бенчмарками**.  \n",
        "9. Опубликовать бенчмарк в открытом формате и разработать веб-интерфейс для тестирования.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Теоретические предпосылки\n",
        "\n",
        "### 2.1. Зачем нужны специализированные бенчмарки?\n",
        "\n",
        "Общие бенчмарки измеряют **широкие способности**: знание фактов, логику, языковую беглость. Однако в прикладных задачах критически важны **узкие компетенции**:\n",
        "\n",
        "- **Формальная точность**: соблюдение орфографических правил, математических формул, юридических норм.  \n",
        "- **Структурное соответствие**: решение должно включать определённые шаги или разделы.  \n",
        "- **Контекстная адекватность**: ответ должен учитывать локальные реалии (например, законодательство РФ или школьную программу).\n",
        "\n",
        "Специализированный бенчмарк — это **мост между исследовательской моделью и её реальным применением**.\n",
        "\n",
        "### 2.2. Архитектура бенчмарка\n",
        "\n",
        "Любой качественный бенчмарк включает:\n",
        "\n",
        "1. **Корпус заданий** — с чёткой таксономией (предмет, тип, сложность).  \n",
        "2. **Эталонные ответы** — возможно, несколько вариантов (для развёрнутых задач).  \n",
        "3. **Критерии оценки** — машинно-интерпретируемые правила или размеченные примеры.  \n",
        "4. **Конвейер оценки** — автоматизированная система сопоставления ответа модели с эталоном.  \n",
        "5. **Метрики** — адаптированные под тип задачи (точность, полнота, согласованность).\n",
        "\n",
        "### 2.3. Типы доменов (примеры)\n",
        "\n",
        "| Домен | Источник данных | Особенности оценки |\n",
        "|------|----------------|-------------------|\n",
        "| **Образование (ОГЭ/ЕГЭ)** | Демоверсии ФИПИ | Строгие критерии, формальные правила |\n",
        "| **Медицина** | Клинические протоколы, MIMIC | Фактическая точность, безопасность |\n",
        "| **Юриспруденция** | Кодексы РФ, судебная практика | Соответствие закону, ссылки на статьи |\n",
        "| **Низкоресурсные языки** | Татарские/башкирские тексты | Поддержка морфологии, орфографии |\n",
        "\n",
        "> **Важно**: ОГЭ в этой работе — **не цель, а методологический пример**, иллюстрирующий, как строить бенчмарк в домене с чёткими формальными правилами.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Методика и порядок выполнения работы\n",
        "\n",
        "### 3.1. Этап 1. Выбор домена и сбор корпуса\n",
        "\n",
        "- **Опция A (ОГЭ)**: сбор заданий по русскому языку и математике из открытых источников (ФИПИ, банк заданий).  \n",
        "- **Опция B (другой домен)**: например, юридические вопросы по ГК РФ, медицинские кейсы, татарские лингвистические задачи.  \n",
        "- **Требования к корпусу**:  \n",
        "  - ≥300 заданий;  \n",
        "  - разнообразие типов (краткие/развёрнутые);  \n",
        "  - наличие эталонных ответов и критериев.\n",
        "\n",
        "### 3.2. Этап 2. Таксономия и аннотация\n",
        "\n",
        "- Разработка схемы классификации заданий (предмет → тип → сложность).  \n",
        "- Аннотация эталонных ответов:  \n",
        "  - для кратких — точный ответ;  \n",
        "  - для развёрнутых — ключевые элементы («должно содержать: определение, пример, вывод»).\n",
        "\n",
        "### 3.3. Этап 3. Конвейер оценки\n",
        "\n",
        "Реализация модуля `domain_evaluator.py`:\n",
        "\n",
        "- **Краткие ответы**: нормализация + точное совпадение или численная близость.  \n",
        "- **Развёрнутые ответы**:  \n",
        "  - **Правило-based**: наличие ключевых фраз/шагов (регулярки, NER);  \n",
        "  - **Модель-based**: BERTScore против эталона;  \n",
        "  - **Классификатор**: fine-tuned RuBERT для оценки по критериям (на основе Работы № 4).\n",
        "\n",
        "### 3.4. Этап 4. Тестирование моделей\n",
        "\n",
        "- **Open-weight LLM**: Llama 3, Mistral, Qwen, DeepSeek, ruT5;  \n",
        "- **Режимы**:  \n",
        "  - Zero-shot,  \n",
        "  - Few-shot (2–3 примера),  \n",
        "  - RAG (с доступом к доменной базе знаний).  \n",
        "- Для ОГЭ: RAG-база = справочник правил и критериев ФИПИ.\n",
        "\n",
        "### 3.5. Этап 5. Оценка и анализ\n",
        "\n",
        "- **Метрики**:  \n",
        "  - Accuracy (краткие),  \n",
        "  - BERTScore + Coverage (развёрнутые),  \n",
        "  - Hallucination Rate,  \n",
        "  - Соответствие критериям (0–100%).  \n",
        "- **Сравнение**: общие бенчмарки vs ваш специализированный.\n",
        "\n",
        "### 3.6. Этап 6. Публикация\n",
        "\n",
        "- Формат: совместим с **EleutherAI LM Evaluation Harness**;  \n",
        "- Публикация: GitHub + Hugging Face Datasets;  \n",
        "- Документация: таксономия, метрики, лицензия.\n",
        "\n",
        "### 3.7. Этап 7. Веб-интерфейс\n",
        "\n",
        "- Выбор домена и типа задания;  \n",
        "- Тестирование модели;  \n",
        "- Отображение: ответ, оценка, критерии, источники (для RAG).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Дополнительные задания\n",
        "\n",
        "1. **Мультидоменный бенчмарк**: объединить ОГЭ, юридические и медицинские задачи в один набор.  \n",
        "2. **Генерация заданий**: fine-tune модель на создании новых заданий по теме.  \n",
        "3. **Оценка на татарском**: адаптация бенчмарка для низкоресурсного языка.  \n",
        "4. **Сравнение RAG и fine-tuning**: что лучше работает в узком домене?\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Требования к отчёту\n",
        "\n",
        "Отчёт по **ГОСТ 7.32–2017** должен включать:\n",
        "\n",
        "1. Обоснование выбора домена (ОГЭ — как пример);  \n",
        "2. Описание таксономии и конвейера оценки;  \n",
        "3. Результаты тестирования моделей;  \n",
        "4. Анализ разрыва между общими и специализированными бенчмарками;  \n",
        "5. Ссылки на публикацию бенчмарка;  \n",
        "6. Рефлексию: насколько методология применима к другим доменам?\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Критерии оценивания\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|----------|\n",
        "| **Отлично (5)** | Полноценный бенчмарк с таксономией, автоматической оценкой, тестированием ≥3 моделей, RAG, публикацией. Чётко показано, что ОГЭ — пример, а не цель. |\n",
        "| **Хорошо (4)** | Работает конвейер оценки, протестированы модели, есть анализ. |\n",
        "| **Удовлетворительно (3)** | Собран корпус и реализована базовая оценка (точное совпадение). |\n",
        "| **Неудовлетворительно (2)** | Отсутствует системный подход к проектированию бенчмарка. |\n"
      ],
      "metadata": {
        "id": "H9NBQ3EZiTEp"
      }
    }
  ]
}