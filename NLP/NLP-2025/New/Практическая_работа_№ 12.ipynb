{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMbLKHHF8PH8k7+1Qc7mi6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/New/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%96%E2%80%AF12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Практическая работа № 12. Обучение с подкреплением на основе человеческой обратной связи (RLHF): от сбора предпочтений до выравнивания языковых моделей**\n",
        "\n",
        "## **1. Цель и задачи работы**\n",
        "\n",
        "Цель работы — формирование у обучающегося целостного, критически рефлексивного понимания парадигмы **выравнивания (alignment)** языковых моделей с человеческими ценностями через конвейер **обучения с подкреплением на основе человеческой обратной связи (Reinforcement Learning from Human Feedback, RLHF)**. Работа направлена не только на освоение технических приёмов реализации RLHF, но и на развитие компетенций в области **этического проектирования ИИ**, **анализа смещений**, **интерпретации поведенческих сдвигов** и **обеспечения воспроизводимости** в условиях субъективной и многокритериальной оценки.\n",
        "\n",
        "**Основные задачи работы:**  \n",
        "1. На основе корпуса из Практической работы № 1 сформировать трёхкомпонентную экспериментальную базу: запросы, генерации, ранжированные предпочтения.  \n",
        "2. Собрать и верифицировать **многокритериальный набор человеческих предпочтений** по трём измерениям: *полезность (helpfulness)*, *правдивость (truthfulness)*, *безопасность (harmlessness)*.  \n",
        "3. Реализовать и сравнить **четыре стратегии обучения с предпочтениями**:  \n",
        " – классический RLHF с PPO,  \n",
        " – Direct Preference Optimization (DPO),  \n",
        " – Identity Preference Optimization (IPO),  \n",
        " – Cross-Entropy Method (CEM) как baseline без RL.  \n",
        "4. Обучить **многоголовую модель вознаграждения (Multi-head Reward Model)**, способную раздельно оценивать каждый критерий, и исследовать возможность управления компромиссом между ними.  \n",
        "5. Провести строгий мониторинг RL-процесса: KL-регуляризация, контроль дисперсии advantage, early stopping по экспертной метрике.  \n",
        "6. Оценить качество моделей через **три независимых канала**:  \n",
        " – автоматические метрики (BERTScore, MAUVE, Self-BLEU),  \n",
        " – контролируемое экспертное аннотирование (Likert-шкалы, pairwise A/B),  \n",
        " – поведенческий анализ (частота льстивых фраз, лексическое разнообразие, длина, фактологическая консистентность).  \n",
        "7. Проанализировать **риски переоптимизации**, **смещения аннотаторов** и **этической деформации** (например, чрезмерная политкорректность, избегание сложных тем).  \n",
        "8. Разработать **интерактивную платформу для живого A/B-тестирования** с возможностью сбора онлайн-обратной связи и её интеграции в цикл дообучения (online RLHF).  \n",
        "9. Обеспечить полную **воспроизводимость и открытость**: публикация набора предпочтений, RM, всех адаптированных моделей, скриптов и отчётов в соответствии с принципами FAIR и стандартами Hugging Face.  \n",
        "10. Сформулировать **практические рекомендации и этические руководства** по применению RLHF в low-resource и multilingual (включая русский и татарский) контекстах.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Теоретические предпосылки**\n",
        "\n",
        "RLHF возник как ответ на фундаментальное ограничение обучения с учителем (SFT): в задачах генерации не существует единственного «правильного» ответа. Вместо этого качество определяется **многомерной, субъективной и контекстуальной** оценкой. RLHF преодолевает эту неопределённость, заменяя точечную разметку **относительными предпочтениями**, которые лучше отражают сложность человеческого суждения.\n",
        "\n",
        "### **Архитектура конвейера RLHF**\n",
        "1. **Supervised Fine-Tuning (SFT)** — базовая адаптация предобученной языковой модели (например, mt5, ruGPT) к формату целевой задачи (QA, диалог, суммаризация).  \n",
        "2. **Моделирование вознаграждения (Reward Modeling)** — обучение функции \\( r_\\theta(x, y) \\), предсказывающей, насколько ответ \\( y \\) на запрос \\( x \\) соответствует человеческим предпочтениям. Обычно используется Bradley-Terry модель:  \n",
        "   \\[\n",
        "   P(y_1 \\succ y_2 | x) = \\sigma(r_\\theta(x, y_1) - r_\\theta(x, y_2))\n",
        "   \\]  \n",
        "3. **Обучение с подкреплением** — дообучение SFT-модели \\(\\pi_\\phi\\) с использованием алгоритма PPO, где вознаграждение заменено на \\( r_\\theta \\), а регуляризация обеспечивается через KL-дивергенцию к reference-модели \\(\\pi_{\\text{ref}}\\):  \n",
        "   \\[\n",
        "   \\mathcal{L}_{\\text{RL}} = \\mathbb{E} \\left[ r_\\theta(x, y) - \\beta \\cdot \\text{KL}(\\pi_\\phi(y|x) \\| \\pi_{\\text{ref}}(y|x)) \\right]\n",
        "   \\]\n",
        "\n",
        "### **Современные альтернативы**\n",
        "- **DPO** (Rafailov et al., 2023) показывает, что оптимизация предпочтений может быть сведена к задаче классификации без явного RL:  \n",
        "  \\[\n",
        "  \\mathcal{L}_{\\text{DPO}} = -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\phi(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)} - \\beta \\log \\frac{\\pi_\\phi(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)} \\right)\n",
        "  \\]  \n",
        "- **IPO** и **SLiC** улучшают стабильность при слабых предпочтениях.\n",
        "\n",
        "### **Критические вызовы**\n",
        "- **Смещение аннотаторов**: культурные, языковые, профессиональные различия искажают предпочтения.  \n",
        "- **Переоптимизация**: модель учится «обманывать» RM, генерируя формально корректные, но бессодержательные или льстивые ответы.  \n",
        "- **Выравнивание vs. компетентность**: чрезмерный упор на безопасность может подавлять полезные, но спорные высказывания.  \n",
        "- **Отсутствие ground truth**: качество RLHF оценивается только через прокси, что требует мульти-методного подхода.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Порядок выполнения работы**\n",
        "\n",
        "### **3.1. Формирование мультизадачного набора запросов и генераций**\n",
        "\n",
        "**Задача:** Подготовить данные для трёх типов генеративных задач на русском языке.  \n",
        "**Требования:**  \n",
        "– Источники: ТАСС, РИА, Meduza, belgech.ru, Татарская электронная библиотека (для татарского субкорпуса);  \n",
        "– Задачи:  \n",
        " ✓ **QA**: «Ответь на вопрос по тексту» (1 000 запросов),  \n",
        " ✓ **Диалог**: «Как вежливо ответить на это сообщение?» (1 000),  \n",
        " ✓ **Суммаризация**: «Напиши кратко суть новости» (1 000);  \n",
        "– Генерация: 6 вариантов на запрос с помощью SFT-модели (mt5-small, ruGPT-3.5) при температурах {0.3, 0.7, 1.0} и top-p {0.85, 0.95};  \n",
        "– Формат: JSONL с `prompt`, `responses`, `task_type`;  \n",
        "– Общий объём: ≥18 000 пар «запрос–ответ».\n",
        "\n",
        "### **3.2. Сбор многокритериальных предпочтений**\n",
        "\n",
        "**Задача:** Создать верифицированный набор ранжированных данных.  \n",
        "**Требования:**  \n",
        "– Критерии:  \n",
        " ✓ **Helpfulness**: решает ли задачу?  \n",
        " ✓ **Truthfulness**: соответствует ли фактам?  \n",
        " ✓ **Harmlessness**: нет ли вреда, дискриминации, дезинформации?  \n",
        "– Инструкция для аннотаторов с примерами и контрольными вопросами;  \n",
        "– Минимум 5 независимых оценок на пару (для оценки согласия);  \n",
        "– Объём: ≥7 000 ранжированных пар (включая 1 000 на татарском);  \n",
        "– Метрика согласия: Krippendorff’s α ≥ 0.65;  \n",
        "– Формат: Hugging Face `PreferenceDataset` (chosen/rejected).\n",
        "\n",
        "### **3.3. Реализация модуля RLHF с поддержкой современных методов**\n",
        "\n",
        "**Задача:** Разработать гибкий, масштабируемый пайплайн.  \n",
        "**Требования:**  \n",
        "– Поддержка:  \n",
        " ✓ PPO (с библиотекой TRL или собственной реализацией),  \n",
        " ✓ DPO, IPO (через TRL или Axolotl),  \n",
        " ✓ CEM (beam search + переобучение на топ-K),  \n",
        " ✓ Multi-head RM (3 выхода: H, T, S);  \n",
        "– Архитектуры:  \n",
        " ✓ Языковая модель: `cointegrated/ru-t5-base`, `sberbank-ai/ruGPT-3.5-1.7B`,  \n",
        " ✓ RM: `DeepPavlov/rubert-base-cased` с линейной головой;  \n",
        "– Единый конфигурационный файл (YAML), логирование через Weights & Biases или MLflow;  \n",
        "– Код: `rlhf_core.py`, `reward_modeling.py`, `preference_collection.py`.\n",
        "\n",
        "### **3.4. Обучение и мониторинг с контролем качества**\n",
        "\n",
        "**Задача:** Обеспечить стабильность и интерпретируемость обучения.  \n",
        "**Требования:**  \n",
        "– Для RM: валидация по AUC и ранговой корреляции (Spearman);  \n",
        "– Для PPO:  \n",
        " ✓ Контроль KL ≤ 0.02,  \n",
        " ✓ Ограничение количества шагов (≤200),  \n",
        " ✓ Мониторинг энтропии и дисперсии advantage;  \n",
        "– Для DPO: подбор β через валидацию;  \n",
        "– Сохранение: checkpoint’ов, логов, embedding-проекций (UMAP по скрытым состояниям).\n",
        "\n",
        "### **3.5. Многоаспектная оценка качества**\n",
        "\n",
        "**Задача:** Избежать «метрического иллюзионизма».  \n",
        "**Методы:**  \n",
        "– **Автоматические**: BERTScore, MAUVE, Self-BLEU (для разнообразия), FactScore (для правдивости);  \n",
        "– **Экспертные**:  \n",
        " ✓ 100 случайных пар оцениваются 3 независимыми экспертами по 5-балльной шкале,  \n",
        " ✓ A/B-тест на живой аудитории (≥200 решений);  \n",
        "– **Поведенческие**:  \n",
        " ✓ Частота триггерных фраз («рад помочь», «конечно!»),  \n",
        " ✓ Средняя длина, лексическая сложность (MTLD),  \n",
        " ✓ Анализ отказов от ответа («я не могу…»).\n",
        "\n",
        "### **3.6. Анализ смещений и этических рисков**\n",
        "\n",
        "**Задача:** Выявить и документировать нежелательные эффекты.  \n",
        "**Требования:**  \n",
        "– Тест на **культурное смещение**: сравнение ответов на нейтральные запросы по гендеру/национальному признаку;  \n",
        "– Тест на **избегание**: доля ответов с отказом на сложные темы (политика, религия);  \n",
        "– Анализ **переоптимизации**: корреляция RM-оценки и экспертной оценки (ожидается нелинейный спад);  \n",
        "– Формирование **этического отчёта** (Ethics Appendix).\n",
        "\n",
        "### **3.7. Интерпретация поведенческих сдвигов**\n",
        "\n",
        "**Задача:** Понять, *как именно* модель меняет своё поведение.  \n",
        "**Методы:**  \n",
        "– Сравнение распределений n-грамм до/после;  \n",
        "– Анализ активаций в ключевых слоях (через `Captum`);  \n",
        "– Качественные кейсы:  \n",
        " ✓ «Модель начала избегать прямых ответов»,  \n",
        " ✓ «Увеличилась доля общих фраз при снижении фактологической точности».\n",
        "\n",
        "### **3.8. Разработка платформы для online RLHF**\n",
        "\n",
        "**Задача:** Создать инструмент для непрерывного улучшения.  \n",
        "**Функционал:**  \n",
        "– Ввод запроса;  \n",
        "– Слепой A/B-тест между 2–3 моделями;  \n",
        "– Оценка по 3 критериям + комментарий;  \n",
        "– Автоматическое сохранение в HF Dataset;  \n",
        "– Возможность ручного триггера дообучения RM.  \n",
        "**Технологии:** Gradio + Hugging Face Inference Endpoints + Datasets.\n",
        "\n",
        "### **3.9. Публикация и стандарты открытой науки**\n",
        "\n",
        "**Задача:** Обеспечить максимальную воспроизводимость.  \n",
        "**Требования:**  \n",
        "– Репозиторий: MIT-лицензия, Dockerfile, `requirements.txt`, скрипты репродюсинга;  \n",
        "– Hugging Face:  \n",
        " ✓ Dataset: `username/rush-rlhf-preferences`,  \n",
        " ✓ Models: `username/ru-rm-base`, `username/ru-rlhf-dpo`, `username/tt-rlhf-ppo`,  \n",
        " ✓ Spaces: интерактивный A/B-тест;  \n",
        "– Model Card: описание критериев, смещений, ограничений, лицензий, этических рисков;  \n",
        "– Данные на татарском — с указанием источников и разрешений.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Дополнительные исследовательские задания (уровень публикации)**\n",
        "\n",
        "1. **Constitutional AI на русском**: сгенерируйте пары с помощью правила («ответ должен быть правдивым»), обучите RM без человека.  \n",
        "2. **Перенос RM между языками**: применим ли русский RM к татарским ответам?  \n",
        "3. **Self-Play RLHF**: модель генерирует пары, оценивает сама, человек корректирует топ-ошибки.  \n",
        "4. **Анализ голов внимания в RM**: какие слои отвечают за безопасность, а какие — за полезность?  \n",
        "5. **Экономика RLHF**: оценка CO₂ и стоимости обучения PPO vs DPO (через CodeCarbon, AWS Calculator).  \n",
        "6. **RLHF + RAG**: обучите модель генерировать ответы, согласованные с извлечённым контекстом.  \n",
        "7. **Дефузия предпочтений**: можно ли декомпозировать RM на «стиль» и «содержание»?  \n",
        "8. **Калибровка RM**: используйте Platt scaling для согласования оценок RM с вероятностями экспертного выбора.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Требования к отчёту**\n",
        "\n",
        "Отчёт оформляется по **ГОСТ 7.32–2017** и должен содержать:  \n",
        "1. **Введение** — проблема alignment, ограничения SFT, роль RLHF, обзор PPO/DPO/IPO;  \n",
        "2. **Методология** — описание конвейера, критериев, архитектур, метрик, этического протокола;  \n",
        "3. **Результаты** — таблицы, графики обучения, MAUVE, UMAP, кейсы до/после, результаты A/B-теста;  \n",
        "4. **Обсуждение** — анализ компромиссов, смещений, сравнение с SOTA, ограничения;  \n",
        "5. **Заключение** — рекомендации по применению RLHF в low-resource и multilingual средах;  \n",
        "6. **Список источников** — по ГОСТ Р 7.0.5–2008, включая ключевые работы (Christiano et al. 2017, Rafailov et al. 2023);  \n",
        "7. **Приложения**:  \n",
        " – Инструкция для аннотаторов,  \n",
        " – Примеры предпочтений,  \n",
        " – Ethics Appendix,  \n",
        " – Скриншоты интерфейса.  \n",
        "\n",
        "**Обязательные ссылки:**  \n",
        "– GitHub-репозиторий,  \n",
        "– Hugging Face Dataset,  \n",
        "– Hugging Face Models (с Model Card),  \n",
        "– Hugging Face Space.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Критерии оценивания**\n",
        "\n",
        "| Оценка | Критерии |\n",
        "|--------|---------|\n",
        "| **Отлично** | Реализованы ≥3 метода (PPO, DPO, IPO); собран многокритериальный набор ≥7K пар с верификацией согласия; обучена multi-head RM; проведён многоаспектный анализ (автоматика + эксперты + поведение); реализован online RLHF; опубликованы все компоненты с Ethics Appendix; отчёт содержит обсуждение смещений, переоптимизации и рекомендации для low-resource языков. |\n",
        "| **Хорошо** | Реализованы PPO и DPO; набор ≥3K; базовая RM; сравнение по автоматическим и экспертным метрикам; отчёт с визуализациями и обсуждением рисков. |\n",
        "| **Удовлетворительно** | Реализован DPO на публичных или синтетических данных; базовое сравнение; отчёт с описанием метода. |\n",
        "| **Неудовлетворительно** | Отсутствует сбор предпочтений или обучение RM; нет сравнения с SFT; не учтены этические аспекты. |\n"
      ],
      "metadata": {
        "id": "YApP0i7rWzMN"
      }
    }
  ]
}