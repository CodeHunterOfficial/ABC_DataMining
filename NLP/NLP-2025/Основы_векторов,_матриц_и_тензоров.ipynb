{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKi6pwHBaU8+XdOi0ngR4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2%2C_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86_%D0%B8_%D1%82%D0%B5%D0%BD%D0%B7%D0%BE%D1%80%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **1. Основы векторов, матриц и тензоров**\n",
        "\n",
        "#### **Векторы**\n",
        "- Определение вектора: понятие, компоненты и размерность.\n",
        "- Вектор как представление слова (word embedding) в задачах НЛП.\n",
        "- Основные операции над векторами: сложение, вычитание, умножение на скаляр.\n",
        "- Длина вектора (норма): определение и значение в контексте сравнения векторов.\n",
        "\n",
        "#### **Матрицы**\n",
        "- Определение матрицы и её размерность (m × n).\n",
        "- Матрица как контейнер для хранения векторов слов или весов модели.\n",
        "- Основные операции: сложение матриц, умножение матрицы на вектор, умножение матриц, транспонирование.\n",
        "- Пример применения: **матрица внимания (Attention Matrix)** — ключевой элемент архитектуры трансформеров.\n",
        "\n",
        "#### **Тензоры**\n",
        "- Определение тензора как обобщения векторов (1D) и матриц (2D) на более высокие измерения.\n",
        "- Роль тензоров в НЛП: работа с многомерными данными (например, батчи последовательностей, скрытые состояния моделей).\n",
        "\n",
        "\n",
        "### **2. Меры сходства и расстояния**\n",
        "\n",
        "#### **Косинусное сходство (Cosine Similarity)**\n",
        "- Определение и формула:  \n",
        "$$\n",
        "  \\text{cos}(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\cdot \\|\\mathbf{B}\\|}\n",
        "$$\n",
        "- Применение в НЛП: оценка семантической близости слов, предложений или документов.\n",
        "\n",
        "#### **Евклидово расстояние (Euclidean Distance)**\n",
        "- Определение и формула:  \n",
        "$$\n",
        "  d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n",
        "  $$\n",
        "- Сравнение с косинусным сходством:  \n",
        "  - Евклидово расстояние учитывает величину векторов.  \n",
        "  - Косинусное сходство фокусируется на направлении (угле между векторами).  \n",
        "- Примеры использования: кластеризация, поиск ближайших соседей.\n",
        "\n",
        "\n",
        "### **3. Ключевые концепции для оптимизации и уменьшения размерности**\n",
        "\n",
        "#### **Сингулярное разложение (SVD — Singular Value Decomposition)**\n",
        "- Разложение матрицы: $A = U \\Sigma V^T$\n",
        "- Интерпретация компонент: левые и правые сингулярные векторы, сингулярные значения.\n",
        "- Применение: анализ латентных признаков, снижение размерности, LSA (Latent Semantic Analysis).\n",
        "\n",
        "#### **Собственные значения и собственные векторы (Eigenvalues and Eigenvectors)**\n",
        "- Определение: $A \\mathbf{v} = \\lambda \\mathbf{v}$\n",
        "- Геометрический смысл: направления, сохраняемые при линейном преобразовании.\n",
        "- Роль в методах анализа данных, включая PCA.\n",
        "\n",
        "#### **Метод главных компонент (PCA — Principal Component Analysis)**\n",
        "- Цель: уменьшение размерности данных с сохранением максимальной дисперсии.\n",
        "- Этапы: центрирование данных, вычисление ковариационной матрицы, нахождение собственных векторов.\n",
        "- Применение: визуализация высокоразмерных данных, предобработка для моделей.\n",
        "\n",
        "#### **Матричная факторизация (Matrix Factorization)**\n",
        "- Общее понятие: разложение матрицы на произведение двух (или более) матриц меньшей размерности.\n",
        "- Примеры: SVD, NMF (Non-negative Matrix Factorization).\n",
        "- Применение:\n",
        "  - Рекомендательные системы (например, алгоритмы коллаборативной фильтрации).\n",
        "  - Тематическое моделирование (например, LSA).\n",
        "  - Представление скрытых признаков в данных.\n"
      ],
      "metadata": {
        "id": "6fcQ4tKs9pO6"
      }
    }
  ]
}