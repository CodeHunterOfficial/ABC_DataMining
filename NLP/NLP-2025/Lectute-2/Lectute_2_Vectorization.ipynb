{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOJO1aNL9RCiNWP5XtcJCz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/Lectute-2/Lectute_2_Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Глава 2. Управление данными и векторизация в NLP\n",
        "\n",
        "В этой главе мы погрузимся в критически важные аспекты подготовки текстовых данных для задач обработки естественного языка (NLP). Эффективное управление данными и их преобразование в числовой формат являются основой для успешного обучения любой NLP-модели. Мы рассмотрим методы сбора, разметки, аугментации данных, а также способы работы с несбалансированными выборками и предотвращения утечки данных. Особое внимание будет уделено различным методам векторизации текста — от простых подходов до современных векторных представлений слов.\n",
        "\n",
        "\n",
        "\n",
        "## §1. Сбор текстовых данных: Полное руководство\n",
        "\n",
        "Сбор текстовых данных является краеугольным камнем любого проекта в области обработки естественного языка (NLP). Качество, релевантность, объём и репрезентативность собранных данных напрямую влияют на успех последующих этапов — обучения, валидации и применения NLP-моделей. Этот процесс требует тщательного планирования и включает определение подходящих источников информации, выбор эффективных методов её извлечения и принятие решения о наиболее оптимальных форматах для хранения, предобработки и анализа.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Источники данных\n",
        "\n",
        "Выбор источников текстовых данных должен быть тесно связан с конкретными целями и задачами вашего NLP-проекта. Разнообразие доступных источников позволяет адаптировать подход к сбору данных под широкий спектр применений.\n",
        "\n",
        "#### 1.1. Веб-страницы  \n",
        "Интернет представляет собой колоссальное и постоянно растущее хранилище текстовой информации, доступное для сбора.\n",
        "\n",
        "- **Новостные порталы и блоги**:  \n",
        "  Эти источники предоставляют актуальную информацию о текущих событиях, общественном мнении и трендах. Они идеально подходят для обучения систем суммаризации текста, анализа новостного потока, определения тональности и извлечения ключевых сущностей. Язык здесь обычно формальный или полуформальный.\n",
        "\n",
        "- **Форумы и социальные сети**:  \n",
        "  Эти платформы являются богатейшим источником неформального языка, сленга, диалектов, пользовательских мнений и интерактивных дискуссий. Они незаменимы для задач анализа настроений (sentiment analysis), обнаружения спама, изучения особенностей разговорной речи, а также для создания и обучения чат-ботов и систем поддержки клиентов. При работе с данными из социальных сетей крайне важно учитывать строгие ограничения, налагаемые API платформ, и неукоснительно соблюдать политику конфиденциальности пользователей и законодательство о защите персональных данных.\n",
        "\n",
        "- **Электронные книги и научные статьи**:  \n",
        "  Эти источники характеризуются высокой степенью структурированности, качеством текста и часто проходят рецензирование. Они идеально подходят для задач извлечения информации (Information Extraction), построения обширных баз знаний, а также для обучения моделей, предназначенных для специализированных доменов (например, медицина, юриспруденция, инженерия), где требуется глубокое понимание специфической терминологии и концепций.\n",
        "\n",
        "- **Сайты электронной коммерции (отзывы, описания товаров)**:  \n",
        "  Отзывы покупателей, подробные описания продуктов, вопросы и ответы могут быть использованы для разработки систем рекомендаций, глубокого анализа потребительского поведения, автоматической классификации товаров, выявления часто встречающихся проблем или преимуществ продуктов.\n",
        "\n",
        "\n",
        "#### 1.2. Базы данных  \n",
        "Корпоративные и частные базы данных часто содержат уникальные и конфиденциальные текстовые данные, которые, как правило, не доступны публично.\n",
        "\n",
        "- **Записи клиентской поддержки**:  \n",
        "  Включают логи диалогов с чат-ботами, электронные письма от клиентов и транскрипции телефонных разговоров. Эти данные могут быть использованы для автоматизации ответов на часто задаваемые вопросы (FAQ), повышения качества обслуживания клиентов, выявления типичных проблем и улучшения общего пользовательского опыта.\n",
        "\n",
        "- **Отчёты и внутренняя документация**:  \n",
        "  Эти источники содержат специфическую терминологию, внутренний жаргон и уникальную структуру, что часто требует разработки специализированных NLP-моделей, адаптированных под конкретный домен или организацию. Примеры: юридические документы, финансовые отчёты, технические спецификации.\n",
        "\n",
        "- **Медицинские записи**:  \n",
        "  Анамнезы, результаты лабораторных исследований, заключения врачей и истории болезни являются чрезвычайно ценными данными для медицинского NLP. Однако их использование требует строжайшего соблюдения международных и локальных правил конфиденциальности и защиты персональных данных (например, HIPAA в США, GDPR в ЕС), а также часто требует деидентификации данных.\n",
        "\n",
        "\n",
        "#### 1.3. API (Application Programming Interface)  \n",
        "Многие онлайн-сервисы и платформы предоставляют программные интерфейсы, которые позволяют разработчикам получать доступ к их данным в структурированном, контролируемом и стандартизированном виде. Использование API является наиболее предпочтительным и эффективным методом сбора данных.\n",
        "\n",
        "**Принцип работы**:  \n",
        "API предоставляет набор предопределённых правил и протоколов (например, REST, GraphQL) для взаимодействия с сервисом. Вы отправляете запросы в определённом формате (часто JSON или XML) и получаете структурированный ответ, который легко парсится и обрабатывается.\n",
        "\n",
        "**Преимущества API**:\n",
        "- Более надёжны, так как специально разработаны для программного доступа.\n",
        "- Менее подвержены сбоям при изменениях дизайна сайта.\n",
        "- Данные предоставляются в уже очищенном и хорошо структурированном виде.\n",
        "- Являются более этичным и законным способом получения данных, поскольку вы действуете в рамках правил, установленных провайдером (соблюдение лимитов запросов, условий использования).\n",
        "\n",
        "**Примеры**:\n",
        "- **Twitter API**: Позволяет собирать твиты по ключевым словам, пользователям, хэштегам или геолокации. Идеален для анализа трендов, мониторинга общественного мнения и анализа настроений в реальном времени. Включает ограничения по количеству запросов (rate limits) и объёму данных.\n",
        "- **Reddit API**: Предоставляет доступ к постам и комментариям из различных сабреддитов, что полезно для изучения сообществ, дискуссий и выявления популярных тем.\n",
        "- **Wikipedia API**: Позволяет извлекать статьи, их разделы, ссылки и метаданные. Полезен для создания энциклопедических баз знаний, обучения моделей на структурированном тексте и задач извлечения фактов.\n",
        "- **Google Books Ngram API**: Предоставляет данные о частоте встречаемости N-грамм в огромном корпусе книг, опубликованных за несколько столетий. Бесценный ресурс для лингвистических исследований, анализа эволюции языка и культурных особенностей.\n",
        "\n",
        "\n",
        "\n",
        "#### 1.4. Публичные датасеты  \n",
        "Существует множество готовых, часто уже размеченных датасетов, созданных для исследовательских целей, бенчмарков и обучения NLP-моделей.\n",
        "\n",
        "Использование таких датасетов позволяет значительно ускорить начало работы над проектом, минуя трудоёмкие этапы сбора и первичной разметки данных. Они часто хорошо документированы и уже прошли определённую очистку.\n",
        "\n",
        "**Примеры**:\n",
        "- **IMDB reviews**: Широко используемый датасет для задач анализа настроений, содержащий тысячи отзывов о фильмах с бинарными метками «положительный» или «отрицательный».\n",
        "- **SQuAD (Stanford Question Answering Dataset)**: Датасет для задач ответов на вопросы, где модель должна найти точный ответ на вопрос в предоставленном контекстном тексте.\n",
        "- **CoNLL (Conference on Natural Language Learning)**: Серии датасетов, используемых для различных задач, включая распознавание именованных сущностей (NER) и синтаксический анализ.\n",
        "- **Common Crawl**: Огромный открытый веб-корпус, содержащий миллиарды веб-страниц, который может использоваться для предварительного обучения больших языковых моделей и создания общих языковых представлений.\n",
        "\n",
        "**Лицензирование**:  \n",
        "При использовании публичных датасетов всегда проверяйте условия лицензирования (например, MIT, Apache, Creative Commons), чтобы убедиться, что их использование соответствует вашим целям (коммерческим или некоммерческим).\n",
        "\n",
        "\n",
        "#### 1.5. Корпусы текстов  \n",
        "Это обширные, часто специально собранные, очищенные и аннотированные коллекции текстов, предназначенные для лингвистических исследований, разработки NLP-систем и обучения языковых моделей.\n",
        "\n",
        "- **Национальный корпус русского языка (НКРЯ)**:  \n",
        "  Один из крупнейших корпусов русского языка, содержащий тексты различных жанров и эпох, тщательно размеченные по морфологии, синтаксису и семантике.\n",
        "\n",
        "- **Google Books Ngram Corpus**:  \n",
        "  Коллекция N-грамм из миллионов книг, опубликованных за несколько столетий, позволяет отслеживать изменения в языке и культурных особенностях.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fW43QEaTyhBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Методы сбора\n",
        "\n",
        "Выбор метода сбора данных определяется источником, структурой данных, их объёмом, а также доступными техническими ресурсами и этическими соображениями.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1. Веб-скрейпинг (Web Scraping)\n",
        "\n",
        "Процесс автоматического извлечения данных с веб-сайтов. Это мощный инструмент, особенно когда отсутствует доступный API или данные представлены в неструктурированном HTML-формате.\n",
        "\n",
        "**Принцип работы**:  \n",
        "Скрейпинг включает отправку HTTP-запросов к веб-серверу для получения HTML-кода страницы. После получения HTML-кода он парсится (анализируется) для извлечения необходимой информации с помощью специализированных инструментов.\n",
        "\n",
        "**Основные шаги**:\n",
        "1. **Отправка HTTP-запроса**:  \n",
        "   Использование библиотеки `requests` для получения HTML-содержимого страницы.\n",
        "2. **Парсинг HTML**:  \n",
        "   Анализ структуры HTML-документа для нахождения нужных элементов. Для этого используются:\n",
        "   - **BeautifulSoup**: Простая и гибкая библиотека для извлечения данных из HTML и XML. Позволяет искать элементы по тегам, классам, ID и другим атрибутам.\n",
        "   - **lxml**: Более быстрая и мощная библиотека для работы с XML и HTML, поддерживающая XPath и CSS-селекторы.\n",
        "   - **XPath / CSS-селекторы**: Языки запросов для навигации по структуре документа и выбора конкретных элементов.\n",
        "3. **Извлечение данных**:  \n",
        "   Получение текстового содержимого, ссылок, атрибутов из найденных элементов.\n",
        "4. **Сохранение**:  \n",
        "   Запись извлечённых данных в выбранный формат (JSON, CSV, TXT и т.д.).\n",
        "\n",
        "**Обработка динамического контента (JavaScript)**:  \n",
        "Многие современные веб-сайты загружают контент асинхронно с помощью JavaScript. Обычные HTTP-запросы в этом случае не возвращают полное содержимое страницы. Для таких случаев используются:\n",
        "- **Selenium**: Автоматизирует управление полноценным веб-браузером (Chrome, Firefox), позволяя имитировать действия пользователя (клики, прокрутка, ввод текста) и дожидаться загрузки динамического контента.\n",
        "- **Playwright / Puppeteer**: Более современные и быстрые headless-браузеры, которые также позволяют программно управлять браузером и получать доступ к DOM после выполнения JavaScript.\n",
        "\n",
        "**Борьба с анти-скрейпинг мерами**:  \n",
        "Веб-сайты часто используют различные методы для предотвращения автоматизированного сбора данных:\n",
        "- **Rate Limiting**: Ограничение количества запросов с одного IP-адреса за определённый период.  \n",
        "  *Решение*: использование задержек между запросами (`time.sleep()`), ротация IP-адресов (прокси-серверы).\n",
        "- **CAPTCHA**: Проверка, является ли пользователь человеком.  \n",
        "  *Решение*: использование сервисов для автоматического распознавания CAPTCHA (хотя это может быть этически спорно).\n",
        "- **User-Agent / Headers**: Блокировка запросов с подозрительными заголовками.  \n",
        "  *Решение*: имитация заголовков реальных браузеров.\n",
        "- **Изменение структуры HTML**: Регулярные изменения в разметке сайта, требующие постоянного обновления скриптов.\n",
        "\n",
        "**Этические и юридические аспекты**:  \n",
        "Крайне важно строго соблюдать правила использования веб-сайтов, которые часто указываются в файлах `robots.txt` (информирующих автоматических ботов о разрешённых и запрещённых разделах сайта) и пользовательских соглашениях. Также необходимо учитывать законодательство об авторском праве и защите персональных данных (например, GDPR в ЕС, CCPA в США), особенно если собираются данные, содержащие личную информацию. Несанкционированный скрейпинг может привести к юридическим последствиям.\n",
        "\n",
        "\n",
        "\n",
        "### 2.2. Использование API (Application Programming Interface)\n",
        "\n",
        "Наиболее предпочтительный, структурированный и контролируемый способ получения данных от онлайн-сервисов.\n",
        "\n",
        "**Принцип работы**:  \n",
        "API предоставляет набор предопределённых правил и протоколов (например, RESTful API, GraphQL API) для взаимодействия с сервисом. Вы отправляете запросы в определённом формате (часто JSON или XML) и получаете структурированный ответ, который легко парсится и обрабатывается.\n",
        "\n",
        "API специально разработаны для программного доступа и менее подвержены сбоям из-за изменений в дизайне или структуре сайта. Данные обычно предоставляются в уже очищенном и хорошо структурированном виде, что значительно упрощает их дальнейшую обработку. Использование API часто является более этичным и законным способом получения данных, так как вы действуете в рамках правил, установленных провайдером (соблюдение лимитов запросов, условий использования).\n",
        "\n",
        "**Ключевые аспекты работы с API**:\n",
        "- **Аутентификация**: Многие API требуют аутентификации (например, API-ключи, OAuth-токены) для доступа к данным.\n",
        "- **Лимиты запросов (Rate Limits)**: Большинство API ограничивают количество запросов за определённый период. Необходимо реализовать логику для соблюдения этих лимитов (например, задержки, повторные попытки с экспоненциальной выдержкой).\n",
        "- **Пагинация**: Для больших объёмов данных API часто используют пагинацию — возвращают данные частями. Необходимо реализовать логику для запроса всех страниц.\n",
        "- **Обработка ошибок**: Важно обрабатывать различные HTTP-коды ошибок (например, 403 Forbidden, 404 Not Found, 429 Too Many Requests, 500 Internal Server Error).\n",
        "- **Специализированные SDK**: Многие крупные сервисы предоставляют официальные клиентские библиотеки (SDK) для различных языков программирования, которые упрощают взаимодействие с API, абстрагируя детали HTTP-запросов и парсинга ответов.\n",
        "\n",
        "\n",
        "\n",
        "### 2.3. Прямое скачивание\n",
        "\n",
        "Для публичных датасетов и корпусов, а также для данных, полученных от третьих сторон или исследовательских организаций, часто предусмотрена возможность прямого скачивания.\n",
        "\n",
        "**Форматы**:  \n",
        "Данные обычно доступны в виде сжатых архивов (ZIP, GZ, TAR.GZ, 7z), содержащих один или несколько текстовых файлов или файлов в структурированных форматах, таких как CSV, JSON, XML.\n",
        "\n",
        "Этот метод является самым простым, так как не требует написания сложного кода для извлечения данных — только для их распаковки и загрузки в память для дальнейшей обработки.\n",
        "\n",
        "**Источники**:  \n",
        "Крупные репозитории данных, такие как Kaggle, Hugging Face Datasets, UCI Machine Learning Repository, а также сайты исследовательских групп и университетов.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Форматы хранения текстовых данных\n",
        "\n",
        "Выбор формата хранения собранных текстовых данных — важное решение, которое зависит от их внутренней структуры, объёма, требований к производительности при чтении/записи, а также от того, как данные будут использоваться в последующих этапах NLP-пайплайна.\n",
        "\n",
        "\n",
        "\n",
        "### 3.1. TXT (Plain Text)\n",
        "\n",
        "Самый простой формат для хранения чистого текста без дополнительной разметки или структуры. Файл содержит последовательность символов. Каждая строка может представлять собой отдельный документ, предложение или абзац — в зависимости от принятого соглашения.\n",
        "\n",
        "**Преимущества**:\n",
        "- Максимально прост и универсален.\n",
        "- Читается любым текстовым редактором.\n",
        "- Легко обрабатывается любым языком программирования.\n",
        "- Занимает минимальный объём дискового пространства (при отсутствии метаданных).\n",
        "- Идеально подходит для хранения очень больших объёмов текста, где каждый файл или каждая строка (при построчном разделении) представляет собой отдельный документ или фрагмент.\n",
        "\n",
        "**Недостатки**:\n",
        "- Не позволяет хранить метаданные (автор, дата, категория) напрямую внутри файла.\n",
        "- Не поддерживает иерархические или сложные структуры данных.\n",
        "\n",
        "**Решение**: Если метаданные необходимы, их приходится хранить в отдельных файлах или использовать соглашения об именовании файлов и папок.\n",
        "\n",
        "**Пример работы с TXT-файлом (Python)**:\n"
      ],
      "metadata": {
        "id": "huLl50i87yNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание TXT-файла\n",
        "with open(\"document_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"Это первый документ, посвященный основам обработки естественного языка.\\n\")\n",
        "    f.write(\"Он содержит важную информацию для студентов.\\n\")\n",
        "\n",
        "# Чтение TXT-файла\n",
        "with open(\"document_1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "    print(\"Содержимое TXT-файла:\")\n",
        "    print(content)"
      ],
      "metadata": {
        "id": "wGaLqWgl70xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.2. CSV (Comma Separated Values)\n",
        "\n",
        "Простой текстовый формат, в котором значения разделены запятыми (или другими разделителями, такими как точка с запятой, табуляция). Подходит для табличных данных. Каждая строка в файле представляет собой одну запись, а поля в этой записи разделяются определённым символом-разделителем. Первая строка часто содержит заголовки столбцов.\n",
        "\n",
        "CSV-файлы широко поддерживаются практически всеми программами для работы с данными (табличные редакторы, базы данных) и библиотеками (например, Pandas в Python), что делает их удобными для обмена данными. Они легко читаются человеком и эффективны для хранения больших объёмов данных с простой, плоской, табличной структурой.\n",
        "\n",
        "Однако CSV плохо подходит для иерархических или вложенных данных. Возникают трудности при обработке текстовых полей, содержащих символы-разделители (например, запятые в тексте), что требует использования кавычек для экранирования или изменения разделителя. Также существует множество «диалектов» CSV, что иногда приводит к проблемам с парсингом.\n",
        "\n",
        "**Пример работы с CSV (Python)**:\n",
        "\n"
      ],
      "metadata": {
        "id": "d20devPO9clc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Данные для записи\n",
        "data = [\n",
        "    {\n",
        "        \"id\": \"doc1\",\n",
        "        \"text\": \"Это первый документ, посвященный NLP и векторизации. Он очень важен.\",\n",
        "        \"category\": \"Технологии\",\n",
        "        \"tags\": \"NLP;векторизация;данные\",\n",
        "        \"date_published\": \"2023-01-15\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc2\",\n",
        "        \"text\": \"Второй документ содержит информацию о сборе данных, включая веб-скрейпинг.\",\n",
        "        \"category\": \"Исследования\",\n",
        "        \"tags\": \"сбор данных;веб-скрейпинг\",\n",
        "        \"date_published\": \"2023-02-20\"\n",
        "    }\n",
        "]\n",
        "fieldnames = [\"id\", \"text\", \"category\", \"tags\", \"date_published\"]\n",
        "\n",
        "# Создание CSV-файла\n",
        "with open(\"documents.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=',')\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "print(\"CSV-файл успешно создан.\")\n",
        "\n",
        "# Чтение CSV-файла\n",
        "with open(\"documents.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter=',')\n",
        "    print(\"Содержимое CSV-файла:\")\n",
        "    for row in reader:\n",
        "        print(row)"
      ],
      "metadata": {
        "id": "YC10Hnxc9fxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### 3.3. JSON (JavaScript Object Notation)\n",
        "\n",
        "Лёгкий, удобочитаемый и широко используемый формат обмена данными, идеально подходящий для хранения структурированных данных. JSON основан на двух базовых структурах: объектах (неупорядоченная коллекция пар «ключ-значение») и массивах (упорядоченная последовательность значений).\n",
        "\n",
        "JSON-файлы чрезвычайно гибки, так как поддерживают вложенные структуры. Это позволяет хранить текстовые фрагменты вместе с их разнообразными метаданными (например, уникальный идентификатор документа, автор, дата публикации, категория, список тегов, геоданные). Формат широко поддерживается, легко парсится и генерируется в большинстве современных языков программирования и активно используется в веб-разработке и API. Относительно прост в восприятии человеком.\n",
        "\n",
        "Однако для очень больших файлов, которые не помещаются в оперативную память, чтение всего JSON-файла целиком может быть неэффективным или невозможным, так как он должен быть загружен полностью для корректного парсинга. Кроме того, JSON может быть более «многословным» по сравнению с бинарными форматами, что приводит к увеличению размера файла.\n",
        "\n",
        "**Пример работы с JSON (Python)**:\n",
        "\n"
      ],
      "metadata": {
        "id": "bi0hnv9i9f8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Данные для записи\n",
        "data = [\n",
        "    {\n",
        "        \"id\": \"news_article_001\",\n",
        "        \"title\": \"Новые достижения в области ИИ\",\n",
        "        \"text\": \"Исследователи объявили о прорыве в разработке самообучающихся алгоритмов. Это открывает новые перспективы.\",\n",
        "        \"metadata\": {\n",
        "            \"category\": \"Технологии\",\n",
        "            \"keywords\": [\"ИИ\", \"машинное обучение\", \"алгоритмы\"],\n",
        "            \"publication_date\": \"2024-07-23\",\n",
        "            \"author\": {\n",
        "                \"name\": \"Анна Иванова\",\n",
        "                \"affiliation\": \"Университет XYZ\"\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blog_post_005\",\n",
        "        \"title\": \"Как начать изучать NLP\",\n",
        "        \"text\": \"Для новичков в NLP важно начать с основ обработки текста. Это поможет заложить прочный фундамент.\",\n",
        "        \"metadata\": {\n",
        "            \"category\": \"Образование\",\n",
        "            \"keywords\": [\"NLP\", \"учебник\", \"старт\"],\n",
        "            \"publication_date\": \"2024-07-20\",\n",
        "            \"author\": {\n",
        "                \"name\": \"Иван Петров\",\n",
        "                \"affiliation\": \"Блог NLP-Эксперт\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Создание JSON-файла\n",
        "with open(\"documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "print(\"JSON-файл успешно создан.\")\n",
        "\n",
        "# Чтение JSON-файла\n",
        "with open(\"documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    loaded_data = json.load(f)\n",
        "    print(\"Содержимое JSON-файла:\")\n",
        "    for item in loaded_data:\n",
        "        print(item)"
      ],
      "metadata": {
        "id": "-5TaLFOz8RFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.4. JSON Lines (JSONL или LDJSON)\n",
        "\n",
        "Формат, в котором каждая строка файла является отдельным, самодостаточным JSON-объектом, разделённым символом новой строки. В отличие от стандартного JSON, который обычно представляет собой один большой массив объектов, JSONL — это последовательность JSON-объектов, каждый из которых находится на новой строке.\n",
        "\n",
        "Этот формат идеально подходит для потоковой обработки очень больших наборов данных, поскольку каждую строку можно читать и обрабатывать построчно, не загружая весь файл в память целиком. Это делает его особенно эффективным для работы с гигантскими текстовыми корпусами, которые не помещаются в оперативную память.\n",
        "\n",
        "JSONL также удобен для добавления новых записей в конец файла без необходимости перезаписывать весь файл, а также легко распараллеливается, так как каждая строка может быть обработана независимо.\n",
        "\n",
        "Однако JSONL не может быть распарсен стандартным JSON-парсером как единый документ — требуется построчное чтение и парсинг каждой строки отдельно. Кроме того, у него отсутствует единый корневой элемент.\n",
        "\n",
        "**Пример работы с JSONL (Python)**:\n"
      ],
      "metadata": {
        "id": "wHmfxzxI9vym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Данные для записи\n",
        "data_lines = [\n",
        "    {\"id\": \"review_001\", \"text\": \"Отличный продукт, очень доволен его качеством!\", \"sentiment\": \"positive\", \"rating\": 5},\n",
        "    {\"id\": \"review_002\", \"text\": \"Доставка заняла слишком много времени. Это расстроило.\", \"sentiment\": \"negative\", \"rating\": 2},\n",
        "    {\"id\": \"review_003\", \"text\": \"В целом неплохо, но есть куда расти. Могло быть лучше.\", \"sentiment\": \"neutral\", \"rating\": 3}\n",
        "]\n",
        "\n",
        "# Создание JSONL-файла\n",
        "with open(\"reviews.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in data_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "print(\"JSONL-файл успешно создан.\")\n",
        "\n",
        "# Чтение JSONL-файла\n",
        "print(\"Содержимое JSONL-файла:\")\n",
        "with open(\"reviews.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line.strip())\n",
        "        print(item)"
      ],
      "metadata": {
        "id": "YkYADzWwAqy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### 3.5. XML (Extensible Markup Language)\n",
        "\n",
        "Язык разметки, предназначенный для хранения и передачи структурированных данных. XML использует теги (аналогичные HTML) для определения элементов данных и их иерархической структуры. Каждый документ имеет корневой элемент, а данные вкладываются в теги.\n",
        "\n",
        "XML отлично подходит для сложных, иерархических данных, где важна строгая структура. Пользователи могут определять собственные теги, что делает формат очень гибким. XML-документы могут быть валидированы с помощью DTD (Document Type Definition) или XML Schema, что обеспечивает соответствие данных заданной структуре.\n",
        "\n",
        "Формат широко используется в лингвистике — многие лингвистические корпусы и стандарты (например, TEI — Text Encoding Initiative) применяют XML для аннотации текста.\n",
        "\n",
        "**Недостатки**:\n",
        "- XML-файлы часто значительно больше по размеру, чем их JSON-эквиваленты, из-за повторяющихся тегов.\n",
        "- Парсинг XML может быть сложнее, чем парсинг JSON, особенно для новичков.\n",
        "- Из-за многословности и сложности обработка больших XML-файлов может быть медленнее.\n",
        "\n",
        "**Пример работы с XML (Python)**:\n"
      ],
      "metadata": {
        "id": "jsQPcU3PAq_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Создание корневого элемента\n",
        "root = ET.Element(\"documents\")\n",
        "\n",
        "# Добавление первого документа\n",
        "doc1 = ET.SubElement(root, \"document\", id=\"news_article_001\")\n",
        "ET.SubElement(doc1, \"title\").text = \"Новые достижения в области ИИ\"\n",
        "ET.SubElement(doc1, \"text\").text = \"Исследователи объявили о прорыве в разработке самообучающихся алгоритмов.\"\n",
        "metadata1 = ET.SubElement(doc1, \"metadata\")\n",
        "ET.SubElement(metadata1, \"category\").text = \"Технологии\"\n",
        "keywords1 = ET.SubElement(metadata1, \"keywords\")\n",
        "ET.SubElement(keywords1, \"keyword\").text = \"ИИ\"\n",
        "ET.SubElement(keywords1, \"keyword\").text = \"машинное обучение\"\n",
        "ET.SubElement(keywords1, \"keyword\").text = \"алгоритмы\"\n",
        "ET.SubElement(metadata1, \"publication_date\").text = \"2024-07-23\"\n",
        "author1 = ET.SubElement(metadata1, \"author\")\n",
        "ET.SubElement(author1, \"name\").text = \"Анна Иванова\"\n",
        "ET.SubElement(author1, \"affiliation\").text = \"Университет XYZ\"\n",
        "\n",
        "# Добавление второго документа\n",
        "doc2 = ET.SubElement(root, \"document\", id=\"blog_post_005\")\n",
        "ET.SubElement(doc2, \"title\").text = \"Как начать изучать NLP\"\n",
        "ET.SubElement(doc2, \"text\").text = \"Для новичков в NLP важно начать с основ обработки текста.\"\n",
        "metadata2 = ET.SubElement(doc2, \"metadata\")\n",
        "ET.SubElement(metadata2, \"category\").text = \"Образование\"\n",
        "keywords2 = ET.SubElement(metadata2, \"keywords\")\n",
        "ET.SubElement(keywords2, \"keyword\").text = \"NLP\"\n",
        "ET.SubElement(keywords2, \"keyword\").text = \"учебник\"\n",
        "ET.SubElement(keywords2, \"keyword\").text = \"старт\"\n",
        "ET.SubElement(metadata2, \"publication_date\").text = \"2024-07-20\"\n",
        "author2 = ET.SubElement(metadata2, \"author\")\n",
        "ET.SubElement(author2, \"name\").text = \"Иван Петров\"\n",
        "ET.SubElement(author2, \"affiliation\").text = \"Блог NLP-Эксперт\"\n",
        "\n",
        "# Создание XML-дерева и запись в файл\n",
        "tree = ET.ElementTree(root)\n",
        "ET.indent(tree, space=\"  \", level=0)  # Для красивого форматирования\n",
        "tree.write(\"documents.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "print(\"XML-файл успешно создан.\")\n",
        "\n",
        "# Чтение XML-файла\n",
        "tree = ET.parse(\"documents.xml\")\n",
        "root = tree.getroot()\n",
        "print(\"Содержимое XML-файла:\")\n",
        "for doc in root.findall('document'):\n",
        "    doc_id = doc.get('id')\n",
        "    title = doc.find('title').text\n",
        "    text = doc.find('text').text\n",
        "    category = doc.find('metadata/category').text\n",
        "    keywords = [kw.text for kw in doc.findall('metadata/keywords/keyword')]\n",
        "    print(f\"ID: {doc_id}, Title: {title}, Text: {text[:50]}..., Category: {category}, Keywords: {keywords}\")"
      ],
      "metadata": {
        "id": "EwZz6U-DAyBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### 3.6. Бинарные колоночные форматы (Parquet, ORC)\n",
        "\n",
        "Эти форматы предназначены для эффективного хранения и обработки очень больших объёмов данных в распределённых системах (например, Apache Spark, Hadoop). В отличие от построчного хранения, они хранят данные **по столбцам** — все значения одного столбца хранятся вместе.\n",
        "\n",
        "Они включают встроенную схему и метаданные, используют эффективные алгоритмы сжатия (что значительно уменьшает размер файлов) и обеспечивают высокую производительность при аналитических запросах, выбирающих подмножество столбцов (не нужно читать весь файл).\n",
        "\n",
        "Форматы идеально подходят для экосистем больших данных и ETL-процессов, обеспечивая строгую схему и высокую скорость чтения.\n",
        "\n",
        "**Недостатки**:\n",
        "- Более сложны в использовании по сравнению с текстовыми форматами.\n",
        "- Непосредственно нечитаемы человеком.\n",
        "- Не предназначены для частой дозаписи отдельных записей.\n",
        "\n",
        "В NLP такие форматы могут использоваться для хранения больших таблиц, где один из столбцов содержит текст, а другие — метаданные или признаки, извлечённые из текста.\n",
        "\n"
      ],
      "metadata": {
        "id": "I7YA8ym4AyL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Создаем Spark-сессию\n",
        "spark = SparkSession.builder.appName(\"ParquetExample\").getOrCreate()\n",
        "\n",
        "# Определяем схему данных\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"text\", StringType(), True),\n",
        "    StructField(\"label\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Пример данных для записи\n",
        "data = [(1, \"Пример текста для NLP\", 0),\n",
        "        (2, \"Еще один текст\", 1)]\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Записываем в Parquet\n",
        "df.write.parquet(\"text_data.parquet\")\n",
        "\n",
        "# Читаем Parquet-файл\n",
        "parquet_df = spark.read.parquet(\"text_data.parquet\")\n",
        "parquet_df.show()"
      ],
      "metadata": {
        "id": "mAvPr6IPBJ35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение Parquet-файла в Pandas"
      ],
      "metadata": {
        "id": "9hA7LKIZBf--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Чтение Parquet-файла (используется библиотека pyarrow)\n",
        "df = pd.read_parquet(\"text_data.parquet\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "TmGWiENZBeXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 3.7. Базы данных (SQL / NoSQL)\n",
        "\n",
        "Для хранения текстовых данных могут использоваться как традиционные, так и нереляционные базы данных.\n",
        "\n",
        "#### SQL (реляционные базы данных):\n",
        "Текст может храниться в столбцах типа `VARCHAR`, `TEXT` или `NVARCHAR`.  \n",
        "**Преимущества**:\n",
        "- Строгая схема данных.\n",
        "- Поддержка транзакций.\n",
        "- Мощные возможности запросов (SQL).\n",
        "\n",
        "**Недостатки**:\n",
        "- Могут быть неоптимальны для очень больших неструктурированных текстов.\n",
        "- Сложно адаптировать под часто меняющиеся схемы.\n",
        "\n",
        "**Примеры**: PostgreSQL, MySQL, SQLite.\n"
      ],
      "metadata": {
        "id": "PbMBgUiCBKCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Подключение к базе данных SQLite (файл создается автоматически)\n",
        "conn = sqlite3.connect(\"nlp_db.sqlite\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Создание таблицы для хранения текстов\n",
        "cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS texts (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        content TEXT,\n",
        "        label INTEGER\n",
        "    )\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "# Вставка данных\n",
        "cursor.execute(\n",
        "    \"INSERT INTO texts (content, label) VALUES (?, ?)\",\n",
        "    (\"Пример текста из SQLite\", 1)\n",
        ")\n",
        "conn.commit()\n",
        "\n",
        "# Чтение данных\n",
        "cursor.execute(\"SELECT * FROM texts\")\n",
        "rows = cursor.fetchall()\n",
        "for row in rows:\n",
        "    print(row)\n",
        "\n",
        "# Закрытие соединения\n",
        "cursor.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "Unt8Y1fIBMnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### NoSQL (нереляционные базы данных):\n",
        "\n",
        "- **Документоориентированные базы данных** (например, MongoDB, Couchbase):  \n",
        "  Хранят данные в формате, похожем на JSON. Идеальны для гибких схем и иерархических данных.\n",
        "\n",
        "- **Ключ-значение хранилища** (например, Redis, DynamoDB):  \n",
        "  Просты и быстры, но менее гибки для сложных запросов.\n",
        "\n",
        "- **Графовые базы данных** (например, Neo4j):  \n",
        "  Полезны для хранения текстовых данных, связанных с графами знаний (например, извлечённые сущности и их отношения).\n",
        "\n",
        "**Преимущества NoSQL**:\n",
        "- Гибкость схемы.\n",
        "- Масштабируемость.\n",
        "- Высокая производительность для определённых типов запросов.\n",
        "\n",
        "**Недостатки**:\n",
        "- Менее строгая консистентность.\n",
        "- Отсутствие стандартизированного языка запросов (в отличие от SQL).\n",
        "\n",
        "\n",
        "```\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Подключение к MongoDB\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"nlp_database\"]\n",
        "collection = db[\"texts\"]\n",
        "\n",
        "# Вставка документа\n",
        "doc = {\n",
        "    \"text\": \"Пример текста в MongoDB\",\n",
        "    \"label\": 0,\n",
        "    \"metadata\": {\"source\": \"web\", \"lang\": \"ru\"}\n",
        "}\n",
        "collection.insert_one(doc)\n",
        "\n",
        "# Поиск документов\n",
        "for record in collection.find({\"label\": 0}):\n",
        "    print(record)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "v102gE50BM0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Заключение по форматам хранения\n",
        "\n",
        "Выбор формата хранения должен определяться балансом между:\n",
        "- удобством обработки,\n",
        "- объёмом данных,\n",
        "- необходимостью хранения метаинформации,\n",
        "- требованиями к производительности при чтении/записи,\n",
        "- интеграцией с существующей инфраструктурой.\n",
        "\n",
        "Часто в крупных и сложных NLP-проектах используется комбинация форматов: например, основной текст хранится в `TXT` или `JSONL` для потоковой обработки, а структурированные метаданные и результаты анализа — в `JSON`, `Parquet` или реляционной базе данных.\n"
      ],
      "metadata": {
        "id": "6OGQa--GBOvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# §2. Разметка (аннотация) данных: Подробное описание\n",
        "\n",
        "Разметка данных, или аннотация, является критически важным этапом в жизненном цикле разработки NLP-моделей, особенно для задач обучения с учителем. Этот процесс заключается в добавлении к необработанным текстовым данным метаинформации — меток, тегов или других атрибутов, которые указывают на определённые характеристики текста, его частей или отношений между ними. Качество и согласованность разметки напрямую влияют на способность модели обучаться и обобщать знания на новые, невидимые данные.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Принципы ручной и полуавтоматической разметки\n",
        "\n",
        "Выбор между ручной и полуавтоматической разметкой, а также их комбинацией, зависит от объёма данных, сложности задачи, доступных ресурсов и требуемой точности.\n",
        "\n",
        "### 1.1. Ручная разметка\n",
        "\n",
        "Это процесс, при котором люди-аннотаторы вручную просматривают и помечают текстовые данные в соответствии с заранее определёнными правилами и инструкциями.\n",
        "\n",
        "Ручная разметка незаменима для создания высококачественных «золотых стандартов» (ground truth) для обучения и оценки моделей. Она часто используется на начальных этапах проекта, когда автоматические методы ещё не разработаны или недостаточно точны, а также для сложных задач, требующих глубокого понимания контекста, здравого смысла или экспертных знаний.\n",
        "\n",
        "Ручная разметка является наиболее точной, но при этом самой дорогой и трудоёмкой. Она требует значительных временных и финансовых затрат, особенно при работе с большими объёмами данных. Кроме того, существует риск субъективности и несогласованности между разными аннотаторами.\n",
        "\n",
        "Для обеспечения согласованности и высокого качества ручной разметки крайне важно разработать чёткие и исчерпывающие руководства (guidelines) для аннотаторов. Эти руководства должны включать:\n",
        "- определения всех категорий меток,\n",
        "- примеры их применения,\n",
        "- правила разрешения неоднозначностей,\n",
        "- описание часто встречающихся пограничных случаев.\n",
        "\n",
        "Регулярное обучение аннотаторов и калибровка их понимания правил также являются ключевыми элементами успешной ручной разметки.\n",
        "\n",
        "### 1.2. Полуавтоматическая разметка (Human-in-the-Loop)\n",
        "\n",
        "Этот подход комбинирует ручную разметку с автоматическими методами машинного обучения, что позволяет значительно ускорить процесс и снизить затраты, сохраняя при этом высокое качество.\n",
        "\n",
        "Принцип работы заключается в том, что модель машинного обучения (часто предварительно обученная на небольшом наборе вручную размеченных данных) используется для предварительной разметки неразмеченных данных. Затем человек-аннотатор просматривает и корректирует эти автоматические метки, исправляя ошибки и уточняя неоднозначности.\n",
        "\n",
        "#### Методы полуавтоматической разметки:\n",
        "\n",
        "- **Активное обучение (Active Learning)**:  \n",
        "  Модель активно выбирает наиболее «информативные» или «сложные» неразмеченные примеры для ручной аннотации. К ним относятся, например, те, по которым модель наименее уверена в своём предсказании, или примеры, находящиеся близко к границе принятия решений. Аннотация таких примеров приносит максимальную пользу для улучшения модели и минимизирует объём ручной работы.\n",
        "\n",
        "- **Программная разметка (Programmatic Labeling / Weak Supervision)**:  \n",
        "  Используются эвристические правила, регулярные выражения, словари или другие простые алгоритмы для автоматического присвоения меток данным. Эти «слабые» метки могут быть менее точными, чем ручные, но они генерируются быстро и в больших объёмах. Затем они могут использоваться для обучения «мета-модели», которая комбинирует слабые сигналы, или для предварительного обучения более сложной модели.\n",
        "\n",
        "- **Интерактивные инструменты**:  \n",
        "  Современные инструменты аннотации (например, Label Studio) включают встроенные функции полуавтоматической разметки, позволяя аннотаторам быстро принимать или отклонять предложенные моделью метки.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Инструменты для аннотации\n",
        "\n",
        "Эффективная разметка данных невозможна без специализированных инструментов, которые упрощают процесс, обеспечивают согласованность и позволяют управлять проектами аннотации.\n",
        "\n",
        "### 2.1. Label Studio\n",
        "\n",
        "Мощный инструмент с открытым исходным кодом, поддерживающий широкий спектр типов данных (текст, изображения, аудио, видео) и задач аннотации.\n",
        "\n",
        "Label Studio позволяет настраивать интерфейс под конкретные нужды проекта и поддерживает различные типы разметки текста: классификацию текста, распознавание именованных сущностей (NER), разметку отношений, тегирование частей речи, анализ настроений. Он также предоставляет функции для управления проектами, отслеживания прогресса аннотаторов и экспорта размеченных данных в различных форматах. Поддерживает интеграцию с моделями машинного обучения для полуавтоматической разметки.\n",
        "\n",
        "### 2.2. Doccano\n",
        "\n",
        "Ещё один популярный инструмент с открытым исходным кодом, специально ориентированный на текстовые данные.\n",
        "\n",
        "Doccano поддерживает основные задачи текстовой аннотации: классификацию текста, распознавание именованных сущностей (NER) и разметку пар предложений (например, для задач определения логического следования). Имеет простой и интуитивно понятный веб-интерфейс, что делает его удобным для быстрого старта.\n",
        "\n",
        "### 2.3. Prodigy\n",
        "\n",
        "Платный инструмент для аннотации от компании Explosion AI, создателей библиотеки spaCy. Ориентирован на эффективность и скорость разметки с использованием активного обучения.\n",
        "\n",
        "Prodigy отличается высокой производительностью и возможностью интеграции с моделями spaCy для умной предварительной разметки и выбора наиболее ценных для аннотации примеров. Предназначен для разработчиков и исследователей, которым нужен гибкий инструмент, управляемый из командной строки и позволяющий быстро и итеративно размечать данные.\n",
        "\n",
        "### 2.4. Hugging Face Ecosystem\n",
        "\n",
        "Платформа Hugging Face, известная своим репозиторием моделей и датасетов, также играет важную роль в процессе аннотации данных, предоставляя инструменты и инфраструктуру для управления и совместной работы.\n",
        "\n",
        "- **Hugging Face Spaces**:  \n",
        "  Позволяет хостить и запускать веб-приложения, включая инструменты для аннотации, такие как Label Studio. Это даёт возможность быстро развернуть инструмент для разметки и предоставить к нему доступ команде.\n",
        "\n",
        "- **Hugging Face Hub (библиотека Datasets)**:  \n",
        "  Хотя сама по себе не является инструментом для аннотации, библиотека `datasets` и Hugging Face Hub служат централизованным хранилищем для размеченных данных. Это облегчает загрузку, обработку, совместное использование и версионирование датасетов, полученных в результате аннотации.\n",
        "\n",
        "- **Интеграции с инструментами аннотации**:  \n",
        "  Многие популярные инструменты (например, Argilla и CVAT) предлагают интеграцию с Hugging Face Hub. Это позволяет легко экспортировать размеченные данные на платформу Hugging Face для дальнейшего использования в обучении моделей или для обмена с сообществом. Также существуют руководства и «кулинарные книги» (cookbooks) на Hugging Face, демонстрирующие, как использовать различные инструменты для аннотации, включая методы активного обучения.\n"
      ],
      "metadata": {
        "id": "BZ-p71sWEfxM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8_TZ70fFoLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 2.5. Другие инструменты и подходы\n",
        "\n",
        "- **In-house инструменты**:  \n",
        "  Некоторые компании разрабатывают собственные инструменты для аннотации, которые точно соответствуют их уникальным требованиям и интегрируются с существующими внутренними системами.\n",
        "\n",
        "- **Коммерческие платформы**:  \n",
        "  Существуют коммерческие платформы для аннотации данных (например, Amazon Mechanical Turk, Figure Eight / Appen, Scale AI), которые предоставляют не только инструменты, но и доступ к большой базе квалифицированных аннотаторов. Это удобно для аутсорсинга больших объёмов разметки.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Оценка качества разметки\n",
        "\n",
        "Оценка качества разметки является критически важным шагом, поскольку ошибки или несогласованность в размеченных данных могут привести к обучению неэффективных или предвзятых моделей.\n",
        "\n",
        "\n",
        "\n",
        "### 3.1. Интераннотаторская согласованность (Inter-Annotator Agreement, IAA)\n",
        "\n",
        "Интераннотаторская согласованность (IAA) — это количественная мера степени совпадения решений между двумя или более аннотаторами при разметке одного и того же набора данных. Она играет ключевую роль в оценке качества и надёжности аннотированных данных, особенно в задачах машинного обучения, обработки естественного языка, медицинской диагностики и социологических исследованиях.\n",
        "\n",
        "**Высокий уровень IAA свидетельствует о том, что**:\n",
        "- инструкции по разметке чётко и однозначно сформулированы,\n",
        "- аннотаторы единообразно понимают поставленную задачу,\n",
        "- разметка является воспроизводимой и надёжной.\n",
        "\n",
        "**Низкая IAA может указывать на**:\n",
        "- неоднозначность в определениях или критериях разметки,\n",
        "- недостаточную подготовку или инструктаж аннотаторов,\n",
        "- высокую субъективность или сложность самой задачи (например, распознавание эмоций, оценка стиля текста),\n",
        "- наличие ошибок или шумов в данных.\n",
        "\n",
        "Таким образом, IAA служит важным индикатором как качества процесса аннотации, так и пригодности данных для дальнейшего использования в обучении моделей. Повышение IAA зачастую требует уточнения инструкций, проведения дополнительных раундов тренировки аннотаторов и анализа спорных примеров.\n",
        "\n",
        "\n",
        "\n",
        "### Методы измерения IAA\n",
        "\n",
        "Для количественной оценки IAA используются различные статистические метрики, выбор которых зависит от типа данных и числа аннотаторов:\n",
        "\n",
        "- **Коэффициент Каппа Коэна** — для двух аннотаторов и номинальных шкал.  \n",
        "- **Взвешенная Каппа** — когда важно учитывать степень различия между метками (например, в порядковых шкалах).  \n",
        "- **Коэффициент Флайса (Fleiss’ Kappa)** — для оценки согласия между тремя и более аннотаторами.  \n",
        "- **Корреляция (Пирсон, Кендалл, Спирмен)** — для количественных или порядковых оценок.\n",
        "\n",
        "\n",
        "\n",
        "#### Коэффициент Каппа Коэна (Cohen’s Kappa)\n",
        "\n",
        "Коэффициент Каппа Коэна — это статистическая мера степени согласия между двумя аннотаторами, скорректированная с учётом совпадений, возникающих случайно. Он особенно полезен при анализе категориальных данных (например, классификация текстов, разметка изображений и т.д.).\n",
        "\n",
        "Формула коэффициента Каппа Коэна:\n",
        "$$\n",
        "\\kappa = \\frac{P_o - P_e}{1 - P_e}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "\n",
        "- $P_o$ (**Observed Agreement**) — наблюдаемая доля согласия:\n",
        "$$\n",
        "  P_o = \\frac{\\text{Число совпадающих оценок}}{\\text{Общее число оценок}}\n",
        "$$\n",
        "\n",
        "- $P_e$ (**Expected Agreement**) — ожидаемая доля случайного согласия:\n",
        "$$\n",
        "  P_e = \\sum_{i=1}^{k} p_{\\text{анн1},i} \\cdot p_{\\text{анн2},i}\n",
        "$$\n",
        "  где:\n",
        "  - $k$ — количество категорий,\n",
        "  - $p_{\\text{анн1},i}$ — доля меток категории $i$, присвоенных первым аннотатором,\n",
        "  - $p_{\\text{анн2},i}$ — доля меток категории $i$, присвоенных вторым аннотатором.\n",
        "\n",
        "\n",
        "\n",
        "#### Интерпретация значений Каппа\n",
        "\n",
        "Значения коэффициента Каппа Коэна находятся в диапазоне от $-1$ до $1$:\n",
        "\n",
        "- $\\kappa = 1$ — полное согласие между аннотаторами.  \n",
        "- $\\kappa \\geq 0.8$ — очень хорошее (почти идеальное) согласие.  \n",
        "- $0.6 \\leq \\kappa < 0.8$ — хорошее согласие.  \n",
        "- $0.4 \\leq \\kappa < 0.6$ — умеренное согласие.  \n",
        "- $0.2 \\leq \\kappa < 0.4$ — слабое согласие.  \n",
        "- $0 < \\kappa < 0.2$ — незначительное согласие.  \n",
        "- $\\kappa = 0$ — согласие на уровне случайности.  \n",
        "- $\\kappa < 0$ — согласие хуже, чем можно было бы ожидать случайно (указывает на систематические расхождения, возможные ошибки в разметке или недопонимание инструкций).\n",
        "\n",
        "> **Примечание**: Отрицательные значения Каппа встречаются крайне редко и могут сигнализировать о серьёзных проблемах в процессе аннотации (например, аннотаторы систематически дают противоположные метки).\n",
        "\n",
        "\n",
        "\n",
        "#### Пример вычисления Каппа Коэна на Python\n",
        "\n"
      ],
      "metadata": {
        "id": "N6PUI3IWEf0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Пример разметки двумя аннотаторами\n",
        "annotator1 = [1, 0, 2, 1, 2, 0, 1, 2]\n",
        "annotator2 = [1, 0, 1, 1, 2, 1, 1, 2]\n",
        "\n",
        "# Вычисление Каппа Коэна\n",
        "kappa = cohen_kappa_score(annotator1, annotator2)\n",
        "\n",
        "print(f\"Коэффициент Каппа Коэна: {kappa:.3f}\")"
      ],
      "metadata": {
        "id": "ld1-N6l_HBQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Вывод**:  \n",
        "```\n",
        "Коэффициент Каппа Коэна: 0.610\n",
        "```\n",
        "\n",
        "**Интерпретация**: Значение $\\kappa = 0.610$ указывает на **хорошее согласие** между аннотаторами.\n",
        "\n",
        "\n",
        "\n",
        "#### Ограничения Каппа Коэна\n",
        "\n",
        "- Применим только для **двух аннотаторов**.  \n",
        "- Предполагает, что категории **номинальные** (не учитывает порядок).  \n",
        "- Для порядковых шкал (например, оценки от 1 до 5) рекомендуется использовать **взвешенную Каппу** (например, линейную или квадратичную).  \n",
        "- Чувствителен к распределению меток — при сильной несбалансированности классов значение $P_e$ может быть высоким, что «понижает» значение $\\kappa$.\n",
        "\n",
        "\n",
        "\n",
        "#### Альтернативы\n",
        "\n",
        "- **Fleiss’ Kappa** — для оценки согласия между более чем двумя аннотаторами.  \n",
        "- **Корреляция по Пирсону / Кендаллу** — для порядковых данных.  \n",
        "- **Взвешенная Каппа** — когда важно учитывать степень несовпадения (например, разница между оценкой 1 и 2 менее критична, чем между 1 и 5).\n"
      ],
      "metadata": {
        "id": "5-yajh_GHBbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import random\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Определение класса для проекта разметки данных\n",
        "class AnnotationProject:\n",
        "    \"\"\"\n",
        "    Класс для симуляции и оценки проекта разметки данных.\n",
        "\n",
        "    Этот класс инкапсулирует логику создания небольшого набора данных,\n",
        "    симуляции ручной разметки двумя \"аннотаторами\" и вычисления\n",
        "    меж-аннотаторской согласованности (IAA) с использованием\n",
        "    коэффициента Каппа Коэна.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, categories: List[str]):\n",
        "        \"\"\"\n",
        "        Инициализация проекта разметки.\n",
        "\n",
        "        Args:\n",
        "            categories (List[str]): Список возможных категорий для разметки (например, ['позитивный', 'негативный']).\n",
        "        \"\"\"\n",
        "        self.categories = categories\n",
        "        self.dataset = self._create_synthetic_dataset()\n",
        "        self.annotations = {} # Словарь для хранения разметки аннотаторов\n",
        "\n",
        "    def _create_synthetic_dataset(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Создает небольшой синтетический набор данных для демонстрации.\n",
        "\n",
        "        Набор данных состоит из 100 текстовых примеров.\n",
        "        \"\"\"\n",
        "        print(\"Создание синтетического набора данных...\")\n",
        "        sentences = [\n",
        "            \"Этот продукт просто ужасен, никогда больше его не куплю.\",\n",
        "            \"Отличный сервис и быстрая доставка!\",\n",
        "            \"Фильм был скучным и затянутым, не рекомендую.\",\n",
        "            \"Прекрасный день для прогулки.\",\n",
        "            \"Всё работает как ожидалось, без нареканий.\",\n",
        "            \"Очень разочарован качеством товара.\",\n",
        "            \"Я абсолютно в восторге от этой книги.\",\n",
        "            \"Нейтральная новость о погоде.\",\n",
        "            \"Сложно сказать, понравилось или нет.\",\n",
        "            \"Просто обычный обед.\"\n",
        "        ]\n",
        "\n",
        "        # Расширяем набор данных, чтобы было 100 примеров\n",
        "        data = []\n",
        "        for i in range(100):\n",
        "            text = f\"Пример {i+1}: {random.choice(sentences)}\"\n",
        "            # Простое назначение \"истинной\" метки\n",
        "            if \"ужасен\" in text or \"скучным\" in text or \"разочарован\" in text:\n",
        "                true_label = \"Негативный\"\n",
        "            elif \"Отличный\" in text or \"Прекрасный\" in text or \"восторг\" in text:\n",
        "                true_label = \"Позитивный\"\n",
        "            else:\n",
        "                true_label = \"Нейтральный\"\n",
        "\n",
        "            data.append({\"id\": i, \"text\": text, \"true_label\": true_label})\n",
        "\n",
        "        print(f\"Синтетический набор данных создан с {len(data)} примерами.\")\n",
        "        return data\n",
        "\n",
        "    def simulate_manual_annotation(self, annotator_id: str, error_rate: float = 0.1) -> List[str]:\n",
        "        \"\"\"\n",
        "        Симулирует процесс ручной разметки одним аннотатором.\n",
        "\n",
        "        Каждый аннотатор имеет определенный процент ошибок, который имитирует\n",
        "        человеческий фактор и возможные расхождения.\n",
        "\n",
        "        Args:\n",
        "            annotator_id (str): Идентификатор аннотатора (например, 'Аннотатор 1').\n",
        "            error_rate (float): Процент ошибок в разметке (от 0.0 до 1.0).\n",
        "\n",
        "        Returns:\n",
        "            List[str]: Список размеченных меток.\n",
        "        \"\"\"\n",
        "        print(f\"\\nСимуляция разметки для {annotator_id}...\")\n",
        "        annotations = []\n",
        "        for item in self.dataset:\n",
        "            true_label = item[\"true_label\"]\n",
        "\n",
        "            # С вероятностью error_rate аннотатор делает ошибку\n",
        "            if random.random() < error_rate:\n",
        "                # Случайным образом выбираем неправильную метку\n",
        "                wrong_categories = [cat for cat in self.categories if cat != true_label]\n",
        "                annotated_label = random.choice(wrong_categories)\n",
        "            else:\n",
        "                annotated_label = true_label\n",
        "\n",
        "            annotations.append(annotated_label)\n",
        "\n",
        "        self.annotations[annotator_id] = annotations\n",
        "        print(f\"Разметка для {annotator_id} завершена. Количество меток: {len(annotations)}\")\n",
        "        return annotations\n",
        "\n",
        "    def calculate_iaa(self, annotator1_id: str, annotator2_id: str) -> float:\n",
        "        \"\"\"\n",
        "        Вычисляет коэффициент Каппа Коэна для оценки меж-аннотаторской согласованности.\n",
        "\n",
        "        Args:\n",
        "            annotator1_id (str): Идентификатор первого аннотатора.\n",
        "            annotator2_id (str): Идентификатор второго аннотатора.\n",
        "\n",
        "        Returns:\n",
        "            float: Значение коэффициента Каппа Коэна.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Если разметка для одного из аннотаторов отсутствует.\n",
        "        \"\"\"\n",
        "        if annotator1_id not in self.annotations or annotator2_id not in self.annotations:\n",
        "            raise ValueError(\"Разметка для одного или обоих аннотаторов отсутствует. Запустите симуляцию разметки сначала.\")\n",
        "\n",
        "        ann1_labels = self.annotations[annotator1_id]\n",
        "        ann2_labels = self.annotations[annotator2_id]\n",
        "\n",
        "        print(f\"\\nВычисление коэффициента Каппа Коэна для {annotator1_id} и {annotator2_id}...\")\n",
        "        # Каппа Коэна требует, чтобы метки были одинакового типа и длины\n",
        "        kappa = cohen_kappa_score(ann1_labels, ann2_labels)\n",
        "\n",
        "        return kappa\n",
        "\n",
        "    def run_project(self):\n",
        "        \"\"\"\n",
        "        Запускает полный цикл симуляции проекта разметки.\n",
        "        \"\"\"\n",
        "        print(\"--- Запуск проекта разметки ---\")\n",
        "\n",
        "        # Симуляция разметки для двух аннотаторов с разным процентом ошибок\n",
        "        self.simulate_manual_annotation(\"Аннотатор 1\", error_rate=0.1)\n",
        "        self.simulate_manual_annotation(\"Аннотатор 2\", error_rate=0.2)\n",
        "\n",
        "        # Вычисление и вывод коэффициента Каппа Коэна\n",
        "        kappa_score = self.calculate_iaa(\"Аннотатор 1\", \"Аннотатор 2\")\n",
        "        print(f\"\\nКоэффициент Каппа Коэна между аннотатором 1 и 2: {kappa_score:.3f}\")\n",
        "\n",
        "        # Интерпретация результата\n",
        "        if kappa_score >= 0.8:\n",
        "            interpretation = \"Очень хорошее (почти идеальное) согласие.\"\n",
        "        elif 0.6 <= kappa_score < 0.8:\n",
        "            interpretation = \"Хорошее согласие.\"\n",
        "        elif 0.4 <= kappa_score < 0.6:\n",
        "            interpretation = \"Умеренное согласие.\"\n",
        "        else:\n",
        "            interpretation = \"Слабое или незначительное согласие. Инструкции, возможно, требуют уточнения.\"\n",
        "\n",
        "        print(f\"Интерпретация: {interpretation}\")\n",
        "\n",
        "# --- Запуск демонстрации ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Определяем категории для разметки (например, для задачи анализа тональности)\n",
        "    sentiment_categories = [\"Позитивный\", \"Негативный\", \"Нейтральный\"]\n",
        "\n",
        "    # Создаем экземпляр класса AnnotationProject\n",
        "    project = AnnotationProject(categories=sentiment_categories)\n",
        "\n",
        "    # Запускаем полный цикл проекта\n",
        "    project.run_project()"
      ],
      "metadata": {
        "id": "B97ESlgPMsd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# §3. Аугментация текстовых данных: Подробное описание\n",
        "\n",
        "Аугментация данных (Data Augmentation) — это совокупность методов, направленных на искусственное увеличение объёма обучающих данных путём создания новых, изменённых версий уже существующих примеров. В контексте обработки естественного языка (NLP) это означает генерацию вариаций исходных текстовых данных, которые сохраняют их семантическое значение, но отличаются по форме.\n",
        "\n",
        "Этот подход особенно ценен в ситуациях, когда доступен ограниченный объём размеченных данных — что является частой проблемой в NLP. Эффективная аугментация помогает:\n",
        "- улучшить обобщающую способность модели,\n",
        "- снизить риск переобучения,\n",
        "- повысить устойчивость к небольшим изменениям во входных данных.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Методы аугментации текстовых данных\n",
        "\n",
        "Методы аугментации текста варьируются от простых операций на уровне слов до сложных подходов с использованием генеративных моделей. Выбор метода зависит от задачи, типа данных и доступных ресурсов.\n",
        "\n",
        "### 1.1. Замена синонимами (Synonym Replacement)\n",
        "\n",
        "Метод заключается в замене слов в предложении их синонимами. Цель — ввести лексическое разнообразие, сохранив при этом исходный смысл.\n",
        "\n",
        "**Пример**:  \n",
        "- Исходное предложение: «Кот быстро бежит по траве».  \n",
        "- Аугментированное предложение: «Кот стремительно бежит по траве».\n",
        "\n",
        "**Особенности реализации**:\n",
        "- Важно использовать **контекстно-зависимые синонимы**, чтобы избежать искажения смысла. Простая замена по словарю может привести к нелепым или грамматически некорректным предложениям. Например, синоним слова «банк» (финансовое учреждение) не должен быть «берег реки».\n",
        "- Для более продвинутой замены могут использоваться **векторные представления слов** (например, Word2Vec, GloVe) для поиска ближайших по смыслу слов в векторном пространстве.\n",
        "\n",
        "**Пример на Python (с использованием nltk и WordNet для синонимов)**:\n"
      ],
      "metadata": {
        "id": "P3KMB5WgEf3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Загрузка необходимых данных\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"Возвращает список синонимов для английского слова.\"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            synonyms.add(synonym.lower())\n",
        "    # Удаляем исходное слово\n",
        "    synonyms.discard(word.lower())\n",
        "    return list(synonyms)\n",
        "\n",
        "def synonym_replacement(sentence, n=1):\n",
        "    \"\"\"Заменяет n случайных слов в предложении их синонимами.\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) == 0:\n",
        "        return sentence\n",
        "\n",
        "    new_words = words.copy()\n",
        "    indices = random.sample(range(len(words)), min(n, len(words)))\n",
        "\n",
        "    for idx in indices:\n",
        "        word = words[idx].strip('.,!?\";')\n",
        "        synonyms = get_synonyms(word.lower())\n",
        "        if synonyms:\n",
        "            new_word = random.choice(synonyms)\n",
        "            # Сохраняем заглавную букву\n",
        "            if words[idx][0].isupper():\n",
        "                new_word = new_word.capitalize()\n",
        "            # Возвращаем пунктуацию\n",
        "            punct = words[idx][-1] if words[idx][-1] in '.,!?\";' else ''\n",
        "            new_word = new_word + punct\n",
        "            new_words[idx] = new_word\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Пример\n",
        "sentence = \"This is a very good movie, I liked it.\"\n",
        "augmented = synonym_replacement(sentence, n=2)\n",
        "print(f\"Original: {sentence}\")\n",
        "print(f\"Augmented: {augmented}\")"
      ],
      "metadata": {
        "id": "rssiOWRFJRSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Примечание**: WordNet для русского языка может быть ограничен по объёму синонимов. Для русского рекомендуется использовать альтернативные источники (например, библиотеку `pymorphy2`, словари синонимов или эмбеддинги).\n",
        "\n",
        "\n",
        "\n",
        "### 1.2. Перефразирование (Paraphrasing)\n",
        "\n",
        "Метод заключается в переписывании предложений или фраз с сохранением смысла, но с изменением синтаксической структуры, лексики или порядка слов. Перефразирование делает модель более устойчивой к разным формулировкам одного и того же содержания.\n",
        "\n",
        "**Пример**:  \n",
        "- Исходное предложение: «Из-за сильного дождя матч был отложен».  \n",
        "- Аугментированное предложение: «Матч был перенесён по причине обильных осадков».\n",
        "\n",
        "**Особенности реализации**:\n",
        "- Может выполняться вручную (трудоёмко) или с помощью **моделей генерации текста**, таких как T5, BART, Pegasus.\n",
        "- Требует качественных моделей и значительных вычислительных ресурсов.\n",
        "- Часто используется в паре с контролем семантической эквивалентности (например, через cosine similarity векторов эмбеддингов).\n",
        "\n",
        "\n",
        "\n",
        "### 1.3. Обратный перевод (Back-translation)\n",
        "\n",
        "Метод включает перевод исходного текста на другой язык, а затем обратный перевод полученного текста на исходный язык. Процесс часто приводит к небольшим изменениям формулировки при сохранении основного смысла, создавая семантически эквивалентную версию.\n",
        "\n",
        "**Пример**:  \n",
        "- Исходное предложение (русский): «Я люблю изучать обработку естественного языка».  \n",
        "- Перевод на английский: «I love studying natural language processing».  \n",
        "- Обратный перевод на русский: «Мне нравится изучать обработку естественного языка».\n",
        "\n",
        "**Особенности реализации**:\n",
        "- Требуются качественные модели машинного перевода.\n",
        "- Выбор промежуточного языка влияет на степень изменения текста (например, английский → немецкий → русский может дать большее разнообразие).\n",
        "- Использование нескольких промежуточных языков увеличивает вариативность.\n",
        "\n",
        "> **Примечание**: Библиотека `googletrans` использует Google Translate API, который не предназначен для промышленного использования без ключа API и может быть нестабильным при больших объёмах. Для реальных проектов рекомендуется использовать официальные API (Google Cloud Translation, DeepL API и т.д.).\n",
        "\n"
      ],
      "metadata": {
        "id": "b9Cn2LwkJRcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers sentencepiece\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate(texts, src_lang, tgt_lang):\n",
        "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    translated = model.generate(**inputs)\n",
        "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "# Пример: Обратный перевод\n",
        "original_text = \"Я люблю изучать обработку естественного языка.\"\n",
        "\n",
        "# Шаг 1: Русский → Английский\n",
        "en_text = translate([original_text], src_lang=\"ru\", tgt_lang=\"en\")[0]\n",
        "print(\"EN translation:\", en_text)\n",
        "\n",
        "# Шаг 2: Английский → Русский\n",
        "back_translated = translate([en_text], src_lang=\"en\", tgt_lang=\"ru\")[0]\n",
        "print(\"Back-translation:\", back_translated)"
      ],
      "metadata": {
        "id": "sWfcMSSzJyFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1.4. Случайная вставка слов (Random Insertion)\n",
        "\n",
        "Метод заключается в случайной вставке синонимов или релевантных слов в предложение. Для каждого слова в предложении с определённой вероятностью выбирается место для вставки синонима этого слова или другого слова, связанного по смыслу.\n",
        "\n",
        "**Цель**: увеличить разнообразие текстов, не нарушая семантику.\n",
        "\n",
        "**Пример**:  \n",
        "- Исходное предложение: «Он читает книгу».  \n",
        "- После вставки: «Он спокойно читает интересную книгу».\n",
        "\n",
        "**Особенности реализации**:\n",
        "- Часто используется в комбинации с методом замены синонимами.\n",
        "- Важно избегать вставки слов, которые искажают смысл или нарушают грамматику.\n",
        "- Может быть реализован с помощью эмбеддингов (вставка ближайших по смыслу слов) или тематических словарей.\n",
        "\n",
        "Пример на питон:\n"
      ],
      "metadata": {
        "id": "TOSc-L6JJyOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 1: Установка (если нужно)\n",
        "# !pip install nltk\n",
        "\n",
        "# Шаг 2: Импорт библиотек\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Шаг 3: Скачивание необходимых данных\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Для многозначных синонимов\n",
        "\n",
        "# Функция: получение синонимов слова\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Возвращает список синонимов для английского слова.\n",
        "    \"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ').lower()\n",
        "            if synonym != word.lower():\n",
        "                synonyms.add(synonym)\n",
        "    return list(synonyms)\n",
        "\n",
        "# Функция: случайная вставка синонимов\n",
        "def random_insertion(sentence, n=1):\n",
        "    \"\"\"\n",
        "    Вставляет n синонимов случайных слов в случайные позиции предложения.\n",
        "\n",
        "    :param sentence: исходное предложение (str)\n",
        "    :param n: количество вставок\n",
        "    :return: аугментированное предложение (str)\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) == 0:\n",
        "        return sentence\n",
        "\n",
        "    new_words = words.copy()\n",
        "\n",
        "    for _ in range(n):\n",
        "        # Выбираем случайное слово из исходного предложения\n",
        "        random_word = random.choice(words)\n",
        "        synonyms = get_synonyms(random_word)\n",
        "\n",
        "        if synonyms:\n",
        "            # Выбираем случайный синоним\n",
        "            synonym = random.choice(synonyms)\n",
        "            # Вставляем в случайную позицию (0 = начало, len(new_words) = конец)\n",
        "            insert_pos = random.randint(0, len(new_words))\n",
        "            new_words.insert(insert_pos, synonym)\n",
        "\n",
        "    # Восстанавливаем пунктуацию в конце\n",
        "    punctuation = sentence[-1] if sentence[-1] in '.!?' else ''\n",
        "    result = ' '.join(new_words)\n",
        "    if punctuation:\n",
        "        result += punctuation\n",
        "\n",
        "    return result\n",
        "\n",
        "# === Пример использования ===\n",
        "if __name__ == \"__main__\":\n",
        "    original_sentence = \"I am learning NLP.\"\n",
        "    augmented_sentence = random_insertion(original_sentence, n=2)\n",
        "\n",
        "    print(f\"Original: {original_sentence}\")\n",
        "    print(f\"Augmented: {augmented_sentence}\")\n"
      ],
      "metadata": {
        "id": "fcwUzAFnJ6a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Особенности реализации: Важно контролировать количество вставляемых слов, чтобы не исказить смысл предложения и не создать чрезмерный \"шум\". Вставляемые слова должны быть релевантны контексту."
      ],
      "metadata": {
        "id": "L9zjuG8lHHHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.5. Случайное удаление слов (Random Deletion)\n",
        "\n",
        "Метод заключается в случайном удалении слов из предложения с определённой вероятностью. Это помогает модели стать более устойчивой к шуму и неполным данным, а также учиться выделять наиболее важные слова.\n",
        "\n",
        "**Пример**:  \n",
        "- Исходное предложение: «Этот очень длинный текст содержит много ненужных слов».  \n",
        "- Аугментированное предложение: «Этот длинный текст много слов».\n",
        "\n",
        "**Особенности реализации**:  \n",
        "Вероятность удаления должна быть тщательно подобрана. Слишком высокая вероятность может привести к потере важной информации и искажению смысла.\n",
        "\n",
        "**Пример на Python**:\n",
        "\n"
      ],
      "metadata": {
        "id": "9fsh3NNcHHK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_deletion(sentence, p=0.1):\n",
        "    \"\"\"Удаляет слова из предложения с вероятностью p.\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) == 0:\n",
        "        return sentence\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.random() > p:  # Сохраняем слово с вероятностью (1-p)\n",
        "            new_words.append(word)\n",
        "\n",
        "    # Если все слова удалены, возвращаем случайное слово из оригинала\n",
        "    if not new_words:\n",
        "        return random.choice(words) if words else \"\"\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "sentence = \"Это очень длинное предложение для демонстрации случайного удаления слов.\"\n",
        "augmented_sentence = random_deletion(sentence, p=0.2)\n",
        "print(f\"Исходное: {sentence}\")\n",
        "print(f\"Аугментированное (случайное удаление): {augmented_sentence}\")"
      ],
      "metadata": {
        "id": "gfZydLbOLqPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1.6. Случайная перестановка слов (Random Swap)\n",
        "\n",
        "Метод заключается в случайной перестановке двух слов в предложении. Это помогает модели стать более устойчивой к изменениям в порядке слов, сохраняя при этом тот же набор лексических единиц.\n",
        "\n",
        "**Пример**:  \n",
        "- Исходное предложение: «Кошка находится на коврике».  \n",
        "- Аугментированное предложение: «Кошка коврике на находится».\n",
        "\n",
        "**Особенности реализации**:  \n",
        "Количество перестановок должно быть ограничено, чтобы не нарушить грамматику и смысл предложения слишком сильно.\n",
        "\n",
        "**Пример на Python**:\n",
        "\n"
      ],
      "metadata": {
        "id": "PyIEKvS6LqaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_swap(sentence, n=1):\n",
        "    \"\"\"Случайно меняет местами n пар слов в предложении.\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) < 2:\n",
        "        return sentence\n",
        "\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
        "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "sentence = \"Это простое предложение для тестирования перестановки слов.\"\n",
        "augmented_sentence = random_swap(sentence, n=2)\n",
        "print(f\"Исходное: {sentence}\")\n",
        "print(f\"Аугментированное (случайная перестановка): {augmented_sentence}\")"
      ],
      "metadata": {
        "id": "Su-HTVALLwlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7. Аугментация с использованием генеративных моделей (Text Generation Augmentation)\n",
        "\n",
        "Продвинутый метод, использующий большие языковые модели (LLM), такие как GPT-3, T5, BART и их аналоги, для генерации новых текстов, семантически близких к исходным, но отличающихся по формулировкам, стилю или структуре.\n",
        "\n",
        "**Принцип работы**:  \n",
        "Исходный текст подаётся на вход генеративной модели, которая создаёт одну или несколько новых версий — будь то перефразирование, расширение контекста или вариации, сохраняющие основную идею.\n",
        "\n",
        "**Применение**:  \n",
        "Особенно полезно для увеличения разнообразия обучающих данных и создания примеров для редких классов.\n",
        "\n",
        "**Особенности реализации**:  \n",
        "- Требует доступа к мощным генеративным моделям (через API или локально).  \n",
        "- Необходим контроль качества сгенерированного текста, чтобы избежать **галлюцинаций** (генерации фактически неверной информации) или потери смысла.  \n",
        "- Часто требуется ручная фильтрация или дополнительная валидация.\n",
        "\n",
        "**Пример на Python (демонстрация вызова API Gemini)**:\n"
      ],
      "metadata": {
        "id": "TnK0IFoPLwwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install -q transformers sentencepiece\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Загружаем модель T5, дообученную на задаче перефразирования\n",
        "model_name = \"ramsrigouthamg/t5_paraphraser\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def paraphrase_t5(input_text, max_length=256, num_return_sequences=3, num_beams=10):\n",
        "    \"\"\"\n",
        "    Генерирует перефразированные варианты текста с помощью модели T5.\n",
        "    \"\"\"\n",
        "    # Добавляем инструкцию, на которой модель обучалась\n",
        "    input_text = \"paraphrase: \" + input_text + \" </s>\"\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        padding='max_length',\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_masks,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        top_k=120,\n",
        "        top_p=0.95,\n",
        "        early_stopping=True,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        num_beams=num_beams\n",
        "    )\n",
        "\n",
        "    paraphrased_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return paraphrased_texts\n",
        "\n",
        "# === Пример использования ===\n",
        "original = \"Клиент выразил недовольство скоростью обслуживания.\"\n",
        "paraphrased_versions = paraphrase_t5(original)\n",
        "\n",
        "print(\"Исходный текст:\")\n",
        "print(original)\n",
        "print(\"\\nАугментированные (перефразированные) версии:\")\n",
        "for i, p in enumerate(paraphrased_versions, 1):\n",
        "    print(f\"{i}. {p}\")"
      ],
      "metadata": {
        "id": "0GHIZFuUL2VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. Цели и ограничения аугментации\n",
        "\n",
        "Аугментация данных, несмотря на свою эффективность, должна применяться обдуманно, с учётом как преимуществ, так и потенциальных рисков.\n",
        "\n",
        "### 2.1. Цели аугментации\n",
        "\n",
        "- **Увеличение размера обучающей выборки**:  \n",
        "  Основная цель — расширить объём данных, особенно когда сбор размеченных примеров затруднён. Больший объём данных способствует более надёжному и обобщающему обучению.\n",
        "\n",
        "- **Повышение робастности (устойчивости) модели**:  \n",
        "  Аугментация делает модель менее чувствительной к небольшим изменениям во входных данных (синонимы, перестановки, опечатки), повышая её устойчивость к \"шуму\".\n",
        "\n",
        "- **Снижение переобучения (Overfitting)**:  \n",
        "  Расширение выборки снижает вероятность \"запоминания\" конкретных примеров, способствуя обобщению.\n",
        "\n",
        "- **Работа с несбалансированными выборками**:  \n",
        "  Позволяет искусственно увеличить число примеров миноритарного класса, снижая предвзятость модели в пользу мажоритарного.\n",
        "\n",
        "### 2.2. Ограничения аугментации\n",
        "\n",
        "- **Сохранение смысла и метки**:  \n",
        "  Главный вызов — не исказить семантику или метку. Некорректная замена (например, «банк» → «берег реки») может привести к появлению \"шумных\" данных, ухудшающих обучение.\n",
        "\n",
        "- **Качество генерируемых данных**:  \n",
        "  Генеративные модели могут создавать грамматически некорректные или бессмысленные предложения. Требуется валидация и фильтрация.\n",
        "\n",
        "- **Избыточность и ограниченное разнообразие**:  \n",
        "  Чрезмерная аугментация может порождать слишком похожие примеры, не добавляющие реального разнообразия, что не решает проблему переобучения.\n",
        "\n",
        "- **Вычислительные затраты**:  \n",
        "  Методы, использующие LLM или многократный перевод, могут быть ресурсоёмкими и медленными.\n",
        "\n",
        "- **Риск утечки данных (Data Leakage)**:  \n",
        "  Аугментация должна применяться **только к обучающей выборке**. Если она применяется ко всему датасету до разделения, аугментированные версии тестовых примеров могут попасть в обучение, что исказит оценку модели.\n"
      ],
      "metadata": {
        "id": "UdYn3JyvL2fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# §4. Работа с несбалансированными выборками: Подробное описание\n",
        "\n",
        "В задачах обработки естественного языка (NLP), как и в машинном обучении в целом, часто возникает проблема несбалансированных выборок (imbalanced datasets). Это происходит, когда количество примеров одного класса (мажоритарного) значительно превышает количество примеров другого класса (миноритарного).\n",
        "\n",
        "Типичные сценарии включают:\n",
        "- обнаружение спама (большинство писем — не спам),\n",
        "- выявление мошенничества (большинство транзакций — легитимны),\n",
        "- диагностику редких заболеваний,\n",
        "- определение редких событий в тексте.\n",
        "\n",
        "Обучение моделей на таких данных без специальных подходов может привести к неоптимальным результатам, поскольку модель склонна предсказывать мажоритарный класс, игнорируя миноритарный.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Влияние дисбаланса на обучение моделей\n",
        "\n",
        "Несбалансированность данных оказывает существенное негативное влияние на процесс обучения и последующую производительность моделей:\n",
        "\n",
        "- **Предвзятость модели**:  \n",
        "  Модель, обученная на несбалансированных данных, будет склонна предсказывать мажоритарный класс, поскольку он встречается гораздо чаще. Алгоритмы оптимизации, такие как градиентный спуск, минимизируют общую ошибку, что при дисбалансе означает минимизацию ошибок на мажоритарном классе за счёт миноритарного.\n",
        "\n",
        "- **Низкая производительность на миноритарном классе**:  \n",
        "  В результате предвзятости модель будет плохо распознавать примеры миноритарного класса. Это критично в задачах, где миноритарный класс представляет наиболее важные события (например, мошенничество, болезнь, критическая ошибка).\n",
        "\n",
        "- **Ошибочная оценка производительности**:  \n",
        "  Традиционные метрики, такие как **общая точность (accuracy)**, могут вводить в заблуждение. Например, если 99% данных относятся к классу \"А\", а 1% — к классу \"Б\", модель, всегда предсказывающая \"А\", достигнет точности 99%. Это ложный показатель её реальной эффективности.  \n",
        "  Вместо accuracy рекомендуется использовать **F1-меру**, **точность (precision)**, **полноту (recall)** и **ROC-AUC**, особенно в контексте миноритарного класса.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Стратегии работы с несбалансированными выборками\n",
        "\n",
        "Для эффективной работы с несбалансированными данными применяются различные стратегии, которые можно разделить на несколько категорий.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1. Методы передискретизации (Resampling Techniques)\n",
        "\n",
        "Методы передискретизации направлены на изменение распределения классов в обучающей выборке путём модификации количества примеров.\n",
        "\n",
        "#### 2.1.1. Oversampling (увеличение количества примеров миноритарного класса)\n",
        "\n",
        "Методы оверсэмплинга увеличивают количество примеров миноритарного класса, чтобы сбалансировать распределение.\n",
        "\n",
        "##### • Случайный оверсэмплинг (Random Oversampling)\n",
        "\n",
        "Простейший подход — случайное дублирование (копирование) существующих примеров миноритарного класса до достижения желаемого соотношения.\n",
        "\n",
        "**Преимущества**:\n",
        "- Лёгкость реализации.\n",
        "\n",
        "**Недостатки**:\n",
        "- Может привести к **переобучению**, так как модель видит одни и те же примеры несколько раз.\n",
        "- Не добавляет нового разнообразия в данные.\n",
        "\n",
        "**Пример на Python (с использованием `imblearn.over_sampling.RandomOverSampler`)**:\n"
      ],
      "metadata": {
        "id": "9qwYk7sTLngE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Создание несбалансированного синтетического датасета\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=2, n_informative=2,\n",
        "    n_redundant=0, n_repeated=0, n_classes=2,\n",
        "    n_clusters_per_class=1, weights=[0.90, 0.10],\n",
        "    flip_y=0, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Исходное распределение классов: {Counter(y)}\")\n",
        "\n",
        "# Применение случайного оверсэмплинга\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "print(f\"Распределение классов после Random Oversampling: {Counter(y_resampled)}\")"
      ],
      "metadata": {
        "id": "OqRzgaJoU4HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "##### SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "\n",
        "Более продвинутый метод, который генерирует **синтетические** (новые, не дублированные) примеры миноритарного класса.  \n",
        "SMOTE интерполирует между существующими примерами: для каждого примера миноритарного класса находятся его $k$ ближайших соседей (из того же класса), затем новый синтетический пример создается вдоль линии, соединяющей исходный пример и одного из соседей.\n",
        "\n",
        "**Преимущества**:\n",
        "- Создаёт новые, \"реалистичные\" примеры, снижая риск переобучения.\n",
        "- Добавляет разнообразие в данные.\n",
        "\n",
        "**Недостатки**:\n",
        "- Может создавать \"шумные\" или нерелевантные примеры, особенно если классы сильно перекрываются.\n",
        "- Уязвим к выбросам.\n",
        "\n",
        "**Пример на Python (с использованием `imblearn.over_sampling.SMOTE`)**:\n"
      ],
      "metadata": {
        "id": "F8WWRfY7U4gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=2, n_informative=2,\n",
        "    n_redundant=0, n_repeated=0, n_classes=2,\n",
        "    n_clusters_per_class=1, weights=[0.90, 0.10],\n",
        "    flip_y=0, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Исходное распределение классов: {Counter(y)}\")\n",
        "\n",
        "# Применение SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(f\"Распределение классов после SMOTE: {Counter(y_resampled)}\")"
      ],
      "metadata": {
        "id": "sQBh_OJqVBQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#####ADASYN (Adaptive Synthetic Sampling)\n",
        "\n",
        "Усовершенствование SMOTE, которое **динамически адаптирует** количество синтетических примеров.  \n",
        "ADASYN генерирует больше синтетических образцов для тех миноритарных примеров, которые **сложнее классифицировать** (т.е. у которых больше мажоритарных соседей).\n",
        "\n",
        "**Преимущества**:\n",
        "- Фокусируется на \"трудных\" зонах классификации.\n",
        "- Может способствовать более чётким границам решений.\n",
        "\n",
        "**Недостатки**:\n",
        "- Как и SMOTE, чувствителен к шуму и выбросам.\n",
        "- Может переобучаться на сложных, но не релевантных участках.\n"
      ],
      "metadata": {
        "id": "WM5zcTF9VBc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q imbalanced-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# 1. Генерация несбалансированного датасета\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=10, n_redundant=5,\n",
        "                           n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0.01, random_state=42)\n",
        "\n",
        "print(\"До применения ADASYN:\", np.bincount(y))\n",
        "\n",
        "# 2. Применение ADASYN\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_res, y_res = adasyn.fit_resample(X, y)\n",
        "\n",
        "print(\"После применения ADASYN:\", np.bincount(y_res))\n",
        "\n",
        "# 3. Обучение модели до и после балансировки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_res, y_res, stratify=y_res, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# До балансировки\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_orig = model.predict(X_test)\n",
        "print(\"\\nОценка модели ДО применения ADASYN:\")\n",
        "print(classification_report(y_test, y_pred_orig))\n",
        "\n",
        "# После балансировки\n",
        "model.fit(X_res_train, y_res_train)\n",
        "y_pred_res = model.predict(X_test)\n",
        "print(\"\\nОценка модели ПОСЛЕ применения ADASYN:\")\n",
        "print(classification_report(y_test, y_pred_res))\n",
        "\n",
        "# 4. Визуализация матриц ошибок\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_orig, ax=axs[0], colorbar=False)\n",
        "axs[0].set_title(\"До ADASYN\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_res, ax=axs[1], colorbar=False)\n",
        "axs[1].set_title(\"После ADASYN\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xCKPghXAVuwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 2.1.2. Undersampling (уменьшение количества примеров мажоритарного класса)\n",
        "\n",
        "Методы андерсэмплинга уменьшают количество примеров мажоритарного класса для балансировки распределения классов.\n",
        "\n",
        "#### • Случайное удаление (Random Undersampling)\n",
        "\n",
        "Простейший подход, заключающийся в случайном удалении примеров из мажоритарного класса до достижения желаемого баланса.\n",
        "\n",
        "**Преимущества**:\n",
        "- Лёгкость реализации.\n",
        "- Уменьшает объём данных, что может ускорить обучение.\n",
        "\n",
        "**Недостатки**:\n",
        "- Может привести к потере важной информации, содержащейся в удалённых примерах мажоритарного класса.\n",
        "- Снижает обобщающую способность модели, особенно если данных и так недостаточно.\n",
        "\n",
        "**Пример на Python (с использованием `imblearn.under_sampling.RandomUnderSampler`)**:\n",
        "\n"
      ],
      "metadata": {
        "id": "7Zj1x4baWPie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Создание несбалансированного синтетического датасета\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=2, n_informative=2,\n",
        "    n_redundant=0, n_repeated=0, n_classes=2,\n",
        "    n_clusters_per_class=1, weights=[0.90, 0.10],\n",
        "    flip_y=0, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Исходное распределение классов: {Counter(y)}\")\n",
        "\n",
        "# Применение случайного андерсэмплинга\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "print(f\"Распределение классов после Random Undersampling: {Counter(y_resampled)}\")"
      ],
      "metadata": {
        "id": "lZhC2_rEW3mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Tomek Links\n",
        "\n",
        "Tomek Links — это пары ближайших соседей разных классов, которые являются взаимно ближайшими друг к другу. То есть, если примеры $x_i$ и $x_j$ образуют Tomek Link, то $x_i$ — ближайший сосед $x_j$, и наоборот, при этом они принадлежат разным классам. Удаление мажоритарных примеров, участвующих в таких парах, помогает \"очистить\" границы между классами.\n",
        "\n",
        "**Преимущества**:\n",
        "- Удаляет шумные или пограничные мажоритарные примеры.\n",
        "- Улучшает чёткость границы принятия решений.\n",
        "\n",
        "**Недостатки**:\n",
        "- Может значительно сократить размер мажоритарного класса.\n",
        "- Эффективен только вблизи границ классов, не влияет на внутренние области.\n",
        "\n",
        "**Пример на Python (с использованием `imblearn.under_sampling.TomekLinks`)**:\n"
      ],
      "metadata": {
        "id": "WoC-ttmgW3xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "# Применение Tomek Links\n",
        "tl = TomekLinks(sampling_strategy='majority')  # Удаляем только мажоритарные образцы\n",
        "X_resampled, y_resampled = tl.fit_resample(X, y)\n",
        "\n",
        "print(f\"Распределение классов после Tomek Links: {Counter(y_resampled)}\")\n",
        "print(f\"Количество образцов после Tomek Links: {len(X_resampled)}\")"
      ],
      "metadata": {
        "id": "a_w_SGK8XATy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Edited Nearest Neighbors (ENN)\n",
        "\n",
        "Метод ENN удаляет примеры из мажоритарного класса, если их классификация по методу K-ближайших соседей (KNN) не совпадает с их истинным классом. То есть, если мажоритарный пример окружён в основном миноритарными примерами, он считается \"шумным\" и удаляется.\n",
        "\n",
        "**Преимущества**:\n",
        "- Эффективен для удаления шума и очистки границ классов.\n",
        "- Повышает качество обучающей выборки.\n",
        "\n",
        "**Недостатки**:\n",
        "- Может удалить слишком много данных, особенно при сильном перекрытии классов.\n",
        "- Чувствителен к выбору параметра $k$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y3Znl5HJXAeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q imbalanced-learn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "\n",
        "# 1. Генерация несбалансированного датасета\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=10, n_redundant=5,\n",
        "                           n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0.01, random_state=42)\n",
        "\n",
        "print(\"До ENN:\", np.bincount(y))\n",
        "\n",
        "# 2. Применение Edited Nearest Neighbors\n",
        "enn = EditedNearestNeighbours(n_neighbors=3)  # параметр k (кол-во соседей)\n",
        "X_res, y_res = enn.fit_resample(X, y)\n",
        "\n",
        "print(\"После ENN:\", np.bincount(y_res))\n",
        "\n",
        "# 3. Разделение и обучение модели до и после применения ENN\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_res, y_res, stratify=y_res, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# До ENN\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_orig = model.predict(X_test)\n",
        "print(\"\\nОценка модели ДО ENN:\")\n",
        "print(classification_report(y_test, y_pred_orig))\n",
        "\n",
        "# После ENN\n",
        "model.fit(X_res_train, y_res_train)\n",
        "y_pred_res = model.predict(X_test)\n",
        "print(\"\\nОценка модели ПОСЛЕ ENN:\")\n",
        "print(classification_report(y_test, y_pred_res))\n",
        "\n",
        "# 4. Визуализация матриц ошибок\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_orig, ax=axs[0], colorbar=False)\n",
        "axs[0].set_title(\"До ENN\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_res, ax=axs[1], colorbar=False)\n",
        "axs[1].set_title(\"После ENN\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n1xRJe7WXH2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### NearMiss\n",
        "\n",
        "Семейство методов андерсэмплинга, которые выбирают подмножество мажоритарных примеров на основе их расстояния до миноритарных. Например:\n",
        "- **NearMiss-1**: выбирает мажоритарные примеры, среднее расстояние до $k$ ближайших миноритарных соседей которых минимально (т.е. ближайшие к миноритарному классу).\n",
        "- **NearMiss-2**: выбирает те, у которых максимальное среднее расстояние.\n",
        "- **NearMiss-3**: для каждого миноритарного примера выбирает ближайших мажоритарных соседей.\n",
        "\n",
        "**Преимущества**:\n",
        "- Целенаправленно отбирает релевантные мажоритарные примеры, близкие к границе.\n",
        "- Сохраняет структуру распределения.\n",
        "\n",
        "**Недостатки**:\n",
        "- Вычислительно затратен.\n",
        "- Может быть чувствителен к выбросам и шуму.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TJds4OGaXIB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q imbalanced-learn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "# 1. Генерация несбалансированного датасета\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=10, n_redundant=5,\n",
        "                           n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0.01, random_state=42)\n",
        "\n",
        "print(\"До NearMiss:\", np.bincount(y))\n",
        "\n",
        "# 2. Применение NearMiss (можно менять версию: 1, 2 или 3)\n",
        "near_miss = NearMiss(version=1)  # Или version=2 / version=3\n",
        "X_res, y_res = near_miss.fit_resample(X, y)\n",
        "\n",
        "print(\"После NearMiss:\", np.bincount(y_res))\n",
        "\n",
        "# 3. Разделение и обучение модели до и после\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_res, y_res, stratify=y_res, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# До андерсэмплинга\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_orig = model.predict(X_test)\n",
        "print(\"\\nОценка модели ДО NearMiss:\")\n",
        "print(classification_report(y_test, y_pred_orig))\n",
        "\n",
        "# После андерсэмплинга\n",
        "model.fit(X_res_train, y_res_train)\n",
        "y_pred_res = model.predict(X_test)\n",
        "print(\"\\nОценка модели ПОСЛЕ NearMiss:\")\n",
        "print(classification_report(y_test, y_pred_res))\n",
        "\n",
        "# 4. Визуализация матриц ошибок\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_orig, ax=axs[0], colorbar=False)\n",
        "axs[0].set_title(\"До NearMiss\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_res, ax=axs[1], colorbar=False)\n",
        "axs[1].set_title(\"После NearMiss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UDXkNHQxXKso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.2. Алгоритмические подходы\n",
        "\n",
        "Эти подходы модифицируют алгоритм обучения или функцию потерь, чтобы учитывать дисбаланс классов, не изменяя при этом обучающие данные.\n",
        "\n",
        "#### 2.2.1. Взвешивание классов (Cost-Sensitive Learning)\n",
        "\n",
        "Заключается в присвоении различных весов ошибкам классификации для разных классов. Ошибки на миноритарном классе получают больший вес, что заставляет модель уделять им больше внимания.\n",
        "\n",
        "**Принцип работы**:\n",
        "Для каждого класса устанавливается вес, обратно пропорциональный его частоте. Многие алгоритмы (логистическая регрессия, SVM, деревья решений, случайный лес, нейросети) поддерживают параметр `class_weight`.\n",
        "\n",
        "**Пример на Python (с использованием `LogisticRegression`)**:\n",
        "\n"
      ],
      "metadata": {
        "id": "mIGBfjv1XK2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Распределение классов в обучающей выборке: {Counter(y_train)}\")\n",
        "\n",
        "# Модель без взвешивания\n",
        "model_no_weight = LogisticRegression(random_state=42, solver='liblinear')\n",
        "model_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = model_no_weight.predict(X_test)\n",
        "print(\"\\nОтчет классификации без взвешивания:\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "# Модель со взвешиванием\n",
        "model_with_weight = LogisticRegression(\n",
        "    random_state=42, solver='liblinear', class_weight='balanced'\n",
        ")\n",
        "model_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = model_with_weight.predict(X_test)\n",
        "print(\"\\nОтчет классификации со взвешиванием:\")\n",
        "print(classification_report(y_test, y_pred_with_weight))"
      ],
      "metadata": {
        "id": "i8byD7R5XNoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 2.2.2. Ансамблевые методы (Ensemble Methods)\n",
        "\n",
        "Некоторые ансамблевые методы адаптированы для работы с несбалансированными данными.\n",
        "\n",
        "- **Bagging с ресэмплингом**:  \n",
        "  Например, `BalancedBaggingClassifier` (из `imblearn`) создаёт несколько подвыборок с андерсэмплингом мажоритарного класса и обучает отдельный классификатор на каждой. Предсказания агрегируются.\n",
        "\n",
        "- **Boosting с модификациями**:  \n",
        "  Алгоритмы вроде **LightGBM** и **CatBoost** имеют встроенные параметры для борьбы с дисбалансом (например, `scale_pos_weight` в LightGBM, `class_weights` в CatBoost).\n",
        "\n",
        "- **EasyEnsemble / BalanceCascade**:  \n",
        "  Эти методы (из `imblearn`) создают ансамбль моделей, каждая из которых обучается на сбалансированной подвыборке, полученной случайным андерсэмплингом мажоритарного класса.\n"
      ],
      "metadata": {
        "id": "tEsAhNc-XNz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 2.2.3. Сдвиг порога классификации (Threshold Moving)\n",
        "\n",
        "Большинство классификаторов выдают вероятность принадлежности к классу. По умолчанию, если вероятность превышает 0.5, объект относится к позитивному классу. При несбалансированных данных этот порог может быть неоптимальным.\n",
        "\n",
        "**Сдвиг порога** заключается в изменении этого значения (например, на 0.3 или 0.7), чтобы увеличить **полноту (recall)** миноритарного класса за счёт возможного снижения **точности (precision)**, или наоборот — в зависимости от целей задачи.\n",
        "\n",
        "#### Принцип работы:\n",
        "После обучения модели выбирается новый порог, который оптимизирует желаемую метрику (например, F1-меру, recall) на валидационной выборке, вместо использования стандартного порога 0.5.\n",
        "\n",
        "#### Пример на Python (сдвиг порога для максимизации F1-меры):\n"
      ],
      "metadata": {
        "id": "LiA1mMIaLnjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Генерация несбалансированного датасета\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=2, n_informative=2,\n",
        "    n_redundant=0, n_repeated=0, n_classes=2,\n",
        "    n_clusters_per_class=1, weights=[0.90, 0.10],\n",
        "    flip_y=0, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Обучение модели\n",
        "model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Получение вероятностей для позитивного класса\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Построение PR-кривой\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Расчёт F1-меры для каждого порога\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "optimal_threshold_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_threshold_idx]\n",
        "\n",
        "print(f\"Оптимальный порог для максимизации F1-меры: {optimal_threshold:.3f}\")\n",
        "\n",
        "# Применение нового порога\n",
        "y_pred_optimal = (y_probs >= optimal_threshold).astype(int)\n",
        "\n",
        "print(\"\\nОтчет классификации с оптимальным порогом:\")\n",
        "print(classification_report(y_test, y_pred_optimal))"
      ],
      "metadata": {
        "id": "P2mkN7wdYsGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Использование синтетических данных (помимо SMOTE)\n",
        "\n",
        "Помимо SMOTE, можно использовать более продвинутые методы генерации текста для создания новых примеров миноритарного класса, особенно в NLP.\n",
        "\n",
        "#### • Генеративные модели (LLM)\n",
        "\n",
        "Большие языковые модели (LLM), такие как **GPT-3/4**, **Llama**, **Gemini**, могут генерировать новые текстовые примеры миноритарного класса, создавая более разнообразные и реалистичные синтетические данные по сравнению с методами интерполяции.\n",
        "\n",
        "**Принцип работы**:\n",
        "- Модель обучается или \"подсказывается\" на существующих примерах миноритарного класса.\n",
        "- С помощью промптов (например, *«Перефразируй этот текст, сохранив смысл»*) генерируются новые варианты.\n",
        "- Возможно применение тонкой настройки (fine-tuning) LLM на данных миноритарного класса для лучшего соответствия стилю и тематике.\n",
        "\n",
        "**Вызовы**:\n",
        "- **Контроль качества**: риск галлюцинаций, искажения смысла, грамматических ошибок.\n",
        "- **Сохранение семантики**: важно, чтобы сгенерированные тексты действительно соответствовали целевому классу.\n",
        "- **API-ограничения**: как показано в ошибке ниже, доступ к некоторым моделям (например, `gemini-2.0-flash`) может быть ограничен:\n",
        "\n",
        "> ```\n",
        "> Error 403 (Forbidden)\n",
        "> Your client does not have permission to get URL from this server.\n",
        "> ```\n",
        "\n",
        "**Рекомендации**:\n",
        "- Используйте официальные API (Google AI Studio, OpenAI, Anthropic) с валидными ключами.\n",
        "- Всегда проводите **ручную или автоматическую валидацию** сгенерированных данных.\n",
        "- Рассмотрите использование **локальных моделей** (например, Llama 3, Mistral) для избежания ограничений доступа.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Оценка моделей на несбалансированных выборках\n",
        "\n",
        "При работе с несбалансированными данными общая точность (**accuracy**) не является адекватной метрикой, так как может быть искусственно завышена за счёт предсказания мажоритарного класса. Необходимо использовать метрики, которые уделяют внимание производительности на миноритарном классе.\n",
        "\n",
        "\n",
        "\n",
        "### 3.1. Матрица ошибок (Confusion Matrix)\n",
        "\n",
        "Матрица ошибок — это таблица, визуализирующая производительность классификатора по классам.\n",
        "\n",
        "**Компоненты (для бинарной классификации)**:\n",
        "- **TP (True Positives)**: правильно предсказанные положительные примеры.\n",
        "- **TN (True Negatives)**: правильно предсказанные отрицательные примеры.\n",
        "- **FP (False Positives)**: отрицательные примеры, ошибочно предсказанные как положительные (ошибка I рода).\n",
        "- **FN (False Negatives)**: положительные примеры, ошибочно предсказанные как отрицательные (ошибка II рода).\n",
        "\n",
        "**Пример на Python**:\n",
        "\n"
      ],
      "metadata": {
        "id": "u-L-FKGrYsUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Генерация синтетических данных с дисбалансом классов (90% vs 10%)\n",
        "X, y = make_classification(\n",
        "    n_samples=100,\n",
        "    n_features=2,\n",
        "    n_informative=2,  # только информативные признаки (не больше, чем n_features)\n",
        "    n_redundant=0,    # убираем избыточные признаки\n",
        "    weights=[0.90, 0.10],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки с сохранением пропорций классов\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y  # стратификация для сохранения баланса классов\n",
        ")\n",
        "\n",
        "# Создание и обучение логистической регрессии\n",
        "model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание на тестовых данных\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Вычисление и вывод матрицы ошибок\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Матрица ошибок:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ESn9RZrcY0fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 3.2. Метрики оценки качества классификации: Precision, Recall, F1-мера\n",
        "\n",
        "В задачах классификации, особенно при анализе текстовых данных, одной из ключевых проблем является **несбалансированность классов**, когда количество объектов одного класса существенно превышает количество объектов другого. В таких условиях использование традиционной метрики — **точности (accuracy)** — может оказаться недостаточным и даже вводящим в заблуждение, поскольку модель может достигать высокого значения accuracy за счёт доминирующего класса, игнорируя при этом редкий, но значимый класс.\n",
        "\n",
        "Для более адекватной оценки качества классификатора применяются такие метрики, как **точность (precision)**, **полнота (recall)** и **F1-мера (F1-score)**. Эти метрики основаны на анализе **матрицы ошибок (confusion matrix)** и позволяют оценить эффективность модели с учётом специфики каждого класса.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.1. Матрица ошибок\n",
        "\n",
        "Пусть дана задача бинарной классификации, где положительный класс обозначается как \\( y = 1 \\), а отрицательный — как \\( y = 0 \\). Тогда матрица ошибок определяется следующим образом:\n",
        "\n",
        "|                        | Фактически $y = 1$ | Фактически $y = 0$ |\n",
        "|------------------------|------------------------|------------------------|\n",
        "| **Предсказано $\\hat{y} = 1$** | $TP$               | $FP$               |\n",
        "| **Предсказано $\\hat{y} = 0$** | $FN$               | $TN$               |\n",
        "\n",
        "где:  \n",
        "- $TP$ (True Positive) — число правильно классифицированных положительных примеров;  \n",
        "- $FP$ (False Positive) — число отрицательных примеров, ошибочно отнесённых к положительному классу;  \n",
        "- $FN$ (False Negative) — число положительных примеров, ошибочно отнесённых к отрицательному классу;  \n",
        "- $TN$ (True Negative) — число правильно классифицированных отрицательных примеров.\n",
        "\n",
        "На основе этих величин определяются основные метрики качества классификации.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.2. Точность (Precision)\n",
        "\n",
        "**Точность** отражает долю правильно предсказанных положительных примеров среди всех объектов, отнесённых моделью к положительному классу. Она определяется как:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Точность характеризует **достоверность предсказания** положительного класса. Высокое значение precision означает, что модель редко ошибается, когда классифицирует объект как положительный. Эта метрика особенно важна в задачах, где **ложные срабатывания (FP)** имеют высокую стоимость.\n",
        "\n",
        "*Пример.* В задаче фильтрации спама высокая точность означает, что в папку «спам» попадают в основном действительно спам-сообщения, а важные письма не теряются.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.3. Полнота (Recall / Sensitivity / True Positive Rate)\n",
        "\n",
        "**Полнота** показывает долю правильно классифицированных положительных примеров среди всех реально положительных объектов:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Полнота отражает **способность модели выявлять все положительные случаи**. Высокий recall означает, что модель пропускает минимальное число объектов положительного класса. Эта метрика критична в задачах, где **пропуск положительного случая (FN)** недопустим.\n",
        "\n",
        "*Пример.* В задаче выявления негативных отзывов о продукте высокий recall позволяет своевременно реагировать на жалобы клиентов.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.4. F1-мера (F1-score)\n",
        "\n",
        "**F1-мера** представляет собой **гармоническое среднее** между precision и recall и используется для комплексной оценки качества модели в условиях необходимости баланса между достоверностью и полнотой предсказаний:\n",
        "\n",
        "$$\n",
        "\\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "F1-мера достигает максимума при одновременно высоких значениях precision и recall. Она особенно эффективна при оценке моделей на **несбалансированных выборках**, где доминирующий класс может маскировать неудовлетворительную работу классификатора по отношению к редкому классу.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.5. Пример расчёта метрик в задаче классификации текстов\n",
        "\n",
        "Рассмотрим задачу классификации отзывов на продукты по признаку эмоциональной окраски:  \n",
        "- Положительный класс ($y = 1$) — негативные отзывы;  \n",
        "- Отрицательный класс ($y = 0$) — позитивные отзывы.\n",
        "\n",
        "Объём тестовой выборки — 1000 экземпляров, из них:\n",
        "- $200$ — негативные отзывы (фактические $y = 1$);\n",
        "- $800$ — позитивные отзывы (фактические $y = 0$).\n",
        "\n",
        "Результаты работы классификатора:\n",
        "- $TP = 150$ — правильно распознаны как негативные;\n",
        "- $FN = 50$ — ошибочно отнесены к позитивным;\n",
        "- $FP = 100$ — позитивные отзывы ошибочно классифицированы как негативные;\n",
        "- $TN = 700$ — правильно распознаны как позитивные.\n",
        "\n",
        "Вычислим метрики:\n",
        "\n",
        "1. **Точность (Precision)**:\n",
        "$$\n",
        "\\text{Precision} = \\frac{150}{150 + 100} = \\frac{150}{250} = 0{,}600\n",
        "$$\n",
        "\n",
        "2. **Полнота (Recall)**:\n",
        "$$\n",
        "\\text{Recall} = \\frac{150}{150 + 50} = \\frac{150}{200} = 0{,}750\n",
        "$$\n",
        "\n",
        "3. **F1-мера**:\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{0{,}600 \\times 0{,}750}{0{,}600 + 0{,}750} = 2 \\times \\frac{0{,}45}{1{,}35} \\approx 0{,}667\n",
        "$$\n",
        "\n",
        "Полученные значения свидетельствуют о **среднем уровне качества модели**: она находит 75 % негативных отзывов, но при этом только 60 % её «негативных» предсказаний действительно являются таковыми. F1-мера, равная 0,667, указывает на необходимость улучшения модели, особенно за счёт снижения числа ложноположительных срабатываний.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.6. Реализация в Python\n",
        "\n",
        "Для расчёта метрик в библиотеке `scikit-learn` предусмотрены функции `precision_score`, `recall_score` и `f1_score`. Ниже приведён пример кода:\n"
      ],
      "metadata": {
        "id": "tR1lr34DY0on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Истинные и предсказанные метки\n",
        "y_true = [1]*200 + [0]*800\n",
        "y_pred = [1]*150 + [0]*50 + [1]*100 + [0]*700\n",
        "\n",
        "# Расчёт метрик для положительного класса\n",
        "precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"F1-score:  {f1:.3f}\")\n",
        "\n",
        "# Полный отчёт\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Позитив', 'Негатив']))"
      ],
      "metadata": {
        "id": "fzQR1wOEb9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример 2."
      ],
      "metadata": {
        "id": "jCqk8m5ncCpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Создаем синтетический несбалансированный датасет\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Разделяем на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучаем модель логистической регрессии\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Получаем предсказания\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Вычисляем метрики по отдельности\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Отдельные метрики:\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\\n\")\n",
        "\n",
        "# Выводим полный отчет классификации\n",
        "print(\"Полный отчет классификации:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
      ],
      "metadata": {
        "id": "ST-vzPuPZ1wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 3.2.7. Области применения метрик в NLP\n",
        "\n",
        "Выбор приоритетной метрики зависит от предметной области и последствий ошибок классификации:\n",
        "\n",
        "- **Высокий precision** требуется в задачах, где **ложные срабатывания** критичны:  \n",
        "  — фильтрация спама;  \n",
        "  — автоматическая отправка уведомлений по негативным отзывам.\n",
        "\n",
        "- **Высокий recall** необходим в задачах, где **пропуск положительного случая** опасен:  \n",
        "  — выявление токсичных комментариев;  \n",
        "  — диагностика по медицинским текстам.\n",
        "\n",
        "- **F1-score** используется как **обобщающая метрика** при необходимости компромисса между precision и recall, особенно при сравнении моделей или подборе гиперпараметров.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, Precision, recall и F1-мера являются фундаментальными метриками оценки качества бинарных и многоклассовых классификаторов, особенно в условиях несбалансированных данных. В задачах обработки естественного языка (NLP), таких как классификация тональности, детекция спама или тематическая сегментация текстов, эти метрики позволяют получить объективную оценку эффективности модели с учётом специфики распределения классов и стоимости различных типов ошибок. Использование данной группы метрик способствует более осознанному выбору и настройке алгоритмов машинного обучения.\n",
        "\n",
        "> В отчёте особенно важно анализировать **precision**, **recall** и **F1-score** для **миноритарного класса** (обычно класс 1), так как именно его качество определяет эффективность модели в задачах с дисбалансом.\n"
      ],
      "metadata": {
        "id": "DyJRJbnUZ16X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 3.3. ROC-AUC и PR-AUC: Оценка производительности на несбалансированных данных\n",
        "\n",
        "Эти метрики оценивают производительность классификатора по всем возможным порогам классификации.\n",
        "\n",
        "#### • ROC-AUC (Receiver Operating Characteristic — Area Under the Curve)\n",
        "\n",
        "ROC-AUC — это площадь под кривой, построенной по значениям **True Positive Rate (TPR)** на оси Y и **False Positive Rate (FPR)** на оси X при различных порогах.\n",
        "\n",
        "Формулы:\n",
        "$$\n",
        "\\text{TPR (Recall)} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "$$\n",
        "\\text{FPR} = \\frac{FP}{FP + TN}\n",
        "$$\n",
        "\n",
        "**Интерпретация**:\n",
        "- Значение ROC-AUC близко к **1.0** — модель отлично различает классы.\n",
        "- Значение около **0.5** — модель работает как случайный угадыватель.\n",
        "- Значение ниже 0.5 — модель работает хуже случайной.\n",
        "\n",
        "**Плюсы**:\n",
        "- ROC-AUC устойчив к дисбалансу классов.\n",
        "- Хорошо показывает общую способность модели к разделению классов.\n",
        "\n",
        "**Минусы**:\n",
        "- Может быть **малоинформативен при сильном дисбалансе**, так как FPR зависит от большого числа TN (мажоритарный класс), что может \"замаскировать\" плохую производительность на миноритарном классе.\n",
        "\n",
        "\n",
        "\n",
        "#### • PR-AUC (Precision-Recall Area Under the Curve)\n",
        "\n",
        "PR-AUC — площадь под кривой, где по оси Y откладывается **Precision**, а по оси X — **Recall**.\n",
        "\n",
        "Формула:\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "**Интерпретация**:\n",
        "- PR-AUC близок к 1.0 — высокая точность и полнота.\n",
        "- Близок к 0 — плохая производительность.\n",
        "\n",
        "**Плюсы**:\n",
        "- **Более информативен для сильно несбалансированных данных**, особенно когда миноритарный класс — положительный (например, мошенничество, болезнь).\n",
        "- Не зависит от количества истинно отрицательных примеров (TN), что делает его чувствительным к качеству предсказаний на редком классе.\n",
        "\n",
        "**Рекомендация**:  \n",
        "Для сильно несбалансированных данных **PR-AUC предпочтительнее ROC-AUC**.\n",
        "\n",
        "\n",
        "\n",
        "#### Пример на Python (оценка ROC-AUC и PR-AUC):"
      ],
      "metadata": {
        "id": "2oVS-6v-cMOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (roc_curve, auc, precision_recall_curve,\n",
        "                             average_precision_score, roc_auc_score)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# 1. Генерация несбалансированных данных\n",
        "X, y = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=10,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # 90% отрицательных, 10% положительных\n",
        "    flip_y=0.01,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Обучение модели (с учётом дисбаланса)\n",
        "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Получение вероятностей положительного класса\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 3. Вычисление ROC-AUC\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# 4. Вычисление PR-AUC\n",
        "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)\n",
        "pr_auc = average_precision_score(y_test, y_proba)  # Это и есть PR-AUC\n",
        "\n",
        "# 5. Визуализация\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC Curve\n",
        "ax1.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "ax1.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random Classifier')\n",
        "ax1.set_xlim([0.0, 1.0])\n",
        "ax1.set_ylim([0.0, 1.05])\n",
        "ax1.set_xlabel('False Positive Rate (FPR)')\n",
        "ax1.set_ylabel('True Positive Rate (TPR)')\n",
        "ax1.set_title('ROC Curve')\n",
        "ax1.legend(loc=\"lower right\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# PR Curve\n",
        "ax2.plot(recall, precision, color='green', lw=2, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "ax2.set_xlabel('Recall (TPR)')\n",
        "ax2.set_ylabel('Precision')\n",
        "ax2.set_title('Precision-Recall Curve')\n",
        "ax2.legend(loc=\"lower left\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Вывод метрик\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC (Average Precision): {pr_auc:.4f}\")\n",
        "\n",
        "# Интерпретация\n",
        "if roc_auc > 0.9:\n",
        "    print(\"ROC-AUC: Модель отлично разделяет классы.\")\n",
        "elif roc_auc > 0.7:\n",
        "    print(\"ROC-AUC: Умеренная производительность.\")\n",
        "elif roc_auc > 0.5:\n",
        "    print(\"ROC-AUC: Слабая производительность.\")\n",
        "else:\n",
        "    print(\"ROC-AUC: Хуже случайной модели.\")\n",
        "\n",
        "if pr_auc > 0.5:\n",
        "    print(\"PR-AUC: Хорошее качество предсказаний для положительного класса.\")\n",
        "else:\n",
        "    print(\"PR-AUC: Низкое качество обнаружения положительного класса.\")"
      ],
      "metadata": {
        "id": "g245Z7DNca41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Функция `make_classification` из библиотеки `scikit-learn`\n",
        "\n",
        "### Введение\n",
        "\n",
        "В процессе разработки, тестирования и анализа алгоритмов машинного обучения часто возникает необходимость в использовании **контролируемых и воспроизводимых наборов данных**. Реальные данные могут содержать шум, пропуски, смещения и сложные зависимости, что затрудняет интерпретацию поведения моделей. Для решения этой задачи применяются **методы генерации синтетических данных**, позволяющие исследовать поведение алгоритмов в условиях, заданных априори.\n",
        "\n",
        "Одной из наиболее востребованных функций для генерации искусственных данных в контексте задач классификации является `make_classification` из библиотеки `scikit-learn`.\n",
        "\n",
        "\n",
        "\n",
        "Функция `make_classification` предназначена для **синтетической генерации многомерных наборов данных**, предназначенных для обучения и оценки алгоритмов бинарной или многоклассовой классификации. Она позволяет контролировать структуру данных, включая количество информативных признаков, степень разделения классов, наличие шума, дисбаланс классов и корреляции между признаками.\n",
        "\n",
        "Основные области применения:\n",
        "- Исследование устойчивости моделей к дисбалансу классов;\n",
        "- Анализ влияния шума в признаках и метках на качество классификации;\n",
        "- Визуализация и демонстрация принципов работы метрик (например, ROC-AUC, PR-AUC);\n",
        "- Отладка и тестирование новых алгоритмов.\n",
        "\n",
        "\n",
        "\n",
        "Генерация данных осуществляется на основе **модели смеси многомерных нормальных распределений**. Каждый класс моделируется как одна или несколько гауссовских компонент (кластеров), центры которых разнесены в признаковом пространстве. Объекты, принадлежащие каждому классу, генерируются вокруг соответствующих центров с использованием многомерного нормального распределения.\n",
        "\n",
        "Для повышения реалистичности модели вводятся:\n",
        "- **Зависимые признаки** — линейные комбинации информативных признаков;\n",
        "- **Шумовые признаки** — независимые случайные величины, не несущие информативной нагрузки;\n",
        "- **Шум в метках** — случайное изменение истинной метки с заданной вероятностью, имитирующее ошибки разметки.\n",
        "\n",
        "Таким образом, синтетический набор данных имитирует сложную структуру реальных данных, включая избыточность, шум и неидеальную разделимость классов.\n",
        "\n",
        "\n",
        "Функция `make_classification` имеет следующий синтаксис:\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=100,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=2,\n",
        "    n_repeated=0,\n",
        "    n_classes=2,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=None,\n",
        "    flip_y=0.01,\n",
        "    class_sep=1.0,\n",
        "    hypercube=True,\n",
        "    shift=0.0,\n",
        "    scale=1.0,\n",
        "    shuffle=True,\n",
        "    random_state=None\n",
        ")\n",
        "```\n",
        "\n",
        "#### Описание ключевых параметров:\n",
        "\n",
        "| Параметр | Описание |\n",
        "|--------|--------|\n",
        "| `n_samples` | Общее количество объектов в выборке. |\n",
        "| `n_features` | Общее число признаков (размерность признакового пространства). |\n",
        "| `n_informative` | Число признаков, **информативных** для разделения классов (непосредственно участвуют в формировании границы принятия решений). |\n",
        "| `n_redundant` | Число признаков, являющихся **линейными комбинациями** информативных признаков. |\n",
        "| `n_repeated` | Число признаков, являющихся **дубликатами** уже существующих (случайно перемешанных). |\n",
        "| `n_classes` | Количество классов (по умолчанию 2 — бинарная классификация). |\n",
        "| `n_clusters_per_class` | Число кластеров (гауссовских компонент) на один класс. |\n",
        "| `weights` | Список долей объектов для каждого класса. Используется для моделирования **дисбаланса классов**. Пример: `weights=[0.9, 0.1]` означает, что 90% объектов принадлежат первому классу, 10% — второму. |\n",
        "| `flip_y` | Вероятность **инверсии метки** (ошибки разметки). Имитирует шум в целевой переменной. |\n",
        "| `class_sep` | Параметр, контролирующий **степень разделения классов**. Чем выше значение, тем лучше классы разделяются. При малых значениях классы могут сильно перекрываться. |\n",
        "| `random_state` | Сид для генератора случайных чисел. Обеспечивает **воспроизводимость** результатов. |\n",
        "\n",
        "> ⚠️ Примечание: Сумма `n_informative + n_redundant + n_repeated` не должна превышать `n_features`. Остальные признаки будут заполнены шумом.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим пример генерации синтетического набора данных с явно выраженным дисбалансом классов, что типично для задач обнаружения редких событий (например, мошенничества, редких заболеваний):\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    weights=[0.8, 0.2],      # Дисбаланс: 80% / 20%\n",
        "    flip_y=0.01,             # 1% шума в метках\n",
        "    class_sep=0.8,           # Умеренное разделение классов\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Размер выборки: {X.shape}\")\n",
        "print(f\"Число классов: {len(np.unique(y))}\")\n",
        "print(f\"Распределение по классам:\\n{np.bincount(y)}\")\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Размер выборки: (1000, 10)\n",
        "Число классов: 2\n",
        "Распределение по классам:\n",
        "[800 200]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "###Преимущества и ограничения\n",
        "\n",
        "#### Преимущества:\n",
        "- Полный контроль над структурой данных;\n",
        "- Возможность моделирования дисбаланса, шума и коррелированных признаков;\n",
        "- Поддержка многоклассовой классификации;\n",
        "- Воспроизводимость при фиксированном `random_state`.\n",
        "\n",
        "#### Ограничения:\n",
        "- Все признаки — непрерывные (нельзя генерировать категориальные);\n",
        "- Предполагается нормальное распределение внутри кластеров;\n",
        "- Не учитывает сложные зависимости, характерные для реальных данных (например, выбросы, мультимодальность, временные зависимости).\n",
        "\n",
        "\n",
        "\n",
        "###Рекомендации по использованию\n",
        "\n",
        "1. **Для обучения и демонстрации**: `make_classification` идеально подходит для иллюстрации концепций машинного обучения, таких как переобучение, оценка метрик, влияние дисбаланса.\n",
        "2. **Для тестирования моделей**: позволяет проводить сравнительный анализ алгоритмов в контролируемых условиях.\n",
        "3. **Для исследования метрик**: особенно полезен при изучении поведения PR-AUC и ROC-AUC на несбалансированных данных.\n",
        "\n",
        "> 💡 **Рекомендация**: Всегда используйте параметр `random_state` для обеспечения воспроизводимости экспериментов.\n"
      ],
      "metadata": {
        "id": "bZEcD4DCLnoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.4. Метрики качества в задачах мультиклассовой классификации\n",
        "\n",
        "В задачах классификации с более чем двумя классами оценка качества модели требует применения специализированных метрик, адаптированных к многомерной структуре выходов. Особую значимость приобретает выбор стратегии агрегации частных метрик по отдельным классам, особенно в условиях несбалансированности распределения объектов по классам. Ниже рассматриваются три основные стратегии усреднения метрик: микро-усреднение (micro-average), макро-усреднение (macro-average) и взвешенное усреднение (weighted-average).\n",
        "\n",
        "\n",
        "\n",
        "#### 3.4.1. Micro-average (микро-усреднение)\n",
        "\n",
        "Микро-усреднение основано на **агрегации значений истинно положительных (TP), ложно положительных (FP) и ложно отрицательных (FN) результатов** по всем классам перед вычислением обобщённых метрик. Данный подход эквивалентен усреднению на уровне отдельных объектов и придаёт больший вес классам, представленным большим количеством примеров.\n",
        "\n",
        "Пусть $C$ — число классов, $\\text{TP}_i$, $\\text{FP}_i$, $\\text{FN}_i$ — соответствующие значения для $i$-го класса. Тогда суммарные значения вычисляются следующим образом:\n",
        "\n",
        "$$\n",
        "\\text{TP}_{\\text{micro}} = \\sum_{i=1}^{C} \\text{TP}_i, \\quad\n",
        "\\text{FP}_{\\text{micro}} = \\sum_{i=1}^{C} \\text{FP}_i, \\quad\n",
        "\\text{FN}_{\\text{micro}} = \\sum_{i=1}^{C} \\text{FN}_i\n",
        "$$\n",
        "\n",
        "На их основе определяются обобщённые метрики:\n",
        "\n",
        "$$\n",
        "\\text{Precision}_{\\text{micro}} = \\frac{\\text{TP}_{\\text{micro}}}{\\text{TP}_{\\text{micro}} + \\text{FP}_{\\text{micro}}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Recall}_{\\text{micro}} = \\frac{\\text{TP}_{\\text{micro}}}{\\text{TP}_{\\text{micro}} + \\text{FN}_{\\text{micro}}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "F1_{\\text{micro}} = 2 \\cdot \\frac{\\text{Precision}_{\\text{micro}} \\cdot \\text{Recall}_{\\text{micro}}}{\\text{Precision}_{\\text{micro}} + \\text{Recall}_{\\text{micro}}}\n",
        "$$\n",
        "\n",
        "**Свойства микро-усреднения:**\n",
        "- Учитывает объём каждого класса, что делает метрику чувствительной к доминирующим классам.\n",
        "- В случае полного учёта всех классов выполняется равенство:  \n",
        "  $$\n",
        "  \\text{Precision}_{\\text{micro}} = \\text{Recall}_{\\text{micro}} = F1_{\\text{micro}}\n",
        "  $$\n",
        "- Применяется, когда важна **общая эффективность модели** на всём наборе данных, например, в задачах с приоритетом на глобальную точность.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.4.2. Macro-average (макро-усреднение)\n",
        "\n",
        "Макро-усреднение предполагает **независимое вычисление метрик для каждого класса**, после чего находится их **арифметическое среднее**. При этом все классы учитываются с равным весом, независимо от их размера.\n",
        "\n",
        "Формально:\n",
        "\n",
        "$$\n",
        "\\text{Precision}_{\\text{macro}} = \\frac{1}{C} \\sum_{i=1}^{C} P_i, \\quad \\text{где } P_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Recall}_{\\text{macro}} = \\frac{1}{C} \\sum_{i=1}^{C} R_i, \\quad \\text{где } R_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FN}_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "F1_{\\text{macro}} = \\frac{1}{C} \\sum_{i=1}^{C} F1_i, \\quad \\text{где } F1_i = 2 \\cdot \\frac{P_i \\cdot R_i}{P_i + R_i}\n",
        "$$\n",
        "\n",
        "**Свойства макро-усреднения:**\n",
        "- Обеспечивает равный вклад каждого класса в итоговую метрику.\n",
        "- Чувствителен к производительности на **миноритарных классах**.\n",
        "- Рекомендуется при решении задач с **сильным дисбалансом классов**, когда критично качество распознавания редких категорий.\n",
        "\n",
        "\n",
        "\n",
        "#### 3.4.3. Weighted-average (взвешенное усреднение)\n",
        "\n",
        "Взвешенное усреднение представляет собой компромисс между микро- и макро-подходами. Оно учитывает **распределение числа объектов по классам**, присваивая каждому классу вес, пропорциональный его доле в общей выборке.\n",
        "\n",
        "Пусть $w_i = \\frac{N_i}{N}$, где $N_i$ — количество объектов класса $i$, $N$ — общее число объектов. Тогда:\n",
        "\n",
        "$$\n",
        "\\text{Precision}_{\\text{weighted}} = \\sum_{i=1}^{C} w_i \\cdot P_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Recall}_{\\text{weighted}} = \\sum_{i=1}^{C} w_i \\cdot R_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "F1_{\\text{weighted}} = \\sum_{i=1}^{C} w_i \\cdot F1_i\n",
        "$$\n",
        "\n",
        "**Свойства взвешенного усреднения:**\n",
        "- Учитывает как размер класса, так и его вклад в качество классификации.\n",
        "- Подходит для ситуаций, когда необходимо сбалансированно учитывать как частоту класса, так и качество его предсказания.\n",
        "- Широко используется в прикладных задачах, где полное игнорирование размера классов неприемлемо, но и малые классы не должны быть полностью проигнорированы.\n",
        "\n",
        "\n",
        "\n",
        "### 3.4.4. Практическая реализация в Python\n",
        "\n",
        "Для демонстрации применения различных стратегий усреднения рассмотрим пример на основе синтетических данных с несбалансированным распределением классов.\n"
      ],
      "metadata": {
        "id": "UBMoG5Ufd-3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Генерация несбалансированных данных\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_classes=3,\n",
        "    n_informative=5,\n",
        "    n_redundant=1,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.8, 0.15, 0.05],  # дисбаланс классов\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Обучение модели\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Вывод отчёта по метрикам\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Явное вычисление усреднённых метрик\n",
        "print(\"\\nЯвные значения метрик:\")\n",
        "print(f\"Micro — Precision: {precision_score(y_test, y_pred, average='micro'):.3f}, \"\n",
        "      f\"Recall: {recall_score(y_test, y_pred, average='micro'):.3f}, \"\n",
        "      f\"F1: {f1_score(y_test, y_pred, average='micro'):.3f}\")\n",
        "\n",
        "print(f\"Macro — Precision: {precision_score(y_test, y_pred, average='macro'):.3f}, \"\n",
        "      f\"Recall: {recall_score(y_test, y_pred, average='macro'):.3f}, \"\n",
        "      f\"F1: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
        "\n",
        "print(f\"Weighted — Precision: {precision_score(y_test, y_pred, average='weighted'):.3f}, \"\n",
        "      f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.3f}, \"\n",
        "      f\"F1: {f1_score(y_test, y_pred, average='weighted'):.3f}\")"
      ],
      "metadata": {
        "id": "x-h5JCVYeDKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### 3.4.5. Рекомендации по выбору стратегии усреднения\n",
        "\n",
        "| Стратегия       | Преимущества | Недостатки | Рекомендуется при |\n",
        "|------------------|-------------|-----------|-------------------|\n",
        "| **Micro**        | Учитывает объём классов, инвариантна к дисбалансу на уровне объектов | Может маскировать плохое качество на малых классах | Оценке общей эффективности модели |\n",
        "| **Macro**        | Даёт равный вес всем классам, чувствителен к миноритарным | Может переоценивать ошибки в малых классах | Сильном дисбалансе и важности редких классов |\n",
        "| **Weighted**     | Балансирует между размером и качеством класса | Менее чувствителен к миноритарным классам, чем макро | Необходимости учесть распределение классов |\n",
        "\n",
        "\n",
        "\n",
        "Выбор стратегии усреднения метрик в мультиклассовой классификации является важным этапом валидации модели. Он должен определяться характером задачи, степенью дисбаланса классов и приоритетами в интерпретации результатов. Корректное использование микро-, макро- и взвешенных метрик позволяет получить более полное и объективное представление о поведении модели в реальных условиях.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Лучшие практики и соображения\n",
        "\n",
        "- **Комбинирование техник**:  \n",
        "  Часто наилучшие результаты достигаются при комбинации методов, например:  \n",
        "  **SMOTE + взвешивание классов**, или **анализ сдвига порога** после андерсэмплинга.\n",
        "\n",
        "- **Кросс-валидация**:  \n",
        "  При использовании ресэмплинга (SMOTE, undersampling и др.) **применяйте его только внутри обучающих фолдов**.  \n",
        "  Никогда не применяйте ресэмплинг ко всему датасету до разделения — это вызовет **утечку данных** и завышенную оценку.  \n",
        "  Используйте `imblearn.pipeline.Pipeline` для корректной интеграции.\n",
        "\n",
        "- **Выбор метрик**:  \n",
        "  Всегда используйте **Precision, Recall, F1-score, PR-AUC**.  \n",
        "  Избегайте зависимости от **общей точности (accuracy)** в несбалансированных задачах.\n",
        "\n",
        "- **Доменные знания**:  \n",
        "  В задачах, где **пропуск события критичен** (например, диагностика болезней), максимизируйте **Recall**.  \n",
        "  Если **ложные срабатывания дорогостоящи** (например, блокировка легитимных транзакций), фокусируйтесь на **Precision**.\n",
        "\n",
        "- **Итеративный подход**:  \n",
        "  Экспериментируйте с различными стратегиями, оценивайте их влияние и выбирайте ту, что лучше всего соответствует целям проекта.\n"
      ],
      "metadata": {
        "id": "k5ToYw4leDTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# §5. Проблема Data Leakage в NLP: Полное описание\n",
        "\n",
        "В процессе разработки моделей обработки естественного языка (NLP) одной из наиболее коварных и трудноуловимых проблем является **утечка данных (Data Leakage)**. Это явление возникает, когда информация из тестовой или валидационной выборки непреднамеренно «просачивается» в обучающую выборку. Результатом такой утечки становится завышенная, чрезмерно оптимистичная оценка производительности модели на тестовых данных, что создаёт иллюзию высокой эффективности — иллюзию, которая не подтверждается при развертывании модели в реальных условиях на новых, ранее не виданных данных.\n",
        "\n",
        "Понимание механизмов утечки данных и методов её предотвращения критически важно для создания надёжных и обобщающих NLP-систем.\n",
        "\n",
        "\n",
        "## 1. Определение Data Leakage\n",
        "\n",
        "**Утечка данных** — это ситуация, при которой модель машинного обучения получает доступ к информации, которая была бы недоступна ей во время реального применения (инференса). Эта «недоступная» информация может быть связана с целевой переменной или с данными, которые должны быть строго отделены для целей тестирования.\n",
        "\n",
        "Таким образом, модель «подсматривает» ответы или получает неявные подсказки о данных, которые она должна предсказывать, что приводит к искусственному завышению её производительности на этапе разработки и тестирования.\n",
        "\n",
        "Утечка может быть:\n",
        "- **явной** (например, прямое включение тестовых примеров в обучающую выборку),\n",
        "- **скрытой и тонкой**, проявляющейся через неочевидные связи в предобработке, инженерии признаков или выборе алгоритмов.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Типы и примеры Data Leakage в NLP\n",
        "\n",
        "Утечка данных может проявляться на различных этапах NLP-пайплайна. Ниже — наиболее распространённые сценарии.\n",
        "\n",
        "### 2.1. Утечка при разделении данных (Data Splitting Leakage)\n",
        "\n",
        "Одна из самых частых форм утечки, возникающая из-за неправильной последовательности операций при подготовке данных.\n",
        "\n",
        "#### • Некорректная предобработка до разделения\n",
        "\n",
        "Если такие операции, как токенизация, удаление стоп-слов, стемминг, лемматизация, построение словаря или вычисление статистик (например, TF-IDF), выполняются на всём корпусе **до** разделения на обучающую, валидационную и тестовую выборки, происходит утечка. Модель получает информацию о словах и их частотах из тестовых данных, которой она не должна обладать.\n",
        "\n",
        "**Пример: вычисление TF-IDF на всём датасете**\n",
        "- Обучающий документ: «Кот спит на диване».\n",
        "- Тестовый документ: «Собака спит на полу».\n",
        "\n",
        "Если TF-IDF рассчитывается на всём корпусе, слово «спит» получит IDF, основанный на его присутствии в обоих документах. Но если бы разделение было сделано до вычисления, IDF «спит» в обучающей выборке был бы выше (только в одном документе). Модель, таким образом, «знает» о слове из будущего — это и есть утечка.\n",
        "\n",
        "#### Временная утечка (Temporal Leakage)\n",
        "\n",
        "Возникает в задачах с временной зависимостью (анализ новостей, прогнозирование, эволюция языка), когда будущие данные используются для обучения модели, предсказывающей прошлое.\n",
        "\n",
        "**Пример**:  \n",
        "Модель обучается на твитах за 2023 и 2024 годы, а тестируется на твитах за 2024 год. Если часть данных за 2024 год попала в обучение, модель использует «будущее» для предсказания «прошлого» — нарушение причинности.\n",
        "\n",
        "#### • Групповая утечка (Group Leakage)\n",
        "\n",
        "Происходит, когда одна и та же группа (пользователь, автор, продукт) встречается в обучающей и тестовой выборках.\n",
        "\n",
        "**Пример**:  \n",
        "Один пользователь написал 10 отзывов. 7 попали в обучение, 3 — в тест. Модель может «запомнить» стиль этого пользователя, а не научиться обобщать по признакам. Это приведёт к завышенной оценке, но плохой обобщающей способности на новых пользователях.\n",
        "\n",
        "\n",
        "### 2.2. Утечка в инженерии признаков (Feature Engineering Leakage)\n",
        "\n",
        "Происходит, когда признаки создаются с использованием информации, недоступной во время инференса.\n",
        "\n",
        "#### • Признаки, основанные на целевой переменной\n",
        "\n",
        "Если признак строится на основе метки, которую модель должна предсказывать, это прямая утечка.\n",
        "\n",
        "**Пример**:  \n",
        "В задаче классификации спама вы создаете признак «содержит ли письмо слово \"победитель\" в тех письмах, которые были помечены как спам». Такой признак использует информацию, доступную только после разметки — это утечка.\n",
        "\n",
        "#### • Признаки из будущей информации\n",
        "\n",
        "Использование данных, которые будут известны только после момента предсказания.\n",
        "\n",
        "**Пример**:  \n",
        "Предсказание популярности статьи до публикации, но с использованием признака «количество репостов за первую неделю». Этот признак недоступен до выхода статьи — его нельзя использовать при обучении.\n",
        "\n",
        "\n",
        "\n",
        "### 2.3. Утечка в кросс-валидации (Cross-Validation Leakage)\n",
        "\n",
        "Если предобработка или выбор признаков выполняются **вне** циклов кросс-валидации, информация из валидационных фолдов может повлиять на обучение.\n",
        "\n",
        "**Пример**:  \n",
        "Вы определяете оптимальное количество n-грамм для TF-IDF на всём датасете **до** кросс-валидации. Это означает, что информация из тестовых фолдов уже повлияла на выбор гиперпараметра — нарушение изоляции.\n",
        "\n",
        "✅ **Решение**: Используйте `Pipeline` (например, `imblearn.pipeline.Pipeline` или `sklearn.pipeline.Pipeline`), чтобы включить предобработку внутрь процесса валидации.\n",
        "\n",
        "\n",
        "\n",
        "### 2.4. Утечка через внешние данные (External Data Leakage)\n",
        "\n",
        "Использование предварительно обученных моделей или эмбеддингов, обученных на корпусах, которые могут содержать данные из вашей тестовой выборки.\n",
        "\n",
        "**Пример**:  \n",
        "Использование Word2Vec-эмбеддингов, обученных на большом интернет-корпусе, который мог включать часть вашего тестового набора.\n",
        "\n",
        "⚠️ **Примечание**:  \n",
        "Эта утечка часто неизбежна, но её нужно **осознавать**. Польза от предобученных моделей обычно перевешивает риски, но в критических задачах (например, бенчмарки) следует использовать только модели, обученные на данных, строго отделённых от тестового набора.\n",
        "\n",
        "\n",
        "## 3. Последствия Data Leakage\n",
        "\n",
        "Утечка данных может привести к серьёзным проблемам:\n",
        "\n",
        "- **Чрезмерно оптимистичная оценка производительности**:  \n",
        "  Модель показывает высокую точность на тесте, но проваливается в реальности.\n",
        "\n",
        "- **Плохая обобщающая способность**:  \n",
        "  Модель не учится истинным закономерностям, а запоминает специфические паттерны тестовой выборки.\n",
        "\n",
        "- **Потеря доверия**:  \n",
        "  Неудачное развертывание модели подрывает доверие к команде и методологии.\n",
        "\n",
        "- **Неэффективное использование ресурсов**:  \n",
        "  Время и деньги тратятся на оптимизацию модели, которая не будет работать в продакшене.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Методы предотвращения Data Leakage\n",
        "\n",
        "Предотвращение утечки требует строгой дисциплины и соблюдения правильной последовательности операций.\n",
        "\n",
        "### 4.1. Строгое разделение данных\n",
        "\n",
        "- Разделяйте данные на **обучающую**, **валидационную** и **тестовую** выборки **до** любой предобработки.\n",
        "- Тестовая выборка должна быть **полностью изолирована** и использоваться **только один раз** — для финальной оценки.\n",
        "- Валидационная выборка используется для настройки гиперпараметров и выбора модели.\n",
        "\n",
        "### 4.2. Изолированная предобработка\n",
        "\n",
        "- Все операции (токенизация, построение словаря, TF-IDF, нормализация) должны выполняться **только на обучающей выборке**, а затем **применяться** к валидационной и тестовой.\n",
        "- Никогда не используйте статистики, вычисленные на всём датасете.\n",
        "\n",
        "### 4.3. Пайплайны и кросс-валидация\n",
        "\n",
        "- Используйте **конвейеры (pipelines)**, включающие предобработку и модель, чтобы гарантировать, что преобразования применяются только к обучающим фолдам внутри кросс-валидации.\n",
        "- Это исключает утечку при выборе признаков и гиперпараметров.\n",
        "\n",
        "### 4.4. Временная валидация\n",
        "\n",
        "- В задачах с временной зависимостью используйте **хронологическое разделение**: обучение на более ранних данных, тестирование на более поздних.\n",
        "- Никогда не используйте случайное перемешивание, если данные упорядочены во времени.\n",
        "\n",
        "### 4.5. Контроль групп\n",
        "\n",
        "- При наличии групп (пользователи, авторы и т.д.) используйте **групповое разделение (GroupShuffleSplit, GroupKFold)**, чтобы одна группа не попадала и в обучение, и в тест.\n",
        "\n",
        "\n",
        "\n",
        "**Заключение**  \n",
        "Data Leakage — это не просто техническая ошибка, а **фундаментальное нарушение принципов машинного обучения**. Её предотвращение требует дисциплины, строгой архитектуры пайплайна и постоянной бдительности. Только так можно обеспечить, что модель действительно обобщает, а не «подглядывает».\n"
      ],
      "metadata": {
        "id": "W2YomOT2gHPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Генерация и загрузка данных\n",
        "# =============================================================================\n",
        "\n",
        "# Пример размеченных текстовых данных для задачи бинарной классификации\n",
        "# (анализ тональности: позитивный/негативный)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Это хороший фильм\",\n",
        "        \"Мне не понравился этот фильм\",\n",
        "        \"Отличная игра актеров\",\n",
        "        \"Скучный сюжет и плохая режиссура\",\n",
        "        \"Рекомендую к просмотру\",\n",
        "        \"Ужасный фильм, пустая трата времени\",\n",
        "        \"Интересный поворот событий\",\n",
        "        \"Не стоит смотреть\",\n",
        "        \"Захватывающий сюжет\",\n",
        "        \"Очень плохое кино\"\n",
        "    ],\n",
        "    'sentiment': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 — позитивный, 0 — негативный\n",
        "}\n",
        "\n",
        "# Преобразование в DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Исходные данные:\")\n",
        "print(df.head())\n",
        "print(f\"\\nРазмер датасета: {df.shape[0]} образцов, {df.shape[1]} столбца(ов)\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Разделение выборки на обучающую и тестовую\n",
        "# =============================================================================\n",
        "\n",
        "# Важно: разделение выполняется ДО векторизации, чтобы избежать утечки данных\n",
        "X = df['text']  # Признаки (тексты)\n",
        "y = df['sentiment']  # Целевая переменная\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,              # 30% данных — тестовая выборка\n",
        "    random_state=42,            # Воспроизводимость\n",
        "    stratify=y                  # Сохранение пропорций классов в обеих выборках\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"РАЗМЕРЫ ВЫБОРОК ПОСЛЕ РАЗДЕЛЕНИЯ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Обучающая выборка (текст): {len(X_train)} объектов\")\n",
        "print(f\"Тестовая выборка (текст):   {len(X_test)} объектов\")\n",
        "print(f\"Распределение в обучающей: {pd.Series(y_train).value_counts().sort_index().to_dict()}\")\n",
        "print(f\"Распределение в тестовой:   {pd.Series(y_test).value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Векторизация текстов с использованием TF-IDF\n",
        "# =============================================================================\n",
        "\n",
        "# Инициализация векторизатора TF-IDF\n",
        "vectorizer = TfidfVectorizer(\n",
        "    lowercase=True,           # Приведение к нижнему регистру\n",
        "    stop_words=None,          # Можно добавить стоп-слова при необходимости\n",
        "    ngram_range=(1, 1),       # Униграммы (можно расширить до (1,2))\n",
        "    max_features=1000         # Ограничение размерности\n",
        ")\n",
        "\n",
        "# Обучение векторизатора ТОЛЬКО на обучающих текстах\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Применение обученного векторизатора к тестовым текстам (без переобучения!)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"РЕЗУЛЬТАТЫ ВЕКТОРИЗАЦИИ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Размер обучающей выборки (векторизованной): {X_train_vec.shape}\")\n",
        "print(f\"Размер тестовой выборки (векторизованной):   {X_test_vec.shape}\")\n",
        "print(f\"Количество признаков (уникальных терминов):  {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# При желании можно посмотреть ключевые термины\n",
        "print(\"\\nПервые 10 терминов из словаря векторизатора:\")\n",
        "print(vectorizer.get_feature_names_out()[:10].tolist())\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Обучение модели логистической регрессии\n",
        "# =============================================================================\n",
        "\n",
        "model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ОБУЧЕНИЕ МОДЕЛИ\")\n",
        "print(\"=\"*60)\n",
        "model.fit(X_train_vec, y_train)\n",
        "print(\"Модель обучена на векторизованных текстах.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Оценка качества модели\n",
        "# =============================================================================\n",
        "\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ОТЧЁТ ПО КАЧЕСТВУ НА ТЕСТОВОЙ ВЫБОРКЕ\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(\n",
        "    y_test, y_pred,\n",
        "    target_names=['Негативный (0)', 'Позитивный (1)'],\n",
        "    digits=3\n",
        "))\n",
        "\n",
        "# Дополнительно: вероятности предсказаний (опционально)\n",
        "if hasattr(model, \"predict_proba\"):\n",
        "    proba = model.predict_proba(X_test_vec)\n",
        "    print(\"\\nПример вероятностей предсказаний:\")\n",
        "    for i, (text, true, pred, probs) in enumerate(zip(X_test, y_test, y_pred, proba)):\n",
        "        print(f\"[{i+1}] Текст: '{text}'\")\n",
        "        print(f\"      Истинный класс: {true}, Предсказанный: {pred}\")\n",
        "        print(f\"      Вероятности: Негативный={probs[0]:.3f}, Позитивный={probs[1]:.3f}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "TNAIaI9FhhDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.2. Правильная кросс-валидация\n",
        "\n",
        "При использовании кросс-валидации (K-Fold, Stratified K-Fold) каждый шаг предобработки и инженерии признаков должен выполняться **внутри каждого фолда**. Это гарантирует, что валидационный фолд на каждой итерации остаётся «невиданным» для этапов, таких как векторизация или нормализация.\n",
        "\n",
        "Для этого удобно использовать **пайплайны (Pipelines)** из `scikit-learn`, которые инкапсулируют предобработку и модель в единый объект.\n"
      ],
      "metadata": {
        "id": "a4bRHjqrLnsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Пример данных (имитация текстовых данных)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Это хороший фильм\", \"Мне не понравился этот фильм\", \"Отличная игра актеров\",\n",
        "        \"Скучный сюжет и плохая режиссура\", \"Рекомендую к просмотру\",\n",
        "        \"Ужасный фильм, пустая трата времени\", \"Интересный поворот событий\",\n",
        "        \"Не стоит смотреть\", \"Захватывающий сюжет\", \"Очень плохое кино\"\n",
        "    ],\n",
        "    'sentiment': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 — позитивный, 0 — негативный\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df['text']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Создание пайплайна: векторизация -> модель\n",
        "# TfidfVectorizer.fit_transform() будет вызываться только на обучающем фолде\n",
        "# TfidfVectorizer.transform() — на валидационном\n",
        "text_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(random_state=42))\n",
        "])\n",
        "\n",
        "# Использование StratifiedKFold для сохранения пропорций классов\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Вычисление F1-меры для каждого фолда\n",
        "scores = cross_val_score(text_clf, X, y, cv=cv, scoring='f1_weighted')\n",
        "\n",
        "print(f\"F1-меры для каждого фолда: {scores}\")\n",
        "print(f\"Средняя F1-мера по кросс-валидации: {scores.mean():.3f}\")\n",
        "print(f\"Стандартное отклонение F1-меры: {scores.std():.3f}\")"
      ],
      "metadata": {
        "id": "qXVaVgs5i4Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ✅ **Ключевой момент**: В пайплайне `TfidfVectorizer` обучается **только на обучающем фолде**, а не на всём датасете. Это предотвращает утечку.\n",
        "\n",
        "\n",
        "\n",
        "### 4.3. Разделение временных рядов (Time-Series Splitting)\n",
        "\n",
        "Для данных с временной зависимостью (например, новости, посты в соцсетях) необходимо использовать стратегии разделения, сохраняющие хронологический порядок. Обучающая выборка всегда должна предшествовать тестовой.\n",
        "\n"
      ],
      "metadata": {
        "id": "BPgoktkvi4bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import pandas as pd\n",
        "\n",
        "# Пример временных данных\n",
        "data = {\n",
        "    'date': pd.to_datetime(['2023-01-01', '2023-01-05', '2023-01-10', '2023-01-15', '2023-01-20',\n",
        "                            '2023-01-25', '2023-01-30', '2023-02-05', '2023-02-10', '2023-02-15']),\n",
        "    'text': ['Текст1', 'Текст2', 'Текст3', 'Текст4', 'Текст5',\n",
        "             'Текст6', 'Текст7', 'Текст8', 'Текст9', 'Текст10'],\n",
        "    'sentiment': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "}\n",
        "df_ts = pd.DataFrame(data).sort_values(by='date').reset_index(drop=True)\n",
        "\n",
        "X_ts = df_ts['text']\n",
        "y_ts = df_ts['sentiment']\n",
        "\n",
        "# TimeSeriesSplit гарантирует, что тестовые данные всегда идут после обучающих\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "print(\"Разделение временного ряда:\")\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(X_ts)):\n",
        "    print(f\"Фолд {i+1}:\")\n",
        "    print(f\"  Обучающие индексы: {train_index}, Тестовые индексы: {test_index}\")\n",
        "    print(f\"  Обучающие даты: {df_ts.loc[train_index, 'date'].tolist()}\")\n",
        "    print(f\"  Тестовые даты: {df_ts.loc[test_index, 'date'].tolist()}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "xzYy-1cxi_PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ⚠️ **Не используйте случайное перемешивание** для временных данных — это вызовет временную утечку.\n",
        "\n",
        "\n",
        "\n",
        "### 4.4. Разделение по группам (Group-based Splitting)\n",
        "\n",
        "Если данные содержат группы (пользователи, авторы, продукты), необходимо убедиться, что **все примеры одной группы** находятся либо в обучении, либо в тесте. Иначе модель может «запомнить» особенности группы, а не обобщить.\n",
        "\n"
      ],
      "metadata": {
        "id": "EZQbNH9ri_Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
        "import pandas as pd\n",
        "\n",
        "# Пример с группами (пользователями)\n",
        "data = {\n",
        "    'user_id': ['A', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
        "    'text': ['Текст_A1', 'Текст_A2', 'Текст_B1', 'Текст_C1', 'Текст_A3',\n",
        "             'Текст_B2', 'Текст_C2', 'Текст_A4', 'Текст_B3', 'Текст_C3'],\n",
        "    'sentiment': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "df_groups = pd.DataFrame(data)\n",
        "\n",
        "X_groups = df_groups['text']\n",
        "y_groups = df_groups['sentiment']\n",
        "groups = df_groups['user_id']  # Группы для разделения\n",
        "\n",
        "# GroupKFold: гарантирует, что одна группа не попадёт и в train, и в test\n",
        "gkf = GroupKFold(n_splits=3)\n",
        "\n",
        "print(\"Разделение по группам (GroupKFold):\")\n",
        "for i, (train_index, test_index) in enumerate(gkf.split(X_groups, y_groups, groups)):\n",
        "    print(f\"Фолд {i+1}:\")\n",
        "    train_users = set(groups.iloc[train_index])\n",
        "    test_users = set(groups.iloc[test_index])\n",
        "    print(f\"  Обучающие индексы: {train_index}, Тестовые индексы: {test_index}\")\n",
        "    print(f\"  Пользователи в обучении: {train_users}\")\n",
        "    print(f\"  Пользователи в тесте: {test_users}\")\n",
        "    print(f\"  Общие пользователи: {train_users.intersection(test_users)}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Пример одного train/test разделения\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "for train_index, test_index in gss.split(X_groups, y_groups, groups):\n",
        "    X_train_g = X_groups.iloc[train_index]\n",
        "    X_test_g = X_groups.iloc[test_index]\n",
        "    y_train_g = y_groups.iloc[train_index]\n",
        "    y_test_g = y_groups.iloc[test_index]\n",
        "    groups_train = groups.iloc[train_index]\n",
        "    groups_test = groups.iloc[test_index]\n",
        "\n",
        "    print(\"\\nПример одного разделения по группам (GroupShuffleSplit):\")\n",
        "    print(f\"Пользователи в обучении: {set(groups_train)}\")\n",
        "    print(f\"Пользователи в тесте: {set(groups_test)}\")\n",
        "    print(f\"Общие пользователи: {set(groups_train).intersection(set(groups_test))}\")"
      ],
      "metadata": {
        "id": "GfvGMb9zjEwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### 4.5. Тщательная инженерия признаков\n",
        "\n",
        "Создавайте признаки, используя **только ту информацию, которая будет доступна во время инференса**. Если признак зависит от:\n",
        "- будущих данных,\n",
        "- целевой переменной,\n",
        "- статистик, посчитанных на всём датасете,\n",
        "\n",
        "— это **прямая утечка**.\n",
        "\n",
        "✅ **Решение**: Инженерия признаков должна быть частью пайплайна, где `fit` применяется только к обучающим данным.\n",
        "\n",
        "\n",
        "\n",
        "### 4.6. Аудит и проверка пайплайна\n",
        "\n",
        "Регулярно проводите аудит всего пайплайна — от сбора данных до оценки модели. Задавайте себе вопросы:\n",
        "- Какие данные доступны на этом этапе?\n",
        "- Может ли этот шаг использовать информацию, недоступную в продакшене?\n",
        "- Была ли предобработка изолирована?\n",
        "\n",
        "> 🔍 **Совет**: Представьте, что вы — модель в продакшене. Что вы можете видеть? Если на этапе обучения вы используете то, чего бы \"модель-в-реальности\" не знала — это утечка.\n",
        "\n",
        "\n",
        "\n",
        "### 4.7. Использование внешних данных\n",
        "\n",
        "При использовании предварительно обученных моделей (Word2Vec, BERT, LLM) помните: они могли быть обучены на данных, пересекающихся с вашей тестовой выборкой.\n",
        "\n",
        "**Пример**:  \n",
        "Gemini-модели, обученные на большом интернет-корпусе, могут \"знать\" фразы из вашего тестового набора.\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Проблема утечки данных является одной из самых серьёзных угроз для валидности результатов в NLP. Она приводит к искусственно завышенной производительности на этапе разработки, что вводит в заблуждение и может привести к провалу модели в продакшене.\n",
        "\n",
        "**Ключевые меры защиты**:\n",
        "- Разделяйте данные **до** предобработки.\n",
        "- Используйте **пайплайны** и **кросс-валидацию** правильно.\n",
        "- Применяйте **TimeSeriesSplit** для временных данных.\n",
        "- Используйте **GroupKFold** для групповых данных.\n",
        "- Проводите **аудит** пайплайна.\n",
        "\n",
        "Только строгая дисциплина и внимание к деталям гарантируют, что ваша модель действительно **обобщает**, а не «подглядывает».\n"
      ],
      "metadata": {
        "id": "MFN9tDcKjE5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TKTDcxPMr1RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dcwJispZh-w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e8l7bb2Ah-07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LNfRgoIdh-9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KMgWMNYYh_Cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3rVSfBffh_Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y9MtBi99h_LV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qe7NxyyHEf6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# §6. Числовое представление текста и методы векторизации: Подробное описание\n",
        "\n",
        "Для того чтобы алгоритмы машинного обучения могли обрабатывать текстовые данные, их необходимо преобразовать из символьного формата в числовой. Этот процесс, известный как **векторизация** или **эмбеддинг**, является фундаментальным шагом в любом пайплайне обработки естественного языка (NLP). Компьютеры оперируют числами, и эффективное числовое представление текста позволяет моделям выявлять закономерности, семантические связи и синтаксические структуры, скрытые в человеческом языке.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Необходимость векторизации и эволюция подходов\n",
        "\n",
        "Исторически первые подходы к векторизации текста были относительно простыми и основывались на подсчёте слов. Они представляли текст как набор дискретных, независимых признаков. Однако с развитием вычислительных мощностей и появлением нейронных сетей методы векторизации значительно усложнились, став способными улавливать более тонкие семантические и синтаксические отношения между словами и предложениями.\n",
        "\n",
        "Эволюция подходов к числовому представлению текста может быть прослежена через несколько ключевых этапов:\n",
        "\n",
        "1. **Разреженные представления (Sparse Representations)**:  \n",
        "   К ним относятся такие методы, как **One-Hot Encoding** и **Bag-of-Words (BoW)**. Эти подходы создают векторы, большинство элементов которых равны нулю, что делает их «разрежёнными». Они просты в реализации, но не способны улавливать семантические отношения между словами и страдают от проблемы высокой размерности при больших словарях.\n",
        "\n",
        "2. **Плотные представления (Dense Representations) / Векторные представления слов (Word Embeddings)**:  \n",
        "   С появлением нейронных сетей появилась возможность создавать плотные векторы меньшей размерности, где каждый элемент имеет смысловую нагрузку. Такие методы, как **Word2Vec**, **GloVe** и **FastText**, способны улавливать семантическую близость слов (например, «король» и «королева» будут иметь близкие векторы в многомерном пространстве). Эти представления значительно улучшили производительность моделей в различных задачах NLP.\n",
        "\n",
        "3. **Контекстно-зависимые эмбеддинги (Contextual Embeddings)**:  \n",
        "   Современные модели, такие как **ELMo**, **BERT**, **GPT** и их многочисленные преемники, произвели революцию в NLP, генерируя эмбеддинги слов, которые зависят от контекста их употребления. Например, слово «банк» будет иметь разные числовые представления в зависимости от того, используется ли оно в значении «финансовое учреждение» или «берег реки». Это позволяет моделям лучше справляться с полисемией и другими языковыми нюансами.\n",
        "\n",
        "В этом разделе мы сосредоточимся на базовых и классических методах векторизации, которые являются основой для понимания более сложных подходов.\n"
      ],
      "metadata": {
        "id": "Z4lYAprfCI2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#2. Методы векторизации текстовых данных\n",
        "\n",
        "## 2.1. One-Hot Encoding\n",
        "\n",
        "### 2.1.1. Введение\n",
        "\n",
        "Одной из ключевых задач в обработке естественного языка (Natural Language Processing, NLP) является **представление текста в числовом виде**, пригодном для обработки алгоритмами машинного обучения. Поскольку компьютеры не могут напрямую работать с текстовыми символами, необходимо преобразовать слова, предложения или документы в **векторные представления**.\n",
        "\n",
        "Наиболее простым и интуитивно понятным способом векторизации категориальных данных, включая слова, является метод **One-Hot Encoding** (кодирование с одним активным состоянием). Данный метод широко используется на начальных этапах изучения NLP как базовая модель преобразования слов в векторы.\n",
        "\n",
        "В этом разделе мы подробно рассмотрим принцип работы One-Hot Encoding, его математическую основу, практическую реализацию и **критические ограничения**, особенно с точки зрения **объёма занимаемой памяти**. В завершение будет приведён **реалистичный пример масштабного текстового корпуса**, демонстрирующий, почему One-Hot Encoding оказывается **неприемлемым для крупных задач**.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.2. Определение и принцип работы\n",
        "\n",
        "Пусть задан текстовый корпус — совокупность документов (предложений, абзацев, книг), из которого извлекаются все уникальные слова. Совокупность этих слов образует **словарь (vocabulary)**. Обозначим размер словаря как:\n",
        "\n",
        "$$V = |\\text{vocabulary}|$$\n",
        "\n",
        "Каждому слову $w_i$ в словаре присваивается уникальный индекс $i \\in \\{0, 1, 2, \\dots, V-1\\}$.\n",
        "\n",
        "**One-Hot Encoding** слова $w_i$ — это бинарный вектор $\\mathbf{v}_i \\in \\mathbb{R}^V$, определяемый следующим образом:\n",
        "\n",
        "$$\n",
        "v_{i,j} =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } j = i, \\\\\n",
        "0, & \\text{если } j \\ne i.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Таким образом, вектор имеет длину $V$, и только одна компонента (на позиции $i$) равна 1, а все остальные — 0.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.3. Пример построения One-Hot векторов\n",
        "\n",
        "Рассмотрим небольшой словарь из четырёх слов:\n",
        "\n",
        "| Слово    | Индекс |\n",
        "|---------|--------|\n",
        "| кошка   | 0      |\n",
        "| собака  | 1      |\n",
        "| бежит   | 2      |\n",
        "| спит    | 3      |\n",
        "\n",
        "Тогда One-Hot векторы будут:\n",
        "\n",
        "- \"кошка\" → $[1, 0, 0, 0]$\n",
        "- \"собака\" → $[0, 1, 0, 0]$\n",
        "- \"бежит\" → $[0, 0, 1, 0]$\n",
        "- \"спит\" → $[0, 0, 0, 1]$\n",
        "\n",
        "Как видно, каждый вектор однозначно идентифицирует слово, но не содержит никакой информации о его значении, контексте или связи с другими словами.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.4. Свойства One-Hot представления\n",
        "\n",
        "| Свойство | Характеристика |\n",
        "|--------|----------------|\n",
        "| **Размерность** | Равна размеру словаря $V$ |\n",
        "| **Разреженность** | Вектор содержит $V-1$ нулей и один 1 → очень высокая разрежённость |\n",
        "| **Ортогональность** | Все векторы попарно ортогональны: $\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0$ при $i \\ne j$ |\n",
        "| **Семантика** | Не учитывается. Все слова равноудалены в векторном пространстве |\n",
        "| **OOV-слова** | Слова, не вошедшие в словарь, не могут быть закодированы (исключение — специальные стратегии, например, нулевой вектор) |\n",
        "\n",
        "\n",
        "### 2.1.5. Практическая реализация\n",
        "\n",
        "Ниже приведён пример реализации One-Hot Encoding на языке Python с использованием библиотек `scikit-learn` и `numpy`.\n"
      ],
      "metadata": {
        "id": "4HNFuzicRteH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIeyksBmCRgF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Пример текстов\n",
        "sentences = [\n",
        "    \"Я люблю кошек\",\n",
        "    \"Собаки тоже хорошие\"\n",
        "]\n",
        "\n",
        "# Токенизация и создание словаря\n",
        "words = []\n",
        "for sentence in sentences:\n",
        "    cleaned = sentence.lower().replace('.', '').replace(',', '').split()\n",
        "    words.extend(cleaned)\n",
        "\n",
        "unique_words = sorted(set(words))\n",
        "print(f\"Словарь: {unique_words}\")\n",
        "\n",
        "# Обучение One-Hot Encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoder.fit(np.array(unique_words).reshape(-1, 1))\n",
        "\n",
        "# Функция кодирования слова\n",
        "def encode_word(word):\n",
        "    return encoder.transform([[word.lower()]])\n",
        "\n",
        "# Примеры\n",
        "print(\"Вектор 'кошек':\", encode_word(\"кошек\"))\n",
        "print(\"Вектор 'птицы' (OOV):\", encode_word(\"птицы\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Пояснение параметров**:\n",
        "> - `handle_unknown='ignore'` — позволяет обрабатывать слова вне словаря, возвращая вектор из нулей.\n",
        "> - `sparse_output=False` — возвращает плотный массив (удобно для демонстрации).\n",
        "> - При работе с большими словарями рекомендуется использовать `sparse=True` для экономии памяти.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.6. Проблема масштабируемости: анализ объёма памяти\n",
        "\n",
        "Одним из главных недостатков One-Hot Encoding является **экстремально высокое потребление памяти** при увеличении размера словаря. Рассмотрим это на примере крупного текстового корпуса.\n",
        "\n",
        "#### 📘 Пример: корпус из 1000 книг\n",
        "\n",
        "Представим, что у нас есть коллекция из **1000 книг**, каждая объёмом:\n",
        "\n",
        "- **250 страниц**\n",
        "- **35 строк на странице**\n",
        "- **~10 слов в строке**\n",
        "\n",
        "##### Шаг 1: Общее количество слов\n",
        "\n",
        "$$1000 \\times 250 \\times 35 \\times 10 = 87\\,500\\,000$$\n",
        "\n",
        "Итого: **87.5 миллионов слов**.\n",
        "\n",
        "##### Шаг 2: Оценка размера словаря\n",
        "\n",
        "Пусть средний словарь одной книги — около 20 000 уникальных слов. Учитывая пересечение лексики между книгами, общее количество уникальных слов во всём корпусе оценим как:\n",
        "\n",
        "$$V = 100\\,000$$\n",
        "\n",
        "> Это реалистичная оценка для разнообразного корпуса (художественная и научная литература).\n",
        "\n",
        "##### Шаг 3: Память на один One-Hot вектор\n",
        "\n",
        "Каждый вектор имеет длину $V = 100\\,000$. Если использовать тип `float32` (4 байта на элемент), то объём на один вектор:\n",
        "\n",
        "$$100\\,000 \\times 4 = 400\\,000\\ \\text{байт} = 400\\ \\text{КБ}$$\n",
        "\n",
        "##### Шаг 4: Общий объём памяти для всего корпуса\n",
        "\n",
        "Для кодирования **87.5 млн слов** потребуется:\n",
        "\n",
        "$$87\\,500\\,000 \\times 400\\,000 = 35\\,000\\,000\\,000\\,000\\ \\text{байт} = 35\\ \\text{ТБ}$$\n",
        "\n",
        "> 💥 **Итого: 35 терабайт оперативной памяти или дискового пространства.**\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.7. Анализ результатов\n",
        "\n",
        "| Параметр | Значение |\n",
        "|--------|--------|\n",
        "| Количество слов | 87.5 млн |\n",
        "| Размер словаря $V$ | 100 000 |\n",
        "| Память на одно слово | 400 КБ |\n",
        "| **Общий объём** | **35 ТБ** |\n",
        "\n",
        "#### Выводы:\n",
        "- **Один вектор** занимает **400 КБ**, хотя содержит только **одну единицу**.\n",
        "- **99.999% данных** — это нули, что делает представление крайне **неэффективным**.\n",
        "- Хранение полных векторов в плотном формате **непрактично даже для средних корпусов**.\n",
        "- Даже при использовании **разреженных матриц**, где хранятся только индексы единиц, объём можно сократить до:\n",
        "  $$87\\,500\\,000 \\times 4\\ \\text{байта (на индекс)} = 350\\ \\text{МБ}$$\n",
        "  — но это уже **не векторы отдельных слов**, а сжатое представление.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.8. Ограничения метода\n",
        "\n",
        "На основе проведённого анализа можно выделить следующие **фундаментальные недостатки** One-Hot Encoding:\n",
        "\n",
        "1. **Неэффективность по памяти**  \n",
        "   Объём памяти растёт пропорционально $V \\times N$, где $N$ — количество слов. Уже при $V > 10^4$ метод становится неприменимым.\n",
        "\n",
        "2. **Отсутствие семантической информации**  \n",
        "   Все слова находятся на одинаковом \"расстоянии\". Например, косинусное расстояние между любыми двумя разными векторами:\n",
        "   $$\\cos(\\mathbf{v}_i, \\mathbf{v}_j) = 0 \\quad \\text{при} \\quad i \\ne j$$\n",
        "   Это означает, что модель не может отличить близкие по смыслу слова (например, \"кошка\" и \"собака\") от совершенно разных (\"кошка\" и \"бежит\").\n",
        "\n",
        "3. **Невозможность обобщения**  \n",
        "   Метод не учитывает морфологию, синонимы или контекст.\n",
        "\n",
        "4. **Проблема OOV (Out-of-Vocabulary)**  \n",
        "   Любое новое слово, не вошедшее в обучающий словарь, не может быть корректно закодировано.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.9. Когда можно использовать One-Hot?\n",
        "\n",
        "Несмотря на ограничения, One-Hot Encoding имеет право на существование в следующих случаях:\n",
        "\n",
        "- **Малые словари**: например, в задачах классификации с ограниченным числом категорий (например, 10–100 классов).\n",
        "- **Обучающие цели**: как вводный метод для понимания векторизации.\n",
        "- **Входной слой нейросетей**: в современных архитектурах One-Hot векторы часто используются неявно — через **embedding-слои**, которые сразу преобразуют индекс слова в плотный вектор.\n",
        "\n",
        "\n",
        "\n",
        "### 2.1.10. Заключение\n",
        "\n",
        "One-Hot Encoding — это **простой, но крайне неэффективный** способ векторизации слов. Он служит важным концептуальным шагом в изучении NLP, демонстрируя, как текст можно преобразовать в числовой формат. Однако его применение в реальных задачах с большими объёмами текста **практически исключено** из-за огромных требований к памяти и отсутствия семантической структуры.\n",
        "\n",
        "Приведённый пример с 1000 книгами показывает, что даже при скромных предположениях объём данных может достигать **десятков терабайт**, что делает метод **непригодным для масштабных приложений**.\n",
        "\n",
        "В следующих главах мы рассмотрим более эффективные методы векторизации, такие как **Word2Vec**, **GloVe** и **контекстные эмбеддинги**, которые решают эти проблемы за счёт компактности и способности улавливать смысл слов.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jfj5P9UJRySE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2.2. Bag-of-Words (мешок слов)\n",
        "\n",
        "## 2.2.1. Определение и принцип работы\n",
        "\n",
        "**Bag-of-Words (BoW)** — это классическая модель представления текстовых данных, при которой документ описывается как **неупорядоченный набор (мультимножество) слов**, с учётом их частоты, но **без учёта грамматики и порядка**.\n",
        "\n",
        "Каждый документ преобразуется в числовой вектор, размерность которого равна размеру словаря $V$, где $V$ — общее количество уникальных слов во всём корпусе. Элемент вектора на позиции $j$ содержит количество вхождений слова $w_j$ в данный документ.\n",
        "\n",
        "Формально, пусть:\n",
        "- $\\mathcal{D} = \\{d_1, d_2, \\dots, d_N\\}$ — коллекция из $N$ документов,\n",
        "- $\\mathcal{V} = \\{w_1, w_2, \\dots, w_V\\}$ — словарь корпуса,\n",
        "- $f_{ij}$ — частота слова $w_j$ в документе $d_i$.\n",
        "\n",
        "Тогда векторное представление документа $d_i$ в модели BoW имеет вид:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_i = [f_{i1}, f_{i2}, \\dots, f_{iV}]\n",
        "$$\n",
        "\n",
        "Такой подход позволяет преобразовать текст в структурированный числовой формат, пригодный для использования в алгоритмах машинного обучения.\n",
        "\n",
        "\n",
        "\n",
        "## 2.2.2. Пример построения BoW-векторов\n",
        "\n",
        "Рассмотрим следующие документы:\n",
        "\n",
        "- Документ 1: *\"Кошка бежит\"*\n",
        "- Документ 2: *\"Собака спит\"*\n",
        "\n",
        "Построим словарь (лексикографически):\n",
        "\n",
        "$$\n",
        "\\text{vocabulary} = \\{\\text{кошка}: 0, \\text{собака}: 1, \\text{бежит}: 2, \\text{спит}: 3\\}\n",
        "$$\n",
        "\n",
        "Тогда BoW-векторы будут:\n",
        "\n",
        "- Вектор для Документа 1: $[1, 0, 1, 0]$\n",
        "- Вектор для Документа 2: $[0, 1, 0, 1]$\n",
        "\n",
        "> Обратите внимание: в отличие от One-Hot Encoding, где каждый вектор содержит только одну единицу, BoW позволяет **множественные ненулевые значения**, отражающие **реальную частоту слов**.\n",
        "\n",
        "\n",
        "\n",
        "## 2.2.3. Алгоритм построения BoW\n",
        "\n",
        "1. **Токенизация**  \n",
        "   Каждый документ разбивается на отдельные слова (токены), обычно с приведением к нижнему регистру и удалением знаков препинания.\n",
        "\n",
        "2. **Построение словаря**  \n",
        "   Из всех токенов корпуса формируется упорядоченный список уникальных слов. Каждому слову присваивается фиксированный индекс.\n",
        "\n",
        "3. **Векторизация**  \n",
        "   Для каждого документа строится вектор длины $V$, в котором на позиции $j$ записывается количество вхождений слова $w_j$ в документ.\n",
        "\n",
        "\n",
        "\n",
        "## 2.2.4. Практическая реализация на Python\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRg5fPsPSEZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Пример документов\n",
        "documents = [\n",
        "    \"Кот спит на диване\",\n",
        "    \"Собака бежит по улице\",\n",
        "    \"Кот и собака играют\"\n",
        "]\n",
        "\n",
        "# Создаём векторизатор BoW\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Обучаем и преобразуем документы\n",
        "X_bow = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Получаем словарь\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(f\"Словарь (feature names): {feature_names}\")\n",
        "\n",
        "# Выводим BoW-векторы\n",
        "print(\"\\nBoW векторы для документов:\")\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# Пример для нового документа\n",
        "new_doc = [\"Собака спит на коврике\"]\n",
        "new_doc_bow = vectorizer.transform(new_doc)\n",
        "print(f\"\\nBoW вектор для нового документа '{new_doc[0]}':\")\n",
        "print(new_doc_bow.toarray())"
      ],
      "metadata": {
        "id": "Yyg7z3mlTbfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Пояснение**:\n",
        "> - `CountVectorizer` автоматически обрабатывает текст: токенизирует, приводит к нижнему регистру, строит словарь.\n",
        "> - Результат `X_bow` — разреженная матрица, что позволяет эффективно хранить большие корпусы.\n",
        "> - Новые документы векторизуются с использованием **того же словаря**, что и при обучении.\n",
        "\n",
        "\n",
        "\n",
        "## 2.2.5. Преимущества BoW перед One-Hot Encoding\n",
        "\n",
        "Несмотря на схожую структуру (оба метода используют векторы длины $V$), **Bag-of-Words значительно превосходит One-Hot Encoding** по нескольким ключевым параметрам.\n",
        "\n",
        "### 1. **Учёт частоты слов**\n",
        "\n",
        "- **One-Hot**: каждое слово представлено вектором с одной единицей, независимо от того, сколько раз оно встречается.\n",
        "- **BoW**: если слово встречается дважды, его счётчик будет равен 2 — это **дополнительная семантическая информация**.\n",
        "\n",
        "> Например, в документе *\"кошка кошка кошка\"* слово \"кошка\" явно играет важную роль. One-Hot не различит его от документа с одним вхождением, а BoW — да.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Эффективное представление целых документов**\n",
        "\n",
        "- **One-Hot**: кодирует **одно слово**.\n",
        "- **BoW**: кодирует **целый документ** как сумму вкладов всех слов.\n",
        "\n",
        "> Это делает BoW **готовым к использованию в задачах классификации, кластеризации и поиска**, в то время как One-Hot требует дополнительных шагов (например, суммирования векторов слов).\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Более информативные векторы**\n",
        "\n",
        "Рассмотрим два документа:\n",
        "- $d_1$: *\"кошка бежит\"*\n",
        "- $d_2$: *\"кошка бежит бежит\"*\n",
        "\n",
        "| Метод | Вектор $d_1$ | Вектор $d_2$ |\n",
        "|------|--------------|--------------|\n",
        "| One-Hot (на слово) | $[1,0,1,0]$ | $[1,0,1,0] + [1,0,1,0] = [2,0,2,0]$ |\n",
        "| BoW (на документ) | $[1,0,1,0]$ | $[1,0,2,0]$ |\n",
        "\n",
        "> В BoW видно, что слово *\"бежит\"* употреблено дважды — это отражено напрямую.  \n",
        "> В One-Hot при суммировании теряется связь между счётчиком и конкретным словом (векторы просто складываются).\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Лучшее поведение в задачах машинного обучения**\n",
        "\n",
        "- BoW векторы содержат **количественную информацию**, что позволяет алгоритмам (например, Naive Bayes, SVM) лучше различать документы.\n",
        "- One-Hot векторы для документов (при суммировании) становятся **небинарными**, но при этом остаются **разрежёнными и несбалансированными**.\n",
        "\n",
        "\n",
        "\n",
        "### 5. **Поддержка разреженных матриц**\n",
        "\n",
        "Оба метода могут использовать разреженное хранение, но BoW **по умолчанию** работает с матрицами документов, что делает его **естественнее масштабируемым**.\n",
        "\n",
        "Например, для корпуса из $N$ документов и словаря размера $V$:\n",
        "- BoW: матрица $N \\times V$ (разреженная),\n",
        "- One-Hot: $N \\times L \\times V$, где $L$ — средняя длина документа (если хранить все слова отдельно).\n",
        "\n",
        "> То есть BoW **в $L$ раз компактнее** при хранении коллекции документов.\n",
        "\n",
        "\n",
        "\n",
        "## 2.2.6. Ограничения модели BoW\n",
        "\n",
        "Несмотря на преимущества, BoW имеет свои недостатки:\n",
        "\n",
        "| Ограничение | Пояснение |\n",
        "|-----------|----------|\n",
        "| **Игнорирование порядка слов** | Не различает *\"собака кусает человека\"* и *\"человек кусает собаку\"* |\n",
        "| **Отсутствие семантики** | Не учитывает синонимы, морфологию, контекст |\n",
        "| **Рост размерности** | При увеличении корпуса $V$ растёт, что увеличивает требования к памяти |\n",
        "| **Чувствительность к шуму** | Частотные, но малозначимые слова (предлоги, союзы) могут доминировать |\n",
        "\n",
        "\n",
        "## 2.2.7. Заключение\n",
        "\n",
        "Модель **Bag-of-Words** является **существенным улучшением по сравнению с One-Hot Encoding** в контексте векторизации текстов. В отличие от One-Hot, который кодирует отдельные слова и игнорирует их частоту, BoW:\n",
        "- позволяет представлять **целые документы**,\n",
        "- учитывает **частоту слов**,\n",
        "- формирует **информативные и интерпретируемые векторы**,\n",
        "- эффективно масштабируется за счёт разреженных матриц.\n",
        "\n",
        "Хотя BoW по-прежнему **не учитывает порядок слов и семантику**, он служит важным шагом на пути к более сложным моделям. Его простота, эффективность и хорошая производительность в задачах классификации делают BoW **одним из базовых инструментов в NLP**.\n",
        "\n",
        "> В следующих главах мы рассмотрим методы, преодолевающие ограничения BoW, включая **TF-IDF**, **плотные эмбеддинги** и **контекстные модели**.\n"
      ],
      "metadata": {
        "id": "GBvdoY4cTbpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 2.3. TF-IDF (Term Frequency – Inverse Document Frequency)\n",
        "\n",
        "## Определение и назначение\n",
        "\n",
        "**TF-IDF** (от англ. *Term Frequency – Inverse Document Frequency*) — это статистическая мера, используемая для оценки **важности слова** в документе относительно всей коллекции документов (корпуса). Метод является **улучшением модели Bag-of-Words (BoW)**, поскольку учитывает не только частоту слова в конкретном документе, но и его **редкость в корпусе в целом**.\n",
        "\n",
        "Основная идея TF-IDF заключается в следующем:\n",
        "- Слова, которые часто встречаются в одном документе, но редко — в других, являются **информативными** и должны иметь **высокий вес**.\n",
        "- Слова, часто употребляемые во многих документах (например, \"и\", \"в\", \"на\", \"это\"), считаются **малозначимыми** (стоп-словами) и получают **пониженный вес**.\n",
        "\n",
        "Таким образом, TF-IDF позволяет **автоматически подавлять шум** и выделять **ключевые термины**, характерные для конкретного документа.\n",
        "\n",
        "## Математическая формула\n",
        "\n",
        "TF-IDF вычисляется как произведение двух компонент:\n",
        "\n",
        "1. **TF** (*Term Frequency*) — частота термина в документе,\n",
        "2. **IDF** (*Inverse Document Frequency*) — мера редкости термина в корпусе.\n",
        "\n",
        "Формально, вес слова $t$ в документе $d$ относительно корпуса $\\mathcal{D}$ определяется как:\n",
        "\n",
        "$$\n",
        "\\mathrm{TF\\text{-}IDF}(t, d, \\mathcal{D}) = \\mathrm{TF}(t, d) \\times \\mathrm{IDF}(t, \\mathcal{D})\n",
        "$$\n",
        "\n",
        "\n",
        "### 1. Term Frequency (TF)\n",
        "\n",
        "**TF** — это мера того, насколько часто слово $t$ встречается в документе $d$. Существует несколько способов вычисления TF. Наиболее распространённые:\n",
        "\n",
        "- **Абсолютная частота**:\n",
        "  $$\n",
        "  \\mathrm{TF}(t, d) = \\text{количество вхождений } t \\text{ в } d\n",
        "  $$\n",
        "\n",
        "- **Нормализованная частота** (рекомендуется):\n",
        "  $$\n",
        "  \\mathrm{TF}(t, d) = \\frac{\\text{количество вхождений } t \\text{ в } d}{\\text{общее число слов в } d}\n",
        "  $$\n",
        "\n",
        "Нормализация предотвращает предвзятость в пользу длинных документов, в которых слова могут встречаться чаще просто из-за объёма текста.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Inverse Document Frequency (IDF)\n",
        "\n",
        "**IDF** — это мера того, насколько слово $t$ является **уникальным** или **редким** в корпусе $\\mathcal{D}$. Чем реже слово встречается среди документов, тем выше его IDF.\n",
        "\n",
        "Стандартная формула:\n",
        "\n",
        "$$\n",
        "\\mathrm{IDF}(t, \\mathcal{D}) = \\log \\left( \\frac{N}{\\mathrm{df}(t)} \\right)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $N$ — общее количество документов в корпусе,\n",
        "- $\\mathrm{df}(t)$ — число документов, содержащих слово $t$ (document frequency).\n",
        "\n",
        "> **Примечание**:\n",
        "> - Логарифм (обычно $\\ln$ или $\\log_{10}$) сглаживает значения и уменьшает влияние экстремально редких слов.\n",
        "> - Чтобы избежать деления на ноль (если слово отсутствует в корпусе), часто используется сглаживание:\n",
        ">   $$\n",
        ">   \\mathrm{IDF}(t, \\mathcal{D}) = \\log \\left( \\frac{N + 1}{\\mathrm{df}(t) + 1} \\right)\n",
        ">   $$\n",
        "\n",
        "\n",
        "\n",
        "## Аналитический пример вычисления TF-IDF\n",
        "\n",
        "Рассмотрим корпус из трёх документов:\n",
        "\n",
        "- $D_1$: *\"Кот спит на диване\"*\n",
        "- $D_2$: *\"Собака бежит по улице\"*\n",
        "- $D_3$: *\"Кот и собака играют\"*\n",
        "\n",
        "Общее число документов: $N = 3$.\n",
        "\n",
        "### Шаг 1: Вычисление Term Frequency (TF)\n",
        "\n",
        "Словарь:  \n",
        "{\"кот\", \"спит\", \"на\", \"диване\", \"собака\", \"бежит\", \"по\", \"улице\", \"и\", \"играют\"}\n",
        "\n",
        "Используем **нормализованную частоту** (длина каждого документа — 4 слова).\n",
        "\n",
        "| Слово | $D_1$ | $D_2$ | $D_3$ |\n",
        "|------|-------|-------|-------|\n",
        "| кот | $1/4 = 0.25$ | 0 | $0.25$ |\n",
        "| спит | $0.25$ | 0 | 0 |\n",
        "| на | $0.25$ | 0 | 0 |\n",
        "| диване | $0.25$ | 0 | 0 |\n",
        "| собака | 0 | $0.25$ | $0.25$ |\n",
        "| бежит | 0 | $0.25$ | 0 |\n",
        "| по | 0 | $0.25$ | 0 |\n",
        "| улице | 0 | $0.25$ | 0 |\n",
        "| и | 0 | 0 | $0.25$ |\n",
        "| играют | 0 | 0 | $0.25$ |\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление Inverse Document Frequency (IDF)\n",
        "\n",
        "Используем натуральный логарифм: $\\mathrm{IDF}(t) = \\ln(3 / \\mathrm{df}(t))$\n",
        "\n",
        "| Слово | $\\mathrm{df}(t)$ | $\\mathrm{IDF}(t)$ |\n",
        "|------|------------------|-------------------|\n",
        "| кот | 2 | $\\ln(3/2) \\approx 0.405$ |\n",
        "| спит | 1 | $\\ln(3/1) \\approx 1.098$ |\n",
        "| на | 1 | $\\approx 1.098$ |\n",
        "| диване | 1 | $\\approx 1.098$ |\n",
        "| собака | 2 | $\\approx 0.405$ |\n",
        "| бежит | 1 | $\\approx 1.098$ |\n",
        "| по | 1 | $\\approx 1.098$ |\n",
        "| улице | 1 | $\\approx 1.098$ |\n",
        "| и | 1 | $\\approx 1.098$ |\n",
        "| играют | 1 | $\\approx 1.098$ |\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление TF-IDF\n",
        "\n",
        "Перемножим значения TF и IDF.\n",
        "\n",
        "**Документ $D_1$**:\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{кот}, D_1) = 0.25 \\times 0.405 = 0.101$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{спит}, D_1) = 0.25 \\times 1.098 = 0.275$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{на}, D_1) = 0.25 \\times 1.098 = 0.275$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{диване}, D_1) = 0.25 \\times 1.098 = 0.275$\n",
        "\n",
        "**Документ $D_2$**:\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{собака}, D_2) = 0.25 \\times 0.405 = 0.101$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{бежит}, D_2) = 0.25 \\times 1.098 = 0.275$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{по}, D_2) = 0.25 \\times 1.098 = 0.275$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{улице}, D_2) = 0.25 \\times 1.098 = 0.275$\n",
        "\n",
        "**Документ $D_3$**:\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{кот}, D_3) = 0.25 \\times 0.405 = 0.101$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{и}, D_3) = 0.25 \\times 1.098 = 0.275$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{собака}, D_3) = 0.25 \\times 0.405 = 0.101$\n",
        "- $\\mathrm{TF\\text{-}IDF}(\\text{играют}, D_3) = 0.25 \\times 1.098 = 0.275$\n",
        "\n",
        "\n",
        "\n",
        "## Анализ результатов\n",
        "\n",
        "- Слова **\"кот\"** и **\"собака\"**, встречающиеся в двух документах, имеют **низкий IDF** ($\\approx 0.405$) и, следовательно, **низкий TF-IDF** ($\\approx 0.101$).\n",
        "- Слова, встречающиеся **только в одном документе** (например, \"спит\", \"диване\", \"и\", \"играют\"), имеют **высокий IDF** ($\\approx 1.098$) и **высокий TF-IDF** ($\\approx 0.275$).\n",
        "\n",
        "> Это демонстрирует, как TF-IDF **автоматически снижает вес общих слов** и **повышает вес уникальных терминов**, что делает документы более различимыми и информативными для задач классификации, поиска и кластеризации.\n",
        "\n",
        "\n",
        "\n",
        "## Преимущества и ограничения TF-IDF\n",
        "\n",
        "| Преимущество | Пояснение |\n",
        "|-------------|----------|\n",
        "| **Учёт информативности слов** | Редкие, но значимые слова получают более высокий вес |\n",
        "| **Подавление стоп-слов** | Частотные, малозначимые слова (например, \"на\", \"и\") получают низкий IDF |\n",
        "| **Интерпретируемость** | Высокие веса легко интерпретировать как ключевые слова документа |\n",
        "| **Эффективность** | Поддерживает разреженные матрицы, что позволяет работать с большими корпусами |\n",
        "\n",
        "| Ограничение | Пояснение |\n",
        "|-----------|----------|\n",
        "| **Игнорирование порядка слов** | Не различает *\"собака кусает человека\"* и *\"человек кусает собаку\"* |\n",
        "| **Отсутствие семантики** | Не учитывает синонимы, морфологию, контекст |\n",
        "| **Зависимость от корпуса** | IDF вычисляется на обучающем корпусе и не обновляется |\n",
        "| **Разреженность векторов** | Векторы остаются разрежёнными, хотя и менее, чем в One-Hot Encoding |\n",
        "\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "TF-IDF является **ключевым улучшением модели BoW**, позволяя учитывать **информационную значимость слов** в контексте всего корпуса. Метод эффективно:\n",
        "- снижает вес часто встречающихся, но малозначимых слов,\n",
        "- повышает вес редких и информативных терминов,\n",
        "- формирует более качественные векторные представления документов.\n",
        "\n",
        "Хотя TF-IDF по-прежнему **не учитывает порядок слов и семантику**, он остаётся **широко применимым** в задачах:\n",
        "- классификации текстов,\n",
        "- информационного поиска,\n",
        "- кластеризации документов.\n",
        "\n",
        "В следующем разделе мы рассмотрим, как **расширение BoW и TF-IDF на N-граммы** позволяет частично учесть синтаксический контекст и улучшить качество векторизации.\n"
      ],
      "metadata": {
        "id": "eswZcgOzTqTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 2.2.3. Учёт униграмм и N-грамм\n",
        "\n",
        "## Общая концепция\n",
        "\n",
        "Модели **Bag-of-Words (BoW)** и **TF-IDF** по умолчанию основываются на **уникальных словах (униграммах)**, что приводит к полной потере информации о **порядке слов** и **синтаксических связях** между ними. Для частичного учёта контекста и улучшения качества векторного представления текста применяется расширение этих моделей с использованием **N-грамм** — непрерывных последовательностей из $N$ слов, извлекаемых из текста.\n",
        "\n",
        "Пусть предложение состоит из $L$ слов: $w_1, w_2, \\dots, w_L$. Тогда **N-грамма** порядка $N$ определяется как подпоследовательность:\n",
        "\n",
        "$$\n",
        "(w_i, w_{i+1}, \\dots, w_{i+N-1}), \\quad \\text{где} \\quad 1 \\leq i \\leq L - N + 1\n",
        "$$\n",
        "\n",
        "Количество N-грамм в предложении длины $L$ равно $L - N + 1$. Ниже рассмотрены наиболее употребительные типы N-грамм: **униграммы** ($N=1$), **биграммы** ($N=2$) и **триграммы** ($N=3$), с примерами реализации на основе библиотек `nltk` и `PySpark`.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Униграммы (1-граммы)\n",
        "\n",
        "**Униграммы** — это отдельные слова, рассматриваемые как независимые признаки. Это базовый уровень представления текста, лежащий в основе классических моделей BoW и TF-IDF.\n",
        "\n",
        "Множество униграмм:\n",
        "$$\n",
        "\\{w_i \\mid 1 \\leq i \\leq L\\}\n",
        "$$\n",
        "\n",
        "#### Пример\n",
        "Предложение: *\"Я люблю NLP\"*  \n",
        "Длина $L = 3$  \n",
        "Униграммы: `\"Я\"`, `\"люблю\"`, `\"NLP\"`\n",
        "\n",
        "#### Реализация с использованием `nltk`\n",
        "\n"
      ],
      "metadata": {
        "id": "bNWmJ9stUzgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Загрузка ресурсов (один раз)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Текст\n",
        "sentence = \"Я люблю NLP\"\n",
        "tokens = word_tokenize(sentence.lower())  # Токенизация\n",
        "L = len(tokens)  # Длина предложения\n",
        "\n",
        "# Генерация униграмм (N=1)\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "print(f\"Униграммы (L={L}): {unigrams}\")\n",
        "# Вывод: [('я',), ('люблю',), ('nlp',)]"
      ],
      "metadata": {
        "id": "jwj5gEHdgkNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Примечание**: Униграммы часто используются как базовый признаковый набор. Каждое слово становится отдельным признаком в векторе.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Биграммы (2-граммы)\n",
        "\n",
        "**Биграммы** — это последовательные пары слов. Они позволяют учитывать **связи между соседними словами**, что критически важно для различения фраз с одинаковыми словами, но разным смыслом.\n",
        "\n",
        "Множество биграмм:\n",
        "$$\n",
        "\\{(w_i, w_{i+1}) \\mid 1 \\leq i \\leq L-1\\}\n",
        "$$\n",
        "\n",
        "#### Пример\n",
        "Предложение: *\"Я люблю NLP\"*  \n",
        "$L = 3$  \n",
        "Биграммы: `\"Я люблю\"`, `\"люблю NLP\"`\n",
        "\n",
        "#### Семантическая значимость\n",
        "Биграммы помогают различать:\n",
        "- *\"горячая собака\"* (еда),\n",
        "- *\"горячий пес\"* (животное).\n",
        "\n",
        "При использовании только униграмм эти фразы будут представлены одинаково, что приводит к потере семантики.\n",
        "\n",
        "#### Реализация с использованием `nltk`\n",
        "\n"
      ],
      "metadata": {
        "id": "77FoCI21gkYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams  # Алиас для ngrams(tokens, 2)\n",
        "\n",
        "# Генерация биграмм\n",
        "bigram_list = list(bigrams(tokens))\n",
        "print(f\"Биграммы (L={L}, число: {L-1}): {bigram_list}\")"
      ],
      "metadata": {
        "id": "hRvS9ndpgxb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Альтернатива**: `ngrams(tokens, 2)` даёт тот же результат.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Триграммы (3-граммы)\n",
        "\n",
        "**Триграммы** — это последовательности из трёх слов. Они захватывают **более широкий контекст**, что полезно для анализа сложных синтаксических конструкций и идиом.\n",
        "\n",
        "Множество триграмм:\n",
        "$$\n",
        "\\{(w_i, w_{i+1}, w_{i+2}) \\mid 1 \\leq i \\leq L-2\\}\n",
        "$$\n",
        "\n",
        "#### Пример\n",
        "Предложение: *\"Я люблю NLP\"*  \n",
        "$L = 3$  \n",
        "Триграмма: `\"Я люблю NLP\"`\n",
        "\n",
        "#### Семантическая значимость\n",
        "Триграммы позволяют учитывать отрицания:\n",
        "- *\"не очень хорошо\"* — триграмма `\"не очень хорошо\"` передаёт **отрицательную оценку**.\n",
        "- Без триграмм модель может интерпретировать это как просто \"хорошо\".\n",
        "\n",
        "#### Реализация с использованием `nltk`\n",
        "\n"
      ],
      "metadata": {
        "id": "wIheVMTEgxk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация триграмм\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(f\"Триграммы (L={L}, число: {L-2}): {trigrams}\")"
      ],
      "metadata": {
        "id": "yohYeg_ag3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Расширение: N-граммы в распределённых системах (PySpark)\n",
        "\n",
        "Для обработки больших корпусов текстов в промышленных приложениях часто используется **Apache Spark**, в частности модуль `pyspark.ml.feature.NGram`.\n",
        "\n",
        "#### Пример с `pyspark.ml.feature.NGram`\n",
        "\n"
      ],
      "metadata": {
        "id": "O4Lab-zTg3tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import NGram\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# Создание сессии Spark\n",
        "spark = SparkSession.builder.appName(\"NGramExample\").getOrCreate()\n",
        "\n",
        "# Исходные данные\n",
        "data = [(0, [\"Я\", \"люблю\", \"NLP\"]), (1, [\"NLP\", \"очень\", \"интересна\"])]\n",
        "df = spark.createDataFrame(data, [\"id\", \"words\"])\n",
        "\n",
        "# Генерация биграмм\n",
        "ngram_transformer = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "ngram_df = ngram_transformer.transform(df)\n",
        "\n",
        "# Просмотр результатов\n",
        "ngram_df.select(\"ngrams\").show(truncate=False)"
      ],
      "metadata": {
        "id": "RTDVv2uqg98j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Примечание**: `NGram` в PySpark автоматически генерирует N-граммы для каждого документа и поддерживает масштабирование на большие данные.\n",
        "\n",
        "\n",
        "\n",
        "## Обобщение: Диапазон N-грамм\n",
        "\n",
        "На практике редко используется только один тип N-грамм. Обычно задаётся **диапазон**, например:\n",
        "$$\n",
        "\\text{ngram\\_range} = (1, 2) \\quad \\text{или} \\quad (1, 3)\n",
        "$$\n",
        "\n",
        "Это позволяет модели учитывать **униграммы, биграммы и триграммы** одновременно, что обеспечивает баланс между **семантической точностью** и **вычислительной эффективностью**.\n",
        "\n",
        "\n",
        "## Преимущества и недостатки N-грамм\n",
        "\n",
        "| Преимущество | Пояснение |\n",
        "|-------------|----------|\n",
        "| **Учёт локального контекста** | Модель видит, какие слова стоят рядом |\n",
        "| **Различение фраз** | Позволяет отличать схожие по словам, но разные по смыслу конструкции |\n",
        "| **Улучшение качества классификации** | Особенно полезно в задачах анализа тональности, детектирования спама и поиска |\n",
        "\n",
        "| Недостаток | Пояснение |\n",
        "|----------|----------|\n",
        "| **Рост размерности словаря** | Каждая уникальная N-грамма — новый признак. Словарь растёт экспоненциально |\n",
        "| **Разреженность векторов** | Многие N-граммы встречаются редко, что приводит к разрежённым векторам |\n",
        "| **Ограниченный контекст** | Учитываются только локальные связи, но не глобальный смысл предложения |\n",
        "\n",
        "\n",
        "## Практическая реализация на Python\n",
        "\n",
        "Ниже приведён **подробный и прокомментированный код**, демонстрирующий, как использовать TF-IDF с разными типами N-грамм.\n"
      ],
      "metadata": {
        "id": "x9Eouj-1g-Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Пример корпуса документов\n",
        "documents = [\n",
        "    \"Кот спит на диване\",\n",
        "    \"Собака бежит по улице\",\n",
        "    \"Кот и собака играют\",\n",
        "    \"Собака спит на коврике\"\n",
        "]\n",
        "\n",
        "print(\"=== 1. УНИГРАММЫ (1-граммы) ===\")\n",
        "# Создаём векторизатор для униграмм: ngram_range=(1,1)\n",
        "tfidf_unigram = TfidfVectorizer(ngram_range=(1, 1), lowercase=True)\n",
        "# Обучаем и преобразуем\n",
        "X_unigram = tfidf_unigram.fit_transform(documents)\n",
        "# Получаем словарь и векторы\n",
        "vocab_unigram = tfidf_unigram.get_feature_names_out()\n",
        "print(f\"Словарь униграмм: {list(vocab_unigram)}\")\n",
        "print(\"TF-IDF матрица (каждая строка — документ):\")\n",
        "print(X_unigram.toarray())\n",
        "print()\n",
        "\n",
        "print(\"=== 2. БИГРАММЫ (2-граммы) ===\")\n",
        "# Создаём векторизатор для биграмм: ngram_range=(2,2)\n",
        "tfidf_bigram = TfidfVectorizer(ngram_range=(2, 2), lowercase=True)\n",
        "X_bigram = tfidf_bigram.fit_transform(documents)\n",
        "vocab_bigram = tfidf_bigram.get_feature_names_out()\n",
        "print(f\"Словарь биграмм: {list(vocab_bigram)}\")\n",
        "print(\"TF-IDF матрица:\")\n",
        "print(X_bigram.toarray())\n",
        "print()\n",
        "\n",
        "print(\"=== 3. УНИГРАММЫ + БИГРАММЫ (диапазон 1–2) ===\")\n",
        "# Создаём векторизатор, учитывающий и 1-граммы, и 2-граммы\n",
        "tfidf_12 = TfidfVectorizer(ngram_range=(1, 2), lowercase=True)\n",
        "X_12 = tfidf_12.fit_transform(documents)\n",
        "vocab_12 = tfidf_12.get_feature_names_out()\n",
        "print(f\"Словарь (униграммы + биграммы): {list(vocab_12)}\")\n",
        "print(\"TF-IDF матрица:\")\n",
        "print(X_12.toarray())\n",
        "print()\n",
        "\n",
        "print(\"=== 4. ПРИМЕР: ВЕКТОРИЗАЦИЯ НОВОГО ДОКУМЕНТА ===\")\n",
        "# Новый документ (не в обучающем корпусе)\n",
        "new_doc = [\"Кот спит\"]\n",
        "# Преобразуем с помощью обученного векторизатора\n",
        "new_vec = tfidf_12.transform(new_doc)\n",
        "print(f\"Новый документ: '{new_doc[0]}'\")\n",
        "print(f\"Его TF-IDF вектор (размерность: {new_vec.shape[1]}):\")\n",
        "print(new_vec.toarray())\n",
        "print(\"Ненулевые признаки:\")\n",
        "# Находим индексы ненулевых элементов\n",
        "nonzero_idx = new_vec.nonzero()[1]\n",
        "for idx in nonzero_idx:\n",
        "    feature = vocab_12[idx]\n",
        "    weight = new_vec[0, idx]\n",
        "    print(f\"  '{feature}': {weight:.4f}\")"
      ],
      "metadata": {
        "id": "XE1n98LlhjlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Анализ результата\n",
        "\n",
        "Для документа `\"Кот спит\"` векторизатор с `ngram_range=(1,2)` создаст признаки:\n",
        "- Униграммы: `\"кот\"`, `\"спит\"`\n",
        "- Биграммы: `\"кот спит\"`\n",
        "\n",
        "Если биграмма `\"кот спит\"` встречалась в обучающем корпусе, она получит **собственный вес**, отражающий её информативность. Это позволяет модели учитывать **фразы как единые смысловые единицы**.\n"
      ],
      "metadata": {
        "id": "D6OhhA58hpyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Расширение моделей BoW и TF-IDF с помощью **N-грамм** позволяет частично преодолеть их ключевой недостаток — **игнорирование порядка слов**. Использование униграмм, биграмм и триграмм обеспечивает разный уровень контекстуального охвата:\n",
        "\n",
        "| Тип | Число N-грамм | Контекст |\n",
        "|-----|----------------|----------|\n",
        "| Униграммы | $L$ | Отдельные слова |\n",
        "| Биграммы | $L-1$ | Пары слов |\n",
        "| Триграммы | $L-2$ | Тройки слов |\n",
        "\n",
        "\n",
        "\n",
        "Хотя это приводит к **росту размерности** и **разрежённости**, на практике использование диапазона `(1,2)` или `(1,3)` часто даёт **значительный прирост качества** при разумных вычислительных затратах.\n",
        "\n",
        "Для научных и образовательных задач рекомендуется использовать `nltk.util.ngrams`, а для обработки больших данных — `pyspark.ml.feature.NGram`. В следующих главах мы рассмотрим **плотные векторные эмбеддинги**, которые решают проблему контекста более глубоко."
      ],
      "metadata": {
        "id": "vAlrCDUxhizY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 3. Word2Vec: Подробное описание\n",
        "\n",
        "## Введение\n",
        "\n",
        "**Word2Vec** — это семейство моделей машинного обучения, разработанных Томашем Миколовым и его коллегами в Google в 2013 году, предназначенных для эффективного обучения **плотных векторных представлений слов**, известных как **словные эмбеддинги (word embeddings)**. Эти эмбеддинги представляют слова в непрерывном многомерном векторном пространстве, где **семантическая близость** слов отражается через **геометрическую близость** их векторов.\n",
        "\n",
        "Основное преимущество Word2Vec заключается в способности модели улавливать **семантические и синтаксические отношения** между словами. Например, векторы слов *\"король\"* и *\"королева\"* оказываются близки друг к другу, а арифметические операции вида:\n",
        "$$\n",
        "\\mathrm{vec}(\\text{король}) - \\mathrm{vec}(\\text{мужчина}) + \\mathrm{vec}(\\text{женщина}) \\approx \\mathrm{vec}(\\text{королева})\n",
        "$$\n",
        "демонстрируют, что модель способна усваивать абстрактные лингвистические закономерности.\n",
        "\n",
        "\n",
        "\n",
        "## Теоретическая основа: Гипотеза распределения\n",
        "\n",
        "В основе Word2Vec лежит **гипотеза распределения (Distributional Hypothesis)**, сформулированная Дж. Р. Фирсом:  \n",
        "> *\"Слова, которые появляются в похожих контекстах, имеют схожие значения.\"*\n",
        "\n",
        "Эта гипотеза лежит в основе всех современных методов обучения векторных представлений слов. Word2Vec реализует её, обучаясь предсказывать слова на основе их окружения (контекста), что позволяет модели неявно усваивать лексико-семантические паттерны.\n",
        "\n",
        "\n",
        "\n",
        "## Архитектура модели\n",
        "\n",
        "Word2Vec не является глубокой нейронной сетью в классическом понимании. Это **двухслойная нейросеть прямого распространения (shallow neural network)** с тремя основными слоями:\n",
        "1. **Входной слой** — кодирует входное слово (или контекст) в виде one-hot вектора.\n",
        "2. **Проекционный (скрытый) слой** — содержит $d$ нейронов, где $d$ — размерность эмбеддингов. Веса этого слоя и есть искомые векторные представления слов.\n",
        "3. **Выходной слой** — вычисляет вероятности всех слов в словаре для задачи классификации.\n",
        "\n",
        "Модель обучается решению **вспомогательной задачи (proxy task)** — предсказанию слова по контексту или наоборот. В процессе обучения веса проекционного слоя настраиваются так, чтобы максимизировать вероятность правильного предсказания. После обучения эти веса используются как **словные эмбеддинги**.\n",
        "\n",
        "\n",
        "\n",
        "## Две основные архитектуры\n",
        "\n",
        "Word2Vec предлагает две архитектуры, различающиеся направлением предсказания:\n",
        "\n",
        "### 1. CBOW (Continuous Bag-of-Words)  \n",
        "Предсказывает целевое (центральное) слово на основе его контекстных слов.\n",
        "\n",
        "### 2. Skip-gram  \n",
        "Предсказывает контекстные слова на основе целевого (центрального) слова.\n",
        "\n",
        "\n",
        "## Математические основы модели Skip-gram\n",
        "\n",
        "### 1. Введение в векторные представления слов (Word Embeddings)\n",
        "\n",
        "В области обработки естественного языка (NLP) традиционные методы представления слов, такие как унитарное (one-hot) кодирование, страдают от проблемы *\"проклятия размерности\"* и неспособности улавливать семантические или синтаксические отношения между словами. Каждое слово представляется как независимая сущность, что не позволяет моделировать сходство между словами, например, между *\"король\"* и *\"королева\"* или *\"быстрый\"* и *\"скорый\"*.\n",
        "\n",
        "Векторные представления слов, или **word embeddings**, решают эту проблему, отображая слова из дискретного пространства в непрерывное векторное пространство низкой размерности. В этом пространстве слова с похожим значением или контекстом располагаются близко друг к другу. Такие представления являются основой для многих современных NLP-задач, включая машинный перевод, анализ настроений и вопросно-ответные системы.\n",
        "\n",
        "Модель **Skip-gram**, разработанная Томашем Миколовым и его коллегами в Google, является одной из наиболее популярных и эффективных архитектур для обучения векторных представлений слов. Она относится к семейству моделей **Word2Vec**.\n",
        "\n",
        "#### 1.1. Основная идея Skip-gram\n",
        "\n",
        "Основная идея модели Skip-gram заключается в предсказании контекстных слов, окружающих данное целевое слово. Если модель может успешно предсказывать контекст, значит, она «понимает» что-то о значении целевого слова, и это «понимание» кодируется в его векторном представлении.\n",
        "\n",
        "Пусть у нас есть предложение: *\"Кот сидит на коврике\"*. Если *\"сидит\"* является целевым словом, то *\"Кот\"*, *\"на\"*, *\"коврике\"* могут быть его контекстными словами в пределах определённого окна. Модель Skip-gram пытается максимизировать вероятность наблюдения контекстных слов $w_c$ при данном целевом слове $w_t$:\n",
        "$$\n",
        "P(w_c \\mid w_t)\n",
        "$$\n",
        "Это отличается от модели **CBOW** (Continuous Bag of Words), которая предсказывает целевое слово на основе его контекста.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Архитектура модели Skip-gram\n",
        "\n",
        "Модель Skip-gram представляет собой простую двухслойную нейронную сеть (без нелинейности в скрытом слое), которая обучается на большом корпусе текста.\n",
        "\n",
        "#### 2.1. Слои модели\n",
        "\n",
        "1. **Входной слой (Input Layer)**: Представляет целевое слово $w_t$ в виде унитарного (one-hot) вектора. Размерность этого вектора равна размеру словаря $V$.\n",
        "2. **Скрытый слой (Hidden Layer)**: Этот слой не имеет функции активации (или имеет линейную функцию активации). Количество нейронов в этом слое равно желаемой размерности векторного представления слова $N$. Выход этого слоя является векторным представлением (эмбеддингом) входного слова.\n",
        "3. **Выходной слой (Output Layer)**: Имеет $V$ нейронов, по одному для каждого слова в словаре. Каждый нейрон выдает оценку (score) того, насколько вероятно, что соответствующее слово является контекстным словом для данного целевого слова. Затем к этим оценкам применяется функция **Softmax** для получения вероятностей.\n",
        "\n",
        "#### 2.2. Весовые матрицы\n",
        "\n",
        "В модели Skip-gram есть две основные весовые матрицы:\n",
        "\n",
        "- **Матрица весов \"вход–скрытый слой\" ($W_{\\text{in}}$)**: Эта матрица имеет размерность $V \\times N$. Каждая строка этой матрицы представляет собой $N$-мерный вектор, который является эмбеддингом соответствующего слова, когда оно выступает в роли входного (целевого) слова. Мы будем называть эти векторы **входными эмбеддингами** и обозначать их как $\\mathbf{v}_w$.\n",
        "\n",
        "- **Матрица весов \"скрытый слой–выход\" ($W_{\\text{out}}$)**: Эта матрица имеет размерность $N \\times V$. Каждый столбец этой матрицы представляет собой $N$-мерный вектор, который является эмбеддингом соответствующего слова, когда оно выступает в роли контекстного слова. Мы будем называть эти векторы **выходными эмбеддингами** и обозначать их как $\\mathbf{u}_w$.\n",
        "\n",
        "> **Важно**: Для каждого слова $w$ в словаре существует два векторных представления: $\\mathbf{v}_w$ (когда $w$ — целевое слово) и $\\mathbf{u}_w$ (когда $w$ — контекстное слово). В конце обучения обычно используется $\\mathbf{v}_w$ в качестве окончательного эмбеддинга слова.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Алгоритм прямого прохода (Forward Pass)\n",
        "\n",
        "Прямой проход — это процесс вычисления выходных вероятностей модели на основе заданного входного (целевого) слова.\n",
        "\n",
        "Пусть $w_t$ — целевое слово, представленное one-hot вектором $\\mathbf{x}_t$ размерности $V$.\n",
        "\n",
        "#### 3.1. Вычисление скрытого слоя\n",
        "\n",
        "One-hot вектор $\\mathbf{x}_t$ имеет единицу на позиции, соответствующей $w_t$, и нули в остальных позициях. При умножении на матрицу $W_{\\text{in}}$ ($V \\times N$) результатом является $N$-мерный вектор, соответствующий строке матрицы $W_{\\text{in}}$, связанной со словом $w_t$. Этот вектор и есть входной эмбеддинг слова $w_t$, обозначаемый как $\\mathbf{v}_{w_t}$:\n",
        "$$\n",
        "\\mathbf{h} = \\mathbf{x}_t^\\top W_{\\text{in}} = \\mathbf{v}_{w_t}\n",
        "$$\n",
        "Здесь $\\mathbf{h}$ — вектор скрытого слоя, который фактически является эмбеддингом целевого слова $w_t$.\n",
        "\n",
        "#### 3.2. Вычисление оценок для выходного слоя\n",
        "\n",
        "Вектор скрытого слоя $\\mathbf{h}$ умножается на матрицу $W_{\\text{out}}$ ($N \\times V$). Результат — $V$-мерный вектор оценок $\\mathbf{z}$, где каждый элемент $z_j$ представляет собой оценку для $j$-го слова в словаре:\n",
        "$$\n",
        "\\mathbf{z} = \\mathbf{h}^\\top W_{\\text{out}} = \\mathbf{v}_{w_t}^\\top W_{\\text{out}}\n",
        "$$\n",
        "Каждый элемент $z_j$ может быть интерпретирован как скалярное произведение входного эмбеддинга целевого слова $\\mathbf{v}_{w_t}$ и выходного эмбеддинга контекстного слова $\\mathbf{u}_{w_j}$:\n",
        "$$\n",
        "z_j = \\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_j}\n",
        "$$\n",
        "Высокое значение $z_j$ означает, что слова $w_t$ и $w_j$ часто встречаются вместе в контексте.\n",
        "\n",
        "#### 3.3. Применение функции Softmax\n",
        "\n",
        "Для преобразования оценок $\\mathbf{z}$ в вероятности $P(w_j \\mid w_t)$, которые суммируются к 1, используется функция **Softmax**:\n",
        "$$\n",
        "P(w_j \\mid w_t) = \\frac{\\exp(z_j)}{\\sum_{k=1}^{V} \\exp(z_k)} = \\frac{\\exp(\\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_j})}{\\sum_{k=1}^{V} \\exp(\\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_k})}\n",
        "$$\n",
        "Это и есть предсказанная вероятность того, что слово $w_j$ является контекстным словом для $w_t$.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Функция потерь (Loss Function)\n",
        "\n",
        "Цель обучения модели Skip-gram — максимизировать вероятность наблюдения фактических контекстных слов для каждого целевого слова. Это эквивалентно минимизации отрицательного логарифма этой вероятности.\n",
        "\n",
        "Для каждой пары (целевое слово $w_t$, контекстное слово $w_c$) в обучающем наборе функция потерь (кросс-энтропия) определяется как:\n",
        "$$\n",
        "\\mathcal{L}(w_t, w_c) = -\\log P(w_c \\mid w_t)\n",
        "$$\n",
        "\n",
        "Для всего обучающего набора, состоящего из пар $(w_t, w_c)$, извлечённых из корпуса текста, общая функция потерь, которую мы хотим минимизировать, имеет вид:\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_{(w_t, w_c) \\in D} \\log P(w_c \\mid w_t)\n",
        "$$\n",
        "где $D$ — множество всех пар (целевое слово, контекстное слово) из обучающего корпуса.\n",
        "\n",
        "\n",
        "\n",
        "> **Примечание**: В реальных реализациях прямое применение Softmax вычислительно затратно из-за необходимости суммирования по всему словарю ($V$). Поэтому на практике часто используются приближённые методы, такие как **Negative Sampling** или **Hierarchical Softmax**, которые значительно ускоряют обучение.\n",
        "\n",
        "\n",
        "# 5. Алгоритм обратного распространения ошибки (Backpropagation)\n",
        "\n",
        "**Обратное распространение ошибки (Backpropagation)** — это ключевой алгоритм в обучении нейронных сетей, позволяющий эффективно вычислять градиенты функции потерь по отношению к весам модели. Эти градиенты используются для обновления весов с целью минимизации потерь с помощью методов оптимизации, таких как стохастический градиентный спуск (SGD).\n",
        "\n",
        "Для модели **Skip-gram** мы хотим вычислить градиенты по двум весовым матрицам:\n",
        "- $\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{in}}}$ — по входной матрице (входные эмбеддинги $\\mathbf{v}_w$),\n",
        "- $\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}}$ — по выходной матрице (выходные эмбеддинги $\\mathbf{u}_w$).\n",
        "\n",
        "Рассмотрим процесс для одной обучающей пары $(w_t, w_c)$, где $w_t$ — целевое слово, $w_c$ — контекстное слово.\n",
        "\n",
        "\n",
        "\n",
        "### 5.1. Градиент по выходному слою\n",
        "\n",
        "Начнём с вычисления градиента функции потерь по отношению к вектору оценок $z_j$, полученных на выходном слое.\n",
        "\n",
        "Функция потерь для одной пары:\n",
        "$$\n",
        "\\mathcal{L} = -\\log P(w_c \\mid w_t) = -\\log \\left( \\frac{\\exp(z_c)}{\\sum_{k=1}^{V} \\exp(z_k)} \\right)\n",
        "$$\n",
        "где $z_c = \\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_c}$ — оценка для истинного контекстного слова $w_c$.\n",
        "\n",
        "Производная потерь по оценке $z_j$ известна из свойств **Softmax + кросс-энтропия**:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_j} = P(w_j \\mid w_t) - y_j\n",
        "$$\n",
        "где:\n",
        "- $P(w_j \\mid w_t)$ — предсказанная вероятность слова $w_j$,\n",
        "- $y_j = \\begin{cases} 1, & \\text{если } j = c \\\\ 0, & \\text{иначе} \\end{cases}$ — истинная метка (one-hot вектор).\n",
        "\n",
        "Обозначим эту разницу как **ошибку на выходе**:\n",
        "$$\n",
        "e_j = P(w_j \\mid w_t) - y_j\n",
        "$$\n",
        "Вектор ошибок $\\mathbf{e} = [e_1, e_2, \\dots, e_V]^\\top$ показывает, насколько модель ошиблась при предсказании каждого слова в словаре.\n",
        "\n",
        "#### Обновление выходных эмбеддингов\n",
        "\n",
        "Напомним: $z_j = \\mathbf{h}^\\top \\mathbf{u}_{w_j} = \\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_j}$, где $\\mathbf{u}_{w_j}$ — $j$-й столбец матрицы $W_{\\text{out}}$.\n",
        "\n",
        "Тогда градиент потерь по выходному эмбеддингу слова $w_j$:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{w_j}} = \\frac{\\partial \\mathcal{L}}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial \\mathbf{u}_{w_j}} = e_j \\cdot \\mathbf{h} = e_j \\cdot \\mathbf{v}_{w_t}\n",
        "$$\n",
        "\n",
        "Обновление весов (с шагом обучения $\\eta$):\n",
        "$$\n",
        "\\mathbf{u}_{w_j}^{\\text{new}} = \\mathbf{u}_{w_j}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{w_j}} = \\mathbf{u}_{w_j}^{\\text{old}} - \\eta \\cdot e_j \\cdot \\mathbf{v}_{w_t}\n",
        "$$\n",
        "\n",
        "> ⚠️ **Важно**: Это обновление применяется ко **всем** словам в словаре, что вычислительно затратно. Именно поэтому на практике используется **Negative Sampling** (см. далее).\n",
        "\n",
        "\n",
        "\n",
        "### 5.2. Градиент по скрытому слою\n",
        "\n",
        "Теперь вычислим градиент функции потерь по вектору скрытого слоя $\\mathbf{h} = \\mathbf{v}_{w_t}$, чтобы передать ошибку назад к входному слою.\n",
        "\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} = \\sum_{j=1}^{V} \\frac{\\partial \\mathcal{L}}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial \\mathbf{h}} = \\sum_{j=1}^{V} e_j \\cdot \\mathbf{u}_{w_j}\n",
        "$$\n",
        "\n",
        "Этот вектор:\n",
        "$$\n",
        "\\delta_{\\mathbf{h}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} = \\sum_{j=1}^{V} e_j \\cdot \\mathbf{u}_{w_j}\n",
        "$$\n",
        "представляет собой **взвешенную сумму выходных эмбеддингов**, где веса — ошибки $e_j$. Он показывает, как нужно изменить эмбеддинг целевого слова $w_t$, чтобы уменьшить потери.\n",
        "\n",
        "\n",
        "### 5.3. Градиент по входному слою (обновление $W_{\\text{in}}$)\n",
        "\n",
        "Напомним: $\\mathbf{h} = \\mathbf{x}_t^\\top W_{\\text{in}}$, где $\\mathbf{x}_t$ — one-hot вектор для $w_t$. Это означает, что $\\mathbf{h}$ — это просто $t$-я строка матрицы $W_{\\text{in}}$, т.е. $\\mathbf{v}_{w_t}$.\n",
        "\n",
        "Следовательно, градиент по $W_{\\text{in}}$ затрагивает **только одну строку** — соответствующую $w_t$:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{in}}} = \\mathbf{x}_t \\cdot \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} \\right)^\\top\n",
        "$$\n",
        "Поскольку $\\mathbf{x}_t$ — one-hot вектор, результат — матрица, у которой только $t$-я строка ненулевая и равна $\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} \\right)^\\top$.\n",
        "\n",
        "Таким образом, обновление входного эмбеддинга:\n",
        "$$\n",
        "\\mathbf{v}_{w_t}^{\\text{new}} = \\mathbf{v}_{w_t}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} = \\mathbf{v}_{w_t}^{\\text{old}} - \\eta \\cdot \\sum_{j=1}^{V} e_j \\cdot \\mathbf{u}_{w_j}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 5.4. Итоговый алгоритм обучения (с полным Softmax)\n",
        "\n",
        "Для каждой пары $(w_t, w_c)$ в контекстном окне:\n",
        "1. **Прямой проход**:\n",
        "   - Получить $\\mathbf{v}_{w_t}$ из $W_{\\text{in}}$.\n",
        "   - Вычислить $z_j = \\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_j}$ для всех $j$.\n",
        "   - Применить Softmax: $P(w_j \\mid w_t) = \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}$.\n",
        "2. **Вычисление ошибок**:\n",
        "   - $e_j = P(w_j \\mid w_t) - y_j$, где $y_j = 1$ только если $j = c$.\n",
        "3. **Обратное распространение**:\n",
        "   - Обновить все выходные эмбеддинги:  \n",
        "     $\\mathbf{u}_{w_j} \\leftarrow \\mathbf{u}_{w_j} - \\eta \\cdot e_j \\cdot \\mathbf{v}_{w_t}$.\n",
        "   - Вычислить $\\delta_{\\mathbf{h}} = \\sum_j e_j \\cdot \\mathbf{u}_{w_j}$.\n",
        "   - Обновить входной эмбеддинг:  \n",
        "     $\\mathbf{v}_{w_t} \\leftarrow \\mathbf{v}_{w_t} - \\eta \\cdot \\delta_{\\mathbf{h}}$.\n",
        "\n",
        "> ❌ **Проблема**: Шаг 3 требует обновления $V$ векторов (по размеру словаря), что крайне неэффективно при $V \\sim 10^5{-}10^6$.\n",
        "\n",
        "\n",
        "\n",
        "# 6. Оптимизация: Negative Sampling (Отрицательное сэмплирование)\n",
        "\n",
        "Для ускорения обучения вместо полного Softmax используется **Negative Sampling (NS)** — приближённый метод, заменяющий многоклассовую задачу на серию бинарных классификаций.\n",
        "\n",
        "## 6.1. Принцип работы\n",
        "\n",
        "Для каждой **положительной пары** $(w_t, w_c)$, которая реально встречается в тексте, модель учится:\n",
        "- **Подтверждать** эту пару («это хороший контекст»),\n",
        "- **Опровергать** $k$ случайно выбранных **отрицательных пар** $(w_t, w_{\\text{neg},i})$, где $w_{\\text{neg},i}$ — слова, **не** входящие в контекст $w_t$.\n",
        "\n",
        "Цель: научить модель отличать \"настоящие\" контексты от \"случайных\".\n",
        "\n",
        "\n",
        "\n",
        "## 6.2. Функция потерь с Negative Sampling\n",
        "\n",
        "Функция потерь строится на основе **логистической регрессии** (сигмоиды):\n",
        "$$\n",
        "\\mathcal{L}_{\\text{NS}} = -\\log \\sigma(\\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_c}) - \\sum_{i=1}^{k} \\log \\sigma(-\\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_{\\text{neg},i}})\n",
        "$$\n",
        "где:\n",
        "- $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ — сигмоида,\n",
        "- $k$ — количество отрицательных примеров (обычно 5–20).\n",
        "\n",
        "- Первый член максимизирует вероятность **наличия** связи $w_t$–$w_c$,\n",
        "- Второй член максимизирует вероятность **отсутствия** связи $w_t$–$w_{\\text{neg},i}$.\n",
        "\n",
        "\n",
        "\n",
        "## 6.3. Обратное распространение с Negative Sampling\n",
        "\n",
        "Градиенты теперь вычисляются только для **малого числа векторов** — $w_t$, $w_c$ и $k$ отрицательных слов.\n",
        "\n",
        "#### Для положительного примера $(w_t, w_c)$:\n",
        "\n",
        "Пусть $s = \\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_c}$. Тогда:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}_{\\text{NS}}}{\\partial \\mathbf{v}_{w_t}} = (\\sigma(s) - 1) \\cdot \\mathbf{u}_{w_c}, \\quad\n",
        "\\frac{\\partial \\mathcal{L}_{\\text{NS}}}{\\partial \\mathbf{u}_{w_c}} = (\\sigma(s) - 1) \\cdot \\mathbf{v}_{w_t}\n",
        "$$\n",
        "\n",
        "#### Для каждого отрицательного примера $(w_t, w_{\\text{neg},i})$:\n",
        "\n",
        "Пусть $s_i = \\mathbf{v}_{w_t}^\\top \\mathbf{u}_{w_{\\text{neg},i}}$. Тогда:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}_{\\text{NS}}}{\\partial \\mathbf{v}_{w_t}} \\mathrel{+}= \\sigma(s_i) \\cdot \\mathbf{u}_{w_{\\text{neg},i}}, \\quad\n",
        "\\frac{\\partial \\mathcal{L}_{\\text{NS}}}{\\partial \\mathbf{u}_{w_{\\text{neg},i}}} = \\sigma(s_i) \\cdot \\mathbf{v}_{w_t}\n",
        "$$\n",
        "\n",
        "> 🔁 **Обновляются только**:\n",
        "> - $\\mathbf{v}_{w_t}$ (входной эмбеддинг целевого слова),\n",
        "> - $\\mathbf{u}_{w_c}$ (выходной эмбеддинг положительного слова),\n",
        "> - $\\mathbf{u}_{w_{\\text{neg},i}}$ (выходные эмбеддинги $k$ отрицательных слов).\n",
        "\n",
        "✅ **Преимущества**:\n",
        "- Вычислительная сложность снизилась с $O(V)$ до $O(k)$.\n",
        "- Обучение становится на порядки быстрее.\n",
        "- Сохраняется качество эмбеддингов.\n",
        "\n",
        "\n",
        "# 7. Заключение\n",
        "\n",
        "Модель **Skip-gram** является мощным и эффективным инструментом для обучения векторных представлений слов. Её математическая основа — комбинация простой архитектуры, гипотезы распределения и градиентного обучения — позволяет модели улавливать как семантические, так и синтаксические закономерности.\n",
        "\n",
        "Ключевые компоненты:\n",
        "- **Прямой проход** формирует вероятности контекстных слов.\n",
        "- **Обратное распространение** позволяет эффективно обновлять веса.\n",
        "- **Negative Sampling** делает обучение масштабируемым, заменяя дорогой Softmax на быстрые бинарные классификации.\n",
        "\n",
        "Понимание этих механизмов критически важно не только для использования Word2Vec, но и для освоения более сложных моделей NLP, таких как **GloVe**, **FastText**, и современных трансформеров, где идеи векторных представлений и контекстного обучения развиваются дальше.\n"
      ],
      "metadata": {
        "id": "038pgJkhiIOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 8. Аналитический пример: Пошаговое вычисление Skip-gram\n",
        "\n",
        "В этом разделе мы проведём **пошаговое вычисление** прямого и обратного прохода модели **Skip-gram** на конкретном примере. Все вычисления будут выполнены вручную, чтобы продемонстрировать, как модель обучается на одной обучающей паре.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Исходные данные и инициализация\n",
        "\n",
        "**Предложение для обучения**:  \n",
        "> \"кот сидит на коврике\"\n",
        "\n",
        "**Словарь** (размер $V = 4$):  \n",
        "$$\n",
        "\\text{слово} \\mapsto \\text{индекс:} \\quad\n",
        "\\begin{cases}\n",
        "\\text{кот} &\\mapsto 0 \\\\\n",
        "\\text{сидит} &\\mapsto 1 \\\\\n",
        "\\text{на} &\\mapsto 2 \\\\\n",
        "\\text{коврике} &\\mapsto 3\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Размерность эмбеддингов** $N = 2$ — каждое слово представляется 2-мерным вектором.\n",
        "\n",
        "**One-hot векторы**:\n",
        "$$\n",
        "\\mathbf{x}_{\\text{кот}} = [1, 0, 0, 0]^\\top, \\quad\n",
        "\\mathbf{x}_{\\text{сидит}} = [0, 1, 0, 0]^\\top, \\\\\n",
        "\\mathbf{x}_{\\text{на}} = [0, 0, 1, 0]^\\top, \\quad\n",
        "\\mathbf{x}_{\\text{коврике}} = [0, 0, 0, 1]^\\top\n",
        "$$\n",
        "\n",
        "**Скорость обучения**: $\\eta = 0.01$\n",
        "\n",
        "\n",
        "\n",
        "### Инициализация весовых матриц\n",
        "\n",
        "Матрицы инициализируются случайными малыми значениями.\n",
        "\n",
        "#### Матрица $W_{\\text{in}}$ (входные эмбеддинги $\\mathbf{v}_w$)\n",
        "\n",
        "Размерность: $V \\times N = 4 \\times 2$  \n",
        "Каждая строка — это входной эмбеддинг слова:\n",
        "\n",
        "$$\n",
        "W_{\\text{in}} =\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{v}_{\\text{кот}} \\\\\n",
        "\\mathbf{v}_{\\text{сидит}} \\\\\n",
        "\\mathbf{v}_{\\text{на}} \\\\\n",
        "\\mathbf{v}_{\\text{коврике}}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.3 \\\\\n",
        "0.5 & 0.7 \\\\\n",
        "0.2 & 0.4 \\\\\n",
        "0.6 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "То есть:\n",
        "- $\\mathbf{v}_{\\text{кот}} = [0.1, 0.3]$\n",
        "- $\\mathbf{v}_{\\text{сидит}} = [0.5, 0.7]$\n",
        "- $\\mathbf{v}_{\\text{на}} = [0.2, 0.4]$\n",
        "- $\\mathbf{v}_{\\text{коврике}} = [0.6, 0.8]$\n",
        "\n",
        "> ⚠️ **Ошибка в оригинале**: в тексте было указано $\\mathbf{v}_{\\text{сидит}} = [0.3, 0.4]$, но это не соответствует второй строке матрицы. Исправлено.\n",
        "\n",
        "#### Матрица $W_{\\text{out}}$ (выходные эмбеддинги $\\mathbf{u}_w$)\n",
        "\n",
        "Размерность: $N \\times V = 2 \\times 4$  \n",
        "Каждый столбец — это выходной эмбеддинг слова:\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} =\n",
        "\\begin{bmatrix}\n",
        "0.05 & 0.15 & 0.25 & 0.35 \\\\\n",
        "0.10 & 0.20 & 0.30 & 0.40\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "То есть:\n",
        "- $\\mathbf{u}_{\\text{кот}} = [0.05, 0.10]^\\top$\n",
        "- $\\mathbf{u}_{\\text{сидит}} = [0.15, 0.20]^\\top$\n",
        "- $\\mathbf{u}_{\\text{на}} = [0.25, 0.30]^\\top$\n",
        "- $\\mathbf{u}_{\\text{коврике}} = [0.35, 0.40]^\\top$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Выбор обучающей пары\n",
        "\n",
        "Выберем:\n",
        "- **Целевое слово** $w_t = \\text{«сидит»}$ (индекс 1)\n",
        "- **Контекстное слово** $w_c = \\text{«кот»}$ (индекс 0)\n",
        "\n",
        "(Предположим, что контекстное окно размером 1 слева и справа.)\n",
        "\n",
        "\n",
        "\n",
        "## 3. Прямой проход (Forward Pass)\n",
        "\n",
        "### 3.1. Вычисление скрытого слоя $\\mathbf{h}$\n",
        "\n",
        "Скрытый слой — это эмбеддинг целевого слова:\n",
        "$$\n",
        "\\mathbf{h} = \\mathbf{x}_{\\text{сидит}}^\\top W_{\\text{in}} = \\text{вторая строка } W_{\\text{in}}\n",
        "$$\n",
        "$$\n",
        "\\mathbf{h} = \\mathbf{v}_{\\text{сидит}} = [0.5, 0.7]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2. Вычисление оценок $z_j$ на выходном слое\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\mathbf{h}^\\top W_{\\text{out}} = [0.5, 0.7]\n",
        "\\begin{bmatrix}\n",
        "0.05 & 0.15 & 0.25 & 0.35 \\\\\n",
        "0.10 & 0.20 & 0.30 & 0.40\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычислим скалярные произведения:\n",
        "- $z_{\\text{кот}} = \\mathbf{v}_{\\text{сидит}}^\\top \\mathbf{u}_{\\text{кот}} = 0.5 \\cdot 0.05 + 0.7 \\cdot 0.10 = 0.025 + 0.070 = 0.095$\n",
        "- $z_{\\text{сидит}} = 0.5 \\cdot 0.15 + 0.7 \\cdot 0.20 = 0.075 + 0.140 = 0.215$\n",
        "- $z_{\\text{на}} = 0.5 \\cdot 0.25 + 0.7 \\cdot 0.30 = 0.125 + 0.210 = 0.335$\n",
        "- $z_{\\text{коврике}} = 0.5 \\cdot 0.35 + 0.7 \\cdot 0.40 = 0.175 + 0.280 = 0.455$\n",
        "\n",
        "Итак:\n",
        "$$\n",
        "\\mathbf{z} = [0.095, 0.215, 0.335, 0.455]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.3. Применение Softmax\n",
        "\n",
        "$$\n",
        "P(w_j \\mid w_t) = \\frac{\\exp(z_j)}{\\sum_{k=0}^{3} \\exp(z_k)}\n",
        "$$\n",
        "\n",
        "Вычислим экспоненты:\n",
        "- $\\exp(0.095) \\approx 1.0996$\n",
        "- $\\exp(0.215) \\approx 1.2399$\n",
        "- $\\exp(0.335) \\approx 1.3977$\n",
        "- $\\exp(0.455) \\approx 1.5758$\n",
        "\n",
        "Сумма:  \n",
        "$$\n",
        "\\sum_k \\exp(z_k) = 1.0996 + 1.2399 + 1.3977 + 1.5758 = 5.3130\n",
        "$$\n",
        "\n",
        "Теперь вероятности:\n",
        "- $P(\\text{кот} \\mid \\text{сидит}) = \\frac{1.0996}{5.3130} \\approx 0.2069$\n",
        "- $P(\\text{сидит} \\mid \\text{сидит}) = \\frac{1.2399}{5.3130} \\approx 0.2334$\n",
        "- $P(\\text{на} \\mid \\text{сидит}) = \\frac{1.3977}{5.3130} \\approx 0.2631$\n",
        "- $P(\\text{коврике} \\mid \\text{сидит}) = \\frac{1.5758}{5.3130} \\approx 0.2966$\n",
        "\n",
        "Проверка: $0.2069 + 0.2334 + 0.2631 + 0.2966 = 1.0000$ ✅\n",
        "\n",
        "\n",
        "## 4. Функция потерь\n",
        "\n",
        "Истинное контекстное слово: $w_c = \\text{«кот»}$ → $y = [1, 0, 0, 0]^\\top$\n",
        "\n",
        "Функция потерь (кросс-энтропия):\n",
        "$$\n",
        "\\mathcal{L} = -\\log P(\\text{кот} \\mid \\text{сидит}) = -\\log(0.2069) \\approx -(-1.575) = 1.575\n",
        "$$\n",
        "\n",
        "> Цель: уменьшить $\\mathcal{L}$, увеличив $P(\\text{кот} \\mid \\text{сидит})$.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Обратное распространение ошибки (Backpropagation)\n",
        "\n",
        "### 5.1. Вычисление ошибок $e_j$\n",
        "\n",
        "$$\n",
        "e_j = P(w_j \\mid w_t) - y_j\n",
        "$$\n",
        "\n",
        "- $e_{\\text{кот}} = 0.2069 - 1 = -0.7931$\n",
        "- $e_{\\text{сидит}} = 0.2334 - 0 = 0.2334$\n",
        "- $e_{\\text{на}} = 0.2631 - 0 = 0.2631$\n",
        "- $e_{\\text{коврике}} = 0.2966 - 0 = 0.2966$\n",
        "\n",
        "Вектор ошибок:  \n",
        "$$\n",
        "\\mathbf{e} = [-0.7931, 0.2334, 0.2631, 0.2966]^\\top\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 5.2. Обновление выходных эмбеддингов $\\mathbf{u}_{w_j}$\n",
        "\n",
        "Градиент:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{w_j}} = e_j \\cdot \\mathbf{h} = e_j \\cdot [0.5, 0.7]\n",
        "$$\n",
        "\n",
        "Обновление:\n",
        "$$\n",
        "\\mathbf{u}_{w_j}^{\\text{new}} = \\mathbf{u}_{w_j}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{w_j}}\n",
        "$$\n",
        "\n",
        "#### Для $\\mathbf{u}_{\\text{кот}}$:\n",
        "- Градиент: $-0.7931 \\cdot [0.5, 0.7] = [-0.39655, -0.55517]$\n",
        "- Обновление:  \n",
        "  $$\n",
        "  \\mathbf{u}_{\\text{кот}}^{\\text{new}} = [0.05, 0.10] - 0.01 \\cdot [-0.39655, -0.55517] = [0.05, 0.10] + [0.0039655, 0.0055517]\n",
        "  $$\n",
        "  $$\n",
        "  \\mathbf{u}_{\\text{кот}}^{\\text{new}} \\approx [0.05397, 0.10555]\n",
        "  $$\n",
        "\n",
        "#### Для $\\mathbf{u}_{\\text{сидит}}$:\n",
        "- Градиент: $0.2334 \\cdot [0.5, 0.7] = [0.1167, 0.16338]$\n",
        "- Обновление:  \n",
        "  $$\n",
        "  \\mathbf{u}_{\\text{сидит}}^{\\text{new}} = [0.15, 0.20] - 0.01 \\cdot [0.1167, 0.16338] = [0.15, 0.20] - [0.001167, 0.0016338]\n",
        "  $$\n",
        "  $$\n",
        "  \\mathbf{u}_{\\text{сидит}}^{\\text{new}} \\approx [0.14883, 0.19837]\n",
        "  $$\n",
        "\n",
        "(Аналогично обновляются $\\mathbf{u}_{\\text{на}}$ и $\\mathbf{u}_{\\text{коврике}}$, но мы их опустим для краткости.)\n",
        "\n",
        "\n",
        "\n",
        "### 5.3. Вычисление градиента по скрытому слою $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} = \\sum_{j=0}^{3} e_j \\cdot \\mathbf{u}_{w_j}^{\\text{old}}\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "- $e_{\\text{кот}} \\cdot \\mathbf{u}_{\\text{кот}} = -0.7931 \\cdot [0.05, 0.10] = [-0.039655, -0.07931]$\n",
        "- $e_{\\text{сидит}} \\cdot \\mathbf{u}_{\\text{сидит}} = 0.2334 \\cdot [0.15, 0.20] = [0.03501, 0.04668]$\n",
        "- $e_{\\text{на}} \\cdot \\mathbf{u}_{\\text{на}} = 0.2631 \\cdot [0.25, 0.30] = [0.065775, 0.07893]$\n",
        "- $e_{\\text{коврике}} \\cdot \\mathbf{u}_{\\text{коврике}} = 0.2966 \\cdot [0.35, 0.40] = [0.10381, 0.11864]$\n",
        "\n",
        "Суммируем:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} =\n",
        "[-0.039655 + 0.03501 + 0.065775 + 0.10381,\\\n",
        "-0.07931 + 0.04668 + 0.07893 + 0.11864]\n",
        "$$\n",
        "$$\n",
        "= [0.16494, 0.16494]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 5.4. Обновление входного эмбеддинга $\\mathbf{v}_{w_t}$\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_{\\text{сидит}}^{\\text{new}} = \\mathbf{v}_{\\text{сидит}}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}}\n",
        "$$\n",
        "$$\n",
        "= [0.5, 0.7] - 0.01 \\cdot [0.16494, 0.16494] = [0.5, 0.7] - [0.0016494, 0.0016494]\n",
        "$$\n",
        "$$\n",
        "\\mathbf{v}_{\\text{сидит}}^{\\text{new}} \\approx [0.49835, 0.69835]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 6. Итоги одной итерации\n",
        "\n",
        "После одной итерации обучения на паре $(\\text{«сидит»}, \\text{«кот»})$:\n",
        "- Эмбеддинг $\\mathbf{v}_{\\text{сидит}}$ немного уменьшился.\n",
        "- Эмбеддинг $\\mathbf{u}_{\\text{кот}}$ изменился так, чтобы **увеличить** скалярное произведение с $\\mathbf{v}_{\\text{сидит}}$.\n",
        "- Эмбеддинги других слов изменились, чтобы **уменьшить** их вероятность в контексте слова «сидит».\n",
        "\n",
        "Этот процесс повторяется для всех пар в корпусе на протяжении многих эпох. Со временем модель формирует осмысленные векторные представления, где семантически близкие слова оказываются близки в векторном пространстве.\n",
        "\n",
        "\n",
        "## 7. Краткое упоминание Negative Sampling\n",
        "\n",
        "Если бы мы использовали **Negative Sampling** с $k=1$ и отрицательным примером $w_{\\text{neg}} = \\text{«на»}$, то:\n",
        "\n",
        "- Функция потерь:\n",
        "  $$\n",
        "  \\mathcal{L}_{\\text{NS}} = -\\log \\sigma(\\mathbf{v}_{\\text{сидит}}^\\top \\mathbf{u}_{\\text{кот}}) - \\log \\sigma(-\\mathbf{v}_{\\text{сидит}}^\\top \\mathbf{u}_{\\text{на}})\n",
        "  $$\n",
        "\n",
        "- Обновлялись бы **только**:\n",
        "  - $\\mathbf{v}_{\\text{сидит}}$,\n",
        "  - $\\mathbf{u}_{\\text{кот}}$,\n",
        "  - $\\mathbf{u}_{\\text{на}}$.\n",
        "\n",
        "- Не требовалось бы вычислять Softmax по всему словарю и обновлять все 4 выходных эмбеддинга.\n",
        "\n",
        "✅ **Преимущество**: обучение становится на порядки быстрее, особенно при большом $V$.\n"
      ],
      "metadata": {
        "id": "GkBsWQcXsByR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Математическая архитектура Continuous Bag-of-Words (CBOW)\n",
        "\n",
        "## Введение: Векторные представления слов в обработке естественного языка\n",
        "\n",
        "В современной обработке естественного языка (NLP) способность компьютера «понимать» смысл слов и их взаимосвязи является фундаментальной. Традиционные методы представления слов, такие как one-hot кодирование, где каждое уникальное слово в словаре представлено вектором, состоящим из одной единицы и множества нулей, страдают от двух основных недостатков:\n",
        "\n",
        "1. **Разреженность**: Векторы становятся чрезвычайно длинными для больших словарей, что приводит к неэффективному использованию памяти и вычислительных ресурсов.  \n",
        "2. **Отсутствие семантики**: One-hot векторы не несут никакой информации о смысловых или синтаксических отношениях между словами. Например, векторы для слов «король» и «королева» будут ортогональны, что не отражает их очевидной смысловой близости.\n",
        "\n",
        "Для преодоления этих ограничений были разработаны векторные представления слов (word embeddings) или векторные вложения слов. Это плотные, низкоразмерные векторы действительных чисел, которые кодируют семантические и синтаксические свойства слов. Основная идея заключается в том, что слова, появляющиеся в схожих контекстах, имеют схожие значения и, следовательно, должны иметь схожие векторные представления в многомерном пространстве. Например, в хорошо обученном пространстве вложений вектор для слова «король» будет находиться близко к вектору «королева», и векторная операция «король» − «мужчина» + «женщина» будет аппроксимировать вектор «королева».\n",
        "\n",
        "Одной из наиболее влиятельных моделей для обучения таких векторных представлений является Continuous Bag-of-Words (CBOW), предложенная Томашем Миколовым и его коллегами из Google в 2013 году. CBOW относится к классу моделей Word2Vec и представляет собой эффективный алгоритм для получения высококачественных векторных вложений из больших текстовых корпусов.\n",
        "\n",
        "## 1. Основная концепция CBOW\n",
        "\n",
        "Центральная идея CBOW заключается в предсказании целевого слова на основе его окружающего контекста. Модель принимает набор слов, находящихся в определённом окне вокруг интересующего слова (целевого слова), и использует их для прогнозирования самого целевого слова.\n",
        "\n",
        "Рассмотрим пример предложения: «Собака гонится за кошкой по двору».  \n",
        "Если «кошкой» является нашим целевым словом, то контекстными словами могут быть «Собака», «гонится», «за», «по», «двору» (в зависимости от размера окна контекста). Модель CBOW принимает эти контекстные слова в качестве входных данных и пытается предсказать «кошкой» как выход.\n",
        "\n",
        "Процесс обучения CBOW можно резюмировать следующим образом:  \n",
        "1. Формирование пар (контекст, целевое слово): из обучающего текстового корпуса извлекаются пары, где контекст состоит из слов, окружающих целевое слово в пределах заданного окна.  \n",
        "2. Прямой проход (Forward Pass): модель обрабатывает входные контекстные слова, агрегирует их представления и вычисляет вероятности для каждого слова в словаре быть целевым словом.  \n",
        "3. Вычисление потерь: сравнивается предсказанное распределение вероятностей с истинным целевым словом, и вычисляется ошибка предсказания.  \n",
        "4. Обратный проход (Backpropagation): ошибка распространяется обратно через сеть, и веса модели (которые и являются векторными представлениями слов) корректируются таким образом, чтобы уменьшить ошибку в будущих предсказаниях.\n",
        "\n",
        "Повторяя этот процесс многократно на обширном текстовом корпусе, модель учится эффективно кодировать семантические и синтаксические отношения в векторных представлениях слов. Чем чаще слово встречается в определённом контексте, тем сильнее модель «запоминает» эту связь, что приводит к более точным и значимым векторным вложениям.\n",
        "\n",
        "## 2. Архитектура нейронной сети CBOW\n",
        "\n",
        "Архитектура CBOW представляет собой относительно простую нейронную сеть, состоящую из трёх основных слоёв:\n",
        "\n",
        "1. **Входной слой (Input Layer)**: этот слой принимает one-hot векторы контекстных слов. Если размер окна контекста составляет $C$ слов до и $C$ слов после целевого слова, то на вход подаётся $2C$ one-hot векторов.  \n",
        "2. **Проекционный (скрытый) слой (Projection/Hidden Layer)**: этот слой является ключевым для создания векторных вложений. Он усредняет one-hot векторы контекстных слов и проецирует их в низкоразмерное векторное пространство. Размерность этого пространства, обозначаемая $N$, определяет размерность получаемых векторных вложений слов. В отличие от традиционных скрытых слоёв в нейронных сетях, здесь отсутствует нелинейная функция активации.  \n",
        "3. **Выходной слой (Output Layer)**: этот слой имеет размер, равный размеру словаря ($V$). Он генерирует вектор оценок (логитов) для каждого слова в словаре, которые затем преобразуются в вероятности с помощью функции Softmax, указывающие на вероятность того, что каждое слово является целевым, учитывая входной контекст.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3. Детальная математическая формулировка\n",
        "\n",
        "Для полного понимания работы CBOW необходимо подробно рассмотреть математические операции, происходящие на каждом этапе.  \n",
        "Пусть $V$ — размер нашего словаря (общее количество уникальных слов), а $N$ — желаемая размерность векторных вложений (например, 100, 300).\n",
        "\n",
        "## 3.1. Входной слой: One-Hot кодирование контекстных слов\n",
        "\n",
        "Каждое слово в словаре представлено уникальным one-hot вектором. One-hot вектор для $i$-го слова $w_i$ — это вектор-столбец размерности $V \\times 1$, в котором $i$-й элемент равен 1, а все остальные элементы равны 0.\n",
        "\n",
        "Например, если словарь содержит 4 слова: {\"кот\": 0, \"сидит\": 1, \"на\": 2, \"коврике\": 3}, то:  \n",
        "- One-hot вектор для \"кот\" будет $\\mathbf{x}_{\\text{кот}} = [1, 0, 0, 0]^\\top$.  \n",
        "- One-hot вектор для \"сидит\" будет $\\mathbf{x}_{\\text{сидит}} = [0, 1, 0, 0]^\\top$.\n",
        "\n",
        "Для данного обучающего примера, состоящего из целевого слова $w_c$ и его контекстных слов, мы имеем $2C$ one-hot векторов контекста: $\\mathbf{x}_{c-C+1}, \\dots, \\mathbf{x}_{c-1}, \\mathbf{x}_{c+1}, \\dots, \\mathbf{x}_{c+C}$.\n",
        "\n",
        "## 3.2. Прямой проход (Forward Pass): От входного к проекционному слою\n",
        "\n",
        "На этом этапе происходит преобразование разреженных one-hot векторов в плотные векторные представления и их агрегация.\n",
        "\n",
        "### Матрица входных весов $W$\n",
        "\n",
        "Между входным и проекционным слоем находится матрица весов $W \\in \\mathbb{R}^{V \\times N}$. Каждая строка этой матрицы $W_i \\in \\mathbb{R}^{1 \\times N}$ представляет собой $N$-мерное векторное вложение $i$-го слова в словаре, когда оно выступает в роли контекстного слова. Эту матрицу можно рассматривать как «таблицу поиска», где по индексу слова мы получаем его векторное представление.\n",
        "\n",
        "### Получение векторного представления контекстного слова ($\\mathbf{v}_j$)\n",
        "\n",
        "Для каждого из $2C$ контекстных слов, представленных one-hot вектором $\\mathbf{x}_j$, его плотное векторное представление $\\mathbf{v}_j$ вычисляется путём умножения транспонированного one-hot вектора на матрицу $W$:\n",
        "$$\n",
        "\\mathbf{v}_j = \\mathbf{x}_j^\\top W\n",
        "$$\n",
        "- $\\mathbf{x}_j^\\top$: транспонированный one-hot вектор $j$-го контекстного слова, имеющий размерность $1 \\times V$.  \n",
        "- $W$: матрица входных весов, размерность $V \\times N$.  \n",
        "- $\\mathbf{v}_j$: векторное представление $j$-го контекстного слова, размерность $1 \\times N$.\n",
        "\n",
        "**Подробное объяснение**: Поскольку $\\mathbf{x}_j$ является one-hot вектором, умножение $\\mathbf{x}_j^\\top W$ является эффективным способом извлечения соответствующей строки из матрицы $W$. Если $\\mathbf{x}_j$ соответствует $k$-му слову в словаре, то $\\mathbf{v}_j$ будет $k$-й строкой матрицы $W$.\n",
        "\n",
        "### Усреднение контекстных векторов ($\\mathbf{h}$)\n",
        "\n",
        "После получения векторных представлений для всех $2C$ контекстных слов ($\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_{2C}$), CBOW усредняет эти векторы для формирования единого контекстного вектора $\\mathbf{h}$:\n",
        "$$\n",
        "\\mathbf{h} = \\frac{1}{2C} \\sum_{j=1}^{2C} \\mathbf{v}_j = \\frac{1}{2C} \\sum_{j=1}^{2C} \\mathbf{x}_j^\\top W\n",
        "$$\n",
        "- $2C$: общее количество контекстных слов в текущем окне.  \n",
        "- $\\sum_{j=1}^{2C} \\mathbf{v}_j$: сумма всех $2C$ векторных представлений контекстных слов.  \n",
        "- $\\mathbf{h}$: усреднённый контекстный вектор, размерность $1 \\times N$.\n",
        "\n",
        "**Подробное объяснение**: Вектор $\\mathbf{h}$ представляет собой агрегированное, плотное представление всего контекста, окружающего целевое слово. Усреднение позволяет модели учитывать вклад каждого контекстного слова в равной степени, формируя единое, семантически насыщенное представление окружения. Этот вектор $\\mathbf{h}$ является выходом проекционного слоя. Важно отметить, что в оригинальной архитектуре CBOW этот «скрытый» слой не имеет нелинейной функции активации; он выполняет только линейную проекцию и усреднение.\n",
        "\n",
        "## 3.3. Прямой проход (Forward Pass): От проекционного к выходному слою\n",
        "\n",
        "Теперь, имея контекстный вектор $\\mathbf{h}$, модель должна предсказать целевое слово.\n",
        "\n",
        "### Матрица выходных весов $W'$\n",
        "\n",
        "Между проекционным и выходным слоями находится вторая матрица весов $W' \\in \\mathbb{R}^{N \\times V}$. Каждая колонка этой матрицы $W'_k \\in \\mathbb{R}^{N \\times 1}$ представляет собой $N$-мерное векторное вложение $k$-го слова в словаре, когда оно выступает в роли целевого слова.\n",
        "\n",
        "### Вычисление оценок $u$ (логитов)\n",
        "\n",
        "Контекстный вектор $\\mathbf{h}$ умножается на транспонированную матрицу $W'^\\top$ для получения вектора оценок $u$ для каждого слова в словаре:\n",
        "$$\n",
        "\\mathbf{u} = W'^\\top \\mathbf{h}\n",
        "$$\n",
        "- $W'^\\top$: транспонированная матрица $W'$, размерность $V \\times N$.  \n",
        "- $\\mathbf{h}$: контекстный вектор, размерность $N \\times 1$.  \n",
        "- $\\mathbf{u}$: вектор оценок (логитов), размерность $V \\times 1$.\n",
        "\n",
        "**Подробное объяснение**: Каждый элемент $u_k$ в векторе $\\mathbf{u}$ является «оценкой» (или «логитом») того, насколько вероятно $k$-е слово в словаре является истинным целевым словом для данного контекста. Эта оценка вычисляется как скалярное произведение $k$-й строки $W'^\\top$ (которая является $k$-й колонкой $W'$) и вектора $\\mathbf{h}$:\n",
        "$$\n",
        "u_k = (W'_k)^\\top \\mathbf{h}\n",
        "$$\n",
        "где $(W'_k)^\\top$ — это транспонированный вектор $k$-й колонки матрицы $W'$. Высокое значение $u_k$ указывает на то, что модель считает $k$-е слово весьма вероятным целевым словом в заданном контексте.\n",
        "\n",
        "## 3.4. Прямой проход (Forward Pass): Выходной слой (Softmax)\n",
        "\n",
        "Оценки $u_k$ могут принимать любые действительные значения. Для преобразования их в вероятности, которые суммируются к 1, используется функция Softmax.\n",
        "\n",
        "### Функция Softmax\n",
        "\n",
        "Вероятность $P(w_k \\mid \\text{контекст})$ того, что $k$-е слово является целевым словом, вычисляется следующим образом:\n",
        "$$\n",
        "P(w_k \\mid \\text{контекст}) = \\frac{\\exp(u_k)}{\\sum_{j=1}^{V} \\exp(u_j)}\n",
        "$$\n",
        "- $\\exp(u_k)$: экспоненциальная функция от оценки $u_k$. Использование экспоненты гарантирует, что все значения будут положительными, а также усиливает различия между оценками, делая большие оценки значительно более доминирующими.  \n",
        "- $\\sum_{j=1}^{V} \\exp(u_j)$: сумма экспонент всех оценок для каждого слова в словаре. Этот член является нормализующим множителем, который обеспечивает, чтобы сумма всех выходных вероятностей для всех слов в словаре была равна 1.\n",
        "\n",
        "**Подробное объяснение**: Функция Softmax преобразует произвольные действительные числа (логиты) в дискретное распределение вероятностей. Она «сглаживает» оценки, делая их интерпретируемыми как вероятности. Слово с наибольшей оценкой $u_k$ получит наибольшую вероятность, но все остальные слова также будут иметь ненулевые вероятности, отражая степень неопределённости предсказания модели.\n",
        "\n",
        "## 3.5. Функция потерь: Кросс-энтропия\n",
        "\n",
        "Для обучения модели необходимо измерить, насколько «плохи» текущие предсказания. Для задач классификации, таких как предсказание целевого слова, широко используется функция потерь кросс-энтропии.\n",
        "\n",
        "Пусть $w_o$ — истинное целевое слово (слово с индексом $o$ в словаре). Его one-hot представление $y_o$ будет иметь 1 на $o$-й позиции и 0 на всех остальных.\n",
        "\n",
        "Функция потерь $E$ для одного обучающего примера определяется как отрицательный логарифм вероятности истинного целевого слова:\n",
        "$$\n",
        "E = -\\log P(w_o \\mid \\text{контекст}) = -\\log \\left( \\frac{\\exp(u_o)}{\\sum_{j=1}^{V} \\exp(u_j)} \\right)\n",
        "$$\n",
        "- $P(w_o \\mid \\text{контекст})$: предсказанная вероятность истинного целевого слова $w_o$ для данного контекста.  \n",
        "- $\\log$: натуральный логарифм.  \n",
        "- $-$: отрицательный знак, поскольку мы стремимся минимизировать потери, а логарифм вероятности (которая находится в диапазоне от 0 до 1) будет отрицательным или нулевым.\n",
        "\n",
        "**Подробное объяснение**: Функция потерь кросс-энтропии наказывает модель тем сильнее, чем ниже предсказанная вероятность истинного целевого слова. Если модель предсказывает правильное слово с вероятностью, близкой к 1, потери будут минимальны (близки к 0). Если же вероятность истинного слова близка к 0, потери будут очень большими (стремящимися к бесконечности). Цель обучения CBOW заключается в минимизации этой функции потерь, что эквивалентно максимизации логарифмической правдоподобности правильного предсказания.\n",
        "\n",
        "# 3.6. Обратный проход (Backpropagation) и оптимизация: Градиентный спуск\n",
        "\n",
        "Для минимизации функции потерь $E$ мы используем алгоритм градиентного спуска. Этот итеративный алгоритм обновляет веса модели ($W$ и $W'$) в направлении, противоположном градиенту функции потерь. Градиент указывает направление наибольшего увеличения функции, поэтому движение в противоположном направлении ведёт к её уменьшению.\n",
        "\n",
        "### Общее правило обновления весов:\n",
        "$$\n",
        "\\text{новые веса} = \\text{старые веса} - \\eta \\cdot \\nabla E\n",
        "$$\n",
        "- $\\eta$ (скорость обучения, *learning rate*): небольшой положительный скаляр, который определяет размер шага при каждом обновлении весов. Выбор адекватной скорости обучения критичен: слишком большая $\\eta$ может привести к «перепрыгиванию» через оптимальный минимум, а слишком маленькая — к чрезмерно медленной сходимости.  \n",
        "- $\\nabla E$: градиент функции потерь $E$ по отношению к конкретным весам. Это вектор, указывающий направление наиболее крутого подъёма функции $E$.\n",
        "\n",
        "Вычисление градиентов для матриц $W$ и $W'$ является центральной частью обратного распространения ошибки и требует применения правила цепи (chain rule) из дифференциального исчисления.\n",
        "\n",
        "## Градиенты для $W'$ (веса выходного слоя)\n",
        "\n",
        "Начнём с вычисления градиента функции потерь по отношению к оценкам $u_k$:\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial u_k} = P(w_k \\mid \\text{контекст}) - y_k\n",
        "$$\n",
        "- $P(w_k \\mid \\text{контекст})$: предсказанная вероятность $k$-го слова.  \n",
        "- $y_k$: истинное значение (1, если $k$-е слово является целевым; 0 в противном случае).\n",
        "\n",
        "**Подробное объяснение**: Этот элегантный результат является прямым следствием использования функции Softmax с кросс-энтропийной потерей. Разница $(P(w_k \\mid \\text{контекст}) - y_k)$ представляет собой «ошибку предсказания» для $k$-го слова. Если предсказанная вероятность $P(w_k \\mid \\text{контекст})$ для истинного слова (где $y_k = 1$) высока, ошибка будет близка к 0. Если она низка, ошибка будет отрицательной и большой по модулю, что указывает на необходимость увеличения весов, ведущих к этому слову. Для неправильных слов (где $y_k = 0$), если $P(w_k \\mid \\text{контекст})$ высока, ошибка будет положительной и большой, указывая на необходимость уменьшения весов, ведущих к этому слову.\n",
        "\n",
        "Теперь, используя правило цепи, мы можем найти градиент для $\\mathbf{w}'_k$ (векторного представления $k$-го слова в $W'$, то есть $k$-й колонки $W'$):\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial \\mathbf{w}'_k} = \\frac{\\partial E}{\\partial u_k} \\cdot \\frac{\\partial u_k}{\\partial \\mathbf{w}'_k} = (P(w_k \\mid \\text{контекст}) - y_k) \\cdot \\mathbf{h}\n",
        "$$\n",
        "- $\\frac{\\partial u_k}{\\partial \\mathbf{w}'_k}$: градиент $u_k$ по $\\mathbf{w}'_k$. Поскольку $u_k = (\\mathbf{w}'_k)^\\top \\mathbf{h}$, то $\\frac{\\partial u_k}{\\partial \\mathbf{w}'_k} = \\mathbf{h}$.\n",
        "\n",
        "### Обновление весов $W'$\n",
        "\n",
        "Обновление весов для каждого векторного представления целевого слова $\\mathbf{w}'_k$ происходит по формуле:\n",
        "$$\n",
        "\\mathbf{w}'_k^{\\text{новое}} = \\mathbf{w}'_k^{\\text{старое}} - \\eta \\cdot (P(w_k \\mid \\text{контекст}) - y_k) \\cdot \\mathbf{h}\n",
        "$$\n",
        "\n",
        "**Подробное объяснение**: Это обновление применяется для всех $V$ слов в словаре. Для каждого слова $w_k$ его векторное представление $\\mathbf{w}'_k$ корректируется пропорционально ошибке предсказания для этого слова и контекстному вектору $\\mathbf{h}$. Если ошибка положительна (модель переоценила вероятность слова), $\\mathbf{w}'_k$ будет сдвигаться в направлении, противоположном $\\mathbf{h}$. Если ошибка отрицательна (модель недооценила вероятность слова), $\\mathbf{w}'_k$ будет сдвигаться в том же направлении, что и $\\mathbf{h}$.\n",
        "\n",
        "## Градиенты для $W$ (веса входного слоя)\n",
        "\n",
        "Распространение ошибки на входную матрицу $W$ требует нескольких шагов, так как $W$ влияет на $\\mathbf{h}$, который затем влияет на $\\mathbf{u}$, который, в свою очередь, влияет на $E$.\n",
        "\n",
        "Сначала вычисляем градиент по $\\mathbf{h}$ (вектору контекста), который представляет собой сумму взвешенных ошибок от выходного слоя:\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial \\mathbf{h}} = \\sum_{k=1}^{V} \\frac{\\partial E}{\\partial u_k} \\cdot \\frac{\\partial u_k}{\\partial \\mathbf{h}} = \\sum_{k=1}^{V} (P(w_k \\mid \\text{контекст}) - y_k) \\cdot \\mathbf{w}'_k\n",
        "$$\n",
        "- $\\frac{\\partial u_k}{\\partial \\mathbf{h}}$: градиент $u_k$ по $\\mathbf{h}$. Поскольку $u_k = (\\mathbf{w}'_k)^\\top \\mathbf{h}$, то $\\frac{\\partial u_k}{\\partial \\mathbf{h}} = \\mathbf{w}'_k$.  \n",
        "- $\\sum_{k=1}^{V}$: суммирование по всем словам в словаре, так как $\\mathbf{h}$ влияет на оценки всех слов.\n",
        "\n",
        "Пусть $\\mathbf{E}_H = \\frac{\\partial E}{\\partial \\mathbf{h}}$. Этот вектор $\\mathbf{E}_H$ представляет собой агрегированную ошибку, которая распространяется обратно в проекционный слой. Он показывает, как изменение контекстного вектора $\\mathbf{h}$ повлияет на общую функцию потерь.\n",
        "\n",
        "Теперь, для каждого контекстного слова $\\mathbf{x}_j$, его векторное представление $\\mathbf{v}_j$ участвует в вычислении $\\mathbf{h}$. Напомним, что:\n",
        "$$\n",
        "\\mathbf{h} = \\frac{1}{2C} \\sum_{j=1}^{2C} \\mathbf{v}_j\n",
        "$$\n",
        "\n",
        "### Градиент для $\\mathbf{v}_j$ (векторного представления $j$-го контекстного слова):\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial \\mathbf{v}_j} = \\frac{\\partial E}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{v}_j} = \\mathbf{E}_H \\cdot \\frac{1}{2C}\n",
        "$$\n",
        "- $\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{v}_j}$: градиент $\\mathbf{h}$ по $\\mathbf{v}_j$. Поскольку $\\mathbf{h}$ является средним значением всех $\\mathbf{v}_j$, производная $\\mathbf{h}$ по любому конкретному $\\mathbf{v}_j$ будет $\\frac{1}{2C}$.\n",
        "\n",
        "**Подробное объяснение**: Этот шаг равномерно распределяет агрегированную ошибку $\\mathbf{E}_H$ между всеми контекстными векторами $\\mathbf{v}_j$, которые внесли вклад в формирование $\\mathbf{h}$.\n",
        "\n",
        "Наконец, градиент для $W$ (в частности, для строки $W_i$, соответствующей $i$-му слову в словаре, если это слово $w_i$ было одним из контекстных слов $\\mathbf{x}_j$):\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial W_i} = \\sum_{j : \\mathbf{x}_j \\text{ is } w_i} \\frac{\\partial E}{\\partial \\mathbf{v}_j} \\cdot \\frac{\\partial \\mathbf{v}_j}{\\partial W_i} = \\sum_{j : \\mathbf{x}_j \\text{ is } w_i} \\frac{1}{2C} \\mathbf{E}_H \\cdot \\mathbf{x}_j\n",
        "$$\n",
        "- $\\frac{\\partial \\mathbf{v}_j}{\\partial W_i}$: градиент $\\mathbf{v}_j$ по $W_i$. Поскольку $\\mathbf{v}_j = \\mathbf{x}_j^\\top W$, и $\\mathbf{x}_j$ является one-hot вектором, то $\\frac{\\partial \\mathbf{v}_j}{\\partial W_i}$ будет ненулевым только для той строки $W_i$, которая соответствует слову $\\mathbf{x}_j$. В этом случае производная эквивалентна $\\mathbf{x}_j$ (вектор-столбец), что при умножении на скаляр и даёт вклад в обновление строки $W_i$.\n",
        "\n",
        "### Обновление весов $W$\n",
        "\n",
        "Обновление весов для каждой строки $W_i$ (т.е. векторного представления контекстного слова) происходит по формуле:\n",
        "$$\n",
        "W_i^{\\text{новое}} = W_i^{\\text{старое}} - \\eta \\cdot \\frac{1}{2C} \\mathbf{E}_H\n",
        "$$\n",
        "\n",
        "**Подробное объяснение**: Это обновление применяется только к тем строкам матрицы $W$, которые соответствуют словам, присутствующим в текущем контексте. Каждая такая строка $W_i$ корректируется пропорционально обратно распространённой ошибке $\\mathbf{E}_H$, делённой на количество контекстных слов $2C$.\n",
        "\n",
        "\n",
        "\n",
        "# 4. Заключение\n",
        "\n",
        "Архитектура Continuous Bag-of-Words (CBOW), несмотря на свою структурную простоту, является чрезвычайно эффективным и мощным инструментом для создания семантически насыщенных векторных представлений слов. Глубокое понимание математических принципов, лежащих в её основе — от начального one-hot кодирования и усреднения контекстных векторов до применения функции Softmax для получения вероятностей и итеративного обновления весов посредством градиентного спуска — позволяет получить всестороннее представление о том, как нейронные сети способны «понимать» и кодировать значения слов из их лингвистического окружения.\n",
        "\n",
        "Ключевые концепции, которые следует прочно усвоить:\n",
        "- **Принцип предсказания**: CBOW учится предсказывать целевое слово на основе его контекстных слов.  \n",
        "- **Две матрицы весов ($W$ и $W'$)**: Эти матрицы содержат обучаемые векторные представления слов. $W$ используется, когда слова выступают в роли контекста, а $W'$ — когда они являются целевыми словами. В идеале, после завершения обучения, эти матрицы будут содержать очень схожие, высококачественные векторные вложения. Для практического использования часто выбирают одну из них или усредняют обе.  \n",
        "- **Усреднение контекста**: Проекционный слой CBOW выполняет простую, но эффективную операцию усреднения векторных представлений всех контекстных слов, формируя единый контекстный вектор.  \n",
        "- **Функция Softmax**: необходима для преобразования произвольных числовых оценок в корректное распределение вероятностей, что позволяет интерпретировать выход модели как вероятность принадлежности к каждому слову в словаре.  \n",
        "- **Градиентный спуск**: фундаментальный алгоритм оптимизации, используемый для итеративной настройки весов модели с целью минимизации функции потерь и, как следствие, повышения точности предсказаний.\n",
        "\n",
        "Полученные с помощью CBOW векторные вложения слов являются ценным ресурсом и находят широкое применение в различных задачах NLP, включая машинный перевод, анализ настроений, вопросно-ответные системы, классификацию текста и многие другие, значительно повышая их производительность и способность к семантическому анализу.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Аналитический пример математической архитектуры Continuous Bag-of-Words (CBOW)\n",
        "\n",
        "Этот раздел посвящён детальному аналитическому примеру работы архитектуры CBOW, шаг за шагом демонстрируя все вычисления прямого и обратного проходов. Цель состоит в том, чтобы сделать каждую формулу и операцию максимально понятной, позволяя любому читателю полностью освоить математические основы CBOW.\n",
        "\n",
        "## 1. Исходные данные и параметры\n",
        "\n",
        "Для нашего примера мы определим минимальный набор данных и параметров:\n",
        "\n",
        "- **Словарь ($V$)**: {\"я\": 0, \"люблю\": 1, \"кошек\": 2, \"собак\": 3}. Размер словаря $V = 4$.  \n",
        "- **Размерность векторных вложений ($N$)**: $N = 2$. Это означает, что каждое слово будет представлено 2-мерным вектором.  \n",
        "- **Размер окна контекста ($C$)**: $C = 1$. Это означает, что мы берём одно слово до и одно слово после целевого слова. Общее количество контекстных слов $2C = 2$.  \n",
        "- **Обучающий пример**: Предложение \"я люблю кошек собак\".  \n",
        "  - Целевое слово ($w_o$): \"кошек\" (индекс 2). Его one-hot вектор $\\mathbf{y}_o = [0, 0, 1, 0]^\\top$.  \n",
        "  - Контекстные слова: \"люблю\" (индекс 1) и \"собак\" (индекс 3).  \n",
        "- **Скорость обучения ($\\eta$)**: $\\eta = 0.01$.\n",
        "\n",
        "### Инициализация матриц весов\n",
        "\n",
        "Мы инициализируем матрицы весов $W$ и $W'$ случайными значениями. Для простоты вычислений выберем небольшие, легко отслеживаемые числа.\n",
        "\n",
        "#### 1. Матрица входных весов $W \\in \\mathbb{R}^{V \\times N}$\n",
        "\n",
        "Каждая строка $W_i$ соответствует векторному представлению $i$-го слова, когда оно является контекстным.\n",
        "$$\n",
        "W =\n",
        "\\begin{bmatrix}\n",
        "W_0 \\\\\n",
        "W_1 \\\\\n",
        "W_2 \\\\\n",
        "W_3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.3 \\\\\n",
        "0.5 & 0.7 \\\\\n",
        "0.2 & 0.4 \\\\\n",
        "0.6 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- $W_0$: вектор для \"я\"  \n",
        "- $W_1$: вектор для \"люблю\"  \n",
        "- $W_2$: вектор для \"кошек\"  \n",
        "- $W_3$: вектор для \"собак\"\n",
        "\n",
        "\n",
        "#### 2. Матрица выходных весов $W' \\in \\mathbb{R}^{N \\times V}$\n",
        "\n",
        "Каждый столбец $W'_k$ соответствует векторному представлению $k$-го слова, когда оно является целевым.\n",
        "$$\n",
        "W' =\n",
        "\\begin{bmatrix}\n",
        "0.9 & 0.8 & 0.7 & 0.6 \\\\\n",
        "0.5 & 0.4 & 0.3 & 0.2\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "\\text{или} \\quad\n",
        "W' = (W'_0\\ W'_1\\ W'_2\\ W'_3)\n",
        "$$\n",
        "- $W'_0$: вектор для \"я\"  \n",
        "- $W'_1$: вектор для \"люблю\"  \n",
        "- $W'_2$: вектор для \"кошек\"  \n",
        "- $W'_3$: вектор для \"собак\"\n",
        "\n",
        "\n",
        "\n",
        "## 2. Прямой проход (Forward Pass)\n",
        "\n",
        "Прямой проход включает вычисление предсказаний модели на основе текущих весов.\n",
        "\n",
        "### 2.1. Входной слой: One-Hot кодирование контекстных слов\n",
        "\n",
        "Контекстные слова: \"люблю\" (индекс 1) и \"собак\" (индекс 3).  \n",
        "Их one-hot векторы:\n",
        "- $\\mathbf{x}_{\\text{люблю}} = [0, 1, 0, 0]^\\top$  \n",
        "- $\\mathbf{x}_{\\text{собак}} = [0, 0, 0, 1]^\\top$\n",
        "\n",
        "\n",
        "\n",
        "### 2.2. От входного к проекционному слою\n",
        "\n",
        "#### Получение векторного представления контекстного слова ($\\mathbf{v}_j$)\n",
        "\n",
        "Для каждого контекстного слова $\\mathbf{x}_j$, его плотное векторное представление $\\mathbf{v}_j$ извлекается из матрицы $W$ как $\\mathbf{v}_j = \\mathbf{x}_j^\\top W$:\n",
        "\n",
        "- Для \"люблю\" (индекс 1):  \n",
        "  $$\n",
        "  \\mathbf{v}_{\\text{люблю}} = \\mathbf{x}_{\\text{люблю}}^\\top W = [0, 1, 0, 0]\n",
        "  \\begin{bmatrix}\n",
        "  0.1 & 0.3 \\\\\n",
        "  0.5 & 0.7 \\\\\n",
        "  0.2 & 0.4 \\\\\n",
        "  0.6 & 0.8\n",
        "  \\end{bmatrix}\n",
        "  = [0.5, 0.7]\n",
        "  $$\n",
        "\n",
        "- Для \"собак\" (индекс 3):  \n",
        "  $$\n",
        "  \\mathbf{v}_{\\text{собак}} = \\mathbf{x}_{\\text{собак}}^\\top W = [0, 0, 0, 1]\n",
        "  \\begin{bmatrix}\n",
        "  0.1 & 0.3 \\\\\n",
        "  0.5 & 0.7 \\\\\n",
        "  0.2 & 0.4 \\\\\n",
        "  0.6 & 0.8\n",
        "  \\end{bmatrix}\n",
        "  = [0.6, 0.8]\n",
        "  $$\n",
        "\n",
        "#### Усреднение контекстных векторов ($\\mathbf{h}$)\n",
        "\n",
        "$$\n",
        "\\mathbf{h} = \\frac{1}{2C} \\sum_{j=1}^{2C} \\mathbf{v}_j = \\frac{1}{2} (\\mathbf{v}_{\\text{люблю}} + \\mathbf{v}_{\\text{собак}})\n",
        "$$\n",
        "$$\n",
        "\\mathbf{h} = \\frac{1}{2} \\left( [0.5, 0.7] + [0.6, 0.8] \\right) = \\frac{1}{2} [1.1, 1.5] = [0.55, 0.75]\n",
        "$$\n",
        "\n",
        "Таким образом, контекстный вектор $\\mathbf{h} = [0.55, 0.75]^\\top$.\n",
        "\n",
        "\n",
        "\n",
        "### 2.3. От проекционного к выходному слою\n",
        "\n",
        "#### Вычисление оценок $u$ (логитов)\n",
        "\n",
        "Умножаем контекстный вектор $\\mathbf{h}$ на транспонированную матрицу $W'^\\top$:\n",
        "\n",
        "$$\n",
        "\\mathbf{u} = W'^\\top \\mathbf{h}, \\quad\n",
        "W'^\\top =\n",
        "\\begin{bmatrix}\n",
        "0.9 & 0.5 \\\\\n",
        "0.8 & 0.4 \\\\\n",
        "0.7 & 0.3 \\\\\n",
        "0.6 & 0.2\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{h} =\n",
        "\\begin{bmatrix}\n",
        "0.55 \\\\\n",
        "0.75\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем каждый элемент $u_k = (W'_k)^\\top \\mathbf{h}$:\n",
        "\n",
        "- $u_0$ (для \"я\"): $0.9 \\cdot 0.55 + 0.5 \\cdot 0.75 = 0.495 + 0.375 = 0.870$  \n",
        "- $u_1$ (для \"люблю\"): $0.8 \\cdot 0.55 + 0.4 \\cdot 0.75 = 0.440 + 0.300 = 0.740$  \n",
        "- $u_2$ (для \"кошек\"): $0.7 \\cdot 0.55 + 0.3 \\cdot 0.75 = 0.385 + 0.225 = 0.610$  \n",
        "- $u_3$ (для \"собак\"): $0.6 \\cdot 0.55 + 0.2 \\cdot 0.75 = 0.330 + 0.150 = 0.480$\n",
        "\n",
        "Таким образом, вектор оценок:\n",
        "$$\n",
        "\\mathbf{u} = [0.870, 0.740, 0.610, 0.480]^\\top\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 2.4. Выходной слой (Softmax)\n",
        "\n",
        "Преобразуем оценки $u_k$ в вероятности $P(w_k \\mid \\text{контекст})$ с помощью функции Softmax:\n",
        "$$\n",
        "P(w_k \\mid \\text{контекст}) = \\frac{\\exp(u_k)}{\\sum_{j=0}^{3} \\exp(u_j)}\n",
        "$$\n",
        "\n",
        "Вычислим экспоненты:\n",
        "- $\\exp(0.870) \\approx 2.386$  \n",
        "- $\\exp(0.740) \\approx 2.096$  \n",
        "- $\\exp(0.610) \\approx 1.840$  \n",
        "- $\\exp(0.480) \\approx 1.616$\n",
        "\n",
        "Сумма:  \n",
        "$$\n",
        "\\sum_j \\exp(u_j) = 2.386 + 2.096 + 1.840 + 1.616 = 7.938\n",
        "$$\n",
        "\n",
        "Теперь вероятности:\n",
        "- $P(\\text{«я»} \\mid \\text{контекст}) = 2.386 / 7.938 \\approx 0.3006$  \n",
        "- $P(\\text{«люблю»} \\mid \\text{контекст}) = 2.096 / 7.938 \\approx 0.2641$  \n",
        "- $P(\\text{«кошек»} \\mid \\text{контекст}) = 1.840 / 7.938 \\approx 0.2318$  \n",
        "- $P(\\text{«собак»} \\mid \\text{контекст}) = 1.616 / 7.938 \\approx 0.2036$\n",
        "\n",
        "Вектор предсказанных вероятностей:  \n",
        "$$\n",
        "\\mathbf{P} = [0.3006, 0.2641, 0.2318, 0.2036]^\\top\n",
        "$$\n",
        "\n",
        "\n",
        "### 2.5. Функция потерь: Кросс-энтропия\n",
        "\n",
        "Истинное целевое слово $w_o$ — \"кошек\" (индекс 2). Его one-hot вектор $\\mathbf{y}_o = [0, 0, 1, 0]^\\top$.  \n",
        "Функция потерь:\n",
        "$$\n",
        "E = -\\log P(w_o \\mid \\text{контекст}) = -\\log(0.2318) \\approx -(-1.462) = 1.462\n",
        "$$\n",
        "\n",
        "Текущее значение функции потерь $E \\approx 1.462$. Наша цель — уменьшить это значение.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Обратный проход (Backpropagation)\n",
        "\n",
        "Обратный проход включает вычисление градиентов функции потерь по отношению к весам и их последующее обновление.\n",
        "\n",
        "### 3.1. Градиенты для $W'$ (веса выходного слоя)\n",
        "\n",
        "Сначала вычислим ошибку предсказания для каждого слова:\n",
        "$$\n",
        "\\delta_k = \\frac{\\partial E}{\\partial u_k} = P(w_k \\mid \\text{контекст}) - y_k\n",
        "$$\n",
        "\n",
        "- $\\delta_0$ (для \"я\"): $0.3006 - 0 = 0.3006$  \n",
        "- $\\delta_1$ (для \"люблю\"): $0.2641 - 0 = 0.2641$  \n",
        "- $\\delta_2$ (для \"кошек\"): $0.2318 - 1 = -0.7682$  \n",
        "- $\\delta_3$ (для \"собак\"): $0.2036 - 0 = 0.2036$\n",
        "\n",
        "Вектор ошибок: $\\boldsymbol{\\delta} = [0.3006, 0.2641, -0.7682, 0.2036]^\\top$\n",
        "\n",
        "Теперь вычислим градиент для каждого столбца $W'_k$:\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial W'_k} = \\delta_k \\cdot \\mathbf{h}\n",
        "$$\n",
        "\n",
        "Напомним: $\\mathbf{h} = [0.55, 0.75]^\\top$\n",
        "\n",
        "- $\\frac{\\partial E}{\\partial W'_0} = 0.3006 \\cdot [0.55, 0.75] = [0.16533, 0.22545]$  \n",
        "- $\\frac{\\partial E}{\\partial W'_1} = 0.2641 \\cdot [0.55, 0.75] = [0.14526, 0.19808]$  \n",
        "- $\\frac{\\partial E}{\\partial W'_2} = -0.7682 \\cdot [0.55, 0.75] = [-0.42251, -0.57615]$  \n",
        "- $\\frac{\\partial E}{\\partial W'_3} = 0.2036 \\cdot [0.55, 0.75] = [0.11198, 0.15270]$\n",
        "\n",
        "#### Обновление весов $W'$\n",
        "\n",
        "$$\n",
        "W'_k^{\\text{новое}} = W'_k^{\\text{старое}} - \\eta \\cdot \\frac{\\partial E}{\\partial W'_k}\n",
        "$$\n",
        "\n",
        "Обновим $W'_2$ (для \"кошек\"):\n",
        "- $W'_2^{\\text{старое}} = [0.7, 0.3]^\\top$  \n",
        "- $W'_2^{\\text{новое}} = [0.7, 0.3] - 0.01 \\cdot [-0.42251, -0.57615] = [0.7 + 0.0042251, 0.3 + 0.0057615] = [0.704225, 0.305762]^\\top$\n",
        "\n",
        "Аналогичные обновления применяются к $W'_0$, $W'_1$, $W'_3$.\n",
        "\n",
        "\n",
        "\n",
        "### 3.2. Градиенты для $W$ (веса входного слоя)\n",
        "\n",
        "#### Вычисление агрегированной ошибки $E_H = \\frac{\\partial E}{\\partial \\mathbf{h}}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial \\mathbf{h}} = \\sum_{k=0}^{3} \\delta_k \\cdot W'_k\n",
        "$$\n",
        "\n",
        "- $\\delta_0 \\cdot W'_0 = 0.3006 \\cdot [0.9, 0.5] = [0.27054, 0.15030]$  \n",
        "- $\\delta_1 \\cdot W'_1 = 0.2641 \\cdot [0.8, 0.4] = [0.21128, 0.10564]$  \n",
        "- $\\delta_2 \\cdot W'_2 = -0.7682 \\cdot [0.7, 0.3] = [-0.53774, -0.23046]$  \n",
        "- $\\delta_3 \\cdot W'_3 = 0.2036 \\cdot [0.6, 0.2] = [0.12216, 0.04072]$\n",
        "\n",
        "Суммируем:\n",
        "- $E_{H_x} = 0.27054 + 0.21128 - 0.53774 + 0.12216 = 0.06624$  \n",
        "- $E_{H_y} = 0.15030 + 0.10564 - 0.23046 + 0.04072 = 0.06620$\n",
        "\n",
        "Таким образом, $E_H \\approx [0.0662, 0.0662]^\\top$\n",
        "\n",
        "#### Градиент для контекстных векторов $\\mathbf{v}_j$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial \\mathbf{v}_j} = \\frac{\\partial E}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{v}_j} = E_H \\cdot \\frac{1}{2C} = E_H \\cdot 0.5\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial \\mathbf{v}_j} = [0.0662, 0.0662] \\cdot 0.5 = [0.0331, 0.0331]\n",
        "$$\n",
        "\n",
        "#### Обновление весов $W$\n",
        "\n",
        "$$\n",
        "W_i^{\\text{новое}} = W_i^{\\text{старое}} - \\eta \\cdot \\frac{\\partial E}{\\partial \\mathbf{v}_j}\n",
        "$$\n",
        "\n",
        "- Для $W_1$ (\"люблю\"):  \n",
        "  $W_1^{\\text{старое}} = [0.5, 0.7]^\\top$  \n",
        "  $W_1^{\\text{новое}} = [0.5, 0.7] - 0.01 \\cdot [0.0331, 0.0331] = [0.499669, 0.699669]^\\top$\n",
        "\n",
        "- Для $W_3$ (\"собак\"):  \n",
        "  $W_3^{\\text{старое}} = [0.6, 0.8]^\\top$  \n",
        "  $W_3^{\\text{новое}} = [0.6, 0.8] - 0.01 \\cdot [0.0331, 0.0331] = [0.599669, 0.799669]^\\top$\n",
        "\n",
        "Строки $W_0$ и $W_2$ не обновляются, так как \"я\" и \"кошек\" не были контекстными словами в этом примере.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Заключение\n",
        "\n",
        "Этот аналитический пример демонстрирует полный цикл прямого и обратного проходов в архитектуре CBOW для одного обучающего примера. Мы пошагово вычислили:\n",
        "- Векторные представления контекстных слов.  \n",
        "- Усреднённый контекстный вектор.  \n",
        "- Оценки (логиты) для всех слов в словаре.  \n",
        "- Вероятности предсказания целевого слова с помощью Softmax.  \n",
        "- Значение функции потерь кросс-энтропии.  \n",
        "- Градиенты для обновления весов в матрицах $W'$ и $W$.  \n",
        "- Примеры обновлённых весов.\n",
        "\n",
        "Этот процесс повторяется миллионы раз на большом текстовом корпусе. С каждым шагом веса $W$ и $W'$ постепенно корректируются, чтобы модель всё точнее предсказывала целевые слова на основе их контекста. В результате этих итераций формируются качественные векторные представления слов, которые кодируют семантические и синтаксические отношения между ними.\n",
        "\n"
      ],
      "metadata": {
        "id": "yZZbptAXsr6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Оптимизации Word2Vec\n",
        "\n",
        "Для повышения эффективности обучения Word2Vec, особенно при работе с очень большими словарями, используются две основные оптимизации:\n",
        "\n",
        "- **Иерархический Softmax (Hierarchical Softmax)**:  \n",
        "  Вместо вычисления вероятностей для всех $V$ слов в выходном слое с помощью стандартного Softmax, иерархический Softmax использует **двоичное дерево Хаффмана**, в котором листьями являются слова словаря. Вероятность слова вычисляется как произведение вероятностей переходов по пути от корня дерева к соответствующему листу. Это снижает вычислительную сложность обновления весов с $O(V)$ до $O(\\log V)$, что особенно эффективно при больших $V$.\n",
        "\n",
        "- **Отрицательное сэмплирование (Negative Sampling)**:  \n",
        "  Это более популярная и вычислительно эффективная оптимизация. Вместо многоклассовой классификации (предсказание одного истинного слова среди $V$), задача преобразуется в **бинарную классификацию**.  \n",
        "  Для каждой положительной пары (целевое слово, контекстное слово) модель обучается предсказывать метку 1. Одновременно выбирается $k$ случайных слов (отрицательные примеры), не входящих в контекст, и для пар (целевое слово, отрицательное слово) модель обучается предсказывать метку 0.  \n",
        "  Таким образом, на каждом шаге обновляются веса только для истинного контекстного слова и $k$ отрицательных слов, а не для всего словаря. Это резко снижает вычислительную нагрузку и ускоряет обучение.\n",
        "\n",
        "\n",
        "\n",
        "#  Преимущества и недостатки Word2Vec (общие)\n",
        "\n",
        "## Преимущества\n",
        "\n",
        "- **Эффективность**:  \n",
        "  Word2Vec способен обучаться на очень больших текстовых корпусах, генерируя качественные эмбеддинги за разумное время.\n",
        "\n",
        "- **Семантические и синтаксические отношения**:  \n",
        "  Эмбеддинги Word2Vec улавливают не только семантическую близость слов (например, «король» и «королева» находятся близко в векторном пространстве), но и линейные аналогии:  \n",
        "  $$\n",
        "  \\mathbf{v}_{\\text{король}} - \\mathbf{v}_{\\text{мужчина}} + \\mathbf{v}_{\\text{женщина}} \\approx \\mathbf{v}_{\\text{королева}}\n",
        "  $$\n",
        "\n",
        "- **Плотные представления**:  \n",
        "  В отличие от разреженных векторов (например, one-hot или TF-IDF), плотные эмбеддинги компактны, эффективны с точки зрения памяти и хорошо работают в составе последующих моделей машинного обучения.\n",
        "\n",
        "- **Гибкость**:  \n",
        "  Модель может использоваться как для обучения с нуля, так и для тонкой настройки (fine-tuning) предварительно обученных эмбеддингов в рамках конкретной задачи.\n",
        "\n",
        "## Недостатки\n",
        "\n",
        "- **Статические эмбеддинги**:  \n",
        "  Каждое слово имеет **один фиксированный вектор**, независимо от контекста его употребления. Это приводит к следующим проблемам:\n",
        "  - **Проблема полисемии**: модель не может различать разные значения одного и того же слова. Например, слово «банк» в значениях *финансовое учреждение* и *берег реки* будет представлено одним и тем же вектором.\n",
        "  - **Отсутствие контекстной чувствительности**: Word2Vec не учитывает, как слово взаимодействует с другими словами в предложении. Это ограничивает его способность понимать нюансы смысла, идиомы, отрицания и другие контекстно-зависимые явления.\n",
        "\n",
        "- **Неспособность к обучению на новых данных без полного переобучения**:  \n",
        "  Новые слова, отсутствующие в исходном словаре (OOV — *out-of-vocabulary*), не имеют векторных представлений. Чтобы добавить их, требуется либо расширение словаря, либо полное переобучение модели на обновлённом корпусе, что неэффективно.\n",
        "\n",
        "\n",
        "\n",
        "Несмотря на эти ограничения, **Word2Vec стал краеугольным камнем в NLP** и проложил путь для развития более сложных моделей, таких как **ELMo**, **BERT** и других, которые используют **контекстно-зависимые эмбеддинги** и решают многие из указанных проблем.\n"
      ],
      "metadata": {
        "id": "g9dGNs1IxyGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# GloVe: Глобальные векторы для представления слов\n",
        "\n",
        "## 1. Введение в векторные представления слов (Word Embeddings)\n",
        "\n",
        "В области обработки естественного языка (NLP) слова традиционно представлялись как дискретные символы. Однако такой подход не позволяет улавливать семантические и синтаксические отношения между словами. Например, слова «король» и «королева» имеют схожие значения, но их дискретные представления не отражают этого сходства.\n",
        "\n",
        "Векторные представления слов, или **word embeddings**, решают эту проблему, отображая слова в плотные векторы вещественных чисел в многомерном пространстве. В этом пространстве слова с похожим значением или контекстом располагаются близко друг к другу. Это позволяет моделям машинного обучения лучше понимать язык и выполнять такие задачи, как машинный перевод, анализ настроений, вопросно-ответные системы и многие другие.\n",
        "\n",
        "Существует два основных подхода к созданию векторных представлений:\n",
        "\n",
        "1. **Прогнозные модели (prediction-based models)**:  \n",
        "   Пример — Word2Vec (Skip-gram и CBOW). Они обучаются предсказывать слово по его контексту или контекст по слову.\n",
        "\n",
        "2. **Модели, основанные на частоте встречаемости (count-based models)**:  \n",
        "   Пример — Latent Semantic Analysis (LSA). Они анализируют статистику встречаемости слов в большом корпусе текстов.\n",
        "\n",
        "Модель **GloVe (Global Vectors for Word Representation)**, разработанная Стэнфордским университетом, представляет собой гибридный подход, который пытается объединить преимущества обоих методов. Она использует глобальную статистику со-встречаемости слов (как в count-based моделях) для обучения векторных представлений, но при этом оптимизирует функцию потерь, похожую на те, что используются в прогнозных моделях.\n",
        "\n",
        "Основная идея GloVe заключается в том, что отношения между словами могут быть закодированы в разностях их векторных представлений. Например:\n",
        "$$\n",
        "\\mathbf{v}_{\\text{король}} - \\mathbf{v}_{\\text{мужчина}} \\approx \\mathbf{v}_{\\text{королева}} - \\mathbf{v}_{\\text{женщина}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Математические основы модели GloVe\n",
        "\n",
        "### 2.1. Матрица со-встречаемости (Co-occurrence Matrix)\n",
        "\n",
        "В основе GloVe лежит **матрица со-встречаемости** $X$. Элемент $X_{ij}$ этой матрицы представляет собой количество раз, когда слово $j$ встречается в контексте слова $i$ в заданном окне. Размер окна определяет, насколько далеко друг от друга могут находиться слова, чтобы считаться «со-встречающимися».\n",
        "\n",
        "#### Пример:\n",
        "Предположим, у нас есть корпус:\n",
        "- «I like deep learning.»\n",
        "- «I like NLP.»\n",
        "- «I love learning.»\n",
        "\n",
        "Используем окно размером 1 (рассматриваем только непосредственных соседей).\n",
        "\n",
        "Матрица $X$ будет выглядеть примерно так:\n",
        "\n",
        "| Word \\ Context | I   | like | deep | learning | NLP | love |\n",
        "|----------------|-----|------|------|----------|-----|------|\n",
        "| I              | 0   | 2    | 0    | 0        | 0   | 0    |\n",
        "| like           | 1   | 0    | 1    | 0        | 0   | 0    |\n",
        "| deep           | 0   | 1    | 0    | 1        | 0   | 0    |\n",
        "| learning       | 0   | 0    | 1    | 0        | 1   | 1    |\n",
        "| NLP            | 0   | 0    | 0    | 1        | 0   | 0    |\n",
        "| love           | 0   | 0    | 0    | 1        | 0   | 0    |\n",
        "\n",
        "> **Примечание**: Обычно матрица со-встречаемости симметрична ($X_{ij} = X_{ji}$), если контекстное окно не учитывает направление. В GloVe часто используется симметричное окно, и $X_{ij}$ суммирует со-встречаемость $i$ с $j$ и $j$ с $i$. Также часто применяется затухающая функция веса со-встречаемости в зависимости от расстояния. Например, если слова находятся на расстоянии $d$, их со-встречаемость может учитываться с весом $1/d$.\n",
        "\n",
        "\n",
        "\n",
        "### 2.2. Функция потерь GloVe (Objective Function)\n",
        "\n",
        "Основная идея GloVe заключается в том, чтобы найти такие векторные представления слов $\\mathbf{v}_i$ и $\\mathbf{v}_j$ (а также контекстные векторы $\\mathbf{v}'_j$), чтобы их скалярное произведение $\\mathbf{v}_i^\\top \\mathbf{v}_j$ хорошо аппроксимировало логарифм частоты со-встречаемости $\\log(X_{ij})$.\n",
        "\n",
        "Функция потерь GloVe определяется следующим образом:\n",
        "$$\n",
        "J = \\sum_{i=1}^{V} \\sum_{j=1}^{V} f(X_{ij}) \\left( \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij}) \\right)^2\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $V$ — размер словаря.\n",
        "- $\\mathbf{v}_i$ — векторное представление слова $i$.\n",
        "- $\\mathbf{v}_j$ — векторное представление слова $j$ (контекстный вектор). В GloVe для каждого слова обучаются два вектора: основной вектор $\\mathbf{v}_i$ и контекстный вектор $\\mathbf{v}'_j$. В конце обучения обычно используется сумма этих векторов ($\\mathbf{v}_i + \\mathbf{v}'_i$) или только основной вектор $\\mathbf{v}_i$. Для простоты обозначим контекстный вектор как $\\mathbf{v}_j$.\n",
        "- $b_i$ и $b_j$ — скалярные смещения (bias terms) для слов $i$ и $j$ соответственно. Они учитывают общую частоту встречаемости слов, которая не улавливается скалярным произведением.\n",
        "- $\\log(X_{ij})$ — логарифм частоты со-встречаемости слова $j$ в контексте слова $i$. На практике используется $\\log(1 + X_{ij})$, чтобы избежать $\\log(0)$ для пар, которые никогда не встречаются.\n",
        "- $f(X_{ij})$ — весовая функция (weighting function). Она предназначена для:\n",
        "  - Присвоения нулевого веса парам, которые никогда не встречаются ($X_{ij} = 0$), чтобы они не влияли на функцию потерь.\n",
        "  - Присвоения меньшего веса очень частым парам (например, «the the»), которые могут нести меньше семантической информации.\n",
        "  - Присвоения большего веса редким, но информативным парам.\n",
        "\n",
        "Типичная весовая функция $f(x)$ имеет вид:\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "(x / x_{\\text{max}})^\\alpha & \\text{если } x < x_{\\text{max}} \\\\\n",
        "1 & \\text{если } x \\geq x_{\\text{max}}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $x$ — это $X_{ij}$,\n",
        "- $x_{\\text{max}}$ — пороговое значение (например, 100). Пары с частотой со-встречаемости выше $x_{\\text{max}}$ получают полный вес 1,\n",
        "- $\\alpha$ — параметр (обычно 0.75).\n",
        "\n",
        "\n",
        "\n",
        "### 2.3. Интуиция за функцией потерь\n",
        "\n",
        "Почему эта функция потерь работает?\n",
        "\n",
        "Цель GloVe — найти векторные представления, которые кодируют **отношения между словами**, проявляющиеся в **отношениях частот со-встречаемости**.\n",
        "\n",
        "Рассмотрим отношение вероятностей того, что слово $j$ встретится в контексте слов $i$ и $k$:\n",
        "$$\n",
        "\\frac{P(j \\mid i)}{P(j \\mid k)}\n",
        "$$\n",
        "GloVe предполагает, что это отношение может быть выражено через разность векторов:\n",
        "$$\n",
        "\\mathbf{v}_i^\\top \\mathbf{v}_j - \\mathbf{v}_k^\\top \\mathbf{v}_j \\approx \\log\\left(\\frac{X_{ij}}{X_{kj}}\\right) = \\log(X_{ij}) - \\log(X_{kj})\n",
        "$$\n",
        "Это можно переписать как:\n",
        "$$\n",
        "\\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j \\approx \\log(X_{ij})\n",
        "$$\n",
        "Таким образом, минимизация квадратичной разницы между скалярным произведением векторов (плюс смещения) и логарифмом частоты со-встречаемости заставляет модель учиться таким образом, чтобы векторные отношения отражали статистические отношения в корпусе.\n",
        "\n",
        "\n",
        "## 3. Оптимизация модели GloVe: Обучение\n",
        "\n",
        "Обучение модели GloVe сводится к минимизации функции потерь $J$ по всем параметрам: векторам слов $\\mathbf{v}_i$, контекстным векторам $\\mathbf{v}_j$ и смещениям $b_i$, $b_j$. Это достигается с использованием алгоритма **стохастического градиентного спуска (Stochastic Gradient Descent, SGD)** или его вариантов (например, AdaGrad, Adam).\n",
        "\n",
        "В SGD параметры обновляются итеративно, делая небольшие шаги в направлении, противоположном градиенту функции потерь.\n",
        "\n",
        "### 3.1. Прямой проход (Forward Pass)\n",
        "\n",
        "Прямой проход для GloVe относительно прост, поскольку он не включает сложной нейронной сети. Для каждой пары слов $(i, j)$ с ненулевой частотой со-встречаемости $X_{ij}$:\n",
        "\n",
        "1. **Извлечение параметров**: Получаем текущие векторные представления $\\mathbf{v}_i$ и $\\mathbf{v}_j$, а также смещения $b_i$ и $b_j$.\n",
        "2. **Вычисление предсказанного значения**: Вычисляем скалярное произведение $\\mathbf{v}_i^\\top \\mathbf{v}_j$ и добавляем смещения:\n",
        "$$\n",
        "   \\text{prediction} = \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j\n",
        "$$\n",
        "3. **Вычисление целевого значения**: Вычисляем логарифм частоты со-встречаемости:\n",
        "$$\n",
        "   \\text{target} = \\log(X_{ij})\n",
        "$$\n",
        "   (На практике используется $\\log(1 + X_{ij})$.)\n",
        "4. **Вычисление ошибки**: Разница между предсказанным и целевым значением:\n",
        "$$\n",
        "   \\text{error} = \\text{prediction} - \\text{target} = \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij})\n",
        "$$\n",
        "5. **Вычисление взвешенной квадратичной ошибки для данной пары**:\n",
        "$$\n",
        "   L_{ij} = f(X_{ij}) \\cdot (\\text{error})^2 = f(X_{ij}) \\left( \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij}) \\right)^2\n",
        "$$\n",
        "   Этот $L_{ij}$ является вкладом одной пары $(i, j)$ в общую функцию потерь $J$.\n",
        "\n",
        "\n",
        "# 3.2. Обратное распространение ошибки (Backpropagation) и обновление параметров\n",
        "\n",
        "Обратное распространение ошибки в GloVe заключается в вычислении частных производных функции потерь $J$ по каждому из обучаемых параметров ($\\mathbf{v}_i$, $\\mathbf{v}_j$, $b_i$, $b_j$). Затем эти градиенты используются для обновления параметров.\n",
        "\n",
        "Для простоты рассмотрим вклад одной пары $(i, j)$ в функцию потерь:\n",
        "$$\n",
        "L_{ij} = f(X_{ij}) \\left( \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij}) \\right)^2\n",
        "$$\n",
        "\n",
        "Обозначим:\n",
        "$$\n",
        "E_{ij} = \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij})\n",
        "$$\n",
        "Тогда:\n",
        "$$\n",
        "L_{ij} = f(X_{ij}) E_{ij}^2\n",
        "$$\n",
        "\n",
        "Мы хотим найти градиенты:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_i},\\\n",
        "\\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_j},\\\n",
        "\\frac{\\partial L_{ij}}{\\partial b_i},\\\n",
        "\\frac{\\partial L_{ij}}{\\partial b_j}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2.1. Градиент по $\\mathbf{v}_i$\n",
        "\n",
        "Применяем правило цепи:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_i} = \\frac{\\partial L_{ij}}{\\partial E_{ij}} \\cdot \\frac{\\partial E_{ij}}{\\partial \\mathbf{v}_i}\n",
        "$$\n",
        "\n",
        "Сначала найдём:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial E_{ij}} = \\frac{\\partial}{\\partial E_{ij}} \\left( f(X_{ij}) E_{ij}^2 \\right) = 2 f(X_{ij}) E_{ij}\n",
        "$$\n",
        "\n",
        "Затем:\n",
        "$$\n",
        "\\frac{\\partial E_{ij}}{\\partial \\mathbf{v}_i} = \\frac{\\partial}{\\partial \\mathbf{v}_i} \\left( \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij}) \\right) = \\mathbf{v}_j\n",
        "$$\n",
        "\n",
        "Таким образом, градиент по $\\mathbf{v}_i$:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_i} = 2 f(X_{ij}) E_{ij} \\cdot \\mathbf{v}_j\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2.2. Градиент по $\\mathbf{v}_j$\n",
        "\n",
        "Аналогично:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_j} = \\frac{\\partial L_{ij}}{\\partial E_{ij}} \\cdot \\frac{\\partial E_{ij}}{\\partial \\mathbf{v}_j}\n",
        "$$\n",
        "\n",
        "Мы уже знаем:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial E_{ij}} = 2 f(X_{ij}) E_{ij}\n",
        "$$\n",
        "\n",
        "Теперь:\n",
        "$$\n",
        "\\frac{\\partial E_{ij}}{\\partial \\mathbf{v}_j} = \\frac{\\partial}{\\partial \\mathbf{v}_j} \\left( \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij}) \\right) = \\mathbf{v}_i\n",
        "$$\n",
        "\n",
        "Таким образом, градиент по $\\mathbf{v}_j$:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_j} = 2 f(X_{ij}) E_{ij} \\cdot \\mathbf{v}_i\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2.3. Градиент по $b_i$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial b_i} = \\frac{\\partial L_{ij}}{\\partial E_{ij}} \\cdot \\frac{\\partial E_{ij}}{\\partial b_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E_{ij}}{\\partial b_i} = \\frac{\\partial}{\\partial b_i} \\left( \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij}) \\right) = 1\n",
        "$$\n",
        "\n",
        "Следовательно:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial b_i} = 2 f(X_{ij}) E_{ij}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2.4. Градиент по $b_j$\n",
        "\n",
        "Аналогично:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial b_j} = \\frac{\\partial L_{ij}}{\\partial E_{ij}} \\cdot \\frac{\\partial E_{ij}}{\\partial b_j}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E_{ij}}{\\partial b_j} = 1\n",
        "$$\n",
        "\n",
        "Следовательно:\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial b_j} = 2 f(X_{ij}) E_{ij}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2.5. Обновление параметров\n",
        "\n",
        "После вычисления градиентов для каждой пары $(i, j)$ с ненулевой частотой со-встречаемости, параметры обновляются по правилу градиентного спуска:\n",
        "$$\n",
        "\\mathbf{v}_i^{\\text{new}} = \\mathbf{v}_i^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_i}\n",
        "$$\n",
        "$$\n",
        "\\mathbf{v}_j^{\\text{new}} = \\mathbf{v}_j^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_j}\n",
        "$$\n",
        "$$\n",
        "b_i^{\\text{new}} = b_i^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{ij}}{\\partial b_i}\n",
        "$$\n",
        "$$\n",
        "b_j^{\\text{new}} = b_j^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{ij}}{\\partial b_j}\n",
        "$$\n",
        "\n",
        "Где $\\eta$ (скорость обучения, *learning rate*) — небольшой положительный скаляр, определяющий размер шага обновления.\n",
        "\n",
        "На практике вместо обработки каждой пары $(i, j)$ по отдельности (что может быть очень медленно для больших корпусов) часто используется **мини-пакетный градиентный спуск (mini-batch gradient descent)**, где градиенты усредняются по небольшому набору пар перед обновлением параметров.\n",
        "\n",
        "\n",
        "\n",
        "# 4. Алгоритм обучения GloVe (пошагово)\n",
        "\n",
        "1. **Сбор корпуса текстов**:  \n",
        "   Подготовьте большой текстовый корпус, на котором будет обучаться модель.\n",
        "\n",
        "2. **Построение словаря**:  \n",
        "   Извлеките все уникальные слова из корпуса и создайте словарь (vocabulary). Каждому слову присваивается уникальный индекс.\n",
        "\n",
        "3. **Построение матрицы со-встречаемости $X$**:\n",
        "   - Инициализируйте матрицу $X$ размером $V \\times V$ (где $V$ — размер словаря) нулями.\n",
        "   - Для каждого слова в корпусе пройдитесь по его контекстному окну (например, 5 слов влево и 5 слов вправо).\n",
        "   - Для каждой пары со-встречающихся слов $(i, j)$ в окне увеличьте $X_{ij}$ на 1 (или на $1/d$, где $d$ — расстояние между словами).\n",
        "   - Примените пороговое значение для редких слов, отбрасывая их или заменяя на токен `<UNK>`.\n",
        "\n",
        "4. **Инициализация параметров**:\n",
        "   - Инициализируйте векторные представления слов $\\mathbf{v}_i$ и контекстные векторы $\\mathbf{v}'_j$ (они могут быть разными или одинаковыми в начале) случайными малыми значениями. Размерность векторов (например, 50, 100, 300) является гиперпараметром.\n",
        "   - Инициализируйте скалярные смещения $b_i$ и $b_j$ нулями или случайными малыми значениями.\n",
        "\n",
        "5. **Итеративное обучение (эпохи)**:  \n",
        "   Повторяйте следующие шаги в течение заданного количества эпох:\n",
        "   - **Перемешивание данных**: Перемешайте список всех пар $(i, j)$ с ненулевой частотой со-встречаемости.\n",
        "   - **Итерация по парам**: Для каждой пары $(i, j)$ из перемешанного списка:\n",
        "     - **Прямой проход**:\n",
        "       - Вычислите $E_{ij} = \\mathbf{v}_i^\\top \\mathbf{v}_j + b_i + b_j - \\log(X_{ij})$.\n",
        "       - Вычислите весовой коэффициент $f(X_{ij})$ с использованием выбранной весовой функции.\n",
        "     - **Обратное распространение ошибки**:\n",
        "       - Вычислите градиенты:\n",
        "$$\n",
        "         \\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_i} = 2 f(X_{ij}) E_{ij} \\cdot \\mathbf{v}_j\n",
        "$$\n",
        "$$\n",
        "         \\frac{\\partial L_{ij}}{\\partial \\mathbf{v}_j} = 2 f(X_{ij}) E_{ij} \\cdot \\mathbf{v}_i\n",
        "$$\n",
        "$$\n",
        "         \\frac{\\partial L_{ij}}{\\partial b_i} = 2 f(X_{ij}) E_{ij}\n",
        "$$\n",
        "$$\n",
        "         \\frac{\\partial L_{ij}}{\\partial b_j} = 2 f(X_{ij}) E_{ij}\n",
        "$$\n",
        "     - **Обновление параметров**:\n",
        "       - Обновите $\\mathbf{v}_i$, $\\mathbf{v}_j$, $b_i$, $b_j$ с использованием вычисленных градиентов и скорости обучения $\\eta$.\n",
        "       - (Опционально) используйте более продвинутые оптимизаторы, такие как AdaGrad, которые адаптируют скорость обучения для каждого параметра.\n",
        "\n",
        "6. **Получение конечных векторов**:  \n",
        "   После завершения обучения, для каждого слова $k$, его окончательное векторное представление может быть получено как $\\mathbf{v}_k + \\mathbf{v}'_k$ (где $\\mathbf{v}'_k$ — контекстный вектор, соответствующий $\\mathbf{v}_k$), или просто как $\\mathbf{v}_k$.\n",
        "\n",
        "\n",
        "\n",
        "# 5. Преимущества и недостатки GloVe\n",
        "\n",
        "## Преимущества\n",
        "\n",
        "- **Эффективность**:  \n",
        "  Обучение GloVe относительно быстро по сравнению с некоторыми другими методами, особенно на больших корпусах, поскольку оно работает с агрегированной статистикой со-встречаемости, а не с отдельными контекстами.\n",
        "\n",
        "- **Использование глобальной статистики**:  \n",
        "  В отличие от Word2Vec, который фокусируется на локальных контекстах, GloVe явно использует глобальную статистику со-встречаемости, что позволяет ему лучше улавливать общие семантические отношения.\n",
        "\n",
        "- **Хорошая производительность**:  \n",
        "  GloVe часто демонстрирует конкурентоспособные результаты на различных задачах NLP, таких как аналогии, классификация и сходство слов.\n",
        "\n",
        "- **Простота и интерпретируемость**:  \n",
        "  Функция потерь GloVe относительно проста и интуитивно понятна, что облегчает её понимание и анализ.\n",
        "\n",
        "## Недостатки\n",
        "\n",
        "- **Зависимость от матрицы со-встречаемости**:  \n",
        "  Производительность GloVe сильно зависит от качества и размера матрицы со-встречаемости, которая может быть очень большой для обширных словарей, требуя значительных вычислительных ресурсов для её построения и хранения.\n",
        "\n",
        "- **Обработка OOV-слов**:  \n",
        "  Как и многие другие методы, GloVe по умолчанию не может генерировать векторы для слов, отсутствующих в словаре (Out-Of-Vocabulary, OOV). Для этого требуются дополнительные механизмы (например, использование субсловных единиц или предварительно обученных векторов).\n",
        "\n",
        "- **Чувствительность к гиперпараметрам**:  \n",
        "  Производительность может быть чувствительна к выбору гиперпараметров, таких как размерность векторов, размер контекстного окна, $x_{\\text{max}}$ и $\\alpha$ в весовой функции, а также скорость обучения.\n",
        "\n",
        "\n",
        "\n",
        "# 6. Заключение\n",
        "\n",
        "Модель GloVe представляет собой мощный и элегантный подход к созданию векторных представлений слов. Объединяя преимущества методов, основанных на частоте встречаемости, и прогнозных моделей, GloVe эффективно кодирует семантические и синтаксические отношения между словами в плотные векторные пространства. Её математическая основа, построенная на минимизации взвешенной квадратичной ошибки между скалярным произведением векторов и логарифмом частоты со-встречаемости, позволяет создавать высококачественные эмбеддинги, которые находят широкое применение в различных задачах обработки естественного языка. Понимание прямого прохода и обратного распространения ошибки является ключом к пониманию того, как модель обучается и адаптирует свои параметры для достижения этой цели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Аналитический пример модели GloVe: Пошаговые вычисления\n",
        "\n",
        "Этот пример демонстрирует один полный шаг обучения (прямой проход, вычисление градиентов и обновление параметров) для одной пары слов в модели GloVe. Мы будем использовать очень маленький корпус и простые значения для наглядности.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Исходные данные\n",
        "\n",
        "### 1.1. Корпус текстов\n",
        "\n",
        "Предположим, у нас есть очень маленький корпус:\n",
        "- «cat chases mouse»\n",
        "- «mouse runs from cat»\n",
        "\n",
        "\n",
        "\n",
        "### 1.2. Словарь\n",
        "\n",
        "Из корпуса извлекаем уникальные слова и присваиваем им индексы:\n",
        "- `cat`: 0  \n",
        "- `chases`: 1  \n",
        "- `mouse`: 2  \n",
        "- `runs`: 3  \n",
        "- `from`: 4  \n",
        "\n",
        "Размер словаря $V = 5$.\n",
        "\n",
        "\n",
        "\n",
        "### 1.3. Матрица со-встречаемости $X$\n",
        "\n",
        "Используем контекстное окно размером 1 (только непосредственные соседи) с симметричным учётом.\n",
        "\n",
        "- «cat chases mouse»:\n",
        "  - (cat, chases): 1\n",
        "  - (chases, cat): 1\n",
        "  - (chases, mouse): 1\n",
        "  - (mouse, chases): 1\n",
        "- «mouse runs from cat»:\n",
        "  - (mouse, runs): 1\n",
        "  - (runs, mouse): 1\n",
        "  - (runs, from): 1\n",
        "  - (from, runs): 1\n",
        "  - (from, cat): 1\n",
        "  - (cat, from): 1\n",
        "\n",
        "Суммируя, получаем матрицу со-встречаемости $X$:\n",
        "\n",
        "| Word \\ Context | cat (0) | chases (1) | mouse (2) | runs (3) | from (4) |\n",
        "|----------------|---------|------------|-----------|----------|----------|\n",
        "| cat (0)        | 0       | 1          | 0         | 0        | 1        |\n",
        "| chases (1)     | 1       | 0          | 1         | 0        | 0        |\n",
        "| mouse (2)      | 0       | 1          | 0         | 1        | 0        |\n",
        "| runs (3)       | 0       | 0          | 1         | 0        | 1        |\n",
        "| from (4)       | 1       | 0          | 0         | 1        | 0        |\n",
        "\n",
        "\n",
        "\n",
        "### 1.4. Инициализация параметров\n",
        "\n",
        "Предположим, размерность векторов $D = 2$.\n",
        "\n",
        "Инициализируем векторы слов $\\mathbf{v}_i$, контекстные векторы $\\mathbf{v}'_j$ и смещения $b_i$, $b_j$ случайными значениями.\n",
        "\n",
        "#### Векторы слов ($\\mathbf{v}_i$):\n",
        "- $\\mathbf{v}_{\\text{cat}} = \\mathbf{v}_0 = [0.1, 0.2]$  \n",
        "- $\\mathbf{v}_{\\text{chases}} = \\mathbf{v}_1 = [0.3, 0.4]$  \n",
        "- $\\mathbf{v}_{\\text{mouse}} = \\mathbf{v}_2 = [0.5, 0.6]$  \n",
        "- $\\mathbf{v}_{\\text{runs}} = \\mathbf{v}_3 = [0.7, 0.8]$  \n",
        "- $\\mathbf{v}_{\\text{from}} = \\mathbf{v}_4 = [0.9, 1.0]$  \n",
        "\n",
        "#### Контекстные векторы ($\\mathbf{v}'_j$):\n",
        "- $\\mathbf{v}'_{\\text{cat}} = \\mathbf{v}'_0 = [0.05, 0.15]$  \n",
        "- $\\mathbf{v}'_{\\text{chases}} = \\mathbf{v}'_1 = [0.25, 0.35]$  \n",
        "- $\\mathbf{v}'_{\\text{mouse}} = \\mathbf{v}'_2 = [0.45, 0.55]$  \n",
        "- $\\mathbf{v}'_{\\text{runs}} = \\mathbf{v}'_3 = [0.65, 0.75]$  \n",
        "- $\\mathbf{v}'_{\\text{from}} = \\mathbf{v}'_4 = [0.85, 0.95]$  \n",
        "\n",
        "> **Примечание**: В оригинальной статье GloVe векторы $\\mathbf{v}_i$ и $\\mathbf{v}'_j$ обучаются независимо. Здесь $\\mathbf{v}'_j$ — контекстный вектор для слова $j$.\n",
        "\n",
        "#### Смещения ($b_i$, $b_j$):\n",
        "- $b_{\\text{cat}} = b_0 = 0.01$  \n",
        "- $b_{\\text{chases}} = b_1 = 0.02$  \n",
        "- $b_{\\text{mouse}} = b_2 = 0.03$  \n",
        "- $b_{\\text{runs}} = b_3 = 0.04$  \n",
        "- $b_{\\text{from}} = b_4 = 0.05$  \n",
        "\n",
        "#### Гиперпараметры:\n",
        "- Скорость обучения: $\\eta = 0.01$  \n",
        "- Параметры весовой функции:\n",
        "  - $x_{\\text{max}} = 10$\n",
        "  - $\\alpha = 0.75$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Выбор пары для обучения\n",
        "\n",
        "Выберем пару слов $(i, j)$ для демонстрации одного шага обучения:  \n",
        "$(\\text{cat}, \\text{chases})$, то есть $i = 0$, $j = 1$.  \n",
        "Из матрицы: $X_{01} = 1$.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Прямой проход (Forward Pass)\n",
        "\n",
        "### 3.1. Извлечение параметров для $(i=0, j=1)$\n",
        "- $\\mathbf{v}_0 = [0.1, 0.2]$  \n",
        "- $\\mathbf{v}'_1 = [0.25, 0.35]$  \n",
        "- $b_0 = 0.01$  \n",
        "- $b_1 = 0.02$  \n",
        "- $X_{01} = 1$\n",
        "\n",
        "\n",
        "\n",
        "### 3.2. Вычисление предсказанного значения\n",
        "\n",
        "$$\n",
        "\\text{prediction} = \\mathbf{v}_0^\\top \\mathbf{v}'_1 + b_0 + b_1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_0^\\top \\mathbf{v}'_1 = (0.1 \\cdot 0.25) + (0.2 \\cdot 0.35) = 0.025 + 0.070 = 0.095\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{prediction} = 0.095 + 0.01 + 0.02 = 0.125\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.3. Вычисление целевого значения\n",
        "\n",
        "$$\n",
        "\\text{target} = \\log(1 + X_{01}) = \\log(1 + 1) = \\log(2) \\approx 0.6931\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.4. Вычисление ошибки\n",
        "\n",
        "$$\n",
        "\\text{error} = \\text{prediction} - \\text{target} = 0.125 - 0.6931 = -0.5681\n",
        "$$\n",
        "\n",
        "\n",
        "### 3.5. Вычисление весового коэффициента $f(X_{01})$\n",
        "\n",
        "Поскольку $X_{01} = 1 < x_{\\text{max}} = 10$:\n",
        "\n",
        "$$\n",
        "f(X_{01}) = \\left( \\frac{X_{01}}{x_{\\text{max}}} \\right)^\\alpha = \\left( \\frac{1}{10} \\right)^{0.75} = 0.1^{0.75} \\approx 0.1778\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.6. Вычисление взвешенной квадратичной ошибки $L_{01}$\n",
        "\n",
        "$$\n",
        "L_{01} = f(X_{01}) \\cdot (\\text{error})^2 = 0.1778 \\cdot (-0.5681)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "(-0.5681)^2 = 0.32276\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{01} = 0.1778 \\cdot 0.32276 \\approx 0.05747\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4. Обратное распространение ошибки (Backpropagation)\n",
        "\n",
        "Общая формула для градиента по параметру $P$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{ij}}{\\partial P} = 2 f(X_{ij}) \\cdot \\text{error} \\cdot \\frac{\\partial (\\text{prediction})}{\\partial P}\n",
        "$$\n",
        "\n",
        "Мы уже знаем:\n",
        "- $f(X_{01}) \\approx 0.1778$\n",
        "- $\\text{error} \\approx -0.5681$\n",
        "\n",
        "Общий множитель:\n",
        "$$\n",
        "2 \\cdot f(X_{01}) \\cdot \\text{error} = 2 \\cdot 0.1778 \\cdot (-0.5681) \\approx -0.2023\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4.1. Градиент по $\\mathbf{v}_0$ (вектор слова «cat»)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{01}}{\\partial \\mathbf{v}_0} = 2 f(X_{01}) \\cdot \\text{error} \\cdot \\mathbf{v}'_1 = -0.2023 \\cdot [0.25, 0.35]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [-0.2023 \\cdot 0.25,\\ -0.2023 \\cdot 0.35] = [-0.050575,\\ -0.070805]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4.2. Градиент по $\\mathbf{v}'_1$ (контекстный вектор слова «chases»)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{01}}{\\partial \\mathbf{v}'_1} = 2 f(X_{01}) \\cdot \\text{error} \\cdot \\mathbf{v}_0 = -0.2023 \\cdot [0.1, 0.2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [-0.2023 \\cdot 0.1,\\ -0.2023 \\cdot 0.2] = [-0.02023,\\ -0.04046]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4.3. Градиент по $b_0$ (смещение слова «cat»)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{01}}{\\partial b_0} = 2 f(X_{01}) \\cdot \\text{error} \\cdot 1 = -0.2023\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4.4. Градиент по $b_1$ (смещение слова «chases»)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_{01}}{\\partial b_1} = 2 f(X_{01}) \\cdot \\text{error} \\cdot 1 = -0.2023\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 5. Обновление параметров\n",
        "\n",
        "Используем скорость обучения $\\eta = 0.01$.\n",
        "\n",
        "\n",
        "\n",
        "### 5.1. Обновление $\\mathbf{v}_0$\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_0^{\\text{new}} = \\mathbf{v}_0^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{01}}{\\partial \\mathbf{v}_0}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [0.1, 0.2] - 0.01 \\cdot [-0.050575, -0.070805]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [0.1 + 0.00050575,\\ 0.2 + 0.00070805] = [0.10050575,\\ 0.20070805]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 5.2. Обновление $\\mathbf{v}'_1$\n",
        "\n",
        "$$\n",
        "\\mathbf{v}'_1^{\\text{new}} = \\mathbf{v}'_1^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{01}}{\\partial \\mathbf{v}'_1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [0.25, 0.35] - 0.01 \\cdot [-0.02023, -0.04046]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [0.25 + 0.0002023,\\ 0.35 + 0.0004046] = [0.2502023,\\ 0.3504046]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 5.3. Обновление $b_0$\n",
        "\n",
        "$$\n",
        "b_0^{\\text{new}} = b_0^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{01}}{\\partial b_0}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 0.01 - 0.01 \\cdot (-0.2023) = 0.01 + 0.002023 = 0.012023\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 5.4. Обновление $b_1$\n",
        "\n",
        "$$\n",
        "b_1^{\\text{new}} = b_1^{\\text{old}} - \\eta \\cdot \\frac{\\partial L_{01}}{\\partial b_1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 0.02 - 0.01 \\cdot (-0.2023) = 0.02 + 0.002023 = 0.022023\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 6. Заключение по примеру\n",
        "\n",
        "Мы выполнили один шаг обновления параметров для пары слов $(\\text{cat}, \\text{chases})$ в модели GloVe. В реальном обучении этот процесс повторяется для всех пар с ненулевой частотой со-встречаемости в течение многих эпох, пока функция потерь не сойдётся к минимуму. С каждым шагом векторы и смещения будут постепенно корректироваться, чтобы лучше отражать статистические отношения между словами в корпусе.\n",
        "\n",
        "Этот пример демонстрирует, как каждый компонент функции потерь и её градиентов влияет на обновление параметров, приближая скалярное произведение векторов к логарифму частоты их со-встречаемости.\n",
        "\n"
      ],
      "metadata": {
        "id": "CppbfjqDyd0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# FastText: Подробное описание  \n",
        "## FastText – Математические основы и алгоритмы\n",
        "\n",
        "### Введение в FastText\n",
        "\n",
        "FastText — это эффективная библиотека для обучения представлений слов (word embeddings) и классификации текста, разработанная Facebook AI Research (FAIR). Она является расширением моделей Word2Vec (Skip-gram и CBOW) и отличается высокой скоростью обучения, а также способностью обрабатывать редкие слова и символы благодаря использованию **символьных n-грамм**. FastText также может использоваться для классификации текста, достигая при этом конкурентоспособных результатов.\n",
        "\n",
        "Основная идея FastText заключается в том, что слово может быть представлено не только как единое целое, но и как сумма векторов его **символьных n-грамм**. Это позволяет модели учитывать морфологическую структуру слов и генерировать осмысленные векторы даже для слов, которые не встречались в обучающем корпусе (out-of-vocabulary, OOV words).\n",
        "\n",
        "\n",
        "\n",
        "### Ключевые концепции\n",
        "\n",
        "1. **Символьные n-граммы**:  \n",
        "   В отличие от Word2Vec, который работает только с целыми словами, FastText разбивает каждое слово на набор символьных n-грамм. Например, для слова «apple» и $n=3$ будут сгенерированы n-граммы: `<ap`, `app`, `ppl`, `ple`, `le>`. Символы `<` и `>` добавляются для обозначения начала и конца слова, что позволяет различать префиксы и суффиксы. Вектор слова затем получается путём усреднения векторов всех его n-грамм.\n",
        "\n",
        "2. **Представления слов и предложений**:\n",
        "   - **Слова**: Вектор слова $\\mathbf{v}_w$ является суммой или средним векторов его символьных n-грамм.\n",
        "   - **Предложения/документы**: Вектор предложения или документа может быть получен путём усреднения векторов слов, входящих в него.\n",
        "\n",
        "3. **Модели Word2Vec как основа**:  \n",
        "   FastText использует архитектуры Skip-gram и CBOW.\n",
        "   - **CBOW (Continuous Bag-of-Words)**: предсказывает текущее слово на основе окружающих его слов (контекста).\n",
        "   - **Skip-gram**: предсказывает окружающие слова (контекст) на основе текущего слова.\n",
        "\n",
        "   В FastText для классификации текста используется модифицированная архитектура CBOW, где «контекстом» является весь документ, а «целевым словом» — метка класса.\n",
        "\n",
        "\n",
        "\n",
        "### Архитектура модели FastText\n",
        "\n",
        "FastText для обучения представлений слов использует архитектуру Skip-gram или CBOW. Для классификации текста используется модифицированная архитектура CBOW.\n",
        "\n",
        "#### Общая структура (для классификации):\n",
        "- **Входной слой**: состоит из векторов символьных n-грамм, принадлежащих словам в документе.\n",
        "- Эти векторы усредняются, чтобы получить вектор представления документа.\n",
        "- **Выходной слой**: линейный классификатор, который предсказывает вероятность принадлежности документа к каждому классу.\n",
        "\n",
        "\n",
        "\n",
        "## Математические основы\n",
        "\n",
        "### 1. Представление слов и n-грамм\n",
        "\n",
        "Пусть $w$ — слово. FastText представляет $w$ как набор символьных n-грамм. Например, для слова «apple» и $n=3$, набор $G_w$ будет:\n",
        "$$\n",
        "G_{\\text{apple}} = \\{\\text{<ap}, \\text{app}, \\text{ppl}, \\text{ple}, \\text{le>}\\}\n",
        "$$\n",
        "\n",
        "Каждой n-грамме $g \\in G_w$ сопоставляется вектор $\\mathbf{v}_g \\in \\mathbb{R}^d$, где $d$ — размерность векторного пространства.\n",
        "\n",
        "Вектор слова $\\mathbf{v}_w$ вычисляется как среднее арифметическое векторов всех его n-грамм:\n",
        "$$\n",
        "\\mathbf{v}_w = \\frac{1}{|G_w|} \\sum_{g \\in G_w} \\mathbf{v}_g\n",
        "$$\n",
        "\n",
        "> На практике часто используется **суммирование**, а не усреднение, или применяется нормализация после суммирования.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Функция скоринга (Scoring Function)\n",
        "\n",
        "Для задачи классификации текста FastText рассматривает документ как «мешок слов» (bag of words). Пусть $D = \\{w_1, w_2, \\dots, w_k\\}$ — набор слов в документе.\n",
        "\n",
        "Вектор документа $\\mathbf{v}_D$ вычисляется как среднее арифметическое векторов слов, входящих в документ:\n",
        "$$\n",
        "\\mathbf{v}_D = \\frac{1}{|D|} \\sum_{i=1}^{k} \\mathbf{v}_{w_i}\n",
        "$$\n",
        "\n",
        "Этот вектор $\\mathbf{v}_D$ подаётся на выходной слой — линейный классификатор.\n",
        "\n",
        "Пусть $C = \\{c_1, c_2, \\dots, c_M\\}$ — набор возможных классов, где $M$ — количество классов.  \n",
        "Каждому классу $c_j$ сопоставляется вектор весов $\\mathbf{u}_{c_j} \\in \\mathbb{R}^d$.\n",
        "\n",
        "Счёт (score) для класса $c_j$ для документа $D$ вычисляется как скалярное произведение:\n",
        "$$\n",
        "s(D, c_j) = \\mathbf{v}_D \\cdot \\mathbf{u}_{c_j}\n",
        "$$\n",
        "\n",
        "Для преобразования в вероятности используется Softmax:\n",
        "$$\n",
        "P(c_j \\mid D) = \\frac{\\exp(s(D, c_j))}{\\sum_{k=1}^{M} \\exp(s(D, c_k))}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 3. Функция потерь (Loss Function)\n",
        "\n",
        "Для обучения FastText использует **отрицательную логарифмическую правдоподобность** (Negative Log-Likelihood) в сочетании с **Negative Sampling** или **Hierarchical Softmax**. Negative Sampling является более распространённым и эффективным методом, особенно при большом числе классов.\n",
        "\n",
        "#### Negative Sampling (Отрицательная выборка)\n",
        "\n",
        "Вместо вычисления вероятностей для всех $M$ классов, задача преобразуется в серию бинарных классификаций.\n",
        "\n",
        "Для каждого обучающего примера $(D, c_{\\text{true}})$ (документ $D$ и его истинный класс $c_{\\text{true}}$) выбираются $K$ «отрицательных» классов $c_{\\text{neg}_1}, \\dots, c_{\\text{neg}_K}$, не являющихся истинным.\n",
        "\n",
        "Цель: максимизировать вероятность истинного класса и минимизировать вероятность отрицательных.\n",
        "\n",
        "Функция потерь для одного примера:\n",
        "$$\n",
        "\\mathcal{L}(D, c_{\\text{true}}, \\{c_{\\text{neg}_k}\\}_{k=1}^K) = -\\log(\\sigma(\\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{true}}})) - \\sum_{k=1}^{K} \\log(\\sigma(-\\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{neg}_k}}))\n",
        "$$\n",
        "\n",
        "где $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ — сигмоидная функция.\n",
        "\n",
        "Минимизация этой функции эквивалентна максимизации:\n",
        "$$\n",
        "\\log(\\sigma(\\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{true}}})) + \\sum_{k=1}^{K} \\log(\\sigma(-\\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{neg}_k}}))\n",
        "$$\n",
        "\n",
        "Это означает, что мы хотим, чтобы:\n",
        "- $\\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{true}}}$ было **большим положительным числом**,\n",
        "- $\\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{neg}_k}}$ было **маленьким или отрицательным**.\n",
        "\n",
        "\n",
        "\n",
        "## Процесс обучения: Forward Pass и Backpropagation\n",
        "\n",
        "Обучение FastText происходит с использованием **стохастического градиентного спуска (SGD)** или его вариантов (например, Adam, Adagrad).\n",
        "\n",
        "### 1. Прямой проход (Forward Pass)\n",
        "\n",
        "Для обучающего примера $(D, c_{\\text{true}})$:\n",
        "\n",
        "1. **Извлечение n-грамм**:  \n",
        "   Для каждого слова $w_i$ в документе $D$ извлекаются его символьные n-граммы $G_{w_i}$.\n",
        "\n",
        "2. **Получение векторов n-грамм**:  \n",
        "   Для каждой n-граммы $g \\in G_{w_i}$ извлекается её вектор $\\mathbf{v}_g$ из матрицы входных весов (таблица n-грамм).\n",
        "\n",
        "3. **Вычисление вектора слова**:  \n",
        "   Для каждого слова $w_i$ вычисляется вектор:\n",
        " $$\n",
        "   \\mathbf{v}_{w_i} = \\sum_{g \\in G_{w_i}} \\mathbf{v}_g \\quad \\text{(или среднее)}\n",
        " $$\n",
        "\n",
        "4. **Вычисление вектора документа**:  \n",
        " $$\n",
        "   \\mathbf{v}_D = \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\mathbf{v}_{w_i}\n",
        " $$\n",
        "   > На практике часто используется суммирование векторов n-грамм всех слов документа, а затем нормализация.\n",
        "\n",
        "5. **Выбор отрицательных классов**:  \n",
        "   Выбираются $K$ отрицательных классов $c_{\\text{neg}_1}, \\dots, c_{\\text{neg}_K}$ из распределения частотности классов.\n",
        "\n",
        "6. **Вычисление счетов и сигмоидов**:\n",
        "   - Счёт для истинного класса: $s_{\\text{true}} = \\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{true}}}$\n",
        "   - Счёт для отрицательного класса: $s_{\\text{neg}_k} = \\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{neg}_k}}$\n",
        "   - Сигмоид для истинного: $p_{\\text{true}} = \\sigma(s_{\\text{true}})$\n",
        "   - Сигмоид для отрицательного: $p_{\\text{neg}_k} = \\sigma(-s_{\\text{neg}_k})$\n",
        "\n",
        "7. **Вычисление функции потерь**:\n",
        " $$\n",
        "   \\mathcal{L} = -\\log(p_{\\text{true}}) - \\sum_{k=1}^{K} \\log(p_{\\text{neg}_k})\n",
        " $$\n",
        "\n",
        "\n",
        "# 2. Backpropagation (Обратное распространение ошибки)\n",
        "\n",
        "Цель обратного распространения ошибки — обновить веса векторов n-грамм (входные веса) и веса классов (выходные веса) для минимизации функции потерь $\\mathcal{L}$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Вычисление градиентов для выходных весов классов $\\mathbf{u}_c$\n",
        "\n",
        "Мы хотим найти $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_c}$.\n",
        "\n",
        "Начнём с производной функции потерь по аргументу сигмоидной функции.\n",
        "\n",
        "Для истинного класса $c_{\\text{true}}$:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial s_{\\text{true}}} = \\frac{\\partial}{\\partial s_{\\text{true}}} \\left( -\\log(\\sigma(s_{\\text{true}})) \\right) = -\\frac{1}{\\sigma(s_{\\text{true}})} \\cdot \\sigma(s_{\\text{true}})(1 - \\sigma(s_{\\text{true}})) = -(1 - \\sigma(s_{\\text{true}})) = \\sigma(s_{\\text{true}}) - 1\n",
        "$$\n",
        "\n",
        "Для отрицательного класса $c_{\\text{neg}_k}$:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial s_{\\text{neg}_k}} = \\frac{\\partial}{\\partial s_{\\text{neg}_k}} \\left( -\\log(\\sigma(-s_{\\text{neg}_k})) \\right) = -\\frac{1}{\\sigma(-s_{\\text{neg}_k})} \\cdot \\sigma(-s_{\\text{neg}_k})(1 - \\sigma(-s_{\\text{neg}_k})) \\cdot (-1) = 1 - \\sigma(-s_{\\text{neg}_k})\n",
        "$$\n",
        "\n",
        "Теперь, используя правило цепи:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{c_{\\text{true}}}} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\text{true}}} \\cdot \\frac{\\partial s_{\\text{true}}}{\\partial \\mathbf{u}_{c_{\\text{true}}}} = (\\sigma(s_{\\text{true}}) - 1) \\cdot \\mathbf{v}_D\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{c_{\\text{neg}_k}}} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\text{neg}_k}} \\cdot \\frac{\\partial s_{\\text{neg}_k}}{\\partial \\mathbf{u}_{c_{\\text{neg}_k}}} = (1 - \\sigma(-s_{\\text{neg}_k})) \\cdot \\mathbf{v}_D\n",
        "$$\n",
        "\n",
        "#### Обновление весов классов:\n",
        "$$\n",
        "\\mathbf{u}_{c_{\\text{true}}} \\leftarrow \\mathbf{u}_{c_{\\text{true}}} - \\eta \\cdot (\\sigma(s_{\\text{true}}) - 1) \\cdot \\mathbf{v}_D\n",
        "$$\n",
        "$$\n",
        "\\mathbf{u}_{c_{\\text{neg}_k}}} \\leftarrow \\mathbf{u}_{c_{\\text{neg}_k}}} - \\eta \\cdot (1 - \\sigma(-s_{\\text{neg}_k})) \\cdot \\mathbf{v}_D\n",
        "$$\n",
        "где $\\eta$ — скорость обучения.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление градиентов для вектора документа $\\mathbf{v}_D$\n",
        "\n",
        "Нам нужно найти $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_D}$.\n",
        "\n",
        "По правилу цепи:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_D} = \\frac{\\partial \\mathcal{L}}{\\partial s_{\\text{true}}} \\cdot \\frac{\\partial s_{\\text{true}}}{\\partial \\mathbf{v}_D} + \\sum_{k=1}^{K} \\frac{\\partial \\mathcal{L}}{\\partial s_{\\text{neg}_k}} \\cdot \\frac{\\partial s_{\\text{neg}_k}}}{\\partial \\mathbf{v}_D}\n",
        "$$\n",
        "\n",
        "Поскольку $s_{\\text{true}} = \\mathbf{v}_D \\cdot \\mathbf{u}_{c_{\\text{true}}}$, то:\n",
        "$$\n",
        "\\frac{\\partial s_{\\text{true}}}{\\partial \\mathbf{v}_D} = \\mathbf{u}_{c_{\\text{true}}}\n",
        "$$\n",
        "\n",
        "Аналогично:\n",
        "$$\n",
        "\\frac{\\partial s_{\\text{neg}_k}}}{\\partial \\mathbf{v}_D} = \\mathbf{u}_{c_{\\text{neg}_k}}\n",
        "$$\n",
        "\n",
        "Таким образом:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_D} = (\\sigma(s_{\\text{true}}) - 1) \\cdot \\mathbf{u}_{c_{\\text{true}}} + \\sum_{k=1}^{K} (1 - \\sigma(-s_{\\text{neg}_k})) \\cdot \\mathbf{u}_{c_{\\text{neg}_k}}\n",
        "$$\n",
        "\n",
        "Обозначим эту сумму как $\\delta_D$. Это «ошибка», которая будет распространяться обратно к входным n-граммам:\n",
        "$$\n",
        "\\delta_D = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_D}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление градиентов для входных весов n-грамм $\\mathbf{v}_g$\n",
        "\n",
        "Вектор документа $\\mathbf{v}_D$ является средним векторов слов, а каждый вектор слова $\\mathbf{v}_{w_i}$ — суммой векторов его n-грамм.\n",
        "\n",
        "Мы хотим найти $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_g}$ для каждой n-граммы $g$.\n",
        "\n",
        "По цепному правилу:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_g} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_D} \\cdot \\frac{\\partial \\mathbf{v}_D}{\\partial \\mathbf{v}_g}\n",
        "$$\n",
        "\n",
        "Поскольку:\n",
        "$$\n",
        "\\mathbf{v}_D = \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\mathbf{v}_{w_i}, \\quad \\mathbf{v}_{w_i} = \\sum_{g' \\in G_{w_i}} \\mathbf{v}_{g'}\n",
        "$$\n",
        "то если $g$ — n-грамма слова $w_i$, то:\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{v}_D}{\\partial \\mathbf{v}_g} = \\frac{1}{|D|}\n",
        "$$\n",
        "\n",
        "Однако **на практике** FastText часто использует **суммирование** векторов n-грамм для слова и **суммирование** векторов слов для документа (без усреднения), а нормализация применяется позже или не требуется. В этом случае:\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{v}_D}{\\partial \\mathbf{v}_g} = 1\n",
        "$$\n",
        "\n",
        "Таким образом, для каждой n-граммы $g$ в документе $D$:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_g} = \\delta_D\n",
        "$$\n",
        "\n",
        "#### Обновление весов n-грамм:\n",
        "\n",
        "Для каждой n-граммы $g$ в документе $D$:\n",
        "$$\n",
        "\\mathbf{v}_g \\leftarrow \\mathbf{v}_g - \\eta \\cdot \\delta_D\n",
        "$$\n",
        "\n",
        "Этот процесс повторяется для каждого обучающего примера в течение нескольких эпох.\n",
        "\n",
        "\n",
        "\n",
        "# FastText для классификации текста\n",
        "\n",
        "Как уже упоминалось, FastText адаптирует архитектуру CBOW для классификации текста. Вместо предсказания центрального слова по контексту, он предсказывает метку класса по содержимому документа.\n",
        "\n",
        "1. **Вход**: Документ представляется как «мешок» символьных n-грамм.\n",
        "2. **Скрытый слой**: Все векторы n-грамм в документе усредняются (или суммируются), формируя вектор представления документа. Этот слой можно рассматривать как «скрытый», хотя он не имеет нелинейной функции активации.\n",
        "3. **Выходной слой**: Вектор документа подаётся на линейный классификатор (выходной слой), который использует Softmax или Negative Sampling для предсказания вероятностей классов.\n",
        "\n",
        "\n",
        "\n",
        "# Преимущества и недостатки FastText\n",
        "\n",
        "## Преимущества\n",
        "\n",
        "- **Скорость**: Очень быстр в обучении и классификации, что делает его пригодным для очень больших корпусов данных.\n",
        "- **Обработка OOV-слов**: Благодаря использованию символьных n-грамм, FastText может генерировать векторы для слов, не встречавшихся в обучающем корпусе. Это значительное преимущество по сравнению с Word2Vec.\n",
        "- **Учёт морфологии**: Символьные n-граммы позволяют модели улавливать морфологические особенности слов (префиксы, суффиксы, корни), что особенно полезно для языков с богатой морфологией.\n",
        "- **Производительность в классификации**: Демонстрирует высокую производительность в задачах классификации текста, часто сравнимую с более сложными нейронными сетями, но с гораздо меньшими вычислительными затратами.\n",
        "\n",
        "## Недостатки\n",
        "\n",
        "- **«Мешок слов»**: Как и другие модели типа «мешок слов», FastText не учитывает порядок слов в предложении, что может быть критично для задач, где важен синтаксис.\n",
        "- **Размер модели**: Модель может быть достаточно большой из-за хранения векторов для всех уникальных символьных n-грамм.\n",
        "- **Менее мощный для сложных задач**: Для очень сложных задач понимания естественного языка, требующих глубокого синтаксического и семантического анализа, более сложные модели (например, на основе трансформеров) могут показывать лучшие результаты.\n",
        "\n",
        "\n",
        "\n",
        "# Заключение\n",
        "\n",
        "FastText представляет собой мощный и эффективный инструмент для создания представлений слов и классификации текста. Его ключевое отличие — использование **символьных n-грамм**, что позволяет ему эффективно работать с редкими и неизвестными словами, а также учитывать морфологическую структуру языка. Простота архитектуры и высокая скорость обучения делают его отличным выбором для широкого круга задач обработки естественного языка, особенно когда ресурсы ограничены или требуется быстрая обработка больших объёмов данных.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Аналитический пример FastText: Пошаговые вычисления\n",
        "\n",
        "Рассмотрим упрощённый пример обучения FastText для задачи классификации текста. Мы используем очень маленький корпус и один обучающий пример, чтобы проиллюстрировать каждый шаг прямого прохода (forward pass).\n",
        "\n",
        "\n",
        "\n",
        "## Сценарий примера\n",
        "\n",
        "- **Задача**: Классификация текста  \n",
        "- **Документ $D_1$**: «хороший фильм»  \n",
        "- **Истинный класс $c_{\\text{true}}$**: «позитив»  \n",
        "- **Возможные классы**: «позитив», «негатив»  \n",
        "\n",
        "### Параметры модели:\n",
        "- Размерность векторов ($d$): 2  \n",
        "- Скорость обучения ($\\eta$): 0.1  \n",
        "- Размер n-грамм ($n$): 3  \n",
        "- Количество отрицательных выборок ($K$): 1  \n",
        "\n",
        "\n",
        "\n",
        "## Инициализация\n",
        "\n",
        "Предположим, что в нашем словаре n-грамм есть следующие 3-граммы и их начальные векторы (инициализированы случайными малыми значениями):\n",
        "\n",
        "| n-грамма | Вектор $\\mathbf{v}_g$ |\n",
        "|----------|------------------------|\n",
        "| `<хор`   | $[0.1, 0.2]$          |\n",
        "| `хор`    | $[0.05, 0.15]$        |\n",
        "| `оро`    | $[0.2, 0.1]$          |\n",
        "| `рог`    | $[0.1, 0.05]$         |\n",
        "| `оги`    | $[0.15, 0.25]$        |\n",
        "| `гий`    | $[0.25, 0.0]$         |\n",
        "| `ий>`    | $[0.0, 0.1]$          |\n",
        "| `<фил`   | $[0.3, 0.1]$          |\n",
        "| `фил`    | $[0.1, 0.3]$          |\n",
        "| `илм`    | $[0.2, 0.2]$          |\n",
        "| `лм>`    | $[0.05, 0.05]$        |\n",
        "\n",
        "### Векторы весов классов:\n",
        "| Класс     | Вектор весов $\\mathbf{u}_c$ |\n",
        "|-----------|-----------------------------|\n",
        "| позитив   | $[0.1, -0.1]$               |\n",
        "| негатив   | $[-0.05, 0.05]$             |\n",
        "\n",
        "\n",
        "## 1. Прямой проход (Forward Pass)\n",
        "\n",
        "**Документ $D_1$**: «хороший фильм»  \n",
        "**Истинный класс $c_{\\text{true}}$**: «позитив»  \n",
        "**Отрицательный класс $c_{\\text{neg}_1}$** (выбран случайно): «негатив»\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Извлечение n-грамм для каждого слова\n",
        "\n",
        "- **Слово \"хороший\"**:\n",
        "  - Полные 3-граммы: `<хор`, `хоро`, `орош`, `роши`, `оший`, `ший>`, `ий>`\n",
        "  - Возьмём только те, что есть в таблице: `<хор`, `оро`, `гий`, `ий>`\n",
        "  - $G_{\\text{хороший}} = \\{\\text{<хор}, \\text{оро}, \\text{гий}, \\text{ий>}\\}$\n",
        "\n",
        "- **Слово \"фильм\"**:\n",
        "  - Полные 3-граммы: `<фил`, `филь`, `ильм`, `лм>`\n",
        "  - Возьмём только те, что есть в таблице: `<фил`, `илм`, `лм>`\n",
        "  - $G_{\\text{фильм}} = \\{\\text{<фил}, \\text{илм}, \\text{лм>}\\}$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Получение векторов n-грамм (из таблицы инициализации)\n",
        "\n",
        "- $\\mathbf{v}_{\\text{<хор}} = [0.1, 0.2]$  \n",
        "- $\\mathbf{v}_{\\text{оро}} = [0.2, 0.1]$  \n",
        "- $\\mathbf{v}_{\\text{гий}} = [0.25, 0.0]$  \n",
        "- $\\mathbf{v}_{\\text{ий>}} = [0.0, 0.1]$  \n",
        "- $\\mathbf{v}_{\\text{<фил}} = [0.3, 0.1]$  \n",
        "- $\\mathbf{v}_{\\text{илм}} = [0.2, 0.2]$  \n",
        "- $\\mathbf{v}_{\\text{лм>}} = [0.05, 0.05]$  \n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление вектора слова $\\mathbf{v}_{w_i}$ (как сумма векторов n-грамм)\n",
        "\n",
        "- **Вектор слова \"хороший\"** ($\\mathbf{v}_{\\text{хороший}}$):\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{хороший}} = \\mathbf{v}_{\\text{<хор}} + \\mathbf{v}_{\\text{оро}} + \\mathbf{v}_{\\text{гий}} + \\mathbf{v}_{\\text{ий>}}\n",
        "$$\n",
        "$$\n",
        "  = [0.1, 0.2] + [0.2, 0.1] + [0.25, 0.0] + [0.0, 0.1] = [0.55, 0.4]\n",
        "$$\n",
        "\n",
        "- **Вектор слова \"фильм\"** ($\\mathbf{v}_{\\text{фильм}}$):\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{фильм}} = \\mathbf{v}_{\\text{<фил}} + \\mathbf{v}_{\\text{илм}} + \\mathbf{v}_{\\text{лм>}}\n",
        "$$\n",
        "$$\n",
        "  = [0.3, 0.1] + [0.2, 0.2] + [0.05, 0.05] = [0.55, 0.35]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Вычисление вектора документа $\\mathbf{v}_D$ (как среднее арифметическое векторов слов)\n",
        "\n",
        "Документ $D_1 = \\{\\text{«хороший»}, \\text{«фильм»}\\}$, $|D_1| = 2$\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_D = \\frac{1}{2} (\\mathbf{v}_{\\text{хороший}} + \\mathbf{v}_{\\text{фильм}})\n",
        "$$\n",
        "$$\n",
        "= \\frac{1}{2} ([0.55, 0.4] + [0.55, 0.35]) = \\frac{1}{2} [1.1, 0.75] = [0.55, 0.375]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Выбор отрицательных классов\n",
        "\n",
        "- **Истинный класс**: $c_{\\text{true}} = \\text{«позитив»}$\n",
        "- **Отрицательный класс**: $c_{\\text{neg}_1} = \\text{«негатив»}$ (выбран случайно)\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 6: Вычисление счетов и сигмоидов\n",
        "\n",
        "Используем векторы весов классов:\n",
        "- $\\mathbf{u}_{\\text{позитив}} = [0.1, -0.1]$\n",
        "- $\\mathbf{u}_{\\text{негатив}} = [-0.05, 0.05]$\n",
        "\n",
        "#### Счёт для истинного класса ($s_{\\text{true}}$):\n",
        "$$\n",
        "s_{\\text{true}} = \\mathbf{v}_D \\cdot \\mathbf{u}_{\\text{позитив}} = [0.55, 0.375] \\cdot [0.1, -0.1]\n",
        "$$\n",
        "$$\n",
        "= (0.55 \\cdot 0.1) + (0.375 \\cdot -0.1) = 0.055 - 0.0375 = 0.0175\n",
        "$$\n",
        "\n",
        "#### Сигмоид для истинного класса ($p_{\\text{true}}$):\n",
        "$$\n",
        "p_{\\text{true}} = \\sigma(s_{\\text{true}}) = \\frac{1}{1 + \\exp(-s_{\\text{true}})} = \\frac{1}{1 + \\exp(-0.0175)}\n",
        "$$\n",
        "$$\n",
        "\\exp(-0.0175) \\approx 0.9826\n",
        "$$\n",
        "$$\n",
        "p_{\\text{true}} = \\frac{1}{1 + 0.9826} = \\frac{1}{1.9826} \\approx 0.5044\n",
        "$$\n",
        "\n",
        "#### Счёт для отрицательного класса ($s_{\\text{neg}_1}$):\n",
        "$$\n",
        "s_{\\text{neg}_1} = \\mathbf{v}_D \\cdot \\mathbf{u}_{\\text{негатив}} = [0.55, 0.375] \\cdot [-0.05, 0.05]\n",
        "$$\n",
        "$$\n",
        "= (0.55 \\cdot -0.05) + (0.375 \\cdot 0.05) = -0.0275 + 0.01875 = -0.00875\n",
        "$$\n",
        "\n",
        "#### Сигмоид для отрицательного класса ($p_{\\text{neg}_1}$):\n",
        "$$\n",
        "p_{\\text{neg}_1} = \\sigma(-s_{\\text{neg}_1}) = \\frac{1}{1 + \\exp(s_{\\text{neg}_1})} = \\frac{1}{1 + \\exp(-0.00875)}\n",
        "$$\n",
        "$$\n",
        "\\exp(-0.00875) \\approx 0.9913\n",
        "$$\n",
        "$$\n",
        "p_{\\text{neg}_1} = \\frac{1}{1 + 0.9913} = \\frac{1}{1.9913} \\approx 0.5022\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 7: Вычисление функции потерь\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\log(p_{\\text{true}}) - \\log(p_{\\text{neg}_1})\n",
        "$$\n",
        "$$\n",
        "= -\\log(0.5044) - \\log(0.5022)\n",
        "$$\n",
        "$$\n",
        "\\log(0.5044) \\approx -0.6844, \\quad \\log(0.5022) \\approx -0.6888\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L} = -(-0.6844) - (-0.6888) = 0.6844 + 0.6888 = 1.3732\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "# 2. Backpropagation (Обратное распространение ошибки)\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Вычисление градиентов для выходных весов классов $\\mathbf{u}_c$\n",
        "\n",
        "#### • Градиент для $\\mathbf{u}_{\\text{позитив}}$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{\\text{позитив}}} = (\\sigma(s_{\\text{true}}) - 1) \\cdot \\mathbf{v}_D\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (0.5044 - 1) \\cdot [0.55, 0.375] = (-0.4956) \\cdot [0.55, 0.375]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [-0.4956 \\cdot 0.55,\\ -0.4956 \\cdot 0.375] = [-0.27258,\\ -0.18585]\n",
        "$$\n",
        "\n",
        "#### • Градиент для $\\mathbf{u}_{\\text{негатив}}$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{\\text{негатив}}} = (1 - \\sigma(-s_{\\text{neg}_1})) \\cdot \\mathbf{v}_D\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (1 - 0.5022) \\cdot [0.55, 0.375] = 0.4978 \\cdot [0.55, 0.375]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= [0.4978 \\cdot 0.55,\\ 0.4978 \\cdot 0.375] = [0.27379,\\ 0.186675]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Обновление весов классов ($\\eta = 0.1$)\n",
        "\n",
        "##### • Новый $\\mathbf{u}_{\\text{позитив}}$:\n",
        "$$\n",
        "\\mathbf{u}_{\\text{позитив}}^{\\text{new}} = \\mathbf{u}_{\\text{позитив}}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{\\text{позитив}}}\n",
        "$$\n",
        "$$\n",
        "= [0.1, -0.1] - 0.1 \\cdot [-0.27258, -0.18585]\n",
        "$$\n",
        "$$\n",
        "= [0.1 - (-0.027258),\\ -0.1 - (-0.018585)] = [0.1 + 0.027258,\\ -0.1 + 0.018585]\n",
        "$$\n",
        "$$\n",
        "\\mathbf{u}_{\\text{позитив}}^{\\text{new}} = [0.127258,\\ -0.081415]\n",
        "$$\n",
        "\n",
        "##### • Новый $\\mathbf{u}_{\\text{негатив}}$:\n",
        "$$\n",
        "\\mathbf{u}_{\\text{негатив}}^{\\text{new}} = \\mathbf{u}_{\\text{негатив}}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_{\\text{негатив}}}\n",
        "$$\n",
        "$$\n",
        "= [-0.05, 0.05] - 0.1 \\cdot [0.27379, 0.186675]\n",
        "$$\n",
        "$$\n",
        "= [-0.05 - 0.027379,\\ 0.05 - 0.0186675] = [-0.077379,\\ 0.0313325]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление градиентов для вектора документа $\\mathbf{v}_D$\n",
        "\n",
        "$$\n",
        "\\delta_D = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_D} = (\\sigma(s_{\\text{true}}) - 1) \\cdot \\mathbf{u}_{\\text{позитив}}^{\\text{old}} + (1 - \\sigma(-s_{\\text{neg}_1})) \\cdot \\mathbf{u}_{\\text{негатив}}^{\\text{old}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (-0.4956) \\cdot [0.1, -0.1] + (0.4978) \\cdot [-0.05, 0.05]\n",
        "$$\n",
        "$$\n",
        "= [-0.04956, 0.04956] + [-0.02489, 0.02489] = [-0.04956 - 0.02489,\\ 0.04956 + 0.02489]\n",
        "$$\n",
        "$$\n",
        "\\delta_D = [-0.07445,\\ 0.07445]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление градиентов для входных весов n-грамм $\\mathbf{v}_g$ и их обновление\n",
        "\n",
        "Поскольку $\\mathbf{v}_D$ — среднее векторов слов, а каждый $\\mathbf{v}_{w_i}$ — сумма векторов n-грамм, то градиент по каждой n-грамме $g$, участвующей в документе, равен:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_g} = \\frac{1}{|D|} \\cdot \\delta_D\n",
        "$$\n",
        "где $|D| = 2$.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_g} = \\frac{1}{2} \\cdot [-0.07445, 0.07445] = [-0.037225,\\ 0.037225]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Обновление весов n-грамм:\n",
        "Для каждой n-граммы $g$ из $G_{\\text{хороший}}$ и $G_{\\text{фильм}}$:\n",
        "$$\n",
        "\\mathbf{v}_g^{\\text{new}} = \\mathbf{v}_g^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_g}\n",
        "$$\n",
        "$$\n",
        "= \\mathbf{v}_g^{\\text{old}} - 0.1 \\cdot [-0.037225, 0.037225] = \\mathbf{v}_g^{\\text{old}} - [-0.0037225, 0.0037225]\n",
        "$$\n",
        "\n",
        "Применим к каждой n-грамме:\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{<хор}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{<хор}}^{\\text{new}} = [0.1, 0.2] - [-0.0037225, 0.0037225] = [0.1037225, 0.1962775]\n",
        "$$\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{оро}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{оро}}^{\\text{new}} = [0.2, 0.1] - [-0.0037225, 0.0037225] = [0.2037225, 0.0962775]\n",
        "$$\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{гий}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{гий}}^{\\text{new}} = [0.25, 0.0] - [-0.0037225, 0.0037225] = [0.2537225, -0.0037225]\n",
        "$$\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{ий>}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{ий>}}^{\\text{new}} = [0.0, 0.1] - [-0.0037225, 0.0037225] = [0.0037225, 0.0962775]\n",
        "$$\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{<фил}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{<фил}}^{\\text{new}} = [0.3, 0.1] - [-0.0037225, 0.0037225] = [0.3037225, 0.0962775]\n",
        "$$\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{илм}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{илм}}^{\\text{new}} = [0.2, 0.2] - [-0.0037225, 0.0037225] = [0.2037225, 0.1962775]\n",
        "$$\n",
        "\n",
        "- **Для $\\mathbf{v}_{\\text{лм>}}$**:\n",
        "$$\n",
        "  \\mathbf{v}_{\\text{лм>}}^{\\text{new}} = [0.05, 0.05] - [-0.0037225, 0.0037225] = [0.0537225, 0.0462775]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "# Заключение по примеру\n",
        "\n",
        "Мы выполнили один шаг обучения (обработку одного обучающего примера) для FastText:\n",
        "\n",
        "- В **прямом проходе** вычислили вектор документа, счета для классов и значение функции потерь.\n",
        "- В **обратном распространении** вычислили градиенты и обновили веса:\n",
        "  - выходного слоя (векторы классов),\n",
        "  - входного слоя (векторы n-грамм).\n",
        "\n",
        "После этого шага веса были скорректированы так, чтобы:\n",
        "- **увеличить** вероятность истинного класса («позитив») для данного документа,\n",
        "- **уменьшить** вероятность отрицательного класса («негатив»).\n",
        "\n",
        "Этот процесс повторяется для всех обучающих примеров в корпусе в течение нескольких эпох, пока функция потерь не сойдётся к минимуму. С каждым шагом модель учится лучше сопоставлять документы с их истинными классами, корректируя векторы n-грамм и веса классов.\n"
      ],
      "metadata": {
        "id": "7QBDwIxU4OJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# §7. Дообучение и публикация Word2Vec, GloVe, FastText моделей на Hugging Face\n",
        "\n",
        "В этом разделе мы подробно рассмотрим процесс **дообучения** (или обучения на пользовательских данных) классических моделей векторных представлений слов — **Word2Vec**, **GloVe** и **FastText**. Также мы покажем, как подготовить эти модели и опубликовать их на **Hugging Face Hub**, что позволит легко делиться ими с сообществом или использовать в других проектах.\n",
        "\n",
        "В качестве исходных данных будет использоваться файл в формате **JSONL**, содержащий несколько тысяч статей.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Подготовка данных: Загрузка и предобработка JSONL-статей\n",
        "\n",
        "Прежде чем обучать модель векторных представлений слов, необходимо загрузить и подготовить текстовые данные. В нашем случае статьи хранятся в формате **JSONL**, где каждая строка — это отдельный JSON-объект.\n",
        "\n",
        "Пример файла `articles.jsonl`:\n",
        "```json\n",
        "{\"id\": 1, \"title\": \"Введение в NLP\", \"text\": \"Обработка естественного языка — это область искусственного интеллекта...\"}\n",
        "{\"id\": 2, \"title\": \"Новые методы векторизации\", \"text\": \"Современные подходы к представлению текста включают Word2Vec, GloVe и FastText...\"}\n",
        "```\n",
        "\n",
        "Для загрузки и предобработки (токенизация, приведение к нижнему регистру) используем Python.\n",
        "\n",
        "### Шаг 1: Загрузка данных из JSONL\n",
        "\n",
        "Прочитаем файл построчно и извлечём текст каждой статьи.\n",
        "\n",
        "### Шаг 2: Токенизация и приведение к нижнему регистру\n",
        "\n",
        "Для обучения моделей Word2Vec, GloVe и FastText текст должен быть разбит на токены и, как правило, приведён к нижнему регистру — это уменьшает размер словаря и улучшает качество эмбеддингов. Мы будем использовать `nltk.word_tokenize` для токенизации.\n",
        "\n"
      ],
      "metadata": {
        "id": "oPFHFhFkh-m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Загрузка токенизатора (если еще не загружен)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def load_and_preprocess_articles(jsonl_filepath, text_field='text'):\n",
        "    \"\"\"\n",
        "    Загружает статьи из JSONL-файла, токенизирует их и приводит к нижнему регистру.\n",
        "\n",
        "    Args:\n",
        "        jsonl_filepath (str): Путь к JSONL-файлу.\n",
        "        text_field (str): Название поля, содержащего текст статьи.\n",
        "\n",
        "    Returns:\n",
        "        list: Список списков токенов (каждый список — одна статья).\n",
        "    \"\"\"\n",
        "    tokenized_sentences = []\n",
        "    print(f\"Загрузка и предобработка статей из '{jsonl_filepath}'...\")\n",
        "\n",
        "    # Подсчёт количества строк для прогресс-бара\n",
        "    total_lines = sum(1 for _ in open(jsonl_filepath, 'r', encoding='utf-8'))\n",
        "\n",
        "    with open(jsonl_filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, total=total_lines, desc=\"Обработка статей\"):\n",
        "            try:\n",
        "                article = json.loads(line.strip())\n",
        "                text = article.get(text_field, \"\")\n",
        "                if text:\n",
        "                    tokens = word_tokenize(text.lower())\n",
        "                    tokenized_sentences.append(tokens)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Ошибка декодирования JSON в строке: {line.strip()}. Ошибка: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Неизвестная ошибка при обработке строки: {line.strip()}. Ошибка: {e}\")\n",
        "\n",
        "    print(f\"Загружено и токенизировано {len(tokenized_sentences)} статей.\")\n",
        "    return tokenized_sentences\n",
        "\n",
        "# Создание фиктивного JSONL-файла для демонстрации\n",
        "dummy_articles_data = [\n",
        "    {\"id\": 1, \"title\": \"Введение в NLP\", \"text\": \"Обработка естественного языка — это область искусственного интеллекта, которая изучает взаимодействие компьютеров и человеческого языка.\"},\n",
        "    {\"id\": 2, \"title\": \"Новые методы векторизации\", \"text\": \"Современные подходы к представлению текста включают Word2Vec, GloVe и FastText. Эти модели преобразуют слова в числовые векторы.\"},\n",
        "    {\"id\": 3, \"title\": \"Применение машинного обучения\", \"text\": \"Машинное обучение используется во многих областях, таких как распознавание изображений, прогнозирование и анализ данных.\"},\n",
        "    {\"id\": 4, \"title\": \"Будущее ИИ\", \"text\": \"Искусственный интеллект продолжает развиваться, открывая новые возможности и вызовы для человечества.\"},\n",
        "    {\"id\": 5, \"title\": \"Основы программирования\", \"text\": \"Изучение основ программирования важно для любого, кто хочет работать с данными или разрабатывать программное обеспечение.\"}\n",
        "]\n",
        "\n",
        "dummy_jsonl_filepath = \"articles.jsonl\"\n",
        "with open(dummy_jsonl_filepath, 'w', encoding='utf-8') as f:\n",
        "    for item in dummy_articles_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# Загрузка и предобработка данных\n",
        "articles_data = load_and_preprocess_articles(dummy_jsonl_filepath)\n",
        "\n",
        "# Проверка: вывод первых нескольких токенизированных статей\n",
        "print(\"\\nПервые 2 токенизированные статьи:\")\n",
        "for i, tokens in enumerate(articles_data[:2]):\n",
        "    print(f\"Статья {i+1}: {tokens[:20]}...\")  # Первые 20 токенов"
      ],
      "metadata": {
        "id": "rLURQoR7j2ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. Дообучение Word2Vec (CBOW, Skip-gram)\n",
        "\n",
        "Word2Vec — это семейство моделей, которые обучаются на гипотезе распределения: *слова, появляющиеся в похожих контекстах, имеют схожие значения*. Модель генерирует плотные векторные представления слов (эмбеддинги), эффективно захватывая семантические и синтаксические отношения.\n",
        "\n",
        "В библиотеке `gensim` термин «дообучение» может означать:\n",
        "- **обучение модели с нуля на вашем корпусе**, или\n",
        "- **продолжение обучения (обновление) уже предварительно обученной модели** на новых данных.\n",
        "\n",
        "### • Обучение с нуля\n",
        "Если у вас есть достаточно большой и тематически специфичный корпус (несколько тысяч статей), часто лучше обучить Word2Vec с нуля. Это позволяет модели уловить доменные особенности, характерные именно для вашего текста.\n",
        "\n",
        "### • Продолжение обучения (fine-tuning)\n",
        "Если у вас уже есть предобученная модель (например, на Wikipedia), и вы хотите адаптировать её к вашему домену, можно продолжить обучение на новых данных. Этот подход эффективен, когда ваш корпус дополняет общую тематику исходного корпуса.\n",
        "\n",
        "Ниже приведён пример **обучения с нуля**, так как для корпуса из нескольких тысяч статей это часто даёт хорошие результаты. Также продемонстрировано, как можно **обновить существующую модель**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5TVOxknCj2i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка библиотеки (только в Colab)\n",
        "#!pip install -q gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import logging\n",
        "\n",
        "# === Включение логирования gensim ===\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# === 0. Пример текстов для обучения (можно заменить своими) ===\n",
        "articles_raw = [\n",
        "    \"Искусственный интеллект — это область информатики.\",\n",
        "    \"Обработка естественного языка — важная задача машинного обучения.\",\n",
        "    \"Нейронные сети широко применяются в анализе текста.\",\n",
        "    \"Язык — один из ключевых элементов человеческой культуры.\",\n",
        "    \"Машинное обучение помогает анализировать большие объёмы данных.\"\n",
        "]\n",
        "\n",
        "# === 1. Токенизация текста ===\n",
        "articles_data = [simple_preprocess(doc) for doc in articles_raw]\n",
        "print(\"Пример токенизированных данных:\", articles_data[:2])\n",
        "\n",
        "# === 2. Параметры модели Word2Vec ===\n",
        "vector_size = 100     # размерность векторов\n",
        "window = 10           # окно контекста\n",
        "min_count = 1         # минимальная частота слов\n",
        "workers = 4           # количество потоков\n",
        "epochs = 10           # эпохи обучения\n",
        "seed = 42             # для воспроизводимости\n",
        "\n",
        "# === 3. Обучение модели Word2Vec (Skip-gram) ===\n",
        "print(\"\\n[1] Обучение Word2Vec (Skip-gram)...\")\n",
        "model_w2v_skipgram = Word2Vec(\n",
        "    sentences=articles_data,\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    sg=1,  # Skip-gram\n",
        "    workers=workers,\n",
        "    epochs=epochs,\n",
        "    seed=seed\n",
        ")\n",
        "print(\"✅ Skip-gram модель обучена.\")\n",
        "\n",
        "# === 4. Тестирование Skip-gram ===\n",
        "test_words = ['интеллект', 'язык']\n",
        "for word in test_words:\n",
        "    try:\n",
        "        print(f\"\\n— Вектор для слова '{word}': {model_w2v_skipgram.wv[word][:5]}\")\n",
        "        print(f\"  Похожие слова: {model_w2v_skipgram.wv.most_similar(word, topn=3)}\")\n",
        "    except KeyError:\n",
        "        print(f\"⚠️ Слово '{word}' отсутствует в словаре.\")\n",
        "\n",
        "# === 5. Обучение модели Word2Vec (CBOW) ===\n",
        "print(\"\\n[2] Обучение Word2Vec (CBOW)...\")\n",
        "model_w2v_cbow = Word2Vec(\n",
        "    sentences=articles_data,\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    sg=0,  # CBOW\n",
        "    workers=workers,\n",
        "    epochs=epochs,\n",
        "    seed=seed\n",
        ")\n",
        "print(\"✅ CBOW модель обучена.\")\n",
        "\n",
        "# === 6. Тестирование CBOW ===\n",
        "try:\n",
        "    print(f\"\\n— Похожие слова к 'анализ': {model_w2v_cbow.wv.most_similar('анализ', topn=3)}\")\n",
        "except KeyError:\n",
        "    print(\"⚠️ Слово 'анализ' отсутствует в словаре.\")\n",
        "\n",
        "# === 7. Дообучение существующей модели Word2Vec ===\n",
        "print(\"\\n[3] Продолжение обучения модели...\")\n",
        "\n",
        "# Базовый корпус\n",
        "base_corpus = [[\"машина\", \"едет\", \"по\", \"дороге\"], [\"человек\", \"идет\", \"по\", \"тротуару\"]]\n",
        "\n",
        "# Создание базовой модели\n",
        "base_model = Word2Vec(\n",
        "    sentences=base_corpus,\n",
        "    vector_size=vector_size,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=workers,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "# Обновление словаря и дообучение\n",
        "base_model.build_vocab(articles_data, update=True)\n",
        "base_model.train(articles_data, total_examples=base_model.corpus_count, epochs=epochs)\n",
        "print(\"✅ Базовая модель дообучена.\")\n",
        "\n",
        "# === 8. Сохранение всех моделей ===\n",
        "model_w2v_skipgram.save(\"word2vec_skipgram.model\")\n",
        "model_w2v_cbow.save(\"word2vec_cbow.model\")\n",
        "base_model.save(\"word2vec_base_extended.model\")\n",
        "\n",
        "print(\"\\n📦 Все модели Word2Vec успешно сохранены:\")\n",
        "print(\" - word2vec_skipgram.model\")\n",
        "print(\" - word2vec_cbow.model\")\n",
        "print(\" - word2vec_base_extended.model\")"
      ],
      "metadata": {
        "id": "SbLj7wsum8An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Дообучение GloVe\n",
        "\n",
        "**GloVe (Global Vectors for Word Representation)** — это модель, которая обучается на **глобальной статистике совместной встречаемости слов**. В отличие от Word2Vec, основывающегося на контекстных окнах и предсказании слов, GloVe использует матрицу слово-слово, где каждый элемент — это частота, с которой одно слово появляется в контексте другого.\n",
        "\n",
        "Библиотека `gensim` не предоставляет встроенной реализации обучения GloVe с нуля. Поэтому \"дообучение\" GloVe в привычном смысле (как у трансформеров) не является стандартной практикой. Если вы хотите адаптировать GloVe к вашему домену, **наиболее эффективный способ — обучить новую модель с нуля на вашем корпусе**.\n",
        "\n",
        "### Процесс обучения GloVe\n",
        "\n",
        "Обучение GloVe включает два основных этапа:\n",
        "\n",
        "1. **Построение матрицы совместной встречаемости**:  \n",
        "   Подсчёт, сколько раз каждое слово встречается в контексте каждого другого слова в корпусе. Учитывается расстояние между словами (ближайшие слова весят больше).\n",
        "\n",
        "2. **Оптимизация функции потерь**:  \n",
        "   Итеративное обновление векторов слов так, чтобы скалярное произведение векторов приближалось к логарифму частоты их совместного появления.\n",
        "\n",
        "Ниже мы покажем, как построить матрицу совместной встречаемости — **первый и наиболее важный шаг**. Для самой оптимизации потребуются специализированные библиотеки (например, `glove-python`, `torch`, `gensim` с кастомным решением).\n",
        "\n"
      ],
      "metadata": {
        "id": "KZPmeCtLo5uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "import logging\n",
        "\n",
        "# Логгирование\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "print(\"\\n--- Подготовка данных для GloVe (матрица совместной встречаемости) ---\")\n",
        "\n",
        "# === Проверка и подготовка данных ===\n",
        "try:\n",
        "    assert 'articles_data' in globals()\n",
        "except AssertionError:\n",
        "    raise ValueError(\"Переменная `articles_data` не определена. Задайте её как список списков токенов.\")\n",
        "\n",
        "# === Параметры ===\n",
        "window_size = 10        # Контекстное окно\n",
        "vector_size = 100       # Размерность векторов (должен совпадать с GloVe embedding размером)\n",
        "\n",
        "# === Создание словаря: слово -> индекс ===\n",
        "word_to_idx = {}\n",
        "idx_to_word = []\n",
        "\n",
        "for sentence in articles_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(idx_to_word)\n",
        "            idx_to_word.append(word)\n",
        "\n",
        "vocab_size = len(word_to_idx)\n",
        "print(f\"📏 Размер словаря для GloVe: {vocab_size} уникальных слов.\")\n",
        "\n",
        "# === Инициализация разреженной матрицы (для экономии памяти) ===\n",
        "co_occurrence_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "# === Функция взвешивания по расстоянию: чем ближе — тем выше вес ===\n",
        "def distance_weight(distance: int) -> float:\n",
        "    return 1.0 / distance if distance != 0 else 0.0\n",
        "\n",
        "# === Построение матрицы совместной встречаемости ===\n",
        "print(\"🔄 Построение матрицы совместной встречаемости...\")\n",
        "for sentence in tqdm(articles_data, desc=\"Building co-occurrence matrix\"):\n",
        "    for i, target_word in enumerate(sentence):\n",
        "        target_idx = word_to_idx[target_word]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(sentence), i + window_size + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if i == j:\n",
        "                continue\n",
        "            context_word = sentence[j]\n",
        "            context_idx = word_to_idx[context_word]\n",
        "            distance = abs(i - j)\n",
        "            weight = distance_weight(distance)\n",
        "\n",
        "            co_occurrence_matrix[target_idx, context_idx] += weight\n",
        "\n",
        "print(\"✅ Матрица совместной встречаемости построена.\")\n",
        "\n",
        "# === (опционально) преобразуем в CSR для последующей работы ===\n",
        "# co_occurrence_matrix = co_occurrence_matrix.tocsr()\n",
        "\n",
        "# === Демонстрация: фиктивные эмбеддинги GloVe (для теста, не обучение) ===\n",
        "print(\"\\n🎲 Генерация фиктивных GloVe-векторов...\")\n",
        "glove_embeddings_dict = {\n",
        "    word: np.random.rand(vector_size) for word in word_to_idx\n",
        "}\n",
        "\n",
        "# === Сохранение в формате KeyedVectors (Gensim совместимо) ===\n",
        "print(\"💾 Сохранение в формате Gensim KeyedVectors...\")\n",
        "glove_kv = KeyedVectors(vector_size=vector_size)\n",
        "glove_kv.add_vectors(\n",
        "    list(glove_embeddings_dict.keys()),\n",
        "    np.array(list(glove_embeddings_dict.values()))\n",
        ")\n",
        "glove_kv.save(\"glove_vectors.kv\")\n",
        "print(\"✅ Фиктивные GloVe-векторы сохранены: glove_vectors.kv\")"
      ],
      "metadata": {
        "id": "f3ZaaL6vqPVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Загрузка сохранённой модели\n",
        "glove_kv = KeyedVectors.load(\"glove_vectors.kv\")\n",
        "\n",
        "word = \"язык\"\n",
        "if word in glove_kv:\n",
        "    vector = glove_kv[word]\n",
        "    print(f\"🔢 Вектор для слова '{word}' (первые 5 значений):\\n{vector[:5]}\")\n",
        "else:\n",
        "    print(f\"⚠️ Слово '{word}' отсутствует в словаре.\")\n",
        "\n",
        "try:\n",
        "    similar_words = glove_kv.most_similar(\"интеллект\", topn=5)\n",
        "    print(\"🔍 Похожие слова к 'интеллект':\")\n",
        "    for word, score in similar_words:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "except KeyError:\n",
        "    print(\"⚠️ Слово 'интеллект' отсутствует в словаре.\")\n",
        "\n",
        "try:\n",
        "    sim = glove_kv.similarity(\"язык\", \"текст\")\n",
        "    print(f\"📏 Косинусная похожесть между 'язык' и 'текст': {sim:.4f}\")\n",
        "except KeyError as e:\n",
        "    print(f\"⚠️ Слово {e} не найдено в словаре.\")\n",
        "\n",
        "# Минимальный тест после загрузки модели\n",
        "glove_kv = KeyedVectors.load(\"glove_vectors.kv\")\n",
        "\n",
        "for test_word in [\"язык\", \"текст\", \"интеллект\"]:\n",
        "    if test_word in glove_kv:\n",
        "        print(f\"\\nСлово: {test_word}\")\n",
        "        print(\"  Вектор (первые 5 значений):\", glove_kv[test_word][:5])\n",
        "        print(\"  Похожие слова:\", glove_kv.most_similar(test_word, topn=3))\n",
        "    else:\n",
        "        print(f\"⚠️ Слово '{test_word}' отсутствует.\")"
      ],
      "metadata": {
        "id": "I3CxWH7cqy5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Дообучение FastText\n",
        "\n",
        "**FastText** — это расширение модели Word2Vec, разработанное командой Facebook AI. В отличие от Word2Vec, FastText учитывает **морфологическую структуру слов**, разбивая их на **символьные n-граммы** (например, слово *\"программирование\"* может быть разбито на `<про`, `про`, `рограм`, `огам` и т.д.). Это позволяет модели:\n",
        "\n",
        "- Генерировать векторы для **неизвестных слов (OOV — out-of-vocabulary)**.\n",
        "- Лучше работать с **морфологически богатыми языками**, такими как русский, арабский или турецкий.\n",
        "- Улавливать семантическое сходство между словами с общими корнями (например, \"программист\", \"программирование\", \"программа\").\n",
        "\n",
        "Процесс дообучения FastText в `gensim` аналогичен Word2Vec и поддерживает два подхода:\n",
        "\n",
        "### • Обучение с нуля  \n",
        "Рекомендуется, если у вас есть достаточно большой и тематически релевантный корпус. Это позволяет модели уловить доменные особенности.\n",
        "\n",
        "### • Продолжение обучения (fine-tuning)  \n",
        "FastText поддерживает дообучение уже существующей модели на новых данных — полезно для адаптации предобученных моделей к вашему домену.\n"
      ],
      "metadata": {
        "id": "sm_eJjSWh-sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "print(\"\\n--- Обучение FastText ---\")\n",
        "\n",
        "# ==== Подготовка данных ====\n",
        "# Пример текстов\n",
        "raw_texts = [\n",
        "    \"Программирование — это процесс создания программ.\",\n",
        "    \"Разработчик пишет код и тестирует программные продукты.\",\n",
        "    \"Искусственный интеллект и машинное обучение развиваются быстро.\"\n",
        "]\n",
        "\n",
        "# Токенизация и очистка\n",
        "stop_words = set(stopwords.words(\"russian\"))\n",
        "articles_data = []\n",
        "for text in raw_texts:\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    articles_data.append(words)\n",
        "\n",
        "# ==== Параметры ====\n",
        "vector_size = 100\n",
        "window = 5\n",
        "min_count = 1\n",
        "workers = 4\n",
        "epochs = 10\n",
        "min_n = 3\n",
        "max_n = 6\n",
        "\n",
        "# ==== Обучение модели ====\n",
        "model_fasttext = FastText(\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    sg=1,\n",
        "    workers=workers,\n",
        "    min_n=min_n,\n",
        "    max_n=max_n,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Построение словаря\n",
        "model_fasttext.build_vocab(corpus_iterable=articles_data)\n",
        "\n",
        "# Обучение модели\n",
        "model_fasttext.train(\n",
        "    corpus_iterable=articles_data,\n",
        "    total_examples=len(articles_data),\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "print(\"Модель FastText обучена.\")\n",
        "\n",
        "# ==== Тестирование ====\n",
        "try:\n",
        "    print(f\"\\nВектор слова 'программирование': {model_fasttext.wv['программирование'][:5]}\")\n",
        "    print(f\"Похожие слова: {model_fasttext.wv.most_similar('программирование', topn=3)}\")\n",
        "except KeyError:\n",
        "    print(\"Слово 'программирование' отсутствует в словаре.\")\n",
        "\n",
        "# ==== OOV ====\n",
        "oov_test_word = \"разработчик\"\n",
        "if oov_test_word in model_fasttext.wv:\n",
        "    print(f\"\\nСлово '{oov_test_word}' найдено в словаре.\")\n",
        "else:\n",
        "    print(f\"\\nСлово '{oov_test_word}' не найдено, FastText сгенерирует вектор.\")\n",
        "print(f\"Вектор для '{oov_test_word}': {model_fasttext.wv[oov_test_word][:5]}\")\n",
        "print(f\"Похожие слова: {model_fasttext.wv.most_similar(oov_test_word, topn=3)}\")\n",
        "\n",
        "# ==== Сохранение ====\n",
        "model_fasttext.save(\"fasttext_model.model\")\n",
        "print(\"\\nМодель FastText сохранена.\")"
      ],
      "metadata": {
        "id": "fog3BvfkrkBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# §8. Ограничения статических эмбеддингов: Подробное описание\n",
        "\n",
        "Модели **Word2Vec**, **GloVe** и **FastText** стали прорывом в обработке естественного языка (NLP), позволив моделям работать с семантикой слов, а не только с их поверхностной формой. Однако все они относятся к категории **статических эмбеддингов** — это означает, что каждому слову в словаре присваивается **один фиксированный вектор**, который **не меняется** в зависимости от контекста.\n",
        "\n",
        "Это фундаментальное ограничение приводит к серьёзным проблемам, мешающим моделям глубоко понимать язык.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Проблема многозначности (полисемии)\n",
        "\n",
        "**Полисемия** — явление, при котором одно слово имеет несколько связанных, но различных значений. В естественном языке это повсеместно. Статические эмбеддинги не могут эффективно справляться с этим, так как генерируют **один усреднённый вектор**, который пытается охватить все значения слова, увиденные в обучающем корпусе.\n",
        "\n",
        "### • Суть проблемы\n",
        "Например, слово **\"банк\"** может означать:\n",
        "- финансовое учреждение,\n",
        "- берег реки,\n",
        "- песчаный бархан.\n",
        "\n",
        "Модель усредняет все эти контексты в один вектор, который оказывается где-то «между» финансовыми и географическими понятиями.\n",
        "\n",
        "### • Последствия\n",
        "- **Неточное представление смысла**: Усреднённый вектор не отражает конкретное значение в предложении.\n",
        "- **Снижение производительности**: В задачах, где важно понимание контекста (например, QA, анализ настроений), модель даёт ошибочные ответы.\n",
        "- **Ошибки в поиске похожих слов**: При запросе \"похожие на 'банк'\" модель может выдать как \"кредит\", так и \"река\", что затрудняет интерпретацию.\n",
        "\n",
        "### • Аналитический пример: слово \"замок\"\n",
        "1. **Архитектурное сооружение**:  \n",
        "   *\"Старинный замок возвышался над долиной.\"*  \n",
        "   → Связано с: *дворец, крепость, башня*.\n",
        "2. **Запирающее устройство**:  \n",
        "   *\"Я потерял замок от двери.\"*  \n",
        "   → Связано с: *ключ, дверь, открывать*.\n",
        "\n",
        "Со статическими эмбеддингами вектор `V_замок` будет компромиссом между этими двумя значениями. При поиске ближайших слов модель может вернуть и \"крепость\", и \"ключ\", не понимая, какое значение актуально в текущем контексте.\n",
        "\n",
        "> 🧠 **Концептуально**: вектор \"замок\" находится между \"крепость\" и \"ключ\", что иллюстрирует его неоднозначность.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Отсутствие контекста\n",
        "\n",
        "Статические эмбеддинги не учитывают, как слово взаимодействует с другими словами в предложении. Вектор остаётся неизменным, даже если контекст кардинально меняет его смысл.\n",
        "\n",
        "### • Суть проблемы\n",
        "Модель не понимает:\n",
        "- синтаксические зависимости,\n",
        "- отрицания,\n",
        "- идиомы,\n",
        "- тонкие нюансы, возникающие из комбинации слов.\n",
        "\n",
        "### • Последствия\n",
        "- **Неспособность понимать отрицание и сарказм**:  \n",
        "  Не может распознать, что *\"не хороший\"* — это отрицание, а не просто наличие слова \"хороший\".\n",
        "- **Проблемы с идиомами**:  \n",
        "  Выражение *\"бить баклуши\"* (бездельничать) теряет смысл, если анализировать \"бить\" и \"баклуши\" отдельно.\n",
        "- **Потеря синтаксической информации**:  \n",
        "  Порядок слов и грамматические конструкции не отражаются в фиксированных векторах.\n",
        "\n",
        "### • Пример: влияние отрицания\n",
        "1. *\"Этот фильм хороший.\"* → **Положительное** настроение.  \n",
        "2. *\"Этот фильм не хороший.\"* → **Отрицательное** настроение.\n",
        "\n",
        "В обоих случаях вектор `V_хороший` одинаков. Модель видит слово \"хороший\" и может ошибочно классифицировать второе предложение как позитивное, игнорируя частицу \"не\".\n",
        "\n",
        "> 🔍 **Концептуальное сравнение**:\n",
        "> - `V_фильм + V_хороший` — положительное.\n",
        "> - `V_фильм + V_не + V_хороший` — всё ещё близко к положительному, если модель не умеет обрабатывать модификаторы.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Неспособность к обучению на новых данных (без полного переобучения)\n",
        "\n",
        "После обучения статические эмбеддинги становятся **фиксированными**. Их нельзя легко обновить или адаптировать к новым словам или изменениям в языке без **полного переобучения** на новом корпусе.\n",
        "\n",
        "### • Суть проблемы\n",
        "Язык динамичен:\n",
        "- появляются неологизмы,\n",
        "- меняются значения слов,\n",
        "- возникает сленг и аббревиатуры.\n",
        "\n",
        "Статические модели не могут естественно адаптироваться к этим изменениям.\n",
        "\n",
        "### • Последствия\n",
        "- **Проблема OOV (Out-of-Vocabulary)**:  \n",
        "  - Word2Vec и GloVe **не могут** генерировать векторы для новых слов.  \n",
        "  - FastText может, но на основе подслов, что не всегда оптимально.\n",
        "- **Устаревание эмбеддингов**:  \n",
        "  Векторы перестают отражать современное употребление слов.\n",
        "- **Высокие затраты на обновление**:  \n",
        "  Переобучение на больших корпусах требует огромных вычислительных ресурсов.\n",
        "- **Неэффективность в динамических доменах**:  \n",
        "  В технологиях, медицине, соцсетях терминология быстро меняется, и статические эмбеддинги быстро устаревают.\n",
        "\n",
        "### • Пример: эволюция слова \"смартфон\"\n",
        "- **2010 год**: контекст — *телефон, мобильный, звонок*.  \n",
        "- **2020 год**: контекст — *приложения, камера, интернет, соцсети*.\n",
        "\n",
        "Если модель не переобучена, её вектор для \"смартфон\" остаётся \"замороженным\" в 2010-х, не отражая современного смысла.  \n",
        "Новое слово *\"зумить\"* (проводить встречу в Zoom) останется OOV, если его не было в исходном корпусе.\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Ограничения статических эмбеддингов — **полисемия**, **отсутствие контекста** и **неспособность к адаптации** — стали катализатором для разработки **контекстно-зависимых эмбеддингов**.\n",
        "\n",
        "Это привело к появлению моделей, таких как **ELMo**, **BERT**, **GPT** и других на основе **архитектуры Трансформера**, которые генерируют векторы **динамически**, в зависимости от контекста. Такие модели:\n",
        "- понимают разные значения одного слова,\n",
        "- учитывают отрицания и идиомы,\n",
        "- могут адаптироваться к новым данным через дообучение.\n",
        "\n",
        "Таким образом, контекстно-зависимые эмбеддинги преодолели ключевые ограничения статических моделей, открыв новую эру в понимании естественного языка.\n"
      ],
      "metadata": {
        "id": "pdo926SOr9XP"
      }
    }
  ]
}