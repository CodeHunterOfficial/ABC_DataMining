{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNHn5Pg2dGvs3B34/Y4PEqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/WordPiece_%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordPiece Токенизатор**  \n",
        "WordPiece Токенизатор представляет собой элегантный гибридный подход, разработанный для нахождения оптимального баланса между токенизацией на уровне слов и символов. Он разбивает слова на более мелкие, но семантически осмысленные единицы, называемые подсловами (subwords). Например, слово \"неизвестные\" может быть токенизировано как \"не\", \"##извест\", \"##ные\". Префикс ## указывает, что данный подтокен является продолжением предыдущего токена, а не началом нового слова.  \n",
        "Основная идея WordPiece: Построить словарь токенов (включающий символы, подслова и, при необходимости, целые слова) таким образом, чтобы он был достаточно компактным, но при этом позволял эффективно представлять как известные, так и неизвестные слова, сохраняя при этом их лингвистический смысл.\n",
        "\n",
        "**1. Принцип Работы WordPiece: Пошаговое Объяснение**  \n",
        "Алгоритм WordPiece, подобно BPE, является итеративным методом, но отличается критерием выбора пар для слияния. Вместо простой частоты встречаемости, WordPiece использует метрику, основанную на вероятности или правдоподобии.  \n",
        "Возьмем наш пример текста для обучения токенизатора: \"В училище учитель учит ученикам по новому учебнику.\"\n",
        "\n",
        "**1.1. Шаг 1: Предварительная Обработка Текста и Создание Начального Словаря**  \n",
        "Перед началом обучения текст подвергается предварительной обработке:  \n",
        " Приведение к нижнему регистру: Все буквы переводятся в строчные, чтобы \"Учитель\" и \"учитель\" считались одним и тем же словом.  \n",
        " Удаление знаков препинания: Знаки препинания (кроме тех, что являются частью слова, например, в сокращениях, но в нашем примере их нет) удаляются или отделяются.  \n",
        " Разделение на слова: Текст сначала разбивается на слова по пробелам.  \n",
        "Обработанный текст: \"в училище учитель учит ученикам по новому учебнику\"  \n",
        "Начальный словарь (V₀): Начинаем с самого простого: каждый уникальный символ в нашем обработанном тексте становится отдельным токеном в нашем словаре. Пробел ( ) также является важным токеном, так как он помогает модели понимать границы слов.  \n",
        "Уникальные символы в тексте: 'в', ' ', 'у', 'ч', 'и', 'л', 'щ', 'е', 'т', 'р', 'ь', 'н', 'к', 'а', 'м', 'п', 'о', 'б', 'ю'.  \n",
        "V₀ = {'в', ' ', 'у', 'ч', 'и', 'л', 'щ', 'е', 'т', 'р', 'ь', 'н', 'к', 'а', 'м', 'п', 'о', 'б', 'ю'} (если бы были другие символы).  \n",
        "Теперь мы можем представить весь наш обработанный текст как последовательность этих начальных токенов. Например, слово \"училище\" будет представлено как ['у', 'ч', 'и', 'л', 'и', 'щ', 'е'].  \n",
        "Полная последовательность токенов в корпусе (после предварительной обработки): ['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'л', 'ь', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']  \n",
        "Общее количество токенов в корпусе: 50.\n",
        "\n",
        "**1.2. Шаг 2: Итеративное Слияние (Обучение Словаря)**  \n",
        "Это ядро алгоритма WordPiece. Мы будем итеративно (повторяющимися шагами) объединять пары соседних токенов (биграммы), чтобы создавать новые, более длинные токены. Цель — найти такие пары, которые, будучи объединенными, максимально увеличивают \"вероятность\" (или \"правдоподобие\") нашего обучающего текста.  \n",
        "\n",
        "**Формула оценки слияния (Score):** Для каждой пары соседних токенов A и B (биграммы AB) мы вычисляем \"оценку слияния\" по следующей формуле:  \n",
        "$$\n",
        "\\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$  \n",
        "Где:  \n",
        "• $f(A, B)$ — это количество раз, когда биграмма AB (токен A, за которым сразу следует токен B) встречается в текущем токенизированном корпусе.  \n",
        "• $f(A)$ — это количество раз, когда токен A встречается в текущем токенизированном корпусе.  \n",
        "• $f(B)$ — это количество раз, когда токен B встречается в текущем токенизированном корпусе.  \n",
        "\n",
        "Данная формула измеряет, насколько часто A и B встречаются **вместе**, по сравнению с тем, насколько часто они встречаются **по отдельности**. Высокий балл означает, что A и B **очень часто появляются вместе**, и их слияние в один токен будет \"полезным\" для модели, так как оно позволяет более компактно (с меньшим количеством токенов) представить текст. Это эквивалентно максимизации правдоподобия обучающих данных, если мы рассматриваем процесс токенизации как генерацию последовательности токенов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Пример итераций с нашим текстом:**\n",
        "\n",
        "**Итерация 1: Поиск первой лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'л', 'ь', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`\n",
        "\n",
        "- **Подсчет частот отдельных токенов (униграмм) в текущем корпусе:**\n",
        "  - $f(\\text{' '}) = 8$\n",
        "  - $f(\\text{'у'}) = 5$\n",
        "  - $f(\\text{'ч'}) = 5$\n",
        "  - $f(\\text{'и'}) = 5$\n",
        "  - $f(\\text{'л'}) = 2$\n",
        "  - $f(\\text{'щ'}) = 1$\n",
        "  - $f(\\text{'е'}) = 4$\n",
        "  - $f(\\text{'т'}) = 2$\n",
        "  - $f(\\text{'ь'}) = 1$\n",
        "  - $f(\\text{'н'}) = 2$\n",
        "  - $f(\\text{'к'}) = 2$\n",
        "  - $f(\\text{'а'}) = 1$\n",
        "  - $f(\\text{'м'}) = 2$\n",
        "  - $f(\\text{'п'}) = 1$\n",
        "  - $f(\\text{'о'}) = 3$\n",
        "  - $f(\\text{'в'}) = 2$ (один раз в начале \"в\", один — в \"новому\")\n",
        "  - $f(\\text{'б'}) = 1$\n",
        "\n",
        "\n",
        "- **Подсчет частот биграмм (пар соседних токенов):**\n",
        "  - $f(\\text{'у', 'ч'}) = 5$ — в \"училище\", \"учитель\", \"учит\" (дважды), \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'ч', 'и'}) = 5$ — аналогично\n",
        "  - $f(\\text{'и', 'л'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'л', 'и'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'и', 'щ'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'щ', 'е'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'е', ' '}) = 2$ — после \"училище\", \"учитель\"\n",
        "  - $f(\\text{'и', 'т'}) = 2$ — в \"учит\" (дважды)\n",
        "  - $f(\\text{'т', 'е'}) = 1$ — в \"учитель\"\n",
        "  - $f(\\text{'е', 'л'}) = 1$ — в \"учитель\"\n",
        "  - $f(\\text{'л', 'ь'}) = 1$ — в \"учитель\"\n",
        "  - $f(\\text{'ч', 'е'}) = 2$ — в \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'е', 'н'}) = 1$ — в \"ученикам\"\n",
        "  - $f(\\text{'н', 'и'}) = 2$ — в \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'и', 'к'}) = 2$ — в \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'к', 'а'}) = 1$ — в \"ученикам\"\n",
        "  - $f(\\text{'а', 'м'}) = 1$ — в \"ученикам\"\n",
        "  - $f(\\text{' ', 'п'}) = 1$ — перед \"по\"\n",
        "  - $f(\\text{'п', 'о'}) = 1$ — в \"по\"\n",
        "  - $f(\\text{'о', ' '}) = 1$ — после \"по\"\n",
        "  - $f(\\text{'н', 'о'}) = 2$ — в \"новому\", \"учебнику\"\n",
        "  - $f(\\text{'о', 'в'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'в', 'о'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'о', 'м'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'м', 'у'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'у', ' '}) = 4$ — после \"в\", \"училище\", \"учитель\", \"новому\"\n",
        "  - $f(\\text{'е', 'б'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'б', 'н'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'н', 'и'}) = 2$ — уже учтено\n",
        "  - $f(\\text{'и', 'к'}) = 2$ — уже учтено\n",
        "  - $f(\\text{'к', 'у'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'в', ' '}) = 1$ — после слова \"в\"\n",
        "\n",
        "\n",
        "- **Вычисление оценок слияния по формуле:**  \n",
        "$$\n",
        "  \\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "\n",
        "  Примеры:\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = \\frac{5}{25} = 0.2$\n",
        "  - $\\text{Score}(\\text{'ч', 'и'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "  - $\\text{Score}(\\text{'л', 'ь'}) = \\frac{1}{2 \\times 1} = \\frac{1}{2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'к', 'а'}) = \\frac{1}{2 \\times 1} = 0.5$\n",
        "  - $\\text{Score}(\\text{'а', 'м'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'п', 'о'}) = \\frac{1}{1 \\times 3} \\approx 0.333$\n",
        "  - $\\text{Score}(\\text{'н', 'о'}) = \\frac{2}{2 \\times 3} \\approx 0.333$\n",
        "  - $\\text{Score}(\\text{'щ', 'е'}) = \\frac{1}{1 \\times 4} = 0.25$\n",
        "  - $\\text{Score}(\\text{'е', 'б'}) = \\frac{1}{4 \\times 1} = 0.25$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $\\text{Score} = 0.5$ имеют биграммы: `'л', 'ь'`, `'к', 'а'`, `'а', 'м'`, `'б', 'н'`.  \n",
        "  При равенстве оценок алгоритм может выбрать первую по порядку в лексикографическом или в порядке появления.  \n",
        "  Предположим, выбирается первая появляющаяся такая пара — `'л', 'ь'` (в слове \"учитель\").\n",
        "\n",
        "- **Слияние `'л', 'ь'`:**  \n",
        "  Все вхождения последовательности `['л', 'ь']` заменяются на новый токен `'ль'`.  \n",
        "  Например, слово \"учитель\" из `['у', 'ч', 'и', 'т', 'е', 'л', 'ь']` становится `['у', 'ч', 'и', 'т', 'е', 'ль']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'ль'` в словарь:  \n",
        "$$\n",
        "  V_1 = V_0 \\cup \\{\\text{'ль'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Итерация 2: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`  \n",
        "  (Токен `'ль'` уже заменил последовательность `'л', 'ь'` в слове \"учитель\".)  \n",
        "  **Общее количество токенов: 49.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'л'}) = 1$ — всё ещё встречается в \"училище\" (`'и', 'л', 'и'`)\n",
        "  - $f(\\text{'ь'}) = 0$ — больше не встречается как отдельный токен\n",
        "  - $f(\\text{'ль'}) = 1$ — новый токен\n",
        "  - Остальные частоты остаются без изменений:\n",
        "    - $f(\\text{'у'}) = 5$, $f(\\text{'ч'}) = 5$, $f(\\text{'и'}) = 5$, $f(\\text{'е'}) = 4$, $f(\\text{'т'}) = 2$, $f(\\text{'к'}) = 2$, $f(\\text{'а'}) = 1$, $f(\\text{'м'}) = 2$, $f(\\text{' '}) = 8$ и т.д.\n",
        "\n",
        "- **Подсчет частот биграмм (обновленные):**\n",
        "  - Биграмма `'л', 'ь'` больше не встречается.\n",
        "  - Появилась новая биграмма: `'е', 'ль'` (из \"учитель\") → $f(\\text{'е', 'ль'}) = 1$\n",
        "  - Остальные биграммы, не затронутые заменой, сохраняют свои частоты:\n",
        "    - $f(\\text{'у', 'ч'}) = 5$, $f(\\text{'ч', 'и'}) = 5$, $f(\\text{'к', 'а'}) = 1$, $f(\\text{'а', 'м'}) = 1$, $f(\\text{'б', 'н'}) = 1$ и т.д.\n",
        "\n",
        "- **Вычисление оценок слияния:**\n",
        "  Используем формулу:\n",
        "$$\n",
        "  \\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "\n",
        "  Примеры:\n",
        "  - $\\text{Score}(\\text{'е', 'ль'}) = \\frac{1}{4 \\times 1} = 0.25$\n",
        "  - $\\text{Score}(\\text{'к', 'а'}) = \\frac{1}{2 \\times 1} = 0.5$\n",
        "  - $\\text{Score}(\\text{'а', 'м'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $\\text{Score} = 0.5$ имеют биграммы: `'к', 'а'`, `'а', 'м'`, `'б', 'н'`.  \n",
        "  Выбираем первую по порядку появления — `'к', 'а'` (в слове \"ученикам\").\n",
        "\n",
        "- **Слияние `'к', 'а'`:**  \n",
        "  Все вхождения последовательности `['к', 'а']` заменяются на новый токен `'ка'`.  \n",
        "  Например, слово \"ученикам\" из `['у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м']` становится `['у', 'ч', 'е', 'н', 'и', 'ка', 'м']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'ка'` в словарь:  \n",
        "$$\n",
        "  V_2 = V_1 \\cup \\{\\text{'ка'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 3: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'ка', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'` и `'ка'` уже вставлены.)  \n",
        "  **Общее количество токенов: 48.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'к'}) = 1$ — встречается в \"учебнику\"\n",
        "  - $f(\\text{'а'}) = 0$ — больше не встречается как отдельный токен\n",
        "  - $f(\\text{'ка'}) = 1$\n",
        "  - $f(\\text{'ам'}) = 1$ — пока нет, но биграмма `'а', 'м'` всё ещё может существовать\n",
        "  - $f(\\text{'а', 'м'}) = 1$ — в последовательности `['ка', 'м']` → `'ка', 'м'`, а не `'а', 'м'`  \n",
        "    → Значит, биграмма `'а', 'м'` **больше не существует**, так как `'а'` исчез как отдельный токен\n",
        "\n",
        "  Однако в слове \"ученикам\" теперь: `['ка', 'м']`, значит:\n",
        "  - Биграмма: `'ка', 'м'`, а не `'а', 'м'`\n",
        "\n",
        "  Следовательно:\n",
        "  - $f(\\text{'а', 'м'}) = 0$ → биграмма исчезла\n",
        "  - $f(\\text{'ка', 'м'}) = 1$ → новая биграмма\n",
        "\n",
        "- **Подсчет частот биграмм:**\n",
        "  - Биграммы `'к', 'а'` и `'а', 'м'` больше не встречаются.\n",
        "  - Появилась новая биграмма: `'ка', 'м'` → $f = 1$\n",
        "  - Остальные биграммы без изменений, например:\n",
        "    - $f(\\text{'б', 'н'}) = 1$\n",
        "    - $f(\\text{'у', 'ч'}) = 5$\n",
        "    - $f(\\text{'ч', 'и'}) = 5$\n",
        "\n",
        "- **Вычисление оценок:**\n",
        "  - $\\text{Score}(\\text{'ка', 'м'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $0.5$ имеют `'ка', 'м'` и `'б', 'н'`.  \n",
        "  Выбираем `'ка', 'м'` как следующую по порядку.\n",
        "\n",
        "- **Слияние `'ка', 'м'`:**  \n",
        "  Заменяем последовательность `['ка', 'м']` на новый токен `'кам'`.  \n",
        "  Слово \"ученикам\" из `['у', 'ч', 'е', 'н', 'и', 'ка', 'м']` становится `['у', 'ч', 'е', 'н', 'и', 'кам']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'кам'` в словарь:  \n",
        "$$\n",
        "  V_3 = V_2 \\cup \\{\\text{'кам'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 4: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'`, `'ка'`, `'ам'` уже объединены в `'кам'`. Уточнение: `'ам'` как отдельный токен не был добавлен — слияние произошло напрямую в `'кам'`.)  \n",
        "  **Общее количество токенов: 47.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'а'}) = 0$ — больше не встречается\n",
        "  - $f(\\text{'м'}) = 1$ — встречается в \"новому\" (`'м', 'у'`)\n",
        "  - $f(\\text{'кам'}) = 1$\n",
        "  - Остальные частоты без изменений.\n",
        "\n",
        "- **Подсчет частот биграмм (обновленные):**\n",
        "  - Биграмма `'и', 'кам'` появляется один раз (в \"ученикам\").\n",
        "  - $f(\\text{'и', 'кам'}) = 1$\n",
        "  - Остальные биграммы, не затронутые слиянием, сохраняют свои значения:\n",
        "    - $f(\\text{'б', 'н'}) = 1$ — в \"учебнику\"\n",
        "    - $f(\\text{'у', 'ч'}) = 5$\n",
        "    - $f(\\text{'ч', 'и'}) = 5$\n",
        "\n",
        "- **Вычисление оценок слияния:**\n",
        "$$\n",
        "  \\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "  - $\\text{Score}(\\text{'и', 'кам'}) = \\frac{1}{5 \\times 1} = 0.2$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $0.5$ имеет биграмма `'б', 'н'`.\n",
        "\n",
        "- **Слияние `'б', 'н'`:**  \n",
        "  Заменяем последовательность `['б', 'н']` на новый токен `'бн'`.  \n",
        "  Слово \"учебнику\" из `['у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']` становится `['у', 'ч', 'е', 'бн', 'и', 'к', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'бн'` в словарь:  \n",
        "$$\n",
        "  V_4 = V_3 \\cup \\{\\text{'бн'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 5: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'`, `'кам'`, `'бн'` добавлены.)  \n",
        "  **Общее количество токенов: 46.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'б'}) = 0$\n",
        "  - $f(\\text{'н'}) = 2$ — встречается в \"ученикам\" (`'е', 'н'`) и \"новому\" (`'н', 'о'`)\n",
        "  - $f(\\text{'бн'}) = 1$\n",
        "\n",
        "\n",
        "- **Подсчет частот биграмм:**\n",
        "  - Появились новые биграммы:\n",
        "    - $f(\\text{'е', 'бн'}) = 1$ — в \"учебнику\"\n",
        "    - $f(\\text{'бн', 'и'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'н', 'о'}) = 2$ — в \"новому\" и \"учебнику\"\n",
        "  - $f(\\text{'о', 'в'}) = 1$, $f(\\text{'в', 'о'}) = 1$, $f(\\text{'о', 'м'}) = 1$\n",
        "\n",
        "- **Вычисление оценок:**\n",
        "  - $\\text{Score}(\\text{'е', 'бн'}) = \\frac{1}{4 \\times 1} = 0.25$\n",
        "  - $\\text{Score}(\\text{'бн', 'и'}) = \\frac{1}{1 \\times 5} = 0.2$\n",
        "  - $\\text{Score}(\\text{'н', 'о'}) = \\frac{2}{2 \\times 3} \\approx 0.333$\n",
        "  - $\\text{Score}(\\text{'о', 'в'}) = \\frac{1}{3 \\times 2} \\approx 0.167$\n",
        "  - $\\text{Score}(\\text{'в', 'о'}) = \\frac{1}{2 \\times 3} \\approx 0.167$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $\\approx 0.333$ имеет `'н', 'о'`.\n",
        "\n",
        "- **Слияние `'н', 'о'`:**  \n",
        "  Заменяем все вхождения `['н', 'о']` на новый токен `'но'`.  \n",
        "  Слово \"новому\" из `['н', 'о', 'в', 'о', 'м', 'у']` становится `['но', 'в', 'о', 'м', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "$$\n",
        "  V_5 = V_4 \\cup \\{\\text{'но'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Итерация 6: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'но', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  **Общее количество токенов: 45.**\n",
        "\n",
        "- **Подсчет частот:**\n",
        "  - $f(\\text{'но'}) = 1$\n",
        "  - $f(\\text{'в'}) = 2$ — в \"в\" и \"новому\"\n",
        "  - $f(\\text{'о'}) = 2$ — в \"по\", \"новому\"\n",
        "\n",
        "- **Частоты биграмм:**\n",
        "  - $f(\\text{'но', 'в'}) = 1$ — в \"новому\"\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'но', 'в'}) = \\frac{1}{1 \\times 2} = 0.5\n",
        "$$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Среди текущих биграмм `'но', 'в'` имеет одну из высоких оценок.\n",
        "\n",
        "- **Слияние `'но', 'в'`:**  \n",
        "  Заменяем `['но', 'в']` на новый токен `'нов'`.  \n",
        "  Слово \"новому\" становится `['нов', 'о', 'м', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "$$\n",
        "  V_6 = V_5 \\cup \\{\\text{'нов'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Итерация 7: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'нов', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  **Общее количество токенов: 44.**\n",
        "\n",
        "- **Частоты:**\n",
        "  - $f(\\text{'нов'}) = 1$\n",
        "  - $f(\\text{'о'}) = 2$\n",
        "\n",
        "- **Биграмма:**\n",
        "  - $f(\\text{'нов', 'о'}) = 1$\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'нов', 'о'}) = \\frac{1}{1 \\times 2} = 0.5\n",
        "$$\n",
        "\n",
        "- **Слияние `'нов', 'о'`:**  \n",
        "  Заменяем `['нов', 'о']` на `'ново'`.  \n",
        "  Слово \"новому\" становится `['ново', 'м', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "$$\n",
        "  V_7 = V_6 \\cup \\{\\text{'ново'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 8: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'ново', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'`, `'кам'`, `'бн'`, `'но'`, `'нов'`, `'ново'` уже добавлены.)  \n",
        "  **Общее количество токенов: 43.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'ново'}) = 1$\n",
        "  - $f(\\text{'м'}) = 1$ — встречается в \"новому\"\n",
        "  - $f(\\text{'нов'}) = 0$ — больше не встречается как отдельный токен\n",
        "  - Остальные частоты без изменений.\n",
        "\n",
        "- **Подсчет частот биграмм:**\n",
        "  - $f(\\text{'ново', 'м'}) = 1$ — в слове \"новому\"\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'ново', 'м'}) = \\frac{f(\\text{'ново', 'м'})}{f(\\text{'ново'}) \\times f(\\text{'м'})} = \\frac{1}{1 \\times 1} = 1.0\n",
        "$$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Биграмма `'ново', 'м'` имеет высокую оценку (1.0).\n",
        "\n",
        "- **Слияние `'ново', 'м'`:**  \n",
        "  Заменяем последовательность `['ново', 'м']` на новый токен `'новом'`.  \n",
        "  Слово \"новому\" из `['ново', 'м', 'у']` становится `['новом', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'новом'` в словарь:  \n",
        "$$\n",
        "  V_8 = V_7 \\cup \\{\\text{'новом'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 9: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'новом', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  (Токены `'новом'` и другие уже включены.)  \n",
        "  **Общее количество токенов: 42.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов:**\n",
        "  - $f(\\text{'новом'}) = 1$\n",
        "  - $f(\\text{'у'}) = 5$ — встречается в \"училище\", \"учитель\", \"учит\", \"ученикам\", \"учебнику\"\n",
        "\n",
        "- **Частота биграммы:**\n",
        "  - $f(\\text{'новом', 'у'}) = 1$\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'новом', 'у'}) = \\frac{1}{1 \\times 5} = 0.2\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  На данном шаге среди всех доступных биграмм `'новом', 'у'` может быть одной из лучших, особенно если других высокочастотных пар не осталось.\n",
        "\n",
        "- **Слияние `'новом', 'у'`:**  \n",
        "  Заменяем `['новом', 'у']` на новый токен `'новому'`.  \n",
        "  Слово \"новому\" теперь представлено как `['новому']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'новому'` в словарь:  \n",
        "$$\n",
        "  V_9 = V_8 \\cup \\{\\text{'новому'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Мы продемонстрировали, как слово *\"новому\"* постепенно собиралось из отдельных символов через последовательные слияния:  \n",
        "`'н'+'о'→'но'`, `'но'+'в'→'нов'`, `'нов'+'о'→'ново'`, `'ново'+'м'→'новом'`, `'новом'+'у'→'новому'`.  \n",
        "Таким образом, WordPiece может формировать **целые слова** как отдельные токены, если они часто встречаются и слияние оправдано с точки зрения правдоподобия.\n",
        "\n",
        "Этот процесс продолжается итеративно до достижения желаемого размера словаря. В реальных моделях, таких как BERT, он проходит **тысячи или десятки тысяч итераций** на огромных текстовых корпусах.\n",
        "\n",
        "В конечном итоге, словарь содержит смесь:\n",
        "- отдельных символов,\n",
        "- подслов (например, `'уч'`, `'кам'`),\n",
        "- целых слов (например, `'новому'`),  \n",
        "что позволяет эффективно кодировать как частотные, так и редкие или неизвестные слова.\n",
        "\n",
        "\n",
        "\n",
        "**2.3. Когда остановить итерацию? (Критерии остановки)**\n",
        "\n",
        "Процесс итеративного слияния в WordPiece обычно останавливается при выполнении одного из следующих условий:\n",
        "\n",
        "- **Достигнут заданный размер словаря:**  \n",
        "  Это — наиболее распространённый критерий. Разработчик заранее определяет максимальное количество токенов в словаре (например, 30 000 для BERT, 50 000 для RoBERTa). Обучение завершается, как только словарь достигает этого предела.\n",
        "\n",
        "- **Невозможно выполнить дальнейшие слияния:**  \n",
        "  Это происходит, когда в корпусе больше нет соседних токенов, которые можно объединить — например, весь текст представлен минимальным числом токенов, или все возможные биграммы уже слиты.\n",
        "\n",
        "- **Оценка лучшей биграммы падает ниже порога:**  \n",
        "  Можно установить минимальное значение \\(\\text{Score}(A, B)\\), ниже которого слияния считаются незначимыми. Если даже лучшая доступная биграмма имеет слишком низкую оценку, процесс останавливается, так как дальнейшие слияния вносят незначительный вклад в правдоподобие текста.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Пример конечного словаря (Vfinal) после множества итераций:**\n",
        "\n",
        "На основе последовательных итераций, наш словарь постепенно будет включать в себя следующие токены:  \n",
        "`{'в', ' ', 'у', 'ч', 'и', 'л', 'щ', 'е', 'т', 'р', 'ь', 'н', 'к', 'а', 'м', 'п', 'о', 'б', 'ль', 'ка', 'ам', 'бн', 'но', 'нов', 'ново', 'новом', 'новому', 'уч', 'чи', 'ит', 'ищ', 'ще', 'е ', 'ни', 'ку', 'учи', 'учил', 'учили', 'училище', 'учитель', 'учит', 'уче', 'учени', 'ученик', 'ученикам', 'учеб', 'учебни', 'учебник', 'учебнику'}`.\n",
        "\n",
        "- **Начальные символы:** Все уникальные символы из текста (например, `'в'`, `' '` , `'у'`, `'ч'` и т.д.) остаются в словаре — они необходимы для токенизации редких или неизвестных слов.\n",
        "- **Итеративные слияния:** В процессе обучения добавляются составные токены: `'ль'`, `'ка'`, `'бн'`, `'но'`, `'нов'`, `'ново'`, `'новом'`, `'новому'` — как результат последовательных оптимальных слияний.\n",
        "- **Более длинные подслова и целые слова:** При продолжении процесса часто встречающиеся фрагменты и целые слова, такие как `'учи'`, `'учил'`, `'учили'`, `'училище'`, `'учитель'`, `'учит'`, `'ученикам'`, `'учебник'`, `'учебнику'`, `'по'`, также становятся отдельными токенами.\n",
        "\n",
        "> **Важное примечание о префиксах:**  \n",
        "> В реальных реализациях WordPiece (например, в BERT) для обозначения продолжения слова используется префикс `##`. Например, слово *\"играющий\"* может быть разбито на `[\"игра\", \"##ющий\"]`. Это позволяет модели различать, где начинается новое слово, а где — продолжается предыдущее.  \n",
        "> В нашем упрощённом примере мы опустили `##` для наглядности, но в реальных системах эта система критически важна для корректной обработки морфологии и редких слов.\n",
        "\n",
        "\n",
        "\n",
        "**3. Токенизация Нового Текста с Использованием Обученного Словаря**\n",
        "\n",
        "После завершения обучения и формирования финального словаря, токенизатор может обрабатывать любой новый текст. Процесс токенизации **одного слова** выглядит следующим образом:\n",
        "\n",
        "1. Ищем **самый длинный подтокен** из словаря, который совпадает с началом (префиксом) слова.\n",
        "2. Если такой подтокен найден, добавляем его в выходную последовательность и повторяем процесс для оставшейся части слова.\n",
        "3. Если часть слова не может быть разбита на известные подтокены, она либо:\n",
        "   - разбивается на отдельные символы (если они есть в словаре),\n",
        "   - либо заменяется специальным токеном `[UNK]` (Unknown), если ни один фрагмент не найден.\n",
        "\n",
        "Этот алгоритм называется **жадным разбиением по наибольшему префиксу** (greedy longest-match-first).\n",
        "\n",
        "\n",
        "**Пример токенизации с гипотетическим Vfinal:**  \n",
        "Текст: `\"в училище учитель учит ученикам по новому учебнику\"`\n",
        "\n",
        "- **Слово \"в\":**  \n",
        "  → \"в\" есть в словаре → `['в']`\n",
        "\n",
        "- **Слово \"училище\":**  \n",
        "  → \"училище\" есть в словаре → `['училище']`\n",
        "\n",
        "- **Слово \"учитель\":**  \n",
        "  → \"учитель\" есть в словаре → `['учитель']`\n",
        "\n",
        "- **Слово \"учит\":**  \n",
        "  → \"учит\" есть в словаре → `['учит']`\n",
        "\n",
        "- **Слово \"ученикам\":**  \n",
        "  → \"ученикам\" есть в словаре → `['ученикам']`\n",
        "\n",
        "- **Слово \"по\":**  \n",
        "  → \"по\" есть в словаре → `['по']`\n",
        "\n",
        "- **Слово \"новому\":**  \n",
        "  → \"новому\" есть в словаре → `['новому']`\n",
        "\n",
        "- **Слово \"учебнику\":**  \n",
        "  → \"учебнику\" есть в словаре → `['учебнику']`\n",
        "\n",
        "\n",
        "**Финальная токенизация (с пробелами как отдельными токенами):**  \n",
        "`['в', ' ', 'училище', ' ', 'учитель', ' ', 'учит', ' ', 'ученикам', ' ', 'по', ' ', 'новому', ' ', 'учебнику']`\n",
        "\n",
        "> ⚠️ Примечание: В большинстве современных реализаций пробелы **не используются как отдельные токены**. Вместо этого они либо удаляются, либо заменяются на `##` при внутрисловном слиянии. Однако в нашем упрощённом примере пробел сохраняется для наглядности.\n",
        "\n",
        "\n",
        "\n",
        "**Случай неизвестного слова:**  \n",
        "Если бы в тексте появилось слово *\"преподавательница\"*, которого нет в словаре, но есть подтокены:\n",
        "- `\"препода\"` → да\n",
        "- `\"##ватель\"` → да\n",
        "- `\"##ница\"` → да\n",
        "\n",
        "То оно было бы токенизировано как:  \n",
        "`['препода', '##ватель', '##ница']`\n",
        "\n",
        "Если же ни один подтокен не найден (например, из-за опечатки), слово может быть заменено на `[UNK]`.\n",
        "\n",
        "\n",
        "**4. Масштабирование WordPiece на Многопредметный Корпус**\n",
        "\n",
        "При обучении на большом корпусе, состоящем из множества предложений или документов, алгоритм WordPiece адаптируется, чтобы сохранить лингвистическую корректность и избежать артефактов.\n",
        "\n",
        "**Ключевые аспекты масштабирования:**\n",
        "\n",
        "- **Обработка по границам предложений:**  \n",
        "  Каждое предложение обрабатывается **независимо**. Подсчёт частот биграмм и слияние токенов **не пересекают границы предложений**. Это предотвращает бессмысленные слияния между последним словом одного предложения и первым — другого.\n",
        "\n",
        "- **Специальные разделительные токены:**  \n",
        "  Часто используются специальные маркеры:\n",
        "  - `<s>` — начало предложения\n",
        "  - `</s>` — конец предложения  \n",
        "  Эти токены помогают модели понимать структуру текста и не допускают слияния через границы.\n",
        "\n",
        "- **Запрет межпредложных слияний:**  \n",
        "  Это принципиально важно. Например, если одно предложение заканчивается словом, оканчивающимся на `\"ок\"`, а следующее начинается с `\"но\"`, WordPiece **не будет** объединять `\"ок\"` и `\"но\"` в `\"окно\"`, потому что они находятся в разных предложениях.  \n",
        "  Это гарантирует, что все формируемые токены имеют **лингвистический смысл в контексте одного предложения**.\n",
        "\n",
        "- **Работа с большим корпусом:**  \n",
        "  На практике WordPiece обучается на **гигантских корпусах** (например, весь Wikipedia). Процесс включает:\n",
        "  - предварительную токенизацию на уровне слов,\n",
        "  - построение начального символьного словаря,\n",
        "  - десятки тысяч итераций слияния,\n",
        "  - остановку по достижению целевого размера словаря (например, 30 000 токенов).\n",
        "\n",
        "\n",
        "\n",
        "### **Пример: Обучение WordPiece на корпусе предложений**\n",
        "\n",
        "Рассмотрим следующий корпус из трёх предложений:\n",
        "\n",
        "1. `\"Ученик учится в школе, а учитель учит ученика.\"`  \n",
        "2. `\"Новый студент изучает грамматику.\"`  \n",
        "3. `\"Учитель и студент читают книги.\"`\n",
        "\n",
        "\n",
        "\n",
        "#### **Начальная настройка**\n",
        "\n",
        "Перед обучением добавим специальные токены для обозначения границ предложений:  \n",
        "- `<s>` — начало предложения  \n",
        "- `</s>` — конец предложения\n",
        "\n",
        "Также проведём предварительную обработку:\n",
        "- Приведение к нижнему регистру\n",
        "- Замена пробелов на символ `_` (для наглядности — в реальных реализациях используется пробел или `##`)\n",
        "- Разделение на символы\n",
        "\n",
        "**Токенизированный корпус (на начальном символьном уровне):**\n",
        "\n",
        "```\n",
        "<s> у _ ч _ е _ н _ и _ к _ _ у _ ч _ и _ т _ с _ я _ _ в _ _ ш _ к _ о _ л _ е _ , _ _ а _ _ у _ ч _ и _ т _ е _ л _ ь _ _ у _ ч _ и _ т _ _ у _ ч _ е _ н _ и _ к _ а _ . </s>\n",
        "<s> _ н _ о _ в _ ы _ й _ _ с _ т _ у _ д _ е _ н _ т _ _ и _ з _ у _ ч _ а _ е _ т _ _ г _ р _ а _ м _ м _ а _ т _ и _ к _ у _ . </s>\n",
        "<s> _ у _ ч _ и _ т _ е _ л _ ь _ _ и _ _ с _ т _ у _ д _ е _ н _ т _ _ ч _ и _ т _ а _ ю _ т _ _ к _ н _ и _ г _ и _ . </s>\n",
        "```\n",
        "\n",
        "> ⚠️ Примечание: В реальных реализациях (например, BERT) пробелы не заменяются на `_`, а обрабатываются как отдельные символы. Здесь `_` используется для наглядности.\n",
        "\n",
        "\n",
        "\n",
        "#### **Итерации WordPiece на корпусе**\n",
        "\n",
        "При обучении частоты биграмм считаются **внутри каждого предложения отдельно**, чтобы избежать слияний через границы. Затем частоты суммируются по всему корпусу.\n",
        "\n",
        "**Пример подсчёта биграмм (гипотетический):**\n",
        "\n",
        "- **Из предложения 1:**  \n",
        "  - `('у', 'ч')`: 5  \n",
        "  - `('и', 'к')`: 2  \n",
        "  - `('_', 'у')`: 3  \n",
        "  - `('а', '_')`: 2  \n",
        "  - `('т', 'е')`: 1  \n",
        "\n",
        "- **Из предложения 2:**  \n",
        "  - `('н', 'о')`: 1  \n",
        "  - `('с', 'т')`: 1  \n",
        "  - `('т', 'у')`: 1  \n",
        "  - `('г', 'р')`: 1  \n",
        "\n",
        "- **Из предложения 3:**  \n",
        "  - `('у', 'ч')`: 1  \n",
        "  - `('т', 'е')`: 2  \n",
        "  - `('ч', 'и')`: 1  \n",
        "  - `('и', '_')`: 1  \n",
        "  - `('к', 'н')`: 1  \n",
        "\n",
        "**Суммарные частоты по корпусу:**\n",
        "- `('у', 'ч')`: 5 + 1 = **6**\n",
        "- `('_', 'у')`: 3 + 1 = **4**\n",
        "- `('т', 'е')`: 1 + 2 = **3**\n",
        "- `('и', 'к')`: 2\n",
        "- `('с', 'т')`: 1 + 1 = **2**\n",
        "\n",
        "\n",
        "#### **Решение об объединении**\n",
        "\n",
        "На первой итерации пара `('у', 'ч')` имеет наивысшую частоту (6). Согласно формуле WordPiece:\n",
        "$$\n",
        "\\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "она также, скорее всего, будет иметь высокую оценку, особенно если `у` и `ч` встречаются вместе значительно чаще, чем можно было бы ожидать случайно.\n",
        "\n",
        "Мы объединяем `у` и `ч` в новый токен **`уч`**.\n",
        "\n",
        "**Обновление корпуса:**  \n",
        "Во всех предложениях заменяем последовательности `['у', 'ч']` на `['уч']`. Например:\n",
        "- Было: `... _ у _ ч _ и _ т ...`  \n",
        "- Стало: `... _ уч _ и _ т ...`\n",
        "\n",
        "Процесс повторяется: пересчитываются частоты, выбирается следующая лучшая биграмма, выполняется слияние — и так до достижения целевого размера словаря.\n",
        "\n",
        "\n",
        "#### **Почему этот подход важен**\n",
        "\n",
        "- **Осмысленные токены:**  \n",
        "  Поскольку слияния не пересекают границы предложений, исключаются артефакты вроде объединения конца одного предложения с началом другого (например, `\"а_н\"` из `\"а.\"</s><s>н\"`). Это гарантирует, что формируемые токены лингвистически осмысленны.\n",
        "\n",
        "- **Целостность предложений:**  \n",
        "  Сохранение границ помогает модели учитывать синтаксическую и семантическую структуру предложений — критично для задач, таких как машинный перевод, классификация или генерация.\n",
        "\n",
        "- **Масштабируемость:**  \n",
        "  Подход позволяет обрабатывать большие или потоковые корпусы, где не все данные доступны одновременно. Это делает WordPiece пригодным для промышленного использования.\n",
        "\n",
        "- **Последовательная токенизация:**  \n",
        "  Одинаковые слова в одинаковом контексте будут токенизированы одинаково, что обеспечивает стабильность и воспроизводимость.\n",
        "\n",
        "\n",
        "### **5. Преимущества и недостатки WordPiece**\n",
        "\n",
        "Хотя WordPiece широко используется (в BERT, DistilBERT и других моделях), у него есть как сильные стороны, так и ограничения.\n",
        "\n",
        "\n",
        "\n",
        "#### **5.1. Преимущества WordPiece**\n",
        "\n",
        "- **Эффективная обработка OOV-слов (Out-of-Vocabulary):**  \n",
        "  WordPiece способен разбивать неизвестные слова на знакомые подслова. Например, *\"программистом\"* → `[\"программ\", \"##ист\", \"##ом\"]`, даже если целое слово отсутствует в словаре. Это обеспечивает частичное понимание и устойчивость к лексической вариативности.\n",
        "\n",
        "- **Контроль размера словаря:**  \n",
        "  Размер словаря задаётся заранее (например, 30 000). Алгоритм итеративно выбирает наиболее \"полезные\" слияния, максимизируя правдоподобие текста. Это позволяет балансировать между компактностью и покрытием.\n",
        "\n",
        "- **Гибридный подход:**  \n",
        "  WordPiece сочетает преимущества символьной и словесной токенизации:\n",
        "  - Частые слова (например, `\"учитель\"`) становятся одним токеном.\n",
        "  - Редкие или сложные слова разбиваются на подслова.\n",
        "  - Все входные данные могут быть представлены — нет \"неизвестных\" символов.\n",
        "\n",
        "- **Универсальность (в варианте Byte-level WordPiece):**  \n",
        "  Байтовый WordPiece (используется в моделях вроде RoBERTa) работает на уровне байтов, а не символов Unicode. Это делает его **инвариантным к кодировке**, способным обрабатывать любой текст (включая эмодзи, редкие символы), и **гарантирует отсутствие [UNK]**.\n",
        "\n",
        "\n",
        "\n",
        "#### **5.2. Недостатки WordPiece**\n",
        "\n",
        "- **Жадный и детерминированный алгоритм:**  \n",
        "  На каждом шаге выбирается пара с максимальной оценкой. Такой подход не учитывает долгосрочных последствий и может привести к субоптимальной сегментации с точки зрения морфологии или семантики.\n",
        "\n",
        "- **Отсутствие лингвистических знаний:**  \n",
        "  WordPiece — статистический алгоритм. Он не \"понимает\" морфемы. Например, может разбить `\"бежать\"` на `[\"беж\", \"##ать\"]`, хотя лингвистически более корректно — `[\"бег\", \"##ать\"]`. Это снижает интерпретируемость и иногда — эффективность.\n",
        "\n",
        "- **Вычислительная сложность:**  \n",
        "  На ранних этапах обучения (при большом количестве символьных токенов) подсчёт частот всех биграмм требует значительных ресурсов. Для больших корпусов это может быть медленным.\n",
        "\n",
        "- **Зависимость от начальной токенизации:**  \n",
        "  Если перед WordPiece используется разбиение по словам, то ошибки на этом этапе (например, неудачная обработка знаков препинания) могут повлиять на итоговый словарь и сегментацию.\n",
        "\n",
        "- **Неоднозначность сегментации:**  \n",
        "  Одно и то же слово может быть разбито по-разному в зависимости от истории слияний. Например, `\"ученик\"` может стать `[\"уч\", \"##еник\"]` или `[\"учени\", \"##к\"]` — в зависимости от того, какие пары были выбраны ранее. Хотя на практике это редко вызывает проблемы, теоретически снижает согласованность.\n",
        "\n",
        "\n",
        "### **Заключение**\n",
        "\n",
        "WordPiece стал **краеугольным камнем** современных NLP-моделей. Его способность эффективно справляться с неизвестными словами, контролировать размер словаря и сохранять баланс между символами и словами сделала его выбором по умолчанию для таких моделей, как **BERT** и **DistilBERT**.\n",
        "\n",
        "Однако его ограничения — жадность, отсутствие морфологического понимания — стимулировали разработку альтернатив:\n",
        "- **SentencePiece** — обучается без предварительной токенизации по словам.\n",
        "- **Unigram Language Model** — использует вероятностную модель для сегментации.\n",
        "- **BoundlessBPE / BBPE** — улучшает морфологическую целостность.\n",
        "\n",
        "Эта эволюция показывает: **нет универсального идеального токенизатора**. Выбор зависит от задачи, языка, требований к размеру модели и компромисса между эффективностью, точностью и универсальностью.\n"
      ],
      "metadata": {
        "id": "Q91FH4o2MxNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.merges = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Предварительная обработка текста: приведение к нижнему регистру и разбиение на слова.\n",
        "        \"\"\"\n",
        "        # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        # Разбиваем на слова по пробелам\n",
        "        words = processed_text.split(' ')\n",
        "        # Добавляем пробелы как отдельные токены между словами\n",
        "        initial_tokens = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word: # Убедимся, что слово не пустое\n",
        "                initial_tokens.extend(list(word))\n",
        "            if i < len(words) - 1:\n",
        "                initial_tokens.append(' ') # Добавляем пробел между словами\n",
        "        return initial_tokens, words # Возвращаем начальные токены и список слов для удобства\n",
        "\n",
        "    def calculate_frequencies(self, tokens):\n",
        "        \"\"\"\n",
        "        Подсчет частот отдельных токенов (униграмм) и биграмм в текущем корпусе.\n",
        "        \"\"\"\n",
        "        unigram_freq = collections.defaultdict(int)\n",
        "        bigram_freq = collections.defaultdict(int)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            unigram_freq[tokens[i]] += 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram_freq[(tokens[i], tokens[i+1])] += 1\n",
        "        return unigram_freq, bigram_freq\n",
        "\n",
        "    def calculate_score(self, unigram_freq, bigram_freq):\n",
        "        \"\"\"\n",
        "        Вычисление оценки слияния для всех возможных биграмм.\n",
        "        Score(A, B) = frequency(AB) / (frequency(A) * frequency(B))\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for (token_a, token_b), freq_ab in bigram_freq.items():\n",
        "            freq_a = unigram_freq[token_a]\n",
        "            freq_b = unigram_freq[token_b]\n",
        "            if freq_a > 0 and freq_b > 0: # Избегаем деления на ноль\n",
        "                score = freq_ab / (freq_a * freq_b)\n",
        "                scores[(token_a, token_b)] = score\n",
        "        return scores\n",
        "\n",
        "    def merge_tokens(self, tokens, best_bigram):\n",
        "        \"\"\"\n",
        "        Слияние лучшей биграммы в корпусе.\n",
        "        \"\"\"\n",
        "        merged_token = \"\".join(best_bigram)\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == best_bigram:\n",
        "                new_tokens.append(merged_token)\n",
        "                i += 2 # Пропускаем оба слитых токена\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        return new_tokens\n",
        "\n",
        "    def train(self, text, target_vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Обучение WordPiece токенизатора.\n",
        "        Итеративно сливает токены до достижения целевого размера словаря\n",
        "        или заданного количества слияний.\n",
        "        \"\"\"\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        current_tokens, _ = self.preprocess_text(text)\n",
        "\n",
        "        # Начальный словарь состоит из всех уникальных символов\n",
        "        self.vocab = set(current_tokens)\n",
        "        print(f\"\\nШаг 1: Начальный словарь (V0) содержит {len(self.vocab)} токенов.\")\n",
        "        print(f\"Начальный корпус: {current_tokens}\")\n",
        "\n",
        "        merges_count = 0\n",
        "        while True:\n",
        "            unigram_freq, bigram_freq = self.calculate_frequencies(current_tokens)\n",
        "            scores = self.calculate_score(unigram_freq, bigram_freq)\n",
        "\n",
        "            if not scores:\n",
        "                print(\"\\nНет больше биграмм для слияния. Остановка.\")\n",
        "                break\n",
        "\n",
        "            # Находим биграмму с наивысшей оценкой\n",
        "            best_bigram = max(scores, key=scores.get)\n",
        "            best_score = scores[best_bigram]\n",
        "\n",
        "            merged_token = \"\".join(best_bigram)\n",
        "\n",
        "            # Критерий остановки: если новый токен уже в словаре\n",
        "            if merged_token in self.vocab:\n",
        "                # Если лучший токен уже существует, удалим его из scores, чтобы найти следующий лучший\n",
        "                del scores[best_bigram]\n",
        "                continue # Продолжаем поиск\n",
        "\n",
        "            # Выводим информацию о текущем слиянии\n",
        "            print(f\"\\nИтерация {merges_count + 1}:\")\n",
        "            print(f\"  Лучшая биграмма для слияния: '{best_bigram[0]}' + '{best_bigram[1]}' -> '{merged_token}' (Score: {best_score:.4f})\")\n",
        "\n",
        "            # Выполняем слияние в корпусе\n",
        "            current_tokens = self.merge_tokens(current_tokens, best_bigram)\n",
        "\n",
        "            # Добавляем новый токен в словарь и сохраняем слияние\n",
        "            self.vocab.add(merged_token)\n",
        "            self.merges.append(best_bigram)\n",
        "            merges_count += 1\n",
        "\n",
        "            print(f\"  Корпус после слияния: {current_tokens}\")\n",
        "            print(f\"  Текущий размер словаря: {len(self.vocab)}\")\n",
        "\n",
        "            # Критерии остановки\n",
        "            if target_vocab_size is not None and len(self.vocab) >= target_vocab_size:\n",
        "                print(f\"\\nДостигнут целевой размер словаря ({target_vocab_size}). Остановка.\")\n",
        "                break\n",
        "            if num_merges is not None and merges_count >= num_merges:\n",
        "                print(f\"\\nДостигнуто заданное количество слияний ({num_merges}). Остановка.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nОбучение завершено. Итоговый словарь содержит {len(self.vocab)} токенов.\")\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Токенизация нового текста с использованием обученного словаря WordPiece.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'vocab') or len(self.vocab) == 0:\n",
        "            raise ValueError(\"Токенизатор не обучен. Сначала вызовите метод train().\")\n",
        "\n",
        "        print(f\"\\n--- Токенизация нового текста ---\")\n",
        "        print(f\"Текст для токенизации: '{text}'\")\n",
        "\n",
        "        # Предварительная обработка текста для токенизации\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        words = processed_text.split(' ')\n",
        "\n",
        "        final_tokens = []\n",
        "        for word in words:\n",
        "            if not word: # Пропускаем пустые строки, если они возникли из-за множественных пробелов\n",
        "                continue\n",
        "\n",
        "            word_tokens = []\n",
        "            remaining_word = word\n",
        "\n",
        "            while remaining_word:\n",
        "                found_match = False\n",
        "                # Ищем самый длинный подтокен из словаря, который является префиксом оставшейся части слова\n",
        "                for i in range(len(remaining_word), 0, -1):\n",
        "                    subword = remaining_word[:i]\n",
        "\n",
        "                    if subword in self.vocab:\n",
        "                        word_tokens.append(subword)\n",
        "                        remaining_word = remaining_word[i:]\n",
        "                        found_match = True\n",
        "                        break\n",
        "\n",
        "                if not found_match:\n",
        "                    # Если не удалось найти соответствующий токен, разбиваем на символы\n",
        "                    # или используем токен [UNK]\n",
        "                    if remaining_word[0] in self.vocab: # Если символ есть в словаре\n",
        "                        word_tokens.append(remaining_word[0])\n",
        "                    else: # Если символ даже не в начальном словаре\n",
        "                        word_tokens.append('[UNK]')\n",
        "                    remaining_word = remaining_word[1:]\n",
        "\n",
        "            final_tokens.extend(word_tokens)\n",
        "            final_tokens.append(' ') # Добавляем пробел между токенизированными словами\n",
        "\n",
        "        # Удаляем последний пробел, если он есть\n",
        "        if final_tokens and final_tokens[-1] == ' ':\n",
        "            final_tokens.pop()\n",
        "\n",
        "        print(f\"Токенизированный текст: {final_tokens}\")\n",
        "        return final_tokens\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Возвращает текущий словарь токенов.\"\"\"\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def get_merges(self):\n",
        "        \"\"\"Возвращает список выполненных слияний.\"\"\"\n",
        "        return self.merges\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    text_to_train = \"В училище учитель учит ученикам по новому учебнику\"\n",
        "\n",
        "    # Создаем экземпляр токенизатора\n",
        "    tokenizer = WordPieceTokenizer()\n",
        "\n",
        "    # Обучаем токенизатор\n",
        "    final_vocab = tokenizer.train(text_to_train, num_merges=50)\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece: {final_vocab}\")\n",
        "\n",
        "    # Токенизируем тот же текст\n",
        "    tokenizer.tokenize(text_to_train)\n",
        "\n",
        "    # Пример токенизации нового слова\n",
        "    new_text = \"учительница\"\n",
        "    # Добавим '##ница' в словарь для демонстрации\n",
        "    if 'ница' not in final_vocab:\n",
        "        tokenizer.vocab.add('ница')\n",
        "        tokenizer.vocab.add('##ница')\n",
        "        final_vocab = tokenizer.get_vocab()\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece (с 'ница' для демонстрации): {final_vocab}\")\n",
        "    tokenizer.tokenize(new_text)\n",
        "\n",
        "    # Демонстрация с упрощенным словарем\n",
        "    simplified_tokenizer = WordPieceTokenizer()\n",
        "    simplified_tokenizer.vocab = set(['у', 'ч', 'и', 'т', 'е', 'л', 'ь', 'н', 'ц', 'а', 'учитель', '##ница', '##тель', '##а'])\n",
        "    print(f\"\\n--- Демонстрация токенизации подслов с упрощенным словарем ---\")\n",
        "    simplified_tokenizer.tokenize(\"учительница\")"
      ],
      "metadata": {
        "id": "ZFIH_P7sO8cg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}