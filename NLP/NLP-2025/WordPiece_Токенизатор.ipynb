{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO028L4Iagowz4I0UjgXnMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/NLP-2025/WordPiece_%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordPiece Токенизатор**  \n",
        "WordPiece Токенизатор представляет собой элегантный гибридный подход, разработанный для нахождения оптимального баланса между токенизацией на уровне слов и символов. Он разбивает слова на более мелкие, но семантически осмысленные единицы, называемые подсловами (subwords). Например, слово \"неизвестные\" может быть токенизировано как \"не\", \"##извест\", \"##ные\". Префикс ## указывает, что данный подтокен является продолжением предыдущего токена, а не началом нового слова.  \n",
        "Основная идея WordPiece: Построить словарь токенов (включающий символы, подслова и, при необходимости, целые слова) таким образом, чтобы он был достаточно компактным, но при этом позволял эффективно представлять как известные, так и неизвестные слова, сохраняя при этом их лингвистический смысл.\n",
        "\n",
        "**1. Принцип Работы WordPiece: Пошаговое Объяснение**  \n",
        "Алгоритм WordPiece, подобно BPE, является итеративным методом, но отличается критерием выбора пар для слияния. Вместо простой частоты встречаемости, WordPiece использует метрику, основанную на вероятности или правдоподобии.  \n",
        "Возьмем наш пример текста для обучения токенизатора: \"В училище учитель учит ученикам по новому учебнику.\"\n",
        "\n",
        "**1.1. Шаг 1: Предварительная Обработка Текста и Создание Начального Словаря**  \n",
        "Перед началом обучения текст подвергается предварительной обработке:  \n",
        " Приведение к нижнему регистру: Все буквы переводятся в строчные, чтобы \"Учитель\" и \"учитель\" считались одним и тем же словом.  \n",
        " Удаление знаков препинания: Знаки препинания (кроме тех, что являются частью слова, например, в сокращениях, но в нашем примере их нет) удаляются или отделяются.  \n",
        " Разделение на слова: Текст сначала разбивается на слова по пробелам.  \n",
        "Обработанный текст: \"в училище учитель учит ученикам по новому учебнику\"  \n",
        "Начальный словарь (V₀): Начинаем с самого простого: каждый уникальный символ в нашем обработанном тексте становится отдельным токеном в нашем словаре. Пробел ( ) также является важным токеном, так как он помогает модели понимать границы слов.  \n",
        "Уникальные символы в тексте: 'в', ' ', 'у', 'ч', 'и', 'л', 'щ', 'е', 'т', 'р', 'ь', 'н', 'к', 'а', 'м', 'п', 'о', 'б', 'ю'.  \n",
        "V₀ = {'в', ' ', 'у', 'ч', 'и', 'л', 'щ', 'е', 'т', 'р', 'ь', 'н', 'к', 'а', 'м', 'п', 'о', 'б', 'ю'} (если бы были другие символы).  \n",
        "Теперь мы можем представить весь наш обработанный текст как последовательность этих начальных токенов. Например, слово \"училище\" будет представлено как ['у', 'ч', 'и', 'л', 'и', 'щ', 'е'].  \n",
        "Полная последовательность токенов в корпусе (после предварительной обработки): ['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'л', 'ь', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']  \n",
        "Общее количество токенов в корпусе: 50.\n",
        "\n",
        "**1.2. Шаг 2: Итеративное Слияние (Обучение Словаря)**  \n",
        "Это ядро алгоритма WordPiece. Мы будем итеративно (повторяющимися шагами) объединять пары соседних токенов (биграммы), чтобы создавать новые, более длинные токены. Цель — найти такие пары, которые, будучи объединенными, максимально увеличивают \"вероятность\" (или \"правдоподобие\") нашего обучающего текста.  \n",
        "\n",
        "**Формула оценки слияния (Score):** Для каждой пары соседних токенов A и B (биграммы AB) мы вычисляем \"оценку слияния\" по следующей формуле:  \n",
        "$$\n",
        "\\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$  \n",
        "Где:  \n",
        "• $f(A, B)$ — это количество раз, когда биграмма AB (токен A, за которым сразу следует токен B) встречается в текущем токенизированном корпусе.  \n",
        "• $f(A)$ — это количество раз, когда токен A встречается в текущем токенизированном корпусе.  \n",
        "• $f(B)$ — это количество раз, когда токен B встречается в текущем токенизированном корпусе.  \n",
        "\n",
        "Данная формула измеряет, насколько часто A и B встречаются **вместе**, по сравнению с тем, насколько часто они встречаются **по отдельности**. Высокий балл означает, что A и B **очень часто появляются вместе**, и их слияние в один токен будет \"полезным\" для модели, так как оно позволяет более компактно (с меньшим количеством токенов) представить текст. Это эквивалентно максимизации правдоподобия обучающих данных, если мы рассматриваем процесс токенизации как генерацию последовательности токенов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Пример итераций с нашим текстом:**\n",
        "\n",
        "**Итерация 1: Поиск первой лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'л', 'ь', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`\n",
        "\n",
        "- **Подсчет частот отдельных токенов (униграмм) в текущем корпусе:**\n",
        "  - $f(\\text{' '}) = 8$\n",
        "  - $f(\\text{'у'}) = 5$\n",
        "  - $f(\\text{'ч'}) = 5$\n",
        "  - $f(\\text{'и'}) = 5$\n",
        "  - $f(\\text{'л'}) = 2$\n",
        "  - $f(\\text{'щ'}) = 1$\n",
        "  - $f(\\text{'е'}) = 4$\n",
        "  - $f(\\text{'т'}) = 2$\n",
        "  - $f(\\text{'ь'}) = 1$\n",
        "  - $f(\\text{'н'}) = 2$\n",
        "  - $f(\\text{'к'}) = 2$\n",
        "  - $f(\\text{'а'}) = 1$\n",
        "  - $f(\\text{'м'}) = 2$\n",
        "  - $f(\\text{'п'}) = 1$\n",
        "  - $f(\\text{'о'}) = 3$\n",
        "  - $f(\\text{'в'}) = 2$ (один раз в начале \"в\", один — в \"новому\")\n",
        "  - $f(\\text{'б'}) = 1$\n",
        "\n",
        "\n",
        "- **Подсчет частот биграмм (пар соседних токенов):**\n",
        "  - $f(\\text{'у', 'ч'}) = 5$ — в \"училище\", \"учитель\", \"учит\" (дважды), \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'ч', 'и'}) = 5$ — аналогично\n",
        "  - $f(\\text{'и', 'л'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'л', 'и'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'и', 'щ'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'щ', 'е'}) = 1$ — в \"училище\"\n",
        "  - $f(\\text{'е', ' '}) = 2$ — после \"училище\", \"учитель\"\n",
        "  - $f(\\text{'и', 'т'}) = 2$ — в \"учит\" (дважды)\n",
        "  - $f(\\text{'т', 'е'}) = 1$ — в \"учитель\"\n",
        "  - $f(\\text{'е', 'л'}) = 1$ — в \"учитель\"\n",
        "  - $f(\\text{'л', 'ь'}) = 1$ — в \"учитель\"\n",
        "  - $f(\\text{'ч', 'е'}) = 2$ — в \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'е', 'н'}) = 1$ — в \"ученикам\"\n",
        "  - $f(\\text{'н', 'и'}) = 2$ — в \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'и', 'к'}) = 2$ — в \"ученикам\", \"учебнику\"\n",
        "  - $f(\\text{'к', 'а'}) = 1$ — в \"ученикам\"\n",
        "  - $f(\\text{'а', 'м'}) = 1$ — в \"ученикам\"\n",
        "  - $f(\\text{' ', 'п'}) = 1$ — перед \"по\"\n",
        "  - $f(\\text{'п', 'о'}) = 1$ — в \"по\"\n",
        "  - $f(\\text{'о', ' '}) = 1$ — после \"по\"\n",
        "  - $f(\\text{'н', 'о'}) = 2$ — в \"новому\", \"учебнику\"\n",
        "  - $f(\\text{'о', 'в'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'в', 'о'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'о', 'м'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'м', 'у'}) = 1$ — в \"новому\"\n",
        "  - $f(\\text{'у', ' '}) = 4$ — после \"в\", \"училище\", \"учитель\", \"новому\"\n",
        "  - $f(\\text{'е', 'б'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'б', 'н'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'н', 'и'}) = 2$ — уже учтено\n",
        "  - $f(\\text{'и', 'к'}) = 2$ — уже учтено\n",
        "  - $f(\\text{'к', 'у'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'в', ' '}) = 1$ — после слова \"в\"\n",
        "\n",
        "\n",
        "- **Вычисление оценок слияния по формуле:**  \n",
        "$$\n",
        "  \\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "\n",
        "  Примеры:\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = \\frac{5}{25} = 0.2$\n",
        "  - $\\text{Score}(\\text{'ч', 'и'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "  - $\\text{Score}(\\text{'л', 'ь'}) = \\frac{1}{2 \\times 1} = \\frac{1}{2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'к', 'а'}) = \\frac{1}{2 \\times 1} = 0.5$\n",
        "  - $\\text{Score}(\\text{'а', 'м'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'п', 'о'}) = \\frac{1}{1 \\times 3} \\approx 0.333$\n",
        "  - $\\text{Score}(\\text{'н', 'о'}) = \\frac{2}{2 \\times 3} \\approx 0.333$\n",
        "  - $\\text{Score}(\\text{'щ', 'е'}) = \\frac{1}{1 \\times 4} = 0.25$\n",
        "  - $\\text{Score}(\\text{'е', 'б'}) = \\frac{1}{4 \\times 1} = 0.25$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $\\text{Score} = 0.5$ имеют биграммы: `'л', 'ь'`, `'к', 'а'`, `'а', 'м'`, `'б', 'н'`.  \n",
        "  При равенстве оценок алгоритм может выбрать первую по порядку в лексикографическом или в порядке появления.  \n",
        "  Предположим, выбирается первая появляющаяся такая пара — `'л', 'ь'` (в слове \"учитель\").\n",
        "\n",
        "- **Слияние `'л', 'ь'`:**  \n",
        "  Все вхождения последовательности `['л', 'ь']` заменяются на новый токен `'ль'`.  \n",
        "  Например, слово \"учитель\" из `['у', 'ч', 'и', 'т', 'е', 'л', 'ь']` становится `['у', 'ч', 'и', 'т', 'е', 'ль']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'ль'` в словарь:  \n",
        "$$\n",
        "  V_1 = V_0 \\cup \\{\\text{'ль'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Итерация 2: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`  \n",
        "  (Токен `'ль'` уже заменил последовательность `'л', 'ь'` в слове \"учитель\".)  \n",
        "  **Общее количество токенов: 49.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'л'}) = 1$ — всё ещё встречается в \"училище\" (`'и', 'л', 'и'`)\n",
        "  - $f(\\text{'ь'}) = 0$ — больше не встречается как отдельный токен\n",
        "  - $f(\\text{'ль'}) = 1$ — новый токен\n",
        "  - Остальные частоты остаются без изменений:\n",
        "    - $f(\\text{'у'}) = 5$, $f(\\text{'ч'}) = 5$, $f(\\text{'и'}) = 5$, $f(\\text{'е'}) = 4$, $f(\\text{'т'}) = 2$, $f(\\text{'к'}) = 2$, $f(\\text{'а'}) = 1$, $f(\\text{'м'}) = 2$, $f(\\text{' '}) = 8$ и т.д.\n",
        "\n",
        "- **Подсчет частот биграмм (обновленные):**\n",
        "  - Биграмма `'л', 'ь'` больше не встречается.\n",
        "  - Появилась новая биграмма: `'е', 'ль'` (из \"учитель\") → $f(\\text{'е', 'ль'}) = 1$\n",
        "  - Остальные биграммы, не затронутые заменой, сохраняют свои частоты:\n",
        "    - $f(\\text{'у', 'ч'}) = 5$, $f(\\text{'ч', 'и'}) = 5$, $f(\\text{'к', 'а'}) = 1$, $f(\\text{'а', 'м'}) = 1$, $f(\\text{'б', 'н'}) = 1$ и т.д.\n",
        "\n",
        "- **Вычисление оценок слияния:**\n",
        "  Используем формулу:\n",
        "$$\n",
        "  \\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "\n",
        "  Примеры:\n",
        "  - $\\text{Score}(\\text{'е', 'ль'}) = \\frac{1}{4 \\times 1} = 0.25$\n",
        "  - $\\text{Score}(\\text{'к', 'а'}) = \\frac{1}{2 \\times 1} = 0.5$\n",
        "  - $\\text{Score}(\\text{'а', 'м'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $\\text{Score} = 0.5$ имеют биграммы: `'к', 'а'`, `'а', 'м'`, `'б', 'н'`.  \n",
        "  Выбираем первую по порядку появления — `'к', 'а'` (в слове \"ученикам\").\n",
        "\n",
        "- **Слияние `'к', 'а'`:**  \n",
        "  Все вхождения последовательности `['к', 'а']` заменяются на новый токен `'ка'`.  \n",
        "  Например, слово \"ученикам\" из `['у', 'ч', 'е', 'н', 'и', 'к', 'а', 'м']` становится `['у', 'ч', 'е', 'н', 'и', 'ка', 'м']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'ка'` в словарь:  \n",
        "$$\n",
        "  V_2 = V_1 \\cup \\{\\text{'ка'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 3: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'ка', 'м', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'` и `'ка'` уже вставлены.)  \n",
        "  **Общее количество токенов: 48.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'к'}) = 1$ — встречается в \"учебнику\"\n",
        "  - $f(\\text{'а'}) = 0$ — больше не встречается как отдельный токен\n",
        "  - $f(\\text{'ка'}) = 1$\n",
        "  - $f(\\text{'ам'}) = 1$ — пока нет, но биграмма `'а', 'м'` всё ещё может существовать\n",
        "  - $f(\\text{'а', 'м'}) = 1$ — в последовательности `['ка', 'м']` → `'ка', 'м'`, а не `'а', 'м'`  \n",
        "    → Значит, биграмма `'а', 'м'` **больше не существует**, так как `'а'` исчез как отдельный токен\n",
        "\n",
        "  Однако в слове \"ученикам\" теперь: `['ка', 'м']`, значит:\n",
        "  - Биграмма: `'ка', 'м'`, а не `'а', 'м'`\n",
        "\n",
        "  Следовательно:\n",
        "  - $f(\\text{'а', 'м'}) = 0$ → биграмма исчезла\n",
        "  - $f(\\text{'ка', 'м'}) = 1$ → новая биграмма\n",
        "\n",
        "- **Подсчет частот биграмм:**\n",
        "  - Биграммы `'к', 'а'` и `'а', 'м'` больше не встречаются.\n",
        "  - Появилась новая биграмма: `'ка', 'м'` → $f = 1$\n",
        "  - Остальные биграммы без изменений, например:\n",
        "    - $f(\\text{'б', 'н'}) = 1$\n",
        "    - $f(\\text{'у', 'ч'}) = 5$\n",
        "    - $f(\\text{'ч', 'и'}) = 5$\n",
        "\n",
        "- **Вычисление оценок:**\n",
        "  - $\\text{Score}(\\text{'ка', 'м'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $0.5$ имеют `'ка', 'м'` и `'б', 'н'`.  \n",
        "  Выбираем `'ка', 'м'` как следующую по порядку.\n",
        "\n",
        "- **Слияние `'ка', 'м'`:**  \n",
        "  Заменяем последовательность `['ка', 'м']` на новый токен `'кам'`.  \n",
        "  Слово \"ученикам\" из `['у', 'ч', 'е', 'н', 'и', 'ка', 'м']` становится `['у', 'ч', 'е', 'н', 'и', 'кам']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'кам'` в словарь:  \n",
        "$$\n",
        "  V_3 = V_2 \\cup \\{\\text{'кам'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 4: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'`, `'ка'`, `'ам'` уже объединены в `'кам'`. Уточнение: `'ам'` как отдельный токен не был добавлен — слияние произошло напрямую в `'кам'`.)  \n",
        "  **Общее количество токенов: 47.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'а'}) = 0$ — больше не встречается\n",
        "  - $f(\\text{'м'}) = 1$ — встречается в \"новому\" (`'м', 'у'`)\n",
        "  - $f(\\text{'кам'}) = 1$\n",
        "  - Остальные частоты без изменений.\n",
        "\n",
        "- **Подсчет частот биграмм (обновленные):**\n",
        "  - Биграмма `'и', 'кам'` появляется один раз (в \"ученикам\").\n",
        "  - $f(\\text{'и', 'кам'}) = 1$\n",
        "  - Остальные биграммы, не затронутые слиянием, сохраняют свои значения:\n",
        "    - $f(\\text{'б', 'н'}) = 1$ — в \"учебнику\"\n",
        "    - $f(\\text{'у', 'ч'}) = 5$\n",
        "    - $f(\\text{'ч', 'и'}) = 5$\n",
        "\n",
        "- **Вычисление оценок слияния:**\n",
        "$$\n",
        "  \\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "  - $\\text{Score}(\\text{'и', 'кам'}) = \\frac{1}{5 \\times 1} = 0.2$\n",
        "  - $\\text{Score}(\\text{'б', 'н'}) = \\frac{1}{1 \\times 2} = 0.5$\n",
        "  - $\\text{Score}(\\text{'у', 'ч'}) = \\frac{5}{5 \\times 5} = 0.2$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $0.5$ имеет биграмма `'б', 'н'`.\n",
        "\n",
        "- **Слияние `'б', 'н'`:**  \n",
        "  Заменяем последовательность `['б', 'н']` на новый токен `'бн'`.  \n",
        "  Слово \"учебнику\" из `['у', 'ч', 'е', 'б', 'н', 'и', 'к', 'у']` становится `['у', 'ч', 'е', 'бн', 'и', 'к', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'бн'` в словарь:  \n",
        "$$\n",
        "  V_4 = V_3 \\cup \\{\\text{'бн'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 5: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'н', 'о', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'`, `'кам'`, `'бн'` добавлены.)  \n",
        "  **Общее количество токенов: 46.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'б'}) = 0$\n",
        "  - $f(\\text{'н'}) = 2$ — встречается в \"ученикам\" (`'е', 'н'`) и \"новому\" (`'н', 'о'`)\n",
        "  - $f(\\text{'бн'}) = 1$\n",
        "\n",
        "\n",
        "- **Подсчет частот биграмм:**\n",
        "  - Появились новые биграммы:\n",
        "    - $f(\\text{'е', 'бн'}) = 1$ — в \"учебнику\"\n",
        "    - $f(\\text{'бн', 'и'}) = 1$ — в \"учебнику\"\n",
        "  - $f(\\text{'н', 'о'}) = 2$ — в \"новому\" и \"учебнику\"\n",
        "  - $f(\\text{'о', 'в'}) = 1$, $f(\\text{'в', 'о'}) = 1$, $f(\\text{'о', 'м'}) = 1$\n",
        "\n",
        "- **Вычисление оценок:**\n",
        "  - $\\text{Score}(\\text{'е', 'бн'}) = \\frac{1}{4 \\times 1} = 0.25$\n",
        "  - $\\text{Score}(\\text{'бн', 'и'}) = \\frac{1}{1 \\times 5} = 0.2$\n",
        "  - $\\text{Score}(\\text{'н', 'о'}) = \\frac{2}{2 \\times 3} \\approx 0.333$\n",
        "  - $\\text{Score}(\\text{'о', 'в'}) = \\frac{1}{3 \\times 2} \\approx 0.167$\n",
        "  - $\\text{Score}(\\text{'в', 'о'}) = \\frac{1}{2 \\times 3} \\approx 0.167$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Наивысшую оценку $\\approx 0.333$ имеет `'н', 'о'`.\n",
        "\n",
        "- **Слияние `'н', 'о'`:**  \n",
        "  Заменяем все вхождения `['н', 'о']` на новый токен `'но'`.  \n",
        "  Слово \"новому\" из `['н', 'о', 'в', 'о', 'м', 'у']` становится `['но', 'в', 'о', 'м', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "$$\n",
        "  V_5 = V_4 \\cup \\{\\text{'но'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Итерация 6: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'но', 'в', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  **Общее количество токенов: 45.**\n",
        "\n",
        "- **Подсчет частот:**\n",
        "  - $f(\\text{'но'}) = 1$\n",
        "  - $f(\\text{'в'}) = 2$ — в \"в\" и \"новому\"\n",
        "  - $f(\\text{'о'}) = 2$ — в \"по\", \"новому\"\n",
        "\n",
        "- **Частоты биграмм:**\n",
        "  - $f(\\text{'но', 'в'}) = 1$ — в \"новому\"\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'но', 'в'}) = \\frac{1}{1 \\times 2} = 0.5\n",
        "$$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Среди текущих биграмм `'но', 'в'` имеет одну из высоких оценок.\n",
        "\n",
        "- **Слияние `'но', 'в'`:**  \n",
        "  Заменяем `['но', 'в']` на новый токен `'нов'`.  \n",
        "  Слово \"новому\" становится `['нов', 'о', 'м', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "$$\n",
        "  V_6 = V_5 \\cup \\{\\text{'нов'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Итерация 7: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'нов', 'о', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  **Общее количество токенов: 44.**\n",
        "\n",
        "- **Частоты:**\n",
        "  - $f(\\text{'нов'}) = 1$\n",
        "  - $f(\\text{'о'}) = 2$\n",
        "\n",
        "- **Биграмма:**\n",
        "  - $f(\\text{'нов', 'о'}) = 1$\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'нов', 'о'}) = \\frac{1}{1 \\times 2} = 0.5\n",
        "$$\n",
        "\n",
        "- **Слияние `'нов', 'о'`:**  \n",
        "  Заменяем `['нов', 'о']` на `'ново'`.  \n",
        "  Слово \"новому\" становится `['ново', 'м', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "$$\n",
        "  V_7 = V_6 \\cup \\{\\text{'ново'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 8: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'ново', 'м', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  (Токены `'ль'`, `'кам'`, `'бн'`, `'но'`, `'нов'`, `'ново'` уже добавлены.)  \n",
        "  **Общее количество токенов: 43.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов (обновленные):**\n",
        "  - $f(\\text{'ново'}) = 1$\n",
        "  - $f(\\text{'м'}) = 1$ — встречается в \"новому\"\n",
        "  - $f(\\text{'нов'}) = 0$ — больше не встречается как отдельный токен\n",
        "  - Остальные частоты без изменений.\n",
        "\n",
        "- **Подсчет частот биграмм:**\n",
        "  - $f(\\text{'ново', 'м'}) = 1$ — в слове \"новому\"\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'ново', 'м'}) = \\frac{f(\\text{'ново', 'м'})}{f(\\text{'ново'}) \\times f(\\text{'м'})} = \\frac{1}{1 \\times 1} = 1.0\n",
        "$$\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  Биграмма `'ново', 'м'` имеет высокую оценку (1.0).\n",
        "\n",
        "- **Слияние `'ново', 'м'`:**  \n",
        "  Заменяем последовательность `['ново', 'м']` на новый токен `'новом'`.  \n",
        "  Слово \"новому\" из `['ново', 'м', 'у']` становится `['новом', 'у']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'новом'` в словарь:  \n",
        "$$\n",
        "  V_8 = V_7 \\cup \\{\\text{'новом'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Итерация 9: Поиск следующей лучшей биграммы**\n",
        "\n",
        "- **Текущий корпус:**  \n",
        "  `['в', ' ', 'у', 'ч', 'и', 'л', 'и', 'щ', 'е', ' ', 'у', 'ч', 'и', 'т', 'е', 'ль', ' ', 'у', 'ч', 'и', 'т', ' ', 'у', 'ч', 'е', 'н', 'и', 'кам', ' ', 'п', 'о', ' ', 'новом', 'у', ' ', 'у', 'ч', 'е', 'бн', 'и', 'к', 'у']`  \n",
        "  (Токены `'новом'` и другие уже включены.)  \n",
        "  **Общее количество токенов: 42.**\n",
        "\n",
        "- **Подсчет частот отдельных токенов:**\n",
        "  - $f(\\text{'новом'}) = 1$\n",
        "  - $f(\\text{'у'}) = 5$ — встречается в \"училище\", \"учитель\", \"учит\", \"ученикам\", \"учебнику\"\n",
        "\n",
        "- **Частота биграммы:**\n",
        "  - $f(\\text{'новом', 'у'}) = 1$\n",
        "\n",
        "- **Оценка слияния:**\n",
        "$$\n",
        "  \\text{Score}(\\text{'новом', 'у'}) = \\frac{1}{1 \\times 5} = 0.2\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "- **Выбор лучшей биграммы:**  \n",
        "  На данном шаге среди всех доступных биграмм `'новом', 'у'` может быть одной из лучших, особенно если других высокочастотных пар не осталось.\n",
        "\n",
        "- **Слияние `'новом', 'у'`:**  \n",
        "  Заменяем `['новом', 'у']` на новый токен `'новому'`.  \n",
        "  Слово \"новому\" теперь представлено как `['новому']`.\n",
        "\n",
        "- **Обновление словаря:**  \n",
        "  Добавляем новый токен `'новому'` в словарь:  \n",
        "$$\n",
        "  V_9 = V_8 \\cup \\{\\text{'новому'}\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Мы продемонстрировали, как слово *\"новому\"* постепенно собиралось из отдельных символов через последовательные слияния:  \n",
        "`'н'+'о'→'но'`, `'но'+'в'→'нов'`, `'нов'+'о'→'ново'`, `'ново'+'м'→'новом'`, `'новом'+'у'→'новому'`.  \n",
        "Таким образом, WordPiece может формировать **целые слова** как отдельные токены, если они часто встречаются и слияние оправдано с точки зрения правдоподобия.\n",
        "\n",
        "Этот процесс продолжается итеративно до достижения желаемого размера словаря. В реальных моделях, таких как BERT, он проходит **тысячи или десятки тысяч итераций** на огромных текстовых корпусах.\n",
        "\n",
        "В конечном итоге, словарь содержит смесь:\n",
        "- отдельных символов,\n",
        "- подслов (например, `'уч'`, `'кам'`),\n",
        "- целых слов (например, `'новому'`),  \n",
        "что позволяет эффективно кодировать как частотные, так и редкие или неизвестные слова.\n",
        "\n",
        "\n",
        "\n",
        "**2.3. Когда остановить итерацию? (Критерии остановки)**\n",
        "\n",
        "Процесс итеративного слияния в WordPiece обычно останавливается при выполнении одного из следующих условий:\n",
        "\n",
        "- **Достигнут заданный размер словаря:**  \n",
        "  Это — наиболее распространённый критерий. Разработчик заранее определяет максимальное количество токенов в словаре (например, 30 000 для BERT, 50 000 для RoBERTa). Обучение завершается, как только словарь достигает этого предела.\n",
        "\n",
        "- **Невозможно выполнить дальнейшие слияния:**  \n",
        "  Это происходит, когда в корпусе больше нет соседних токенов, которые можно объединить — например, весь текст представлен минимальным числом токенов, или все возможные биграммы уже слиты.\n",
        "\n",
        "- **Оценка лучшей биграммы падает ниже порога:**  \n",
        "  Можно установить минимальное значение \\(\\text{Score}(A, B)\\), ниже которого слияния считаются незначимыми. Если даже лучшая доступная биграмма имеет слишком низкую оценку, процесс останавливается, так как дальнейшие слияния вносят незначительный вклад в правдоподобие текста.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Пример конечного словаря (Vfinal) после множества итераций:**\n",
        "\n",
        "На основе последовательных итераций, наш словарь постепенно будет включать в себя следующие токены:  \n",
        "`{'в', ' ', 'у', 'ч', 'и', 'л', 'щ', 'е', 'т', 'р', 'ь', 'н', 'к', 'а', 'м', 'п', 'о', 'б', 'ль', 'ка', 'ам', 'бн', 'но', 'нов', 'ново', 'новом', 'новому', 'уч', 'чи', 'ит', 'ищ', 'ще', 'е ', 'ни', 'ку', 'учи', 'учил', 'учили', 'училище', 'учитель', 'учит', 'уче', 'учени', 'ученик', 'ученикам', 'учеб', 'учебни', 'учебник', 'учебнику'}`.\n",
        "\n",
        "- **Начальные символы:** Все уникальные символы из текста (например, `'в'`, `' '` , `'у'`, `'ч'` и т.д.) остаются в словаре — они необходимы для токенизации редких или неизвестных слов.\n",
        "- **Итеративные слияния:** В процессе обучения добавляются составные токены: `'ль'`, `'ка'`, `'бн'`, `'но'`, `'нов'`, `'ново'`, `'новом'`, `'новому'` — как результат последовательных оптимальных слияний.\n",
        "- **Более длинные подслова и целые слова:** При продолжении процесса часто встречающиеся фрагменты и целые слова, такие как `'учи'`, `'учил'`, `'учили'`, `'училище'`, `'учитель'`, `'учит'`, `'ученикам'`, `'учебник'`, `'учебнику'`, `'по'`, также становятся отдельными токенами.\n",
        "\n",
        "> **Важное примечание о префиксах:**  \n",
        "> В реальных реализациях WordPiece (например, в BERT) для обозначения продолжения слова используется префикс `##`. Например, слово *\"играющий\"* может быть разбито на `[\"игра\", \"##ющий\"]`. Это позволяет модели различать, где начинается новое слово, а где — продолжается предыдущее.  \n",
        "> В нашем упрощённом примере мы опустили `##` для наглядности, но в реальных системах эта система критически важна для корректной обработки морфологии и редких слов.\n",
        "\n",
        "\n",
        "\n",
        "**3. Токенизация Нового Текста с Использованием Обученного Словаря**\n",
        "\n",
        "После завершения обучения и формирования финального словаря, токенизатор может обрабатывать любой новый текст. Процесс токенизации **одного слова** выглядит следующим образом:\n",
        "\n",
        "1. Ищем **самый длинный подтокен** из словаря, который совпадает с началом (префиксом) слова.\n",
        "2. Если такой подтокен найден, добавляем его в выходную последовательность и повторяем процесс для оставшейся части слова.\n",
        "3. Если часть слова не может быть разбита на известные подтокены, она либо:\n",
        "   - разбивается на отдельные символы (если они есть в словаре),\n",
        "   - либо заменяется специальным токеном `[UNK]` (Unknown), если ни один фрагмент не найден.\n",
        "\n",
        "Этот алгоритм называется **жадным разбиением по наибольшему префиксу** (greedy longest-match-first).\n",
        "\n",
        "\n",
        "**Пример токенизации с гипотетическим Vfinal:**  \n",
        "Текст: `\"в училище учитель учит ученикам по новому учебнику\"`\n",
        "\n",
        "- **Слово \"в\":**  \n",
        "  → \"в\" есть в словаре → `['в']`\n",
        "\n",
        "- **Слово \"училище\":**  \n",
        "  → \"училище\" есть в словаре → `['училище']`\n",
        "\n",
        "- **Слово \"учитель\":**  \n",
        "  → \"учитель\" есть в словаре → `['учитель']`\n",
        "\n",
        "- **Слово \"учит\":**  \n",
        "  → \"учит\" есть в словаре → `['учит']`\n",
        "\n",
        "- **Слово \"ученикам\":**  \n",
        "  → \"ученикам\" есть в словаре → `['ученикам']`\n",
        "\n",
        "- **Слово \"по\":**  \n",
        "  → \"по\" есть в словаре → `['по']`\n",
        "\n",
        "- **Слово \"новому\":**  \n",
        "  → \"новому\" есть в словаре → `['новому']`\n",
        "\n",
        "- **Слово \"учебнику\":**  \n",
        "  → \"учебнику\" есть в словаре → `['учебнику']`\n",
        "\n",
        "\n",
        "**Финальная токенизация (с пробелами как отдельными токенами):**  \n",
        "`['в', ' ', 'училище', ' ', 'учитель', ' ', 'учит', ' ', 'ученикам', ' ', 'по', ' ', 'новому', ' ', 'учебнику']`\n",
        "\n",
        "> ⚠️ Примечание: В большинстве современных реализаций пробелы **не используются как отдельные токены**. Вместо этого они либо удаляются, либо заменяются на `##` при внутрисловном слиянии. Однако в нашем упрощённом примере пробел сохраняется для наглядности.\n",
        "\n",
        "\n",
        "\n",
        "**Случай неизвестного слова:**  \n",
        "Если бы в тексте появилось слово *\"преподавательница\"*, которого нет в словаре, но есть подтокены:\n",
        "- `\"препода\"` → да\n",
        "- `\"##ватель\"` → да\n",
        "- `\"##ница\"` → да\n",
        "\n",
        "То оно было бы токенизировано как:  \n",
        "`['препода', '##ватель', '##ница']`\n",
        "\n",
        "Если же ни один подтокен не найден (например, из-за опечатки), слово может быть заменено на `[UNK]`.\n",
        "\n",
        "\n",
        "**4. Масштабирование WordPiece на Многопредметный Корпус**\n",
        "\n",
        "При обучении на большом корпусе, состоящем из множества предложений или документов, алгоритм WordPiece адаптируется, чтобы сохранить лингвистическую корректность и избежать артефактов.\n",
        "\n",
        "**Ключевые аспекты масштабирования:**\n",
        "\n",
        "- **Обработка по границам предложений:**  \n",
        "  Каждое предложение обрабатывается **независимо**. Подсчёт частот биграмм и слияние токенов **не пересекают границы предложений**. Это предотвращает бессмысленные слияния между последним словом одного предложения и первым — другого.\n",
        "\n",
        "- **Специальные разделительные токены:**  \n",
        "  Часто используются специальные маркеры:\n",
        "  - `<s>` — начало предложения\n",
        "  - `</s>` — конец предложения  \n",
        "  Эти токены помогают модели понимать структуру текста и не допускают слияния через границы.\n",
        "\n",
        "- **Запрет межпредложных слияний:**  \n",
        "  Это принципиально важно. Например, если одно предложение заканчивается словом, оканчивающимся на `\"ок\"`, а следующее начинается с `\"но\"`, WordPiece **не будет** объединять `\"ок\"` и `\"но\"` в `\"окно\"`, потому что они находятся в разных предложениях.  \n",
        "  Это гарантирует, что все формируемые токены имеют **лингвистический смысл в контексте одного предложения**.\n",
        "\n",
        "- **Работа с большим корпусом:**  \n",
        "  На практике WordPiece обучается на **гигантских корпусах** (например, весь Wikipedia). Процесс включает:\n",
        "  - предварительную токенизацию на уровне слов,\n",
        "  - построение начального символьного словаря,\n",
        "  - десятки тысяч итераций слияния,\n",
        "  - остановку по достижению целевого размера словаря (например, 30 000 токенов).\n",
        "\n",
        "\n",
        "\n",
        "### **Пример: Обучение WordPiece на корпусе предложений**\n",
        "\n",
        "Рассмотрим следующий корпус из трёх предложений:\n",
        "\n",
        "1. `\"Ученик учится в школе, а учитель учит ученика.\"`  \n",
        "2. `\"Новый студент изучает грамматику.\"`  \n",
        "3. `\"Учитель и студент читают книги.\"`\n",
        "\n",
        "\n",
        "\n",
        "#### **Начальная настройка**\n",
        "\n",
        "Перед обучением добавим специальные токены для обозначения границ предложений:  \n",
        "- `<s>` — начало предложения  \n",
        "- `</s>` — конец предложения\n",
        "\n",
        "Также проведём предварительную обработку:\n",
        "- Приведение к нижнему регистру\n",
        "- Замена пробелов на символ `_` (для наглядности — в реальных реализациях используется пробел или `##`)\n",
        "- Разделение на символы\n",
        "\n",
        "**Токенизированный корпус (на начальном символьном уровне):**\n",
        "\n",
        "```\n",
        "<s> у _ ч _ е _ н _ и _ к _ _ у _ ч _ и _ т _ с _ я _ _ в _ _ ш _ к _ о _ л _ е _ , _ _ а _ _ у _ ч _ и _ т _ е _ л _ ь _ _ у _ ч _ и _ т _ _ у _ ч _ е _ н _ и _ к _ а _ . </s>\n",
        "<s> _ н _ о _ в _ ы _ й _ _ с _ т _ у _ д _ е _ н _ т _ _ и _ з _ у _ ч _ а _ е _ т _ _ г _ р _ а _ м _ м _ а _ т _ и _ к _ у _ . </s>\n",
        "<s> _ у _ ч _ и _ т _ е _ л _ ь _ _ и _ _ с _ т _ у _ д _ е _ н _ т _ _ ч _ и _ т _ а _ ю _ т _ _ к _ н _ и _ г _ и _ . </s>\n",
        "```\n",
        "\n",
        "> ⚠️ Примечание: В реальных реализациях (например, BERT) пробелы не заменяются на `_`, а обрабатываются как отдельные символы. Здесь `_` используется для наглядности.\n",
        "\n",
        "\n",
        "\n",
        "#### **Итерации WordPiece на корпусе**\n",
        "\n",
        "При обучении частоты биграмм считаются **внутри каждого предложения отдельно**, чтобы избежать слияний через границы. Затем частоты суммируются по всему корпусу.\n",
        "\n",
        "**Пример подсчёта биграмм (гипотетический):**\n",
        "\n",
        "- **Из предложения 1:**  \n",
        "  - `('у', 'ч')`: 5  \n",
        "  - `('и', 'к')`: 2  \n",
        "  - `('_', 'у')`: 3  \n",
        "  - `('а', '_')`: 2  \n",
        "  - `('т', 'е')`: 1  \n",
        "\n",
        "- **Из предложения 2:**  \n",
        "  - `('н', 'о')`: 1  \n",
        "  - `('с', 'т')`: 1  \n",
        "  - `('т', 'у')`: 1  \n",
        "  - `('г', 'р')`: 1  \n",
        "\n",
        "- **Из предложения 3:**  \n",
        "  - `('у', 'ч')`: 1  \n",
        "  - `('т', 'е')`: 2  \n",
        "  - `('ч', 'и')`: 1  \n",
        "  - `('и', '_')`: 1  \n",
        "  - `('к', 'н')`: 1  \n",
        "\n",
        "**Суммарные частоты по корпусу:**\n",
        "- `('у', 'ч')`: 5 + 1 = **6**\n",
        "- `('_', 'у')`: 3 + 1 = **4**\n",
        "- `('т', 'е')`: 1 + 2 = **3**\n",
        "- `('и', 'к')`: 2\n",
        "- `('с', 'т')`: 1 + 1 = **2**\n",
        "\n",
        "\n",
        "#### **Решение об объединении**\n",
        "\n",
        "На первой итерации пара `('у', 'ч')` имеет наивысшую частоту (6). Согласно формуле WordPiece:\n",
        "$$\n",
        "\\text{Score}(A, B) = \\frac{f(A, B)}{f(A) \\times f(B)}\n",
        "$$\n",
        "она также, скорее всего, будет иметь высокую оценку, особенно если `у` и `ч` встречаются вместе значительно чаще, чем можно было бы ожидать случайно.\n",
        "\n",
        "Мы объединяем `у` и `ч` в новый токен **`уч`**.\n",
        "\n",
        "**Обновление корпуса:**  \n",
        "Во всех предложениях заменяем последовательности `['у', 'ч']` на `['уч']`. Например:\n",
        "- Было: `... _ у _ ч _ и _ т ...`  \n",
        "- Стало: `... _ уч _ и _ т ...`\n",
        "\n",
        "Процесс повторяется: пересчитываются частоты, выбирается следующая лучшая биграмма, выполняется слияние — и так до достижения целевого размера словаря.\n",
        "\n",
        "\n",
        "#### **Почему этот подход важен**\n",
        "\n",
        "- **Осмысленные токены:**  \n",
        "  Поскольку слияния не пересекают границы предложений, исключаются артефакты вроде объединения конца одного предложения с началом другого (например, `\"а_н\"` из `\"а.\"</s><s>н\"`). Это гарантирует, что формируемые токены лингвистически осмысленны.\n",
        "\n",
        "- **Целостность предложений:**  \n",
        "  Сохранение границ помогает модели учитывать синтаксическую и семантическую структуру предложений — критично для задач, таких как машинный перевод, классификация или генерация.\n",
        "\n",
        "- **Масштабируемость:**  \n",
        "  Подход позволяет обрабатывать большие или потоковые корпусы, где не все данные доступны одновременно. Это делает WordPiece пригодным для промышленного использования.\n",
        "\n",
        "- **Последовательная токенизация:**  \n",
        "  Одинаковые слова в одинаковом контексте будут токенизированы одинаково, что обеспечивает стабильность и воспроизводимость.\n",
        "\n",
        "\n",
        "### **5. Преимущества и недостатки WordPiece**\n",
        "\n",
        "Хотя WordPiece широко используется (в BERT, DistilBERT и других моделях), у него есть как сильные стороны, так и ограничения.\n",
        "\n",
        "\n",
        "\n",
        "#### **5.1. Преимущества WordPiece**\n",
        "\n",
        "- **Эффективная обработка OOV-слов (Out-of-Vocabulary):**  \n",
        "  WordPiece способен разбивать неизвестные слова на знакомые подслова. Например, *\"программистом\"* → `[\"программ\", \"##ист\", \"##ом\"]`, даже если целое слово отсутствует в словаре. Это обеспечивает частичное понимание и устойчивость к лексической вариативности.\n",
        "\n",
        "- **Контроль размера словаря:**  \n",
        "  Размер словаря задаётся заранее (например, 30 000). Алгоритм итеративно выбирает наиболее \"полезные\" слияния, максимизируя правдоподобие текста. Это позволяет балансировать между компактностью и покрытием.\n",
        "\n",
        "- **Гибридный подход:**  \n",
        "  WordPiece сочетает преимущества символьной и словесной токенизации:\n",
        "  - Частые слова (например, `\"учитель\"`) становятся одним токеном.\n",
        "  - Редкие или сложные слова разбиваются на подслова.\n",
        "  - Все входные данные могут быть представлены — нет \"неизвестных\" символов.\n",
        "\n",
        "- **Универсальность (в варианте Byte-level WordPiece):**  \n",
        "  Байтовый WordPiece (используется в моделях вроде RoBERTa) работает на уровне байтов, а не символов Unicode. Это делает его **инвариантным к кодировке**, способным обрабатывать любой текст (включая эмодзи, редкие символы), и **гарантирует отсутствие [UNK]**.\n",
        "\n",
        "\n",
        "\n",
        "#### **5.2. Недостатки WordPiece**\n",
        "\n",
        "- **Жадный и детерминированный алгоритм:**  \n",
        "  На каждом шаге выбирается пара с максимальной оценкой. Такой подход не учитывает долгосрочных последствий и может привести к субоптимальной сегментации с точки зрения морфологии или семантики.\n",
        "\n",
        "- **Отсутствие лингвистических знаний:**  \n",
        "  WordPiece — статистический алгоритм. Он не \"понимает\" морфемы. Например, может разбить `\"бежать\"` на `[\"беж\", \"##ать\"]`, хотя лингвистически более корректно — `[\"бег\", \"##ать\"]`. Это снижает интерпретируемость и иногда — эффективность.\n",
        "\n",
        "- **Вычислительная сложность:**  \n",
        "  На ранних этапах обучения (при большом количестве символьных токенов) подсчёт частот всех биграмм требует значительных ресурсов. Для больших корпусов это может быть медленным.\n",
        "\n",
        "- **Зависимость от начальной токенизации:**  \n",
        "  Если перед WordPiece используется разбиение по словам, то ошибки на этом этапе (например, неудачная обработка знаков препинания) могут повлиять на итоговый словарь и сегментацию.\n",
        "\n",
        "- **Неоднозначность сегментации:**  \n",
        "  Одно и то же слово может быть разбито по-разному в зависимости от истории слияний. Например, `\"ученик\"` может стать `[\"уч\", \"##еник\"]` или `[\"учени\", \"##к\"]` — в зависимости от того, какие пары были выбраны ранее. Хотя на практике это редко вызывает проблемы, теоретически снижает согласованность.\n",
        "\n",
        "\n",
        "### **Заключение**\n",
        "\n",
        "WordPiece стал **краеугольным камнем** современных NLP-моделей. Его способность эффективно справляться с неизвестными словами, контролировать размер словаря и сохранять баланс между символами и словами сделала его выбором по умолчанию для таких моделей, как **BERT** и **DistilBERT**.\n",
        "\n",
        "Однако его ограничения — жадность, отсутствие морфологического понимания — стимулировали разработку альтернатив:\n",
        "- **SentencePiece** — обучается без предварительной токенизации по словам.\n",
        "- **Unigram Language Model** — использует вероятностную модель для сегментации.\n",
        "- **BoundlessBPE / BBPE** — улучшает морфологическую целостность.\n",
        "\n",
        "Эта эволюция показывает: **нет универсального идеального токенизатора**. Выбор зависит от задачи, языка, требований к размеру модели и компромисса между эффективностью, точностью и универсальностью.\n"
      ],
      "metadata": {
        "id": "Q91FH4o2MxNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.merges = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Предварительная обработка текста: приведение к нижнему регистру и разбиение на слова.\n",
        "        \"\"\"\n",
        "        # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        # Разбиваем на слова по пробелам\n",
        "        words = processed_text.split(' ')\n",
        "        # Добавляем пробелы как отдельные токены между словами\n",
        "        initial_tokens = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word: # Убедимся, что слово не пустое\n",
        "                initial_tokens.extend(list(word))\n",
        "            if i < len(words) - 1:\n",
        "                initial_tokens.append(' ') # Добавляем пробел между словами\n",
        "        return initial_tokens, words # Возвращаем начальные токены и список слов для удобства\n",
        "\n",
        "    def calculate_frequencies(self, tokens):\n",
        "        \"\"\"\n",
        "        Подсчет частот отдельных токенов (униграмм) и биграмм в текущем корпусе.\n",
        "        \"\"\"\n",
        "        unigram_freq = collections.defaultdict(int)\n",
        "        bigram_freq = collections.defaultdict(int)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            unigram_freq[tokens[i]] += 1\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram_freq[(tokens[i], tokens[i+1])] += 1\n",
        "        return unigram_freq, bigram_freq\n",
        "\n",
        "    def calculate_score(self, unigram_freq, bigram_freq):\n",
        "        \"\"\"\n",
        "        Вычисление оценки слияния для всех возможных биграмм.\n",
        "        Score(A, B) = frequency(AB) / (frequency(A) * frequency(B))\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for (token_a, token_b), freq_ab in bigram_freq.items():\n",
        "            freq_a = unigram_freq[token_a]\n",
        "            freq_b = unigram_freq[token_b]\n",
        "            if freq_a > 0 and freq_b > 0: # Избегаем деления на ноль\n",
        "                score = freq_ab / (freq_a * freq_b)\n",
        "                scores[(token_a, token_b)] = score\n",
        "        return scores\n",
        "\n",
        "    def merge_tokens(self, tokens, best_bigram):\n",
        "        \"\"\"\n",
        "        Слияние лучшей биграммы в корпусе.\n",
        "        \"\"\"\n",
        "        merged_token = \"\".join(best_bigram)\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == best_bigram:\n",
        "                new_tokens.append(merged_token)\n",
        "                i += 2 # Пропускаем оба слитых токена\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        return new_tokens\n",
        "\n",
        "    def train(self, text, target_vocab_size=None, num_merges=None):\n",
        "        \"\"\"\n",
        "        Обучение WordPiece токенизатора.\n",
        "        Итеративно сливает токены до достижения целевого размера словаря\n",
        "        или заданного количества слияний.\n",
        "        \"\"\"\n",
        "        print(f\"Исходный текст: '{text}'\")\n",
        "        current_tokens, _ = self.preprocess_text(text)\n",
        "\n",
        "        # Начальный словарь состоит из всех уникальных символов\n",
        "        self.vocab = set(current_tokens)\n",
        "        print(f\"\\nШаг 1: Начальный словарь (V0) содержит {len(self.vocab)} токенов.\")\n",
        "        print(f\"Начальный корпус: {current_tokens}\")\n",
        "\n",
        "        merges_count = 0\n",
        "        while True:\n",
        "            unigram_freq, bigram_freq = self.calculate_frequencies(current_tokens)\n",
        "            scores = self.calculate_score(unigram_freq, bigram_freq)\n",
        "\n",
        "            if not scores:\n",
        "                print(\"\\nНет больше биграмм для слияния. Остановка.\")\n",
        "                break\n",
        "\n",
        "            # Находим биграмму с наивысшей оценкой\n",
        "            best_bigram = max(scores, key=scores.get)\n",
        "            best_score = scores[best_bigram]\n",
        "\n",
        "            merged_token = \"\".join(best_bigram)\n",
        "\n",
        "            # Критерий остановки: если новый токен уже в словаре\n",
        "            if merged_token in self.vocab:\n",
        "                # Если лучший токен уже существует, удалим его из scores, чтобы найти следующий лучший\n",
        "                del scores[best_bigram]\n",
        "                continue # Продолжаем поиск\n",
        "\n",
        "            # Выводим информацию о текущем слиянии\n",
        "            print(f\"\\nИтерация {merges_count + 1}:\")\n",
        "            print(f\"  Лучшая биграмма для слияния: '{best_bigram[0]}' + '{best_bigram[1]}' -> '{merged_token}' (Score: {best_score:.4f})\")\n",
        "\n",
        "            # Выполняем слияние в корпусе\n",
        "            current_tokens = self.merge_tokens(current_tokens, best_bigram)\n",
        "\n",
        "            # Добавляем новый токен в словарь и сохраняем слияние\n",
        "            self.vocab.add(merged_token)\n",
        "            self.merges.append(best_bigram)\n",
        "            merges_count += 1\n",
        "\n",
        "            print(f\"  Корпус после слияния: {current_tokens}\")\n",
        "            print(f\"  Текущий размер словаря: {len(self.vocab)}\")\n",
        "\n",
        "            # Критерии остановки\n",
        "            if target_vocab_size is not None and len(self.vocab) >= target_vocab_size:\n",
        "                print(f\"\\nДостигнут целевой размер словаря ({target_vocab_size}). Остановка.\")\n",
        "                break\n",
        "            if num_merges is not None and merges_count >= num_merges:\n",
        "                print(f\"\\nДостигнуто заданное количество слияний ({num_merges}). Остановка.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nОбучение завершено. Итоговый словарь содержит {len(self.vocab)} токенов.\")\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Токенизация нового текста с использованием обученного словаря WordPiece.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'vocab') or len(self.vocab) == 0:\n",
        "            raise ValueError(\"Токенизатор не обучен. Сначала вызовите метод train().\")\n",
        "\n",
        "        print(f\"\\n--- Токенизация нового текста ---\")\n",
        "        print(f\"Текст для токенизации: '{text}'\")\n",
        "\n",
        "        # Предварительная обработка текста для токенизации\n",
        "        processed_text = text.lower().replace('...', '').replace('.', '')\n",
        "        words = processed_text.split(' ')\n",
        "\n",
        "        final_tokens = []\n",
        "        for word in words:\n",
        "            if not word: # Пропускаем пустые строки, если они возникли из-за множественных пробелов\n",
        "                continue\n",
        "\n",
        "            word_tokens = []\n",
        "            remaining_word = word\n",
        "\n",
        "            while remaining_word:\n",
        "                found_match = False\n",
        "                # Ищем самый длинный подтокен из словаря, который является префиксом оставшейся части слова\n",
        "                for i in range(len(remaining_word), 0, -1):\n",
        "                    subword = remaining_word[:i]\n",
        "\n",
        "                    if subword in self.vocab:\n",
        "                        word_tokens.append(subword)\n",
        "                        remaining_word = remaining_word[i:]\n",
        "                        found_match = True\n",
        "                        break\n",
        "\n",
        "                if not found_match:\n",
        "                    # Если не удалось найти соответствующий токен, разбиваем на символы\n",
        "                    # или используем токен [UNK]\n",
        "                    if remaining_word[0] in self.vocab: # Если символ есть в словаре\n",
        "                        word_tokens.append(remaining_word[0])\n",
        "                    else: # Если символ даже не в начальном словаре\n",
        "                        word_tokens.append('[UNK]')\n",
        "                    remaining_word = remaining_word[1:]\n",
        "\n",
        "            final_tokens.extend(word_tokens)\n",
        "            final_tokens.append(' ') # Добавляем пробел между токенизированными словами\n",
        "\n",
        "        # Удаляем последний пробел, если он есть\n",
        "        if final_tokens and final_tokens[-1] == ' ':\n",
        "            final_tokens.pop()\n",
        "\n",
        "        print(f\"Токенизированный текст: {final_tokens}\")\n",
        "        return final_tokens\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Возвращает текущий словарь токенов.\"\"\"\n",
        "        return sorted(list(self.vocab))\n",
        "\n",
        "    def get_merges(self):\n",
        "        \"\"\"Возвращает список выполненных слияний.\"\"\"\n",
        "        return self.merges\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    text_to_train = \"В училище учитель учит ученикам по новому учебнику\"\n",
        "\n",
        "    # Создаем экземпляр токенизатора\n",
        "    tokenizer = WordPieceTokenizer()\n",
        "\n",
        "    # Обучаем токенизатор\n",
        "    final_vocab = tokenizer.train(text_to_train, num_merges=50)\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece: {final_vocab}\")\n",
        "\n",
        "    # Токенизируем тот же текст\n",
        "    tokenizer.tokenize(text_to_train)\n",
        "\n",
        "    # Пример токенизации нового слова\n",
        "    new_text = \"учительница\"\n",
        "    # Добавим '##ница' в словарь для демонстрации\n",
        "    if 'ница' not in final_vocab:\n",
        "        tokenizer.vocab.add('ница')\n",
        "        tokenizer.vocab.add('##ница')\n",
        "        final_vocab = tokenizer.get_vocab()\n",
        "\n",
        "    print(f\"\\nИтоговый словарь WordPiece (с 'ница' для демонстрации): {final_vocab}\")\n",
        "    tokenizer.tokenize(new_text)\n",
        "\n",
        "    # Демонстрация с упрощенным словарем\n",
        "    simplified_tokenizer = WordPieceTokenizer()\n",
        "    simplified_tokenizer.vocab = set(['у', 'ч', 'и', 'т', 'е', 'л', 'ь', 'н', 'ц', 'а', 'учитель', '##ница', '##тель', '##а'])\n",
        "    print(f\"\\n--- Демонстрация токенизации подслов с упрощенным словарем ---\")\n",
        "    simplified_tokenizer.tokenize(\"учительница\")"
      ],
      "metadata": {
        "id": "ZFIH_P7sO8cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Униграмная Языковая Модель: Фундаментальные Принципы и Применение в НЛП**\n",
        "\n",
        "\n",
        "\n",
        "### **1. Введение в Языковое Моделирование**\n",
        "\n",
        "Языковое моделирование — одна из ключевых задач в обработке естественного языка (Natural Language Processing, NLP). Его основная цель — оценить вероятность появления последовательности слов, предсказать следующее слово в контексте или определить, насколько \"естественно\" звучит заданный фрагмент текста.\n",
        "\n",
        "Языковые модели лежат в основе множества NLP-приложений:\n",
        "- машинный перевод,\n",
        "- распознавание речи,\n",
        "- проверка орфографии и грамматики,\n",
        "- генерация текста,\n",
        "- информационный поиск,\n",
        "- классификация и кластеризация текстов.\n",
        "\n",
        "**Униграмная языковая модель (Unigram Language Model)** — это самая простая и базовая вероятностная модель языка. Она не учитывает порядок слов и контекст, но служит важным теоретическим и практическим фундаментом для понимания более сложных моделей, таких как биграммы, триграммы и нейросетевые языковые модели.\n",
        "\n",
        "> ⚠️ **Важно:** Униграмная модель **не выполняет токенизацию**. Токенизация — это предварительный этап, в ходе которого текст разбивается на токены (слова, символы и т.д.). Униграмная модель работает **с уже токенизированными данными**, оценивая вероятности отдельных токенов.\n",
        "\n",
        "\n",
        "\n",
        "### **2. Основные Концепции Униграмной Модели**\n",
        "\n",
        "Центральное предположение униграмной модели — **независимость слов**. Модель считает, что каждое слово в тексте появляется независимо от других слов. То есть:\n",
        "- Вероятность слова не зависит от предыдущих или последующих слов.\n",
        "- Порядок слов в последовательности не имеет значения.\n",
        "\n",
        "Формально, вероятность последовательности слов $w_1, w_2, \\dots, w_n$ в униграмной модели вычисляется как:\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_n) = \\prod_{i=1}^{n} P(w_i)\n",
        "$$\n",
        "где $P(w_i)$ — вероятность отдельного слова $w_i$, оцениваемая по частоте его встречаемости в обучающем корпусе:\n",
        "$$\n",
        "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total\\_words}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **Пример:**\n",
        "\n",
        "Рассмотрим предложение: *\"кот сидит на коврике\"*.  \n",
        "Униграмная модель оценит его вероятность как:\n",
        "$$\n",
        "P(\\text{кот}) \\times P(\\text{сидит}) \\times P(\\text{на}) \\times P(\\text{коврике})\n",
        "$$\n",
        "При этом модель не различает, является ли это осмысленным предложением или просто случайной перестановкой слов — например, *\"на сидит коврике кот\"* будет иметь ту же вероятность.\n",
        "\n",
        "\n",
        "### **2.1. Области Применения Униграмных Моделей**\n",
        "\n",
        "Несмотря на упрощённое предположение о независимости слов, униграмные модели находят практическое применение в задачах, где **частота слов** является ключевым фактором, а контекст — вторичным.\n",
        "\n",
        "#### **• Базовая оценка вероятности текста**  \n",
        "Униграмная модель может дать грубую оценку \"естественности\" текста. Например, в задачах определения авторства или жанра можно сравнивать распределения слов в разных текстах. Хотя это не учитывает синтаксис, оно может выявить стилистические особенности (например, частота местоимений или глаголов).\n",
        "\n",
        "#### **• Тематическое моделирование (Topic Modeling)**  \n",
        "Униграмные модели позволяют выявить наиболее частотные слова в документе. Эти слова часто коррелируют с основной темой. Например, высокая частота слов *\"футбол\"*, *\"гол\"*, *\"матч\"* указывает на спортивную тематику. Этот подход лежит в основе простых методов, таких как **TF-IDF** и **мешок слов (Bag-of-Words)**.\n",
        "\n",
        "#### **• Спам-фильтрация**  \n",
        "В классических фильтрах (например, на основе наивного байесовского классификатора) униграмные вероятности используются для оценки того, насколько \"спамовым\" является письмо. Если вероятность слов *\"выигрыш\"*, *\"бесплатно\"*, *\"акция\"* в спам-корпусе высока, то письмо с их высокой частотой будет классифицировано как спам.\n",
        "\n",
        "#### **• Поиск информации (Information Retrieval)**  \n",
        "В простых системах поиска (например, в ранних версиях TF-IDF) униграмные частоты слов используются для оценки релевантности документа запросу. Документ, содержащий больше слов из запроса, считается более релевантным — независимо от порядка слов.\n",
        "\n",
        "#### **• Оценка сложности текста**  \n",
        "Униграмные модели могут применяться для приблизительной оценки читабельности или сложности текста. Например:\n",
        "- Высокая частота редких слов → более сложный текст.\n",
        "- Большое количество уникальных слов (высокое лексическое разнообразие) → выше сложность.\n",
        "\n",
        "\n",
        "\n",
        "### **3. Математические Формулы Униграмной Модели**\n",
        "\n",
        "\n",
        "\n",
        "#### **3.1. Вероятность последовательности слов**\n",
        "\n",
        "Пусть дана последовательность слов $W = (w_1, w_2, \\dots, w_n)$.  \n",
        "В униграмной языковой модели предполагается, что слова появляются **независимо** друг от друга. Это позволяет выразить вероятность всей последовательности как **произведение вероятностей отдельных слов**:\n",
        "\n",
        "$$\n",
        "P(W) = P(w_1, w_2, \\dots, w_n) = \\prod_{i=1}^{n} P(w_i)\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $P(w_i)$ — вероятность слова $w_i$ в языковой модели,\n",
        "- $\\prod$ — знак произведения по всем словам в последовательности.\n",
        "\n",
        "> ⚠️ **Важно:** Это предположение игнорирует как синтаксис, так и семантику. Например, последовательности *\"кошка спит на диване\"* и *\"на спит диване кошка\"* будут иметь одинаковую вероятность, если все слова одинаковы.\n",
        "\n",
        "\n",
        "\n",
        "#### **3.2. Оценка вероятности слова: Максимальное правдоподобие (MLE)**\n",
        "\n",
        "На практике вероятности слов $P(w)$ неизвестны и должны быть оценены на основе обучающего корпуса. Наиболее распространённый способ — **метод максимального правдоподобия (Maximum Likelihood Estimation, MLE)**.\n",
        "\n",
        "Согласно MLE, вероятность слова $w$ оценивается как его **относительная частота** в обучающем корпусе:\n",
        "\n",
        "$$\n",
        "P(w) = \\frac{\\text{Count}(w)}{N}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $\\text{Count}(w)$ — количество вхождений слова $w$ в обучающий корпус,\n",
        "- $N = \\sum_{w' \\in V} \\text{Count}(w')$ — общее количество слов в корпусе,\n",
        "- $V$ — словарь корпуса (множество всех уникальных слов).\n",
        "\n",
        "> **Интуиция:** Если слово *\"кот\"* встречается 10 раз в корпусе из 1000 слов, то его оценённая вероятность будет:\n",
        ">$$\n",
        "> P(\\text{кот}) = \\frac{10}{1000} = 0.01\n",
        ">$$\n",
        "> Это значение \"максимизирует правдоподобие\" обучающих данных — то есть делает наблюдаемый корпус наиболее вероятным при заданных параметрах модели.\n",
        "\n",
        "\n",
        "\n",
        "#### **3.3. Проблема нулевых частот и сглаживание (Smoothing)**\n",
        "\n",
        "**Критическая проблема MLE:**  \n",
        "Если слово $w$ **не встречалось** в обучающем корпусе, то $\\text{Count}(w) = 0$, и, следовательно, $P(w) = 0$.\n",
        "\n",
        "Это приводит к **катастрофической ошибке**: любая последовательность, содержащая это слово, будет иметь нулевую вероятность:\n",
        "$$\n",
        "P(\\dots, w, \\dots) = \\dots \\times P(w) \\times \\dots = \\dots \\times 0 \\times \\dots = 0\n",
        "$$\n",
        "Модель становится **необобщаемой** — она не может обрабатывать ни одно новое слово, что делает её бесполезной в реальных условиях.\n",
        "\n",
        "\n",
        "\n",
        "##### **Решение: Сглаживание (Smoothing)**\n",
        "\n",
        "Чтобы решить эту проблему, применяются методы **сглаживания** — они перераспределяют часть вероятностной массы от наблюдаемых слов к ненаблюдаемым, гарантируя, что **все возможные слова имеют ненулевую вероятность**.\n",
        "\n",
        "\n",
        "\n",
        "##### **Сглаживание по Лапласу (Laplace Smoothing / Add-One Smoothing)**\n",
        "\n",
        "Один из самых простых методов — **сглаживание по Лапласу**, при котором к счётчику каждого слова (включая не встретившиеся) добавляется 1:\n",
        "\n",
        "$$\n",
        "P_{\\text{Laplace}}(w) = \\frac{\\text{Count}(w) + 1}{N + |V|}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $\\text{Count}(w)$ — частота слова $w$ в корпусе,\n",
        "- $N$ — общее количество слов в корпусе,\n",
        "- $|V|$ — размер словаря (число уникальных слов в корпусе).\n",
        "\n",
        "> ⚠️ **Примечание:** В знаменателе добавляется $|V|$, потому что мы \"добавляем по единице\" для каждого из $|V|$ возможных слов.\n",
        "\n",
        "\n",
        "\n",
        "##### **Интуиция сглаживания по Лапласу:**\n",
        "\n",
        "Представим, что **каждое возможное слово из словаря** встретилось в корпусе **хотя бы один раз**. Это искусственно \"расширяет\" корпус и предотвращает нулевые вероятности.\n",
        "\n",
        "> **Пример:**  \n",
        "> Корпус: `[\"кот\", \"спит\", \"кот\", \"еда\"]`  \n",
        "> - $N = 4$, $|V| = 4$ (слова: \"кот\", \"спит\", \"еда\", и предполагаемое \"новое\")  \n",
        "> - $P_{\\text{MLE}}(\\text{собака}) = 0$  \n",
        "> - $P_{\\text{Laplace}}(\\text{собака}) = \\frac{0 + 1}{4 + 4} = \\frac{1}{8} = 0.125$\n",
        "\n",
        "Теперь даже незнакомые слова имеют шанс.\n",
        "\n",
        "\n",
        "\n",
        "##### **Недостатки сглаживания по Лапласу:**\n",
        "- Сильно **искажает вероятности** частотных слов, особенно при большом $|V|$.\n",
        "- Не подходит для больших словарей (например, в языковых моделях с десятками тысяч слов), так как \"перераспределяет слишком много\" вероятности.\n",
        "- Поэтому на практике чаще используются более продвинутые методы: **Good-Turing**, **Witten-Bell**, **Kneser-Ney smoothing**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **4. Конкретный Пример Применения Униграмной Модели**\n",
        "\n",
        "Рассмотрим применение униграмной языковой модели к следующему тексту:  \n",
        "**\"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"**\n",
        "\n",
        "\n",
        "\n",
        "#### **4.1. Шаг 1: Токенизация и подготовка корпуса**\n",
        "\n",
        "Перед построением модели необходимо предварительно обработать текст:\n",
        "- Привести к нижнему регистру,\n",
        "- Удалить знаки препинания,\n",
        "- Разделить на слова (токены).\n",
        "\n",
        "**Исходный текст:**  \n",
        "\"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"\n",
        "\n",
        "**Токенизированный корпус (после обработки):**  \n",
        "`['ученики', 'пишут', 'диктант', 'учитель', 'диктует', 'мы', 'пишем', 'он', 'пишет', 'они', 'пишут', 'пишите', 'внимательно']`\n",
        "\n",
        "**Общее количество слов в корпусе (N):**  \n",
        "$$\n",
        "N = 13\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **4.2. Шаг 2: Подсчёт частот слов**\n",
        "\n",
        "Составим таблицу частот для каждого уникального слова:\n",
        "\n",
        "| Слово         | Частота $\\text{Count}(w)$ |\n",
        "|---------------|-------------------------------|\n",
        "| ученики       | 1                             |\n",
        "| пишут         | 2                             |\n",
        "| диктант       | 1                             |\n",
        "| учитель       | 1                             |\n",
        "| диктует       | 1                             |\n",
        "| мы            | 1                             |\n",
        "| пишем         | 1                             |\n",
        "| он            | 1                             |\n",
        "| пишет         | 1                             |\n",
        "| они           | 1                             |\n",
        "| пишите        | 1                             |\n",
        "| внимательно   | 1                             |\n",
        "\n",
        "> ⚠️ Обратите внимание: слово **\"пишут\"** встречается дважды — в начале и в середине текста.\n",
        "\n",
        "\n",
        "\n",
        "#### **4.3. Шаг 3: Определение словаря и его размера**\n",
        "\n",
        "**Словарь $V$** — множество всех уникальных слов в корпусе:  \n",
        "$$\n",
        "V = \\{ \\text{'ученики'}, \\text{'пишут'}, \\text{'диктант'}, \\text{'учитель'}, \\text{'диктует'}, \\text{'мы'}, \\text{'пишем'}, \\text{'он'}, \\text{'пишет'}, \\text{'они'}, \\text{'пишите'}, \\text{'внимательно'} \\}\n",
        "$$\n",
        "\n",
        "**Размер словаря $|V|$:**  \n",
        "$$\n",
        "|V| = 12\n",
        "$$\n",
        "\n",
        "\n",
        "#### **4.4. Шаг 4: Расчёт вероятностей слов (MLE)**\n",
        "\n",
        "Используем метод максимального правдоподобия (MLE):  \n",
        "$$\n",
        "P(w) = \\frac{\\text{Count}(w)}{N} = \\frac{\\text{Count}(w)}{13}\n",
        "$$\n",
        "\n",
        "Вычислим вероятности:\n",
        "\n",
        "| Слово         | $\\text{Count}(w)$ | $P(w)$             |\n",
        "|---------------|------------------------|-------------------------|\n",
        "| ученики       | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| пишут         | 2                      | $\\frac{2}{13} \\approx 0.1538$ |\n",
        "| диктант       | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| учитель       | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| диктует       | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| мы            | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| пишем         | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| он            | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| пишет         | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| они           | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| пишите        | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "| внимательно   | 1                      | $\\frac{1}{13} \\approx 0.0769$ |\n",
        "\n",
        "> Слово **\"пишут\"** имеет наибольшую вероятность ($\\approx 0.1538$) из-за своей высокой частоты.\n",
        "\n",
        "\n",
        "\n",
        "#### **4.5. Шаг 5: Расчёт вероятности предложения**\n",
        "\n",
        "Оценим вероятность всей последовательности (все слова из корпуса в порядке их появления):\n",
        "\n",
        "$$\n",
        "P(\\text{предложение}) = \\prod_{i=1}^{13} P(w_i)\n",
        "$$\n",
        "\n",
        "Распишем произведение:\n",
        "$$\n",
        "P = P(\\text{ученики}) \\times P(\\text{пишут}) \\times P(\\text{диктант}) \\times P(\\text{учитель}) \\times P(\\text{диктует}) \\times P(\\text{мы}) \\times P(\\text{пишем}) \\times P(\\text{он}) \\times P(\\text{пишет}) \\times P(\\text{они}) \\times P(\\text{пишут}) \\times P(\\text{пишите}) \\times P(\\text{внимательно})\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "$$\n",
        "P = \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{2}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{2}{13} \\right) \\times \\left( \\frac{1}{13} \\right) \\times \\left( \\frac{1}{13} \\right)\n",
        "$$\n",
        "\n",
        "Упростим:\n",
        "$$\n",
        "P = \\frac{1 \\times 2 \\times 1 \\times 1 \\times 1 \\times 1 \\times 1 \\times 1 \\times 1 \\times 1 \\times 2 \\times 1 \\times 1}{13^{13}} = \\frac{4}{13^{13}}\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "$$\n",
        "13^{13} \\approx 3.028 \\times 10^{14}, \\quad \\Rightarrow P \\approx \\frac{4}{3.028 \\times 10^{14}} \\approx 1.32 \\times 10^{-14}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{P(\\text{предложение}) \\approx 1.32 \\times 10^{-14}}\n",
        "$$\n",
        "\n",
        "> ⚠️ Несмотря на то что текст осмысленный, его вероятность крайне мала — это типично для униграмных моделей из-за умножения множества малых вероятностей.\n",
        "\n",
        "\n",
        "\n",
        "#### **4.6. Пример со сглаживанием по Лапласу**\n",
        "\n",
        "Предположим, мы хотим оценить вероятность слова **\"школа\"**, которого **нет в обучающем корпусе**.\n",
        "\n",
        "- **Без сглаживания (MLE):**  \n",
        "$$\n",
        "  P(\\text{школа}) = \\frac{\\text{Count}(\\text{школа})}{N} = \\frac{0}{13} = 0\n",
        "$$\n",
        "  Это означает, что любое предложение, содержащее слово \"школа\", будет иметь нулевую вероятность.\n",
        "\n",
        "- **Со сглаживанием по Лапласу:**  \n",
        "  Используем формулу:\n",
        "$$\n",
        "  P_{\\text{Laplace}}(w) = \\frac{\\text{Count}(w) + 1}{N + |V|}\n",
        "$$\n",
        "  Подставим значения:\n",
        "  - $\\text{Count}(\\text{школа}) = 0$\n",
        "  - $N = 13$\n",
        "  - $|V| = 12$\n",
        "\n",
        "$$\n",
        "  P_{\\text{Laplace}}(\\text{школа}) = \\frac{0 + 1}{13 + 12} = \\frac{1}{25} = 0.04\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{P_{\\text{Laplace}}(\\text{школа}) = 0.04}\n",
        "$$\n",
        "\n",
        "Теперь слово \"школа\" имеет **ненулевую вероятность**, и модель может обрабатывать новые, ранее не встречавшиеся слова. Это делает её **более робастной** в реальных условиях.\n",
        "\n",
        "\n",
        "\n",
        "### **5. Преимущества и Недостатки Униграмной Модели**\n",
        "\n",
        "Несмотря на свою простоту, униграмная языковая модель обладает как сильными сторонами, так и серьёзными ограничениями. Понимание этих аспектов помогает оценить её место в иерархии языковых моделей и определить области, где она остаётся полезной.\n",
        "\n",
        "\n",
        "#### **5.1. Преимущества**\n",
        "\n",
        "- **Простота и интуитивность**  \n",
        "  Униграмная модель основана на простой идее: частота слова напрямую отражает его вероятность. Это делает модель крайне простой для понимания, объяснения и реализации. Она служит отличной **обучающей платформой** для изучения основ языкового моделирования.\n",
        "\n",
        "- **Вычислительная эффективность**  \n",
        "  Обучение модели сводится к подсчёту частот слов — операции со сложностью $O(N)$, где $N$ — размер корпуса. Использование модели (оценка вероятности последовательности) также требует лишь умножения вероятностей. Это делает униграмные модели **очень быстрыми** и пригодными для обработки **огромных объёмов данных** или встраивания в системы с жёсткими требованиями к производительности.\n",
        "\n",
        "- **Эффективная оценка частот слов**  \n",
        "  Модель идеально подходит для задач, где **ключевую роль играют частоты отдельных слов**, а порядок не важен. Это включает:\n",
        "  - тематическое моделирование,\n",
        "  - фильтрацию спама,\n",
        "  - предварительный анализ текста,\n",
        "  - построение словарей и оценку лексического разнообразия.\n",
        "\n",
        "- **Отсутствие проблемы разреженности (в сравнении с высокими N-граммами)**  \n",
        "  В отличие от биграмм или триграмм, где многие возможные комбинации слов отсутствуют в корпусе (что приводит к разреженности данных), униграмная модель работает с отдельными словами. Для **частотных слов** проблема разреженности минимальна, так как они, как правило, наблюдаются в корпусе.\n",
        "\n",
        "\n",
        "\n",
        "#### **5.2. Недостатки**\n",
        "\n",
        "- **Полное игнорирование контекста и порядка слов**  \n",
        "  Это — **главный и наиболее критичный недостаток**. Униграмная модель не учитывает ни синтаксис, ни семантику. Она не различает осмысленные и бессмысленные последовательности, если они содержат одни и те же слова.  \n",
        "  Например, предложения *\"собака укусила человека\"* и *\"человек укусил собаку\"* будут иметь **одинаковую вероятность**, хотя их смысл диаметрально противоположен.\n",
        "\n",
        "- **Нереалистичное предположение о независимости слов**  \n",
        "  Естественный язык — это последовательность взаимозависимых слов. Вероятность появления слова *\"кофе\"* сильно возрастает после *\"я пью\"*, но почти нулевая после *\"я лаю\"*. Униграмная модель **не может улавливать такие зависимости**, что делает её слишком грубой для задач, требующих понимания смысла.\n",
        "\n",
        "- **Проблема нулевых вероятностей (OOV-слова)**  \n",
        "  Без применения сглаживания любое слово, не встретившееся в обучающем корпусе, получает вероятность 0. Это означает, что **любое предложение с новым словом** будет иметь нулевую вероятность — модель необобщаема. Хотя сглаживание решает эту проблему частично, оно вносит искажения в распределение.\n",
        "\n",
        "- **Ограниченная применимость в современном НЛП**  \n",
        "  Из-за отсутствия контекстной чувствительности униграмные модели **не подходят** для большинства современных задач, таких как:\n",
        "  - генерация текста,\n",
        "  - машинный перевод,\n",
        "  - вопросно-ответные системы,\n",
        "  - распознавание речи,\n",
        "  - суммаризация.  \n",
        "  Эти задачи требуют понимания последовательности, зависимостей и смысла — чего униграмная модель дать не может.\n",
        "\n",
        "\n",
        "### **Заключение**\n",
        "\n",
        "Униграмная языковая модель — это **упрощённая, но важная абстракция**, которая демонстрирует базовые принципы вероятностного моделирования в НЛП. Её сила — в простоте, скорости и интерпретируемости. Её слабость — в полном игнорировании контекста.\n",
        "\n",
        "Однако именно эти ограничения стали **двигателем прогресса** в области языкового моделирования:\n",
        "- от униграммы → к биграммам и триграммам (с учётом контекста),\n",
        "- от n-грамм → к нейросетевым моделям (RNN, LSTM, трансформерам),\n",
        "- от частотных подходов → к распределённым представлениям слов (word embeddings, BERT и др.).\n",
        "\n",
        "Таким образом, униграмная модель — не инструмент для современных сложных систем, а **необходимая ступень на пути к ним**. Она помогает понять, **почему контекст важен**, и почему более сложные модели стали неизбежными в эпоху глубокого обучения.\n"
      ],
      "metadata": {
        "id": "GIaN_NWePT8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "import math\n",
        "\n",
        "class UnigramLanguageModel:\n",
        "    \"\"\"\n",
        "    Реализация Униграмной Языковой Модели.\n",
        "    Эта модель предполагает, что вероятность каждого слова в последовательности\n",
        "    не зависит от других слов.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.word_counts = collections.defaultdict(int) # Счетчик вхождений каждого слова\n",
        "        self.total_words = 0 # Общее количество слов в корпусе\n",
        "        self.vocabulary = set() # Множество уникальных слов (словарь)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Приватный метод для токенизации текста.\n",
        "        Приводит текст к нижнему регистру и разбивает на слова, удаляя знаки препинания.\n",
        "        :param text: Входная строка текста.\n",
        "        :return: Список токенов (слов).\n",
        "        \"\"\"\n",
        "        # Приводим к нижнему регистру\n",
        "        text = text.lower()\n",
        "        # Удаляем знаки препинания (кроме тех, что могут быть частью слова, но для простоты здесь удаляем все)\n",
        "        # Использование регулярного выражения для извлечения только буквенных последовательностей\n",
        "        tokens = re.findall(r'\\b[а-яё]+\\b', text)\n",
        "        return tokens\n",
        "\n",
        "    def train(self, corpus_text):\n",
        "        \"\"\"\n",
        "        Обучает униграмную языковую модель на заданном текстовом корпусе.\n",
        "        Подсчитывает частоты слов и формирует словарь.\n",
        "        :param corpus_text: Строка, представляющая обучающий корпус.\n",
        "        \"\"\"\n",
        "        print(\"--- Начало обучения Униграмной Модели ---\")\n",
        "        tokens = self._tokenize(corpus_text)\n",
        "\n",
        "        if not tokens:\n",
        "            print(\"Предупреждение: Корпус пуст или не содержит слов после токенизации.\")\n",
        "            return\n",
        "\n",
        "        self.total_words = len(tokens)\n",
        "\n",
        "        for word in tokens:\n",
        "            self.word_counts[word] += 1\n",
        "            self.vocabulary.add(word)\n",
        "\n",
        "        print(f\"Обучение завершено. Общее количество слов (N): {self.total_words}\")\n",
        "        print(f\"Размер словаря (|V|): {len(self.vocabulary)}\")\n",
        "        print(f\"Частоты слов: {dict(self.word_counts)}\")\n",
        "        print(\"--- Обучение завершено ---\")\n",
        "\n",
        "    def get_word_probability(self, word, smoothing=None):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность слова P(w).\n",
        "        :param word: Слово, для которого нужно рассчитать вероятность.\n",
        "        :param smoothing: Метод сглаживания ('laplace' для сглаживания по Лапласу, None для MLE).\n",
        "        :return: Вероятность слова.\n",
        "        \"\"\"\n",
        "        # Если модель не обучена или корпус пуст\n",
        "        if self.total_words == 0:\n",
        "            print(\"Ошибка: Модель не обучена или корпус пуст. Невозможно рассчитать вероятность.\")\n",
        "            return 0.0\n",
        "\n",
        "        count_w = self.word_counts[word] # Count(w)\n",
        "\n",
        "        if smoothing == 'laplace':\n",
        "            # Сглаживание по Лапласу: (Count(w) + 1) / (N + |V|)\n",
        "            probability = (count_w + 1) / (self.total_words + len(self.vocabulary))\n",
        "            # print(f\"P_Laplace('{word}') = ({count_w} + 1) / ({self.total_words} + {len(self.vocabulary)}) = {probability:.4f}\")\n",
        "        else:\n",
        "            # Метод максимального правдоподобия (MLE): Count(w) / N\n",
        "            if count_w == 0:\n",
        "                # print(f\"P_MLE('{word}') = 0 (слово не найдено в корпусе)\")\n",
        "                return 0.0 # Если слово не найдено, вероятность 0 без сглаживания\n",
        "            probability = count_w / self.total_words\n",
        "            # print(f\"P_MLE('{word}') = {count_w} / {self.total_words} = {probability:.4f}\")\n",
        "\n",
        "        return probability\n",
        "\n",
        "    def get_sequence_probability(self, sequence_text, smoothing=None):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность последовательности слов P(W).\n",
        "        :param sequence_text: Строка, представляющая последовательность слов.\n",
        "        :param smoothing: Метод сглаживания ('laplace' для сглаживания по Лапласу, None для MLE).\n",
        "        :return: Вероятность последовательности.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Расчет вероятности последовательности: '{sequence_text}' ---\")\n",
        "        tokens = self._tokenize(sequence_text)\n",
        "\n",
        "        if not tokens:\n",
        "            print(\"Предупреждение: Последовательность пуста или не содержит слов после токенизации.\")\n",
        "            return 0.0\n",
        "\n",
        "        sequence_probability = 1.0\n",
        "        probabilities_list = []\n",
        "\n",
        "        for word in tokens:\n",
        "            p_word = self.get_word_probability(word, smoothing)\n",
        "            probabilities_list.append(p_word)\n",
        "            sequence_probability *= p_word\n",
        "\n",
        "            # Если хоть одно слово имеет нулевую вероятность без сглаживания,\n",
        "            # вся последовательность будет иметь нулевую вероятность.\n",
        "            if smoothing != 'laplace' and p_word == 0:\n",
        "                print(f\"Слово '{word}' имеет нулевую вероятность. Вероятность всей последовательности = 0.\")\n",
        "                return 0.0 # Оптимизация: если P(w)=0, то произведение будет 0\n",
        "\n",
        "        print(f\"Токены последовательности: {tokens}\")\n",
        "        print(f\"Вероятности отдельных слов: {probabilities_list}\")\n",
        "        print(f\"Итоговая вероятность последовательности: {sequence_probability}\")\n",
        "        print(\"--- Расчет завершен ---\")\n",
        "        return sequence_probability\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_example = \"Ученики пишут диктант. Учитель диктует. Мы пишем, он пишет, они пишут. Пишите внимательно.\"\n",
        "\n",
        "    # Создаем экземпляр модели\n",
        "    unigram_model = UnigramLanguageModel()\n",
        "\n",
        "    # Обучаем модель на корпусе\n",
        "    unigram_model.train(corpus_example)\n",
        "\n",
        "    print(\"\\n--- Расчет вероятностей отдельных слов (MLE) ---\")\n",
        "    words_to_check = ['пишут', 'учитель', 'школа', 'мы']\n",
        "    for word in words_to_check:\n",
        "        prob = unigram_model.get_word_probability(word, smoothing=None)\n",
        "        print(f\"P_MLE('{word}') = {prob:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Расчет вероятностей отдельных слов (Лаплас) ---\")\n",
        "    for word in words_to_check:\n",
        "        prob = unigram_model.get_word_probability(word, smoothing='laplace')\n",
        "        print(f\"P_Laplace('{word}') = {prob:.4f}\")\n",
        "\n",
        "    # Расчет вероятности предложения (без сглаживания)\n",
        "    sentence_mle = \"Ученики пишут диктант\"\n",
        "    prob_mle = unigram_model.get_sequence_probability(sentence_mle, smoothing=None)\n",
        "    print(f\"Вероятность предложения (MLE): {prob_mle:.10f}\")\n",
        "\n",
        "    sentence_mle_oov = \"Ученики пишут школа\" # Содержит OOV слово 'школа'\n",
        "    prob_mle_oov = unigram_model.get_sequence_probability(sentence_mle_oov, smoothing=None)\n",
        "    print(f\"Вероятность предложения с OOV (MLE): {prob_mle_oov:.10f}\")\n",
        "\n",
        "    # Расчет вероятности предложения (со сглаживанием по Лапласу)\n",
        "    sentence_laplace = \"Ученики пишут диктант\"\n",
        "    prob_laplace = unigram_model.get_sequence_probability(sentence_laplace, smoothing='laplace')\n",
        "    print(f\"Вероятность предложения (Лаплас): {prob_laplace:.10f}\")\n",
        "\n",
        "    sentence_laplace_oov = \"Ученики пишут школа\" # Содержит OOV слово 'школа'\n",
        "    prob_laplace_oov = unigram_model.get_sequence_probability(sentence_laplace_oov, smoothing='laplace')\n",
        "    print(f\"Вероятность предложения с OOV (Лаплас): {prob_laplace_oov:.10f}\")\n",
        "\n",
        "    # Пример из документации: \"Ученики пишут диктант Учитель диктует Мы пишем он пишет они пишут Пишите внимательно\"\n",
        "    full_sentence = \"Ученики пишут диктант Учитель диктует Мы пишем он пишет они пишут Пишите внимательно\"\n",
        "    prob_full_mle = unigram_model.get_sequence_probability(full_sentence, smoothing=None)\n",
        "    print(f\"Вероятность полного предложения (MLE): {prob_full_mle:.15f}\")\n",
        "\n",
        "    prob_full_laplace = unigram_model.get_sequence_probability(full_sentence, smoothing='laplace')\n",
        "    print(f\"Вероятность полного предложения (Лаплас): {prob_full_laplace:.15f}\")"
      ],
      "metadata": {
        "id": "v0GXAfmeVNkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FRzLd16z5BwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "class SentencePieceTokenizer:\n",
        "    \"\"\"\n",
        "    Класс для обучения, сохранения и использования модели токенизации SentencePiece.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_prefix=\"my_sentencepiece_model\"):\n",
        "        \"\"\"\n",
        "        Инициализирует токенизатор.\n",
        "\n",
        "        Args:\n",
        "            model_prefix (str): Префикс для файлов модели и словаря.\n",
        "                                 Будут созданы файлы <model_prefix>.model и <model_prefix>.vocab.\n",
        "        \"\"\"\n",
        "        self.model_prefix = model_prefix\n",
        "        self.sp_processor = None\n",
        "        self.model_path = f\"{model_prefix}.model\"\n",
        "        self.vocab_path = f\"{model_prefix}.vocab\"\n",
        "\n",
        "    def train(self, input_file, vocab_size=8000, model_type=\"unigram\", character_coverage=1.0, num_threads=os.cpu_count()):\n",
        "        \"\"\"\n",
        "        Обучает модель SentencePiece на заданном текстовом корпусе.\n",
        "\n",
        "        Args:\n",
        "            input_file (str): Путь к файлу с необработанным текстовым корпусом.\n",
        "                              Каждый абзац или предложение должен быть на новой строке.\n",
        "            vocab_size (int): Желаемый размер словаря подслов.\n",
        "            model_type (str): Тип алгоритма токенизации ('bpe' или 'unigram').\n",
        "            character_coverage (float): Доля символов, которые должны быть покрыты моделью (от 0.0 до 1.0).\n",
        "            num_threads (int): Количество потоков для параллельного обучения.\n",
        "        \"\"\"\n",
        "        print(f\"Начинаем обучение модели SentencePiece с параметрами:\")\n",
        "        print(f\"  Входной файл: {input_file}\")\n",
        "        print(f\"  Префикс модели: {self.model_prefix}\")\n",
        "        print(f\"  Размер словаря: {vocab_size}\")\n",
        "        print(f\"  Тип модели: {model_type}\")\n",
        "        print(f\"  Покрытие символов: {character_coverage}\")\n",
        "        print(f\"  Количество потоков: {num_threads}\")\n",
        "\n",
        "        try:\n",
        "            spm.SentencePieceTrainer.train(\n",
        "                input=input_file,\n",
        "                model_prefix=self.model_prefix,\n",
        "                vocab_size=vocab_size,\n",
        "                model_type=model_type,\n",
        "                character_coverage=character_coverage,\n",
        "                num_threads=num_threads\n",
        "            )\n",
        "            print(f\"Обучение завершено. Модель сохранена как {self.model_path} и {self.vocab_path}\")\n",
        "            self.load_model() # Загружаем обученную модель сразу после обучения\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обучении модели: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        Загружает обученную модель SentencePiece.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise FileNotFoundError(f\"Файл модели не найден: {self.model_path}. Пожалуйста, сначала обучите модель.\")\n",
        "        try:\n",
        "            self.sp_processor = spm.SentencePieceProcessor(model_file=self.model_path)\n",
        "            print(f\"Модель SentencePiece успешно загружена из {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке модели: {e}\")\n",
        "            raise\n",
        "\n",
        "    def tokenize_as_pieces(self, text):\n",
        "        \"\"\"\n",
        "        Токенизирует текст в подслова (pieces).\n",
        "\n",
        "        Args:\n",
        "            text (str): Входной текст для токенизации.\n",
        "\n",
        "        Returns:\n",
        "            list: Список подслов.\n",
        "        \"\"\"\n",
        "        if self.sp_processor is None:\n",
        "            raise RuntimeError(\"Модель не загружена. Пожалуйста, обучите или загрузите модель.\")\n",
        "        return self.sp_processor.encode_as_pieces(text)\n",
        "\n",
        "    def tokenize_as_ids(self, text):\n",
        "        \"\"\"\n",
        "        Токенизирует текст в ID подслов.\n",
        "\n",
        "        Args:\n",
        "            text (str): Входной текст для токенизации.\n",
        "\n",
        "        Returns:\n",
        "            list: Список ID подслов.\n",
        "        \"\"\"\n",
        "        if self.sp_processor is None:\n",
        "            raise RuntimeError(\"Модель не загружена. Пожалуйста, обучите или загрузите модель.\")\n",
        "        return self.sp_processor.encode_as_ids(text)\n",
        "\n",
        "    def decode_pieces(self, pieces):\n",
        "        \"\"\"\n",
        "        Декодирует список подслов обратно в текст.\n",
        "\n",
        "        Args:\n",
        "            pieces (list): Список подслов.\n",
        "\n",
        "        Returns:\n",
        "            str: Восстановленный текст.\n",
        "        \"\"\"\n",
        "        if self.sp_processor is None:\n",
        "            raise RuntimeError(\"Модель не загружена. Пожалуйста, обучите или загрузите модель.\")\n",
        "        return self.sp_processor.decode_pieces(pieces)\n",
        "\n",
        "    def decode_ids(self, ids):\n",
        "        \"\"\"\n",
        "        Декодирует список ID подслов обратно в текст.\n",
        "\n",
        "        Args:\n",
        "            ids (list): Список ID подслов.\n",
        "\n",
        "        Returns:\n",
        "            str: Восстановленный текст.\n",
        "        \"\"\"\n",
        "        if self.sp_processor is None:\n",
        "            raise RuntimeError(\"Модель не загружена. Пожалуйста, обучите или загрузите модель.\")\n",
        "        return self.sp_processor.decode_ids(ids)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"\n",
        "        Возвращает словарь модели (подслова и их ID).\n",
        "\n",
        "        Returns:\n",
        "            dict: Словарь, где ключ - подслово, значение - его ID.\n",
        "        \"\"\"\n",
        "        if self.sp_processor is None:\n",
        "            raise RuntimeError(\"Модель не загружена. Пожалуйста, обучите или загрузите модель.\")\n",
        "        return {self.sp_processor.id_to_piece(i): i for i in range(self.sp_processor.get_piece_size())}\n",
        "\n",
        "# --- Пример использования класса ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Создаем тестовый файл для обучения\n",
        "    training_data = [\n",
        "        \"SentencePiece - это мощный инструмент для токенизации подслов, разработанный Google.\",\n",
        "        \"Он позволяет работать с необработанным текстом и является языково-агностическим.\",\n",
        "        \"Токенизация - важный шаг в обработке естественного языка.\",\n",
        "        \"Примеры использования включают машинный перевод и большие языковые модели.\"\n",
        "    ]\n",
        "    input_file_name = \"train_corpus.txt\"\n",
        "    with open(input_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in training_data:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "    # 2. Инициализируем токенизатор\n",
        "    tokenizer = SentencePieceTokenizer(model_prefix=\"my_nlp_tokenizer\")\n",
        "\n",
        "    # 3. Обучаем модель\n",
        "    try:\n",
        "        # Изменено: vocab_size уменьшен до 91, чтобы соответствовать размеру корпуса\n",
        "        tokenizer.train(input_file=input_file_name, vocab_size=91, model_type=\"unigram\")\n",
        "\n",
        "        # 4. Используем обученную модель для токенизации\n",
        "        text_to_tokenize = \"SentencePiece упрощает NLP задачи в области машинного перевода.\"\n",
        "        pieces = tokenizer.tokenize_as_pieces(text_to_tokenize)\n",
        "        ids = tokenizer.tokenize_as_ids(text_to_tokenize)\n",
        "\n",
        "        print(f\"\\nИсходный текст: {text_to_tokenize}\")\n",
        "        print(f\"Токены (pieces): {pieces}\")\n",
        "        print(f\"ID токенов (ids): {ids}\")\n",
        "\n",
        "        # 5. Декодируем токены обратно в текст\n",
        "        decoded_text_from_pieces = tokenizer.decode_pieces(pieces)\n",
        "        decoded_text_from_ids = tokenizer.decode_ids(ids)\n",
        "\n",
        "        print(f\"Декодированный текст из pieces: {decoded_text_from_pieces}\")\n",
        "        print(f\"Декодированный текст из ids: {decoded_text_from_ids}\")\n",
        "\n",
        "        # 6. Получаем словарь\n",
        "        model_vocab = tokenizer.get_vocab()\n",
        "        print(f\"\\nРазмер словаря: {len(model_vocab)}\")\n",
        "        # print(f\"Несколько примеров из словаря: {list(model_vocab.items())[:20]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка в процессе: {e}\")\n",
        "    finally:\n",
        "        # Очистка: удаляем созданные файлы модели и словаря\n",
        "        if os.path.exists(input_file_name):\n",
        "            os.remove(input_file_name)\n",
        "        if os.path.exists(tokenizer.model_path):\n",
        "            os.remove(tokenizer.model_path)\n",
        "        if os.path.exists(tokenizer.vocab_path):\n",
        "            os.remove(tokenizer.vocab_path)\n",
        "        print(\"\\nТестовые файлы удалены.\")\n"
      ],
      "metadata": {
        "id": "WxqlDM3w5B3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import React, { useState, useEffect } from 'react';\n",
        "\n",
        "// Основной компонент приложения\n",
        "const App = () => {\n",
        "  // Состояние для текста, используемого для \"обучения\" модели\n",
        "  const [trainingText, setTrainingText] = useState(\n",
        "    \"SentencePiece - это мощный инструмент для токенизации подслов, разработанный Google. Он позволяет работать с необработанным текстом и является языково-агностическим.\"\n",
        "  );\n",
        "  // Состояние для текста, который будет токенизирован\n",
        "  const [inputText, setInputText] = useState(\n",
        "    \"SentencePiece упрощает NLP задачи.\"\n",
        "  );\n",
        "  // Состояние для имитации словаря\n",
        "  const [vocab, setVocab] = useState([]);\n",
        "  // Состояние для токенизированных частей\n",
        "  const [tokenizedPieces, setTokenizedPieces] = useState([]);\n",
        "  // Состояние для ID токенов\n",
        "  const [tokenizedIds, setTokenizedIds] = useState([]);\n",
        "  // Состояние для индикатора загрузки обучения\n",
        "  const [isTraining, setIsTraining] = useState(false);\n",
        "  // Состояние для индикатора загрузки токенизации\n",
        "  const [isTokenizing, setIsTokenizing] = useState(false);\n",
        "  // Состояние для сообщения об ошибке\n",
        "  const [error, setError] = useState('');\n",
        "\n",
        "  // Функция для имитации обучения модели SentencePiece\n",
        "  const handleTrainModel = () => {\n",
        "    setError('');\n",
        "    setIsTraining(true);\n",
        "    // Имитация задержки обучения\n",
        "    setTimeout(() => {\n",
        "      try {\n",
        "        // Очень упрощенная имитация обучения:\n",
        "        // Разбиваем текст на слова, затем на более мелкие части,\n",
        "        // чтобы имитировать подслова.\n",
        "        // В реальной SentencePiece модели используется BPE или Unigram.\n",
        "        const words = trainingText.toLowerCase().split(/\\s+/).filter(Boolean);\n",
        "        const uniqueSubwords = new Set();\n",
        "\n",
        "        // Добавляем пробел как специальный токен\n",
        "        uniqueSubwords.add(' ');\n",
        "\n",
        "        words.forEach(word => {\n",
        "          // Добавляем слово целиком\n",
        "          uniqueSubwords.add(word);\n",
        "          // Добавляем части слова для имитации подслов\n",
        "          for (let i = 1; i <= word.length; i++) {\n",
        "            uniqueSubwords.add(word.substring(0, i));\n",
        "            uniqueSubwords.add(word.substring(word.length - i));\n",
        "          }\n",
        "          // Добавляем биграммы и триграммы\n",
        "          for (let i = 0; i < word.length - 1; i++) {\n",
        "            uniqueSubwords.add(word.substring(i, i + 2));\n",
        "          }\n",
        "          for (let i = 0; i < word.length - 2; i++) {\n",
        "            uniqueSubwords.add(word.substring(i, i + 3));\n",
        "          }\n",
        "        });\n",
        "\n",
        "        // Сортируем для детерминированного ID\n",
        "        const sortedVocab = Array.from(uniqueSubwords).sort((a, b) => b.length - a.length || a.localeCompare(b));\n",
        "        setVocab(sortedVocab);\n",
        "        console.log(\"Обученный словарь (имитация):\", sortedVocab);\n",
        "      } catch (e) {\n",
        "        setError(\"Ошибка при имитации обучения модели: \" + e.message);\n",
        "        console.error(\"Ошибка при имитации обучения модели:\", e);\n",
        "      } finally {\n",
        "        setIsTraining(false);\n",
        "      }\n",
        "    }, 1500); // Имитация времени обучения\n",
        "  };\n",
        "\n",
        "  // Функция для имитации токенизации текста\n",
        "  const handleTokenizeText = () => {\n",
        "    setError('');\n",
        "    if (vocab.length === 0) {\n",
        "      setError(\"Пожалуйста, сначала обучите модель.\");\n",
        "      return;\n",
        "    }\n",
        "    setIsTokenizing(true);\n",
        "    // Имитация задержки токенизации\n",
        "    setTimeout(() => {\n",
        "      try {\n",
        "        const textToTokenize = inputText.toLowerCase();\n",
        "        let currentText = textToTokenize;\n",
        "        const pieces = [];\n",
        "        const ids = [];\n",
        "        let idCounter = 0; // Для присвоения уникальных ID в демонстрации\n",
        "\n",
        "        // Добавляем пробел в начало, как это делает SentencePiece\n",
        "        currentText = ' ' + currentText;\n",
        "\n",
        "        while (currentText.length > 0) {\n",
        "          let foundMatch = false;\n",
        "          // Ищем самый длинный совпадающий токен из словаря\n",
        "          for (const token of vocab) {\n",
        "            if (currentText.startsWith(token)) {\n",
        "              pieces.push(token);\n",
        "              // Присваиваем простой ID для демонстрации\n",
        "              ids.push(vocab.indexOf(token) !== -1 ? vocab.indexOf(token) : idCounter++);\n",
        "              currentText = currentText.substring(token.length);\n",
        "              foundMatch = true;\n",
        "              break; // Нашли самый длинный, переходим к следующему участку текста\n",
        "            }\n",
        "          }\n",
        "          if (!foundMatch && currentText.length > 0) {\n",
        "            // Если не нашли совпадения, берем один символ (имитация UNK или символьного токена)\n",
        "            const char = currentText[0];\n",
        "            pieces.push(char);\n",
        "            ids.push(vocab.indexOf(char) !== -1 ? vocab.indexOf(char) : idCounter++);\n",
        "            currentText = currentText.substring(1);\n",
        "          }\n",
        "          // Если currentText стал пустым, выходим из цикла\n",
        "          if (currentText.length === 0 && !foundMatch) {\n",
        "            break;\n",
        "          }\n",
        "        }\n",
        "        setTokenizedPieces(pieces);\n",
        "        setTokenizedIds(ids);\n",
        "        console.log(\"Токенизированные части (имитация):\", pieces);\n",
        "        console.log(\"ID токенов (имитация):\", ids);\n",
        "      } catch (e) {\n",
        "        setError(\"Ошибка при имитации токенизации: \" + e.message);\n",
        "        console.error(\"Ошибка при имитации токенизации:\", e);\n",
        "      } finally {\n",
        "        setIsTokenizing(false);\n",
        "      }\n",
        "    }, 1000); // Имитация времени токенизации\n",
        "  };\n",
        "\n",
        "  // Функция для очистки всех результатов\n",
        "  const handleClear = () => {\n",
        "    setVocab([]);\n",
        "    setTokenizedPieces([]);\n",
        "    setTokenizedIds([]);\n",
        "    setError('');\n",
        "  };\n",
        "\n",
        "  return (\n",
        "    <div className=\"min-h-screen bg-gray-100 p-4 font-sans flex flex-col items-center\">\n",
        "      <div className=\"w-full max-w-4xl bg-white rounded-lg shadow-xl p-6 space-y-6\">\n",
        "        <h1 className=\"text-3xl font-bold text-center text-gray-800 mb-6\">\n",
        "          Демонстрация токенизации с использованием SentencePiece (имитация)\n",
        "        </h1>\n",
        "\n",
        "        {error && (\n",
        "          <div className=\"bg-red-100 border border-red-400 text-red-700 px-4 py-3 rounded relative\" role=\"alert\">\n",
        "            <strong className=\"font-bold\">Ошибка:</strong>\n",
        "            <span className=\"block sm:inline\"> {error}</span>\n",
        "          </div>\n",
        "        )}\n",
        "\n",
        "        {/* Секция обучения модели */}\n",
        "        <div className=\"bg-blue-50 p-4 rounded-lg border border-blue-200\">\n",
        "          <h2 className=\"text-xl font-semibold text-blue-800 mb-3\">1. Обучение модели SentencePiece</h2>\n",
        "          <p className=\"text-sm text-gray-600 mb-4\">\n",
        "            Введите текст для \"обучения\" модели токенизации. В реальной SentencePiece модели обучение происходит на большом корпусе данных.\n",
        "            Эта демонстрация имитирует процесс, создавая упрощенный словарь подслов.\n",
        "          </p>\n",
        "          <textarea\n",
        "            className=\"w-full p-3 border border-gray-300 rounded-md focus:ring-blue-500 focus:border-blue-500 text-gray-700\"\n",
        "            rows=\"6\"\n",
        "            placeholder=\"Введите текст для обучения модели...\"\n",
        "            value={trainingText}\n",
        "            onChange={(e) => setTrainingText(e.target.value)}\n",
        "            disabled={isTraining}\n",
        "          ></textarea>\n",
        "          <button\n",
        "            onClick={handleTrainModel}\n",
        "            className={`mt-4 w-full py-3 px-6 rounded-md text-white font-semibold transition-all duration-300 ease-in-out\n",
        "              ${isTraining ? 'bg-blue-300 cursor-not-allowed' : 'bg-blue-600 hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2'}`}\n",
        "            disabled={isTraining}\n",
        "          >\n",
        "            {isTraining ? 'Обучение...' : 'Обучить модель'}\n",
        "          </button>\n",
        "        </div>\n",
        "\n",
        "        {/* Секция токенизации текста */}\n",
        "        <div className=\"bg-green-50 p-4 rounded-lg border border-green-200\">\n",
        "          <h2 className=\"text-xl font-semibold text-green-800 mb-3\">2. Токенизация текста</h2>\n",
        "          <p className=\"text-sm text-gray-600 mb-4\">\n",
        "            Введите текст для токенизации с использованием \"обученной\" модели.\n",
        "          </p>\n",
        "          <textarea\n",
        "            className=\"w-full p-3 border border-gray-300 rounded-md focus:ring-green-500 focus:border-green-500 text-gray-700\"\n",
        "            rows=\"4\"\n",
        "            placeholder=\"Введите текст для токенизации...\"\n",
        "            value={inputText}\n",
        "            onChange={(e) => setInputText(e.target.value)}\n",
        "            disabled={isTokenizing || vocab.length === 0}\n",
        "          ></textarea>\n",
        "          <button\n",
        "            onClick={handleTokenizeText}\n",
        "            className={`mt-4 w-full py-3 px-6 rounded-md text-white font-semibold transition-all duration-300 ease-in-out\n",
        "              ${isTokenizing || vocab.length === 0 ? 'bg-green-300 cursor-not-allowed' : 'bg-green-600 hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-offset-2'}`}\n",
        "            disabled={isTokenizing || vocab.length === 0}\n",
        "          >\n",
        "            {isTokenizing ? 'Токенизация...' : 'Токенизировать текст'}\n",
        "          </button>\n",
        "        </div>\n",
        "\n",
        "        {/* Результаты */}\n",
        "        <div className=\"bg-purple-50 p-4 rounded-lg border border-purple-200\">\n",
        "          <h2 className=\"text-xl font-semibold text-purple-800 mb-3\">3. Результаты токенизации</h2>\n",
        "\n",
        "          <div className=\"mb-4\">\n",
        "            <h3 className=\"text-lg font-medium text-gray-700 mb-2\">Обученный словарь (имитация):</h3>\n",
        "            <div className=\"bg-white p-3 rounded-md border border-gray-200 min-h-[50px] max-h-[200px] overflow-y-auto text-gray-800 text-sm\">\n",
        "              {vocab.length > 0 ? (\n",
        "                <span className=\"break-all\">{vocab.join(', ')}</span>\n",
        "              ) : (\n",
        "                <p className=\"text-gray-500\">Словарь пока пуст. Обучите модель.</p>\n",
        "              )}\n",
        "            </div>\n",
        "          </div>\n",
        "\n",
        "          <div className=\"mb-4\">\n",
        "            <h3 className=\"text-lg font-medium text-gray-700 mb-2\">Токены (Pieces):</h3>\n",
        "            <div className=\"bg-white p-3 rounded-md border border-gray-200 min-h-[50px] max-h-[200px] overflow-y-auto text-gray-800 text-sm\">\n",
        "              {tokenizedPieces.length > 0 ? (\n",
        "                <span className=\"break-all\">[{tokenizedPieces.map(p => `'${p}'`).join(', ')}]</span>\n",
        "              ) : (\n",
        "                <p className=\"text-gray-500\">Токены пока не сгенерированы.</p>\n",
        "              )}\n",
        "            </div>\n",
        "          </div>\n",
        "\n",
        "          <div>\n",
        "            <h3 className=\"text-lg font-medium text-gray-700 mb-2\">ID токенов (IDs):</h3>\n",
        "            <div className=\"bg-white p-3 rounded-md border border-gray-200 min-h-[50px] max-h-[200px] overflow-y-auto text-gray-800 text-sm\">\n",
        "              {tokenizedIds.length > 0 ? (\n",
        "                <span className=\"break-all\">[{tokenizedIds.join(', ')}]</span>\n",
        "              ) : (\n",
        "                <p className=\"text-gray-500\">ID токенов пока не сгенерированы.</p>\n",
        "              )}\n",
        "            </div>\n",
        "          </div>\n",
        "\n",
        "          <button\n",
        "            onClick={handleClear}\n",
        "            className=\"mt-6 w-full py-3 px-6 rounded-md bg-gray-500 text-white font-semibold hover:bg-gray-600 focus:outline-none focus:ring-2 focus:ring-gray-400 focus:ring-offset-2 transition-all duration-300 ease-in-out\"\n",
        "          >\n",
        "            Очистить результаты\n",
        "          </button>\n",
        "        </div>\n",
        "\n",
        "        <p className=\"text-center text-gray-500 text-xs mt-8\">\n",
        "          Примечание: Эта демонстрация является упрощенной имитацией работы SentencePiece.\n",
        "          Фактическая библиотека SentencePiece использует сложные алгоритмы (BPE/Unigram) и реализована на C++ для высокой производительности.\n",
        "        </p>\n",
        "      </div>\n",
        "    </div>\n",
        "  );\n",
        "};\n",
        "\n",
        "export default App;\n"
      ],
      "metadata": {
        "id": "9SQrahqE5d2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OfHLbZy45eUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XoN7BJ8zVR9A"
      }
    }
  ]
}