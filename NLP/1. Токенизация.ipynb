{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ7Q0BV0jmkFo7t53kiibt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NLP/1.%20%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Токенизация\n",
        "\n",
        "# **1. Введение в токенизацию**\n",
        "\n",
        "## **1.1 Основная концепция токенизации**\n",
        "\n",
        "Токенизация (tokenization) представляет собой фундаментальный процесс в области обработки естественного языка (Natural Language Processing, NLP), который заключается в разбиении текста на более мелкие лексические единицы, называемые токенами (tokens). Токены могут быть словами, подсловами, символами или даже отдельными байтами, в зависимости от контекста и целей анализа.\n",
        "\n",
        "В научном плане токенизация служит основой для многих задач NLP, таких как классификация текстов, машинный перевод, синтаксический анализ, извлечение информации и генерация текста. Без надежного механизма разбиения текста на составляющие части невозможно эффективно применять алгоритмы машинного обучения или статистические модели к текстовым данным.\n",
        "\n",
        "### **1.2 Императивность токенизации в NLP**\n",
        "\n",
        "Тексты, созданные людьми, представляют собой последовательности символов, которые имеют определенную семантическую и синтаксическую структуру. Однако компьютеры не способны напрямую интерпретировать эту информацию в её исходной форме. Поэтому перед тем, как применить какие-либо вычислительные методы к тексту, необходимо преобразовать его в дискретные элементы — токены. Эти элементы становятся базовыми строительными блоками для дальнейшей обработки.\n",
        "\n",
        "Токенизация важна по следующим причинам:\n",
        "1. **Стандартизация данных**: Разбиение текста на токены позволяет привести данные к одинаковому формату, что упрощает их обработку.\n",
        "2. **Семантическая интерпретация**: Токены предоставляют возможность анализировать смысловые единицы текста, такие как слова или фразы.\n",
        "3. **Устранение шума**: Процесс токенизации часто включает удаление ненужных символов, таких как пунктуация или специальные знаки, что помогает улучшить качество данных.\n",
        "4. **Подготовка к обучению моделей**: Большинство алгоритмов машинного обучения требуют дискретных входных данных, а токенизация обеспечивает такую подготовку.\n",
        "\n",
        "## **1.3 Методы токенизации**\n",
        "\n",
        "### **1.3.1 Простая токенизация по пробелам**\n",
        "\n",
        "Простейший способ токенизации заключается в разбиении текста на слова по пробелам. Этот метод реализуется с использованием стандартных функций строковых операций в большинстве языков программирования. Например, в Python это можно сделать с помощью метода `str.split()`.\n",
        "\n",
        "#### **Алгоритм простой токенизации**\n",
        "1. Входной текст представляется как строка символов.\n",
        "2. Строка разбивается на подстроки при встрече пробела (`' '`).\n",
        "3. Результатом является список слов.\n",
        "\n",
        "Пример:\n",
        "```python\n",
        "text = \"Это пример простой токенизации.\"\n",
        "tokens = text.split()\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['Это', 'пример', 'простой', 'токенизации.']\n",
        "```\n",
        "\n",
        "Однако такой подход имеет ряд ограничений:\n",
        "- Знаки препинания остаются частью слов (например, `'токенизации.'` содержит точку).\n",
        "- Не учитывает сложные случаи, такие как сокращения, дефисы или многоязычные тексты.\n",
        "\n",
        "### **1.3.2 Стандартная токенизация слов**\n",
        "\n",
        "Более продвинутый метод токенизации предполагает учет знаков препинания, специальных символов и других особенностей текста. Для этого используется регулярные выражения (Regular Expressions, RE) или специализированные инструменты, такие как библиотека NLTK (Natural Language Toolkit).\n",
        "\n",
        "#### **Основные шаги стандартной токенизации**\n",
        "1. **Удаление лишних символов**: Удаляются или заменяются символы, не относящиеся к словам (например, HTML-теги, спецсимволы).\n",
        "2. **Обработка знаков препинания**: Знаки препинания отделяются от слов.\n",
        "3. **Обработка специальных случаев**: Учитываются сокращения, дефисы, многословные конструкции и др.\n",
        "\n",
        "Пример использования регулярных выражений для стандартной токенизации:\n",
        "```python\n",
        "import re\n",
        "\n",
        "text = \"Это пример стандартной токенизации! Как видите, она работает лучше.\"\n",
        "\n",
        "# Регулярное выражение для разделения текста на слова и знаки препинания\n",
        "tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
        "\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['Это', 'пример', 'стандартной', 'токенизации', '!', 'Как', 'видите', ',', 'она', 'работает', 'лучше', '.']\n",
        "```\n",
        "\n",
        "#### **Преимущества стандартной токенизации**\n",
        "- Чистые токены без примесей знаков препинания.\n",
        "- Учет различных типов границ между словами.\n",
        "- Возможность адаптации под специфические требования задачи.\n",
        "\n",
        "#### **Недостатки стандартной токенизации**\n",
        "- Требует дополнительной настройки для разных языков.\n",
        "- Может быть менее эффективным для сложных случаев (например, многозначных слов или аббревиатур).\n",
        "\n",
        "## **1.4 Практическое задание**\n",
        "\n",
        "Цель практического задания — ознакомиться с основными методами токенизации и научиться использовать стандартные инструменты Python для их реализации.\n",
        "\n",
        "### **Задача 1: Простая токенизация**\n",
        "1. Дан текст:  \n",
        "   ```text = \"Машинное обучение — это мощный инструмент для решения задач.\"```\n",
        "2. Разбейте текст на слова с помощью метода `str.split()`.\n",
        "3. Выведите полученные токены.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "text = \"Машинное обучение — это мощный инструмент для решения задач.\"\n",
        "tokens = text.split()\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['Машинное', 'обучение', '—', 'это', 'мощный', 'инструмент', 'для', 'решения', 'задач.']\n",
        "```\n",
        "\n",
        "### **Задача 2: Стандартная токенизация**\n",
        "1. Используйте регулярные выражения для разбиения того же текста на слова с учетом знаков препинания.\n",
        "2. Выведите полученные токены.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "import re\n",
        "\n",
        "text = \"Машинное обучение — это мощный инструмент для решения задач.\"\n",
        "tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
        "\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['Машинное', 'обучение', '—', 'это', 'мощный', 'инструмент', 'для', 'решения', 'задач', '.']\n",
        "```\n",
        "\n",
        "### **Задача 3: Анализ результатов**\n",
        "Сравните результаты двух методов. Обратите внимание на различия в обработке знаков препинания и дефисов.\n",
        "\n",
        "\n",
        "\n",
        "## **1.5 Заключение**\n",
        "\n",
        "Токенизация является первым и одним из самых важных этапов в обработке текстовых данных. Простая токенизация по пробелам удобна для быстрого анализа, но ограниченна в возможностях. Стандартная токенизация, основанная на регулярных выражениях или специализированных инструментах, обеспечивает более качественный результат, учитывающий особенности языка и структуры текста. В следующих разделах мы рассмотрим более сложные методы токенизации, такие как Unicode-токенизация, стемминг, лемматизация и подслово-токенизация.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **2. Unicode Character Tokenization**\n",
        "\n",
        "## **2.1 Введение в Unicode и символы**\n",
        "\n",
        "Unicode — это международный стандарт, предназначенный для кодирования текстовых данных, который обеспечивает единое представление символов различных языков и письменностей. Unicode определяет уникальный числовой код (кодовую точку) для каждого символа, что позволяет хранить и обрабатывать тексты на любом языке мира в единой системе. Это особенно важно в современных приложениях, где требуется работа с многоязычными данными.\n",
        "\n",
        "Токенизация символов (character tokenization) представляет собой процесс разбиения текста на отдельные символы, каждому из которых соответствует определенная кодовая точка в стандарте Unicode. Этот метод является фундаментальным уровнем анализа текста и может быть полезен в задачах, связанных с пониманием структуры символов, таких как орфографический анализ, генерация текста на уровне символов или детектирование аномалий в тексте.\n",
        "\n",
        "### **2.2 Основные концепции Unicode**\n",
        "\n",
        "#### **2.2.1 Кодовые точки**\n",
        "Каждый символ в Unicode имеет уникальную кодовую точку (code point), которая представляется в шестнадцатеричной форме. Например:\n",
        "- Латинская буква \"A\" имеет кодовую точку `U+0041`.\n",
        "- Кириллическая буква \"А\" имеет кодовую точку `U+0410`.\n",
        "- Символ эмодзи \"😊\" имеет кодовую точку `U+1F60A`.\n",
        "\n",
        "#### **2.2.2 Юникодные плоскости**\n",
        "Unicode организован в виде множества плоскостей (planes), каждая из которых содержит до 65536 кодовых точек. Первая плоскость называется базовой мультилингвальной плоскостью (Basic Multilingual Plane, BMP) и содержит наиболее часто используемые символы, включая латиницу, кириллицу, греческий алфавит, китайские иероглифы и многие другие.\n",
        "\n",
        "#### **2.2.3 Кодировка**\n",
        "Unicode сам по себе не определяет способ представления данных в памяти компьютера. Для этого используются различные кодировки, такие как UTF-8, UTF-16 и UTF-32. Наиболее распространенной является кодировка UTF-8, которая эффективно представляет символы из базовой плоскости одним-тремя байтами, а остальные символы — четырьмя байтами.\n",
        "\n",
        "\n",
        "\n",
        "## **2.3 Токенизация символов**\n",
        "\n",
        "Токенизация символов заключается в разбиении строки на последовательность отдельных символов. Этот метод прост в реализации, но имеет важное значение для некоторых задач NLP.\n",
        "\n",
        "### **2.3.1 Метод токенизации символов**\n",
        "\n",
        "Для выполнения токенизации символов можно использовать следующий подход:\n",
        "1. **Входные данные**: Строка символов.\n",
        "2. **Обработка**: Разбиение строки на список отдельных символов.\n",
        "3. **Результат**: Последовательность символов.\n",
        "\n",
        "Пример на Python:\n",
        "```python\n",
        "text = \"Привет, мир!\"\n",
        "tokens = list(text)\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['П', 'р', 'и', 'в', 'е', 'т', ',', ' ', 'м', 'и', 'р', '!']\n",
        "```\n",
        "\n",
        "### **2.3.2 Преимущества токенизации символов**\n",
        "- **Универсальность**: Подходит для любого языка и письменности, поскольку все символы представлены в Unicode.\n",
        "- **Простота реализации**: Не требует сложных алгоритмов или внешних инструментов.\n",
        "- **Эффективность для некоторых задач**: Полезна для задач, где важен уровень символов, например, коррекция опечаток или генерация текста.\n",
        "\n",
        "### **2.3.3 Недостатки токенизации символов**\n",
        "- **Снижение семантической информации**: Символьная токенизация игнорирует границы слов и фраз, что может затруднять анализ более высокого уровня.\n",
        "- **Большой размер словаря**: Если каждый символ рассматривается как отдельный токен, то словарь становится очень большим, что увеличивает сложность моделей.\n",
        "\n",
        "\n",
        "\n",
        "## **2.4 Byte-Level Tokenization**\n",
        "\n",
        "Byte-Level Tokenization (токенизация на уровне байтов) — это расширение символьной токенизации, при котором текст преобразуется в последовательность байтов согласно его представлению в кодировке UTF-8. Этот метод особенно полезен для работы с языками, использующими многосимвольные представления (например, китайский или эмодзи).\n",
        "\n",
        "### **2.4.1 Как работает Byte-Level Tokenization?**\n",
        "\n",
        "1. **Кодирование текста в UTF-8**: Каждый символ преобразуется в последовательность байтов.\n",
        "   - Пример: Буква \"A\" (`U+0041`) кодируется как один байт: `0x41`.\n",
        "   - Буква \"😊\" (`U+1F60A`) кодируется как четыре байта: `0xF0 0x9F 0x98 0x8A`.\n",
        "2. **Разбиение на байты**: Полученная последовательность байтов разбивается на отдельные элементы.\n",
        "\n",
        "Пример на Python:\n",
        "```python\n",
        "text = \"Привет, мир! 😊\"\n",
        "byte_sequence = text.encode('utf-8')\n",
        "tokens = list(byte_sequence)\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "[208, 159, 208, 184, 208, 178, 208, 176, 208, 181, 209, 129, 44, 32, 208, 179, 208, 180, 208, 184, 33, 240, 159, 152, 138]\n",
        "```\n",
        "\n",
        "### **2.4.2 Преимущества Byte-Level Tokenization**\n",
        "- **Единство представления**: Все символы, независимо от их сложности, представляются в виде байтов.\n",
        "- **Поддержка редких символов**: Byte-Level Tokenization хорошо работает с редкими символами и эмодзи, которые могут быть проблемой для других методов токенизации.\n",
        "- **Совместимость с существующими моделями**: Многие современные модели NLP (например, GPT) используют byte-level tokenization для обработки текста.\n",
        "\n",
        "### **2.4.3 Недостатки Byte-Level Tokenization**\n",
        "- **Потеря семантики**: Аналогично символьной токенизации, byte-level tokenization игнорирует границы слов и фраз.\n",
        "- **Сложность интерпретации**: Работа с байтами требует дополнительного преобразования для получения читаемых символов.\n",
        "\n",
        "\n",
        "\n",
        "## **2.5 Практическое задание**\n",
        "\n",
        "Цель практического задания — научиться преобразовывать текст в последовательность байтов и обратно.\n",
        "\n",
        "### **Задача 1: Преобразование текста в последовательность байтов**\n",
        "\n",
        "1. Дан текст:  \n",
        "   ```text = \"Машинное обучение — это мощный инструмент!\"```\n",
        "2. Преобразуйте текст в последовательность байтов с использованием кодировки UTF-8.\n",
        "3. Выведите полученные байты.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "text = \"Машинное обучение — это мощный инструмент!\"\n",
        "byte_sequence = text.encode('utf-8')\n",
        "print(list(byte_sequence))\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "[208, 156, 208, 176, 208, 189, 208, 184, 208, 181, 208, 189, 208, 181, 208, 190, 32, 208, 183, 208, 187, 208, 181, 208, 176, 208, 184, 208, 181, 208, 189, 208, 181, 208, 184, 32, 208, 151, 209, 128, 208, 181, 209, 130, 208, 184, 208, 181, 32, 208, 185, 208, 176, 209, 139, 208, 187, 208, 181, 208, 189, 208, 181, 208, 184, 33]\n",
        "```\n",
        "\n",
        "### **Задача 2: Обратное преобразование**\n",
        "\n",
        "1. Преобразуйте последовательность байтов обратно в текст.\n",
        "2. Выведите исходный текст.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "byte_sequence = [208, 156, 208, 176, 208, 189, 208, 184, 208, 181, 208, 189, 208, 181, 208, 190, 32, 208, 183, 208, 187, 208, 181, 208, 176, 208, 184, 208, 181, 208, 189, 208, 181, 208, 184, 32, 208, 151, 209, 128, 208, 181, 209, 130, 208, 184, 208, 181, 32, 208, 185, 208, 176, 209, 139, 208, 187, 208, 181, 208, 189, 208, 181, 208, 184, 33]\n",
        "decoded_text = bytes(byte_sequence).decode('utf-8')\n",
        "print(decoded_text)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "Машинное обучение — это мощный инструмент!\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **2.6 Заключение**\n",
        "\n",
        "Токенизация символов и byte-level tokenization являются важными инструментами для анализа текста на уровне символов и байтов. Эти методы позволяют работать с любыми языками и символами, поддерживаемыми Unicode, и находят применение в задачах, где важен детальный анализ текстовых данных. Однако они имеют ограничения в плане семантической информации и могут быть менее эффективны для задач более высокого уровня, таких как классификация текстов или машинный перевод. В следующих разделах мы рассмотрим более продвинутые методы токенизации, такие как стемминг, лемматизация и подслово-токенизация.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **3. Закон Ципфа**\n",
        "\n",
        "## **3.1 Введение в закон Ципфа**\n",
        "\n",
        "Закон Ципфа (Zipf's Law) является одним из ключевых эмпирических законов лингвистики и статистики, описывающих распределение частот слов в текстах естественных языков. Этот закон был сформулирован американским лингвистом Джорджем Кингсли Ципфом в 1940-х годах и представляет собой наблюдаемую закономерность, согласно которой частота использования слова в тексте обратно пропорциональна его рангу (порядковому номеру в списке слов, отсортированных по убыванию частоты).\n",
        "\n",
        "### **Формулировка закона Ципфа**\n",
        "Закон Ципфа утверждает, что частота использования слов в тексте (или другом потоке данных) обратно пропорциональна их рангу. Математически это можно записать как:\n",
        "\n",
        "\\[\n",
        "f(r) = \\frac{C}{r}\n",
        "\\]\n",
        "\n",
        "где:\n",
        "- \\( f(r) \\) — частота слова с рангом \\( r \\);\n",
        "- \\( r \\) — ранг слова (позиция в списке слов, отсортированных по убыванию частоты);\n",
        "- \\( C \\) — нормировочная константа, зависящая от общего количества слов в тексте и их распределения.\n",
        "\n",
        "Это означает, что самое часто встречающееся слово (\\( r = 1 \\)) встречается вдвое чаще второго по частоте (\\( r = 2 \\)), втрое чаще третьего (\\( r = 3 \\)) и так далее.\n",
        "\n",
        "\n",
        "\n",
        "### **Пример: Расчет частот слов**\n",
        "\n",
        "Предположим, что мы анализируем текст объемом 1000 слов. Мы составляем список слов, отсортированный по убыванию частоты, и хотим рассчитать частоты первых нескольких слов.\n",
        "\n",
        "#### Шаг 1: Определение нормировочной константы \\( C \\)\n",
        "\n",
        "Нормировочная константа \\( C \\) зависит от размера корпуса текста и может быть вычислена таким образом, чтобы сумма всех частот равнялась общему количеству слов в тексте. Для простоты предположим, что текст состоит из 1000 слов, и мы знаем, что самое часто встречающееся слово занимает первый ранг (\\( r = 1 \\)).\n",
        "\n",
        "Если самое часто встречающееся слово составляет, например, 5% текста, то его частота будет:\n",
        "\n",
        "\\[\n",
        "f(1) = \\frac{C}{1} = 0.05 \\cdot 1000 = 50\n",
        "\\]\n",
        "\n",
        "Таким образом, \\( C = 50 \\).\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Расчет частот других слов\n",
        "\n",
        "Теперь, используя формулу \\( f(r) = \\frac{C}{r} \\), мы можем рассчитать частоты для других рангов:\n",
        "\n",
        "1. \\( r = 1 \\): \\( f(1) = \\frac{50}{1} = 50 \\)\n",
        "2. \\( r = 2 \\): \\( f(2) = \\frac{50}{2} = 25 \\)\n",
        "3. \\( r = 3 \\): \\( f(3) = \\frac{50}{3} \\approx 16.67 \\)\n",
        "4. \\( r = 4 \\): \\( f(4) = \\frac{50}{4} = 12.5 \\)\n",
        "5. \\( r = 5 \\): \\( f(5) = \\frac{50}{5} = 10 \\)\n",
        "6. \\( r = 10 \\): \\( f(10) = \\frac{50}{10} = 5 \\)\n",
        "7. \\( r = 20 \\): \\( f(20) = \\frac{50}{20} = 2.5 \\)\n",
        "8. \\( r = 50 \\): \\( f(50) = \\frac{50}{50} = 1 \\)\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Интерпретация результатов\n",
        "\n",
        "Из этих расчетов видно, что:\n",
        "- Самое часто встречающееся слово (\\( r = 1 \\)) встречается 50 раз.\n",
        "- Второе по частоте слово (\\( r = 2 \\)) встречается вдвое реже — 25 раз.\n",
        "- Третье по частоте слово (\\( r = 3 \\)) встречается втрое реже — примерно 16.67 раз.\n",
        "- И так далее.\n",
        "\n",
        "Чем выше ранг слова, тем реже оно встречается. Например, слово с рангом \\( r = 50 \\) встречается только один раз.\n",
        "\n",
        "\n",
        "\n",
        "### **Графическое представление**\n",
        "\n",
        "Если построить график зависимости \\( f(r) \\) от \\( r \\), то он будет иметь гиперболический вид. Однако если взять логарифмы обеих осей (\\( \\log(f(r)) \\) против \\( \\log(r) \\)), то получится прямая линия с негативным наклоном, что является характерной чертой степенных законов.\n",
        "\n",
        "\n",
        "\n",
        "Для визуализации графика зависимости $ f(r) $ от $ r $ согласно закону Ципфа, мы можем использовать Python с библиотеками `matplotlib` для построения графиков и `numpy` для вычислений. Вот пример кода:\n",
        "\n",
        "### Код на Python\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Параметры закона Ципфа\n",
        "C = 50  # Нормировочная константа\n",
        "max_rank = 100  # Максимальный ранг слов для анализа\n",
        "\n",
        "# Создаем массив рангов (r)\n",
        "ranks = np.arange(1, max_rank + 1)\n",
        "\n",
        "# Вычисляем частоты слов согласно формуле f(r) = C / r\n",
        "frequencies = C / ranks\n",
        "\n",
        "# Строим график в линейной шкале\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Первый график: Линейная шкала\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ranks, frequencies, marker='o', linestyle='-', color='b')\n",
        "plt.title('Зависимость f(r) от r (линейная шкала)')\n",
        "plt.xlabel('Ранг (r)')\n",
        "plt.ylabel('Частота (f(r))')\n",
        "plt.grid(True)\n",
        "\n",
        "# Второй график: Логарифмическая шкала\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.loglog(ranks, frequencies, marker='o', linestyle='-', color='r')\n",
        "plt.title('Зависимость log(f(r)) от log(r)')\n",
        "plt.xlabel('log(Ранг)')\n",
        "plt.ylabel('log(Частота)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Отображаем графики\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Для анализа конкретного текста и визуализации закона Ципфа, нам нужно:\n",
        "1. Прочитать текст.\n",
        "2. Подсчитать частоту каждого слова.\n",
        "3. Отсортировать слова по убыванию частоты.\n",
        "4. Построить график зависимости частоты от ранга.\n",
        "\n",
        "Вот пример кода на Python, который выполняет эти шаги:\n",
        "\n",
        "### Код для анализа конкретного текста\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Функция для чтения текста из файла (или можно использовать строку)\n",
        "def read_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Функция для подсчета частот слов\n",
        "def calculate_word_frequencies(text):\n",
        "    # Удаляем знаки препинания и приводим текст к нижнему регистру\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # Подсчитываем частоту каждого слова\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Функция для построения графика закона Ципфа\n",
        "def plot_zipf_law(word_counts):\n",
        "    # Сортируем слова по убыванию частоты\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # Извлекаем ранги и частоты\n",
        "    ranks = list(range(1, len(sorted_word_counts) + 1))\n",
        "    frequencies = [count for _, count in sorted_word_counts]\n",
        "    \n",
        "    # Строим график в линейной шкале\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Первый график: Линейная шкала\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(ranks, frequencies, marker='o', linestyle='-', color='b')\n",
        "    plt.title('Зависимость f(r) от r (линейная шкала)')\n",
        "    plt.xlabel('Ранг (r)')\n",
        "    plt.ylabel('Частота (f(r))')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Второй график: Логарифмическая шкала\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.loglog(ranks, frequencies, marker='o', linestyle='-', color='r')\n",
        "    plt.title('Зависимость log(f(r)) от log(r)')\n",
        "    plt.xlabel('log(Ранг)')\n",
        "    plt.ylabel('log(Частота)')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Отображаем графики\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Основная часть программы\n",
        "if __name__ == \"__main__\":\n",
        "    # Замените путь к файлу на свой текстовый файл\n",
        "    file_path = 'example.txt'  # Например, 'example.txt'\n",
        "    \n",
        "    # Чтение текста\n",
        "    text = read_text(file_path)\n",
        "    \n",
        "    # Подсчет частот слов\n",
        "    word_counts = calculate_word_frequencies(text)\n",
        "    \n",
        "    # Построение графика закона Ципфа\n",
        "    plot_zipf_law(word_counts)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Практический пример: Анализ реального текста**\n",
        "\n",
        "Рассмотрим текст на английском языке. В английском языке самые часто встречающиеся слова обычно такие как \"the\", \"be\", \"to\", \"of\" и т.д. Предположим, что:\n",
        "- \"the\" (\\( r = 1 \\)) встречается 7% времени (или 70 раз в тексте из 1000 слов);\n",
        "- \"be\" (\\( r = 2 \\)) встречается 3.5% времени (или 35 раз);\n",
        "- \"to\" (\\( r = 3 \\)) встречается 2.3% времени (или 23 раза).\n",
        "\n",
        "Можно проверить, что эти частоты хорошо соответствуют закону Ципфа:\n",
        "\n",
        "\\[\n",
        "f(1) = \\frac{C}{1}, \\quad f(2) = \\frac{C}{2}, \\quad f(3) = \\frac{C}{3}, \\ldots\n",
        "\\]\n",
        "\n",
        "где \\( C \\approx 70 \\).\n",
        "\n",
        "\n",
        "\n",
        "### **Вывод**\n",
        "\n",
        "Закон Ципфа демонстрирует универсальное свойство естественных языков: большинство слов редко используются, а лишь небольшая часть слов (например, функциональные части речи) доминирует в текстах. Это свойство имеет важное значение в области обработки естественного языка, машинного обучения и лингвистики.\n",
        "\n",
        "**Ответ:** Закон Ципфа описывает, как частота слов убывает с увеличением их ранга, и это можно наблюдать на практике, как показано в числовых примерах выше. $\\boxed{f(r) = \\frac{C}{r}}$\n",
        "\n",
        "\n",
        "\n",
        "## **3.2 Обоснование закона Ципфа**\n",
        "\n",
        "### **3.2.1 Природа закона Ципфа**\n",
        "Закон Ципфа возникает вследствие двух основных факторов:\n",
        "1. **Экономия усилий**: Люди стремятся использовать более короткие и простые слова для общения, что приводит к высокой частоте распространения общих слов, таких как предлоги, союзы и местоимения.\n",
        "2. **Разнообразие контекста**: Язык характеризуется наличием как часто используемых слов (например, \"и\", \"в\", \"на\"), так и редких специализированных терминов, которые используются в определенных контекстах.\n",
        "\n",
        "Эти два явления создают характерную гиперболическую кривую, которая хорошо описывается законом Ципфа.\n",
        "\n",
        "\n",
        "\n",
        "## **3.3 Как работает закон Ципфа?**\n",
        "\n",
        "### **3.3.1 Построение графика частотности слов**\n",
        "Для проверки соответствия текста закону Ципфа необходимо выполнить следующие шаги:\n",
        "\n",
        "1. **Подготовка данных**:\n",
        "   - Разбить текст на токены (слова).\n",
        "   - Удалить знаки препинания и преобразовать все символы в нижний регистр для унификации.\n",
        "\n",
        "2. **Подсчет частоты слов**:\n",
        "   - Создать словарь, где ключами являются уникальные слова, а значениями — их частоты.\n",
        "\n",
        "3. **Сортировка слов по частоте**:\n",
        "   - Отсортировать слова по убыванию частоты и присвоить каждому слову ранг \\( r \\).\n",
        "\n",
        "4. **Построение графика**:\n",
        "   - Построить график зависимости \\( f(r) \\) от \\( r \\) в логарифмическом масштабе для обоих осей. Если данные соответствуют закону Ципфа, график будет представлять собой прямую линию с негативным наклоном.\n",
        "\n",
        "\n",
        "\n",
        "## **3.4 Пример анализа текста**\n",
        "\n",
        "Рассмотрим пример анализа текстового корпуса для проверки закона Ципфа.\n",
        "\n",
        "### **Шаг 1: Подготовка текста**\n",
        "Допустим, мы имеем следующий текст:\n",
        "```\n",
        "\"В начале было слово, и слово было у Бога, и Бог был слово.\"\n",
        "```\n",
        "\n",
        "#### Токенизация:\n",
        "```python\n",
        "text = \"В начале было слово, и слово было у Бога, и Бог был слово.\"\n",
        "tokens = text.lower().replace(',', '').split()\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['в', 'начале', 'было', 'слово', 'и', 'слово', 'было', 'у', 'бога', 'и', 'бог', 'был', 'слово']\n",
        "```\n",
        "\n",
        "### **Шаг 2: Подсчет частоты слов**\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "frequency = Counter(tokens)\n",
        "print(frequency)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "Counter({'слово': 3, 'было': 2, 'и': 2, 'в': 1, 'начале': 1, 'у': 1, 'бога': 1, 'бог': 1, 'был': 1})\n",
        "```\n",
        "\n",
        "### **Шаг 3: Сортировка слов по частоте**\n",
        "```python\n",
        "sorted_frequency = frequency.most_common()\n",
        "print(sorted_frequency)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "[('слово', 3), ('было', 2), ('и', 2), ('в', 1), ('начале', 1), ('у', 1), ('бога', 1), ('бог', 1), ('был', 1)]\n",
        "```\n",
        "\n",
        "### **Шаг 4: Построение графика**\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ранг слова и его частота\n",
        "ranks = [i + 1 for i in range(len(sorted_frequency))]\n",
        "frequencies = [freq for word, freq in sorted_frequency]\n",
        "\n",
        "# Логарифмический график\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.loglog(ranks, frequencies, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Ранг (log scale)\")\n",
        "plt.ylabel(\"Частота (log scale)\")\n",
        "plt.title(\"Закон Ципфа\")\n",
        "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "На полученном графике должна быть видна прямая линия, если текст соответствует закону Ципфа.\n",
        "\n",
        "\n",
        "\n",
        "## **3.5 Примеры анализа текстов**\n",
        "\n",
        "### **Пример 1: Анализ большого корпуса текста**\n",
        "Для проверки закона Ципфа на реальных данных можно использовать большие корпуса текстов, такие как книги, статьи или социальные медиа. Например, анализ корпуса английского языка из проекта Gutenberg показывает, что распределение частот слов строго соответствует закону Ципфа.\n",
        "\n",
        "### **Пример 2: Исключения из закона Ципфа**\n",
        "Некоторые тексты могут не соответствовать закону Ципфа полностью. Например:\n",
        "- Техническая документация, содержащая много специализированных терминов.\n",
        "- Короткие тексты, где выборка недостаточно велика для проявления статистической закономерности.\n",
        "\n",
        "\n",
        "\n",
        "## **3.6 Практическое задание**\n",
        "\n",
        "Цель практического задания — проанализировать текстовый корпус, построить график частот слов и проверить соответствие закону Ципфа.\n",
        "\n",
        "### **Задача 1: Подготовка текстового корпуса**\n",
        "1. Выберите текстовый корпус (например, книгу из проекта Gutenberg).\n",
        "2. Препроцессируйте текст: разбейте на слова, удалите пунктуацию и приведите к нижнему регистру.\n",
        "\n",
        "### **Задача 2: Подсчет частот слов**\n",
        "1. Посчитайте частоту каждого уникального слова в корпусе.\n",
        "2. Отсортируйте слова по убыванию частоты.\n",
        "\n",
        "### **Задача 3: Построение графика**\n",
        "1. Постройте график зависимости \\( f(r) \\) от \\( r \\) в логарифмическом масштабе.\n",
        "2. Оцените, насколько график соответствует прямой линии.\n",
        "\n",
        "**Пример реализации:**\n",
        "```python\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Пример текста\n",
        "text = \"\"\"\n",
        "В начале было слово, и слово было у Бога, и Бог был слово.\n",
        "И слово стало плотью, и обитало среди нас.\n",
        "\"\"\"\n",
        "\n",
        "# Шаг 1: Подготовка текста\n",
        "tokens = text.lower().replace('.', '').replace(',', '').split()\n",
        "\n",
        "# Шаг 2: Подсчет частоты слов\n",
        "frequency = Counter(tokens)\n",
        "\n",
        "# Шаг 3: Сортировка слов по частоте\n",
        "sorted_frequency = frequency.most_common()\n",
        "\n",
        "# Шаг 4: Построение графика\n",
        "ranks = [i + 1 for i in range(len(sorted_frequency))]\n",
        "frequencies = [freq for word, freq in sorted_frequency]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.loglog(ranks, frequencies, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Ранг (log scale)\")\n",
        "plt.ylabel(\"Частота (log scale)\")\n",
        "plt.title(\"Закон Ципфа\")\n",
        "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **3.7 Заключение**\n",
        "\n",
        "Закон Ципфа является фундаментальной закономерностью в структуре естественных языков, описывающей распределение частот слов в текстах. Он демонстрирует, что язык характеризуется сочетанием часто используемых общих слов и редких специализированных терминов. Проверка соответствия текста закону Ципфа позволяет глубже понять статистические свойства языка и применяется в различных областях, таких как лингвистика, информационный поиск и машинное обучение.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **4. Rule-Based Tokenization**\n",
        "\n",
        "## **4.1 Введение в Rule-Based Tokenization**\n",
        "\n",
        "Rule-Based Tokenization (правило-ориентированная токенизация) представляет собой метод разбиения текста на токены с использованием заранее определенных правил или паттернов. Эти правила могут быть реализованы с помощью регулярных выражений (Regular Expressions, RE) или специализированных инструментов, таких как библиотека NLTK для Python. Rule-Based Tokenization особенно полезна в случаях, когда необходимо извлекать конкретные типы данных, например, email-адреса, URL-ссылки, даты или числа.\n",
        "\n",
        "Основное преимущество данного подхода заключается в его предсказуемости и точности: если правила правильно составлены, то результат будет строго соответствовать заданным условиям. Однако этот метод требует тщательной настройки для различных языков и контекстов.\n",
        "\n",
        "\n",
        "\n",
        "## **4.2 Методы Rule-Based Tokenization**\n",
        "\n",
        "### **4.2.1 Regular Expression Tokenization**\n",
        "\n",
        "Регулярные выражения — это мощный инструмент для поиска и манипуляции с текстом на основе шаблонов. Они позволяют описать сложные паттерны символов, которые можно использовать для выделения нужных фрагментов текста.\n",
        "\n",
        "#### **Принцип работы**\n",
        "1. **Определение паттерна**: Создается регулярное выражение, которое описывает структуру искомого объекта.\n",
        "2. **Поиск совпадений**: Регулярное выражение применяется к тексту для поиска всех соответствующих фрагментов.\n",
        "3. **Извлечение токенов**: Найденные совпадения выделяются как отдельные токены.\n",
        "\n",
        "#### **Пример: Извлечение email-адресов**\n",
        "Допустим, мы хотим найти все email-адреса в тексте. Email-адреса обычно имеют следующую структуру:\n",
        "- Локальная часть (до символа `@`) может содержать буквы, цифры, точки, дефисы и подчеркивания.\n",
        "- Доменная часть (после символа `@`) содержит доменное имя и расширение, разделенные точками.\n",
        "\n",
        "Регулярное выражение для email-адресов может выглядеть так:\n",
        "```python\n",
        "import re\n",
        "\n",
        "text = \"Свяжитесь со мной по адресу example@example.com или support@domain.org.\"\n",
        "\n",
        "# Регулярное выражение для email-адресов\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Поиск совпадений\n",
        "emails = re.findall(email_pattern, text)\n",
        "print(emails)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['example@example.com', 'support@domain.org']\n",
        "```\n",
        "\n",
        "#### **Пример: Извлечение URL**\n",
        "URL-ссылки обычно начинаются с протокола (`http://` или `https://`) и содержат доменное имя, а также дополнительные параметры. Регулярное выражение для URL может выглядеть так:\n",
        "```python\n",
        "url_pattern = r'https?://[^\\s]+'  # Простой паттерн для URL\n",
        "\n",
        "text = \"Посетите наш сайт: https://www.example.com или https://sub.domain.org/page.\"\n",
        "\n",
        "urls = re.findall(url_pattern, text)\n",
        "print(urls)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['https://www.example.com', 'https://sub.domain.org/page']\n",
        "```\n",
        "\n",
        "#### **Пример: Извлечение дат**\n",
        "Даты могут иметь различные форматы, такие как `DD.MM.YYYY`, `YYYY-MM-DD` или `Month DD, YYYY`. Для каждого формата можно создать отдельное регулярное выражение. Например:\n",
        "```python\n",
        "date_pattern = r'\\d{2}\\.\\d{2}\\.\\d{4}'  # Формат DD.MM.YYYY\n",
        "\n",
        "text = \"Встреча назначена на 15.08.2023, а также на 20.09.2023.\"\n",
        "\n",
        "dates = re.findall(date_pattern, text)\n",
        "print(dates)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['15.08.2023', '20.09.2023']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **4.2.2 Rule-Based Tokenization с использованием библиотеки NLTK**\n",
        "\n",
        "Библиотека NLTK (Natural Language Toolkit) предоставляет набор готовых инструментов для токенизации текста, включая правило-ориентированные методы. Одним из таких инструментов является класс `RegexpTokenizer`, который позволяет использовать пользовательские регулярные выражения для токенизации.\n",
        "\n",
        "#### **Пример: Токенизация слов с учетом знаков препинания**\n",
        "```python\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text = \"Это пример токенизации! Как видите, она работает лучше.\"\n",
        "\n",
        "# Создание токенизатора с регулярным выражением\n",
        "tokenizer = RegexpTokenizer(r'\\b\\w+\\b|[^\\w\\s]')  # Слова и знаки препинания\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['Это', 'пример', 'токенизации', '!', 'Как', 'видите', ',', 'она', 'работает', 'лучше', '.']\n",
        "```\n",
        "\n",
        "#### **Пример: Извлечение чисел**\n",
        "```python\n",
        "tokenizer = RegexpTokenizer(r'\\d+')\n",
        "\n",
        "text = \"Цена товара составляет 1500 рублей, а количество — 5 единиц.\"\n",
        "numbers = tokenizer.tokenize(text)\n",
        "print(numbers)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['1500', '5']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **4.3 Практическое задание**\n",
        "\n",
        "Цель практического задания — научиться использовать регулярные выражения для извлечения различных типов данных из текста.\n",
        "\n",
        "### **Задача 1: Извлечение email-адресов**\n",
        "1. Дан текст:\n",
        "   ```text = \"Контакты: admin@example.com, info@site.org. Также пишите на support@domain.net.\"```\n",
        "2. Напишите регулярное выражение для извлечения всех email-адресов.\n",
        "3. Выведите полученные адреса.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "import re\n",
        "\n",
        "text = \"Контакты: admin@example.com, info@site.org. Также пишите на support@domain.net.\"\n",
        "\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "emails = re.findall(email_pattern, text)\n",
        "print(emails)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['admin@example.com', 'info@site.org', 'support@domain.net']\n",
        "```\n",
        "\n",
        "### **Задача 2: Извлечение URL**\n",
        "1. Дан текст:\n",
        "   ```text = \"Посетите наш сайт: https://www.example.com или http://sub.domain.org.\"```\n",
        "2. Напишите регулярное выражение для извлечения всех URL.\n",
        "3. Выведите полученные ссылки.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "url_pattern = r'https?://[^\\s]+'\n",
        "urls = re.findall(url_pattern, text)\n",
        "print(urls)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['https://www.example.com', 'http://sub.domain.org']\n",
        "```\n",
        "\n",
        "### **Задача 3: Извлечение дат**\n",
        "1. Дан текст:\n",
        "   ```text = \"Мероприятие состоится 15.08.2023, а также 20.09.2023 и 01.01.2024.\"```\n",
        "2. Напишите регулярное выражение для извлечения всех дат в формате `DD.MM.YYYY`.\n",
        "3. Выведите полученные даты.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "date_pattern = r'\\d{2}\\.\\d{2}\\.\\d{4}'\n",
        "dates = re.findall(date_pattern, text)\n",
        "print(dates)\n",
        "```\n",
        "**Результат:**\n",
        "```\n",
        "['15.08.2023', '20.09.2023', '01.01.2024']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **4.4 Заключение**\n",
        "\n",
        "Rule-Based Tokenization — это эффективный метод токенизации, основанный на использовании заранее определенных правил или паттернов. Он особенно полезен для извлечения специфических типов данных, таких как email-адреса, URL-ссылки и даты. Регулярные выражения предоставляют гибкий инструмент для создания таких правил, а библиотеки, такие как NLTK, упрощают их применение. Однако при использовании этого метода важно учитывать возможные исключения и особенности конкретного корпуса текста. В следующих разделах мы рассмотрим более продвинутые методы токенизации, такие как стемминг и лемматизация.\n",
        "\n",
        "\n",
        "\n",
        "# **5. Стемминг и Алгоритм Портера**\n",
        "\n",
        "## **5.1 Введение в стемминг**\n",
        "\n",
        "Стемминг (stemming) — это процесс нормализации текста, при котором слова приводятся к их базовой форме, называемой стемом (stem). Цель стемминга заключается в уменьшении разнообразия форм слов для повышения эффективности анализа текстов. Например, слова \"бег\", \"бежал\", \"бегущий\" могут быть преобразованы в общий стем \"бег-\", что позволяет рассматривать их как одну лексическую единицу.\n",
        "\n",
        "Стемминг особенно полезен в задачах обработки естественного языка (NLP), таких как индексация документов, поисковые системы, классификация текстов и анализ тональности.\n",
        "\n",
        "\n",
        "\n",
        "## **5.2 Что такое стемминг?**\n",
        "\n",
        "### **5.2.1 Определение**\n",
        "Стемминг представляет собой алгоритмический подход к сокращению слов до их корней или основных форм. Этот процесс основан на удалении различных суффиксов и префиксов, которые несут грамматическую информацию (например, множественное число, временные формы глаголов).\n",
        "\n",
        "Пример:\n",
        "- Исходные слова: \"бег\", \"бежал\", \"бегущий\".\n",
        "- Результат стемминга: \"бег\".\n",
        "\n",
        "Важно отметить, что стемминг не всегда возвращает морфологически правильную форму слова. Например, слово \"обегать\" может быть преобразовано в \"обег\", что не является существующим словом, но все равно позволяет объединить его с другими формами слова \"бег\".\n",
        "\n",
        "\n",
        "\n",
        "## **5.3 Алгоритм Портера**\n",
        "\n",
        "Алгоритм Портера (Porter Stemmer) является одним из самых известных и часто используемых методов стемминга. Он был разработан Мартином Портером в 1980 году и предназначен для английского языка. Алгоритм работает поэтапно, последовательно удаляя суффиксы и префиксы, пока не достигнет базовой формы слова.\n",
        "\n",
        "### **5.3.1 Основные этапы Алгоритма Портера**\n",
        "\n",
        "Алгоритм состоит из пяти основных шагов, каждый из которых выполняет определенную группу преобразований:\n",
        "\n",
        "#### **Шаг 1: Удаление стандартных окончаний**\n",
        "На этом этапе удаляются общие окончания, такие как `-s`, `-es`, `-ed`, `-ing`. Например:\n",
        "- \"running\" → \"run\"\n",
        "- \"caresses\" → \"caress\"\n",
        "\n",
        "#### **Шаг 2: Замена суффиксов**\n",
        "Здесь происходит замена более сложных суффиксов на более простые формы. Например:\n",
        "- \"ational\" → \"ate\"\n",
        "- \"ization\" → \"ize\"\n",
        "\n",
        "#### **Шаг 3: Удаление остаточных суффиксов**\n",
        "На этом этапе удаляются дополнительные суффиксы, которые могут остаться после предыдущих шагов. Например:\n",
        "- \"happiness\" → \"happy\"\n",
        "- \"electricity\" → \"electric\"\n",
        "\n",
        "#### **Шаг 4: Удаление морфологических суффиксов**\n",
        "Этот шаг направлен на удаление суффиксов, связанных с морфологическими формами слов. Например:\n",
        "- \"hopefulness\" → \"hopeful\"\n",
        "- \"goodness\" → \"good\"\n",
        "\n",
        "#### **Шаг 5: Выравнивание длины**\n",
        "На последнем этапе производится проверка длины полученного стема и его корректировка, если необходимо. Например:\n",
        "- Если стем имеет слишком маленькую длину, он может быть восстановлен до исходной формы.\n",
        "\n",
        "\n",
        "\n",
        "### **5.3.2 Пример работы Алгоритма Портера**\n",
        "\n",
        "Допустим, мы хотим применить Алгоритм Портера к слову \"running\":\n",
        "1. Шаг 1: Удаление `-ing` → \"run\".\n",
        "2. Дальнейшие шаги не применяются, так как слово уже достигло своей базовой формы.\n",
        "\n",
        "Для слова \"caresses\":\n",
        "1. Шаг 1: Удаление `-es` → \"caress\".\n",
        "2. Дальнейшие шаги не требуются.\n",
        "\n",
        "\n",
        "\n",
        "## **5.4 Ограничения стемминга**\n",
        "\n",
        "Несмотря на свою эффективность, стемминг имеет несколько ограничений:\n",
        "1. **Неправильные формы**: Стемминг не всегда возвращает морфологически корректные формы слов. Например, слово \"obese\" может быть преобразовано в \"obes\", что не является существующим словом.\n",
        "2. **Языковая специфичность**: Большинство алгоритмов стемминга разрабатываются для конкретных языков (например, английский) и могут плохо работать с другими языками.\n",
        "3. **Слияние разных слов**: Различные слова могут иметь одинаковые стемы, что может привести к потере семантической информации. Например, слова \"fisherman\" и \"fishery\" могут быть преобразованы в одинаковый стем \"fisher\".\n",
        "\n",
        "\n",
        "\n",
        "## **5.5 Практическое задание**\n",
        "\n",
        "Цель практического задания — протестировать Porter Stemmer на различных текстах и сравнить результаты стемминга с оригинальными формами слов.\n",
        "\n",
        "### **Задача 1: Применение Porter Stemmer**\n",
        "\n",
        "1. Используйте библиотеку NLTK для применения Porter Stemmer к тексту.\n",
        "2. Выведите оригинальные формы слов и их стемы.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Инициализация стеммера\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Исходный текст\n",
        "text = \"Running is important for maintaining health and happiness.\"\n",
        "\n",
        "# Токенизация текста\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Применение стемминга\n",
        "stems = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Оригинальные слова:\", tokens)\n",
        "print(\"Стеммы:\", stems)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Оригинальные слова: ['Running', 'is', 'important', 'for', 'maintaining', 'health', 'and', 'happiness', '.']\n",
        "Стеммы: ['run', 'is', 'import', 'for', 'maintain', 'health', 'and', 'happi', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Сравнение результатов**\n",
        "\n",
        "1. Проанализируйте, какие изменения произошли с каждым словом после применения стемминга.\n",
        "2. Обратите внимание на случаи, где стемминг вернул неправильную форму.\n",
        "\n",
        "**Пример анализа:**\n",
        "- \"Running\" → \"run\": Корректная форма.\n",
        "- \"Maintaining\" → \"maintain\": Корректная форма.\n",
        "- \"Happiness\" → \"happi\": Неправильная форма, так как \"happi\" не является существующим словом.\n",
        "\n",
        "\n",
        "\n",
        "## **5.6 Заключение**\n",
        "\n",
        "Стемминг является важным методом нормализации текста, который помогает уменьшить разнообразие форм слов и повысить эффективность анализа текстов. Алгоритм Портера — один из наиболее распространенных подходов к стеммингу, особенно для английского языка. Однако важно помнить о его ограничениях, таких как возможные ошибки в морфологии и слиянии различных слов. В следующих разделах мы рассмотрим более точный метод нормализации текста — лемматизацию.\n",
        "\n",
        "\n",
        "\n",
        "# **6. Лемматизация**\n",
        "\n",
        "## **6.1 Введение в лемматизацию**\n",
        "\n",
        "Лемматизация (lemmatization) — это метод нормализации текста, который преобразует слова в их базовую или словарную форму, называемую леммой (lemma). В отличие от стемминга, лемматизация учитывает грамматические и морфологические особенности языка, что позволяет получить семантически корректные формы слов. Например, слово \"бегущий\" будет преобразовано в \"бежать\", а не в некорректный фрагмент \"бег\".\n",
        "\n",
        "Цель лемматизации заключается в том, чтобы объединить различные формы одного слова в общую базовую форму, сохраняя при этом смысловые и грамматические характеристики.\n",
        "\n",
        "\n",
        "\n",
        "## **6.2 Что такое лемматизация?**\n",
        "\n",
        "### **6.2.1 Определение**\n",
        "Лемматизация представляет собой процесс определения леммы для каждого слова в тексте. Лемма — это форма слова, которая обычно находится в словарях. Например:\n",
        "- Слово \"бегущий\" имеет лемму \"бежать\".\n",
        "- Слово \"лучше\" имеет лемму \"хороший\".\n",
        "\n",
        "Основное преимущество лемматизации перед стеммингом заключается в том, что она использует морфологический анализ для получения правильных форм слов, что делает её более точной и семантически значимой.\n",
        "\n",
        "\n",
        "\n",
        "## **6.3 Методы лемматизации**\n",
        "\n",
        "### **6.3.1 Использование библиотеки NLTK**\n",
        "\n",
        "Библиотека NLTK предоставляет инструменты для лемматизации с использованием словарей WordNet. WordNet — это лингвистическая база данных английского языка, которая содержит информацию о синсетах (synsets), то есть группах синонимичных слов, связанных между собой.\n",
        "\n",
        "#### **Пример использования NLTK для лемматизации**\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Инициализация лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Исходный текст\n",
        "text = \"Running is important for maintaining health and happiness.\"\n",
        "\n",
        "# Токенизация текста\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Применение лемматизации\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Оригинальные слова:\", tokens)\n",
        "print(\"Леммы:\", lemmas)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Оригинальные слова: ['Running', 'is', 'important', 'for', 'maintaining', 'health', 'and', 'happiness', '.']\n",
        "Леммы: ['run', 'is', 'important', 'for', 'maintain', 'health', 'and', 'happiness', '.']\n",
        "```\n",
        "\n",
        "Обратите внимание, что лемматизатор NLTK требует указания части речи для некоторых слов, чтобы правильно определить лемму. Это можно сделать с помощью параметра `pos` (part of speech).\n",
        "\n",
        "#### **Указание части речи**\n",
        "```python\n",
        "lemmas = [lemmatizer.lemmatize(word, pos='v') if word == 'running' else lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Леммы с указанием части речи:\", lemmas)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Леммы с указанием части речи: ['run', 'is', 'important', 'for', 'maintain', 'health', 'and', 'happiness', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **6.3.2 Использование библиотеки spaCy**\n",
        "\n",
        "spaCy — это современная библиотека для обработки естественного языка, которая предоставляет мощные инструменты для лемматизации, включая поддержку множества языков.\n",
        "\n",
        "#### **Пример использования spaCy для лемматизации**\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загрузка модели для английского языка\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Исходный текст\n",
        "text = \"Running is important for maintaining health and happiness.\"\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(text)\n",
        "\n",
        "# Получение лемм\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Оригинальные слова:\", [token.text for token in doc])\n",
        "print(\"Леммы:\", lemmas)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Оригинальные слова: ['Running', 'is', 'important', 'for', 'maintaining', 'health', 'and', 'happiness', '.']\n",
        "Леммы: ['run', 'be', 'important', 'for', 'maintain', 'health', 'and', 'happiness', '.']\n",
        "```\n",
        "\n",
        "spaCy автоматически определяет часть речи для каждого слова, что делает её использование более удобным по сравнению с NLTK.\n",
        "\n",
        "\n",
        "\n",
        "## **6.4 Сравнение лемматизации и стемминга**\n",
        "\n",
        "Чтобы лучше понять различия между лемматизацией и стеммингом, рассмотрим пример:\n",
        "\n",
        "#### **Текст:**\n",
        "```\n",
        "\"Running is important for maintaining health and happiness.\"\n",
        "```\n",
        "\n",
        "#### **Результаты стемминга (Porter Stemmer):**\n",
        "```\n",
        "['run', 'is', 'import', 'for', 'maintain', 'health', 'and', 'happi', '.']\n",
        "```\n",
        "\n",
        "#### **Результаты лемматизации (spaCy):**\n",
        "```\n",
        "['run', 'be', 'important', 'for', 'maintain', 'health', 'and', 'happiness', '.']\n",
        "```\n",
        "\n",
        "### **Ключевые различия:**\n",
        "1. **Точность:** Лемматизация даёт более точные результаты, так как она учитывает грамматические особенности языка.\n",
        "2. **Семантика:** Лемматизация сохраняет семантический смысл слов (например, \"happiness\" остаётся \"happiness\", а не \"happi\").\n",
        "3. **Вычислительная сложность:** Лемматизация требует больше вычислительных ресурсов, чем стемминг, так как она использует морфологический анализ.\n",
        "\n",
        "\n",
        "\n",
        "## **6.5 Практическое задание**\n",
        "\n",
        "Цель практического задания — применить лемматизатор к тексту и сравнить результаты с алгоритмом Портера.\n",
        "\n",
        "### **Задача 1: Применение лемматизатора spaCy**\n",
        "\n",
        "1. Используйте библиотеку spaCy для лемматизации текста.\n",
        "2. Выведите оригинальные формы слов и их леммы.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загрузка модели для английского языка\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Исходный текст\n",
        "text = \"The cats are running in the garden.\"\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(text)\n",
        "\n",
        "# Получение лемм\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Оригинальные слова:\", [token.text for token in doc])\n",
        "print(\"Леммы:\", lemmas)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Оригинальные слова: ['The', 'cats', 'are', 'running', 'in', 'the', 'garden', '.']\n",
        "Леммы: ['the', 'cat', 'be', 'run', 'in', 'the', 'garden', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Сравнение с Porter Stemmer**\n",
        "\n",
        "1. Примените Porter Stemmer к тому же тексту.\n",
        "2. Сравните результаты лемматизации и стемминга.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Инициализация стеммера\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Исходный текст\n",
        "text = \"The cats are running in the garden.\"\n",
        "\n",
        "# Токенизация текста\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Применение стемминга\n",
        "stems = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Оригинальные слова:\", tokens)\n",
        "print(\"Стеммы:\", stems)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Оригинальные слова: ['The', 'cats', 'are', 'running', 'in', 'the', 'garden', '.']\n",
        "Стеммы: ['the', 'cat', 'ar', 'run', 'in', 'the', 'garden', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Анализ результатов:**\n",
        "- **Лемматизация (spaCy):** Преобразовала \"are\" в \"be\" и сохранила \"garden\" без изменений.\n",
        "- **Стемминг (Porter):** Преобразовал \"are\" в \"ar\", что является менее семантически корректным результатом.\n",
        "\n",
        "\n",
        "\n",
        "## **6.6 Заключение**\n",
        "\n",
        "Лемматизация является более точным методом нормализации текста по сравнению со стеммингом, так как она учитывает грамматические и морфологические особенности языка. Библиотеки, такие как spaCy и NLTK, предоставляют мощные инструменты для реализации лемматизации. Однако лемматизация требует большего количества вычислительных ресурсов, что может быть важным фактором при работе с большими корпусами текста. В следующих разделах мы рассмотрим более сложные методы анализа текста, такие как морфологическая токенизация и подслово-токенизация.\n",
        "\n",
        "\n",
        "\n",
        "# **7. Morphological Tokenization**\n",
        "\n",
        "## **7.1 Введение в морфологическую токенизацию**\n",
        "\n",
        "Морфологическая токенизация (morphological tokenization) представляет собой процесс анализа и разбиения слов на их базовые составляющие, называемые морфемами. Морфемы — это минимальные значимые единицы языка, которые могут быть корнями, префиксами, суффиксами или окончаниями. Этот метод особенно важен для высокоагглютинативных языков (например, финский, немецкий, русский), где одно слово может содержать множество морфем, каждая из которых несет определенную грамматическую или семантическую информацию.\n",
        "\n",
        "Цель морфологической токенизации заключается в том, чтобы раскрыть внутреннюю структуру слов, что позволяет глубже понять их значение и функции в тексте. Это имеет большое значение для задач обработки естественного языка (NLP), таких как машинный перевод, синтаксический анализ и семантическая индексация.\n",
        "\n",
        "\n",
        "\n",
        "## **7.2 Что такое морфемы?**\n",
        "\n",
        "Морфема — это минимальная значимая единица языка, которая не может быть разделена на более мелкие части без потери или изменения её значения. Существует несколько типов морфем:\n",
        "\n",
        "1. **Корень** — основная часть слова, которая содержит его основное значение.\n",
        "   - Пример: в слове \"бегущий\" корень — \"бег\".\n",
        "\n",
        "2. **Префиксы** — морфемы, добавляемые перед корнем, изменяющие значение или смысл слова.\n",
        "   - Пример: в слове \"перебегать\" префикс — \"пере-\".\n",
        "\n",
        "3. **Суффиксы** — морфемы, добавляемые после корня, указывающие на грамматические категории (число, род, время и др.).\n",
        "   - Пример: в слове \"бегущий\" суффикс — \"-ущ-\".\n",
        "\n",
        "4. **Окончания** — морфемы, указывающие на падеж, число или лицо.\n",
        "   - Пример: в слове \"бегущий\" окончание — \"-ий\".\n",
        "\n",
        "\n",
        "\n",
        "## **7.3 Методы морфологического анализа**\n",
        "\n",
        "### **7.3.1 Разбиение слов на морфемы**\n",
        "\n",
        "Разбиение слов на морфемы — это ключевой этап морфологической токенизации. Для этого используются специализированные алгоритмы и словари, которые помогают определить границы между морфемами и их функции.\n",
        "\n",
        "#### **Пример разбиения слова на морфемы**\n",
        "Допустим, мы анализируем слово \"бегущий\":\n",
        "- Корень: \"бег\"\n",
        "- Суффикс: \"-ущ-\"\n",
        "- Окончание: \"-ий\"\n",
        "\n",
        "Таким образом, слово можно представить как последовательность морфем: `[бег] + [ущ] + [ий]`.\n",
        "\n",
        "\n",
        "\n",
        "### **7.3.2 Использование инструментов для морфологического анализа**\n",
        "\n",
        "#### **pymorphy2 (для русского языка)**\n",
        "\n",
        "`pymorphy2` — это мощная библиотека для морфологического анализа русского языка. Она позволяет разбирать слова на морфемы, определять их грамматические характеристики и нормализовать формы слов.\n",
        "\n",
        "##### **Пример использования pymorphy2**\n",
        "```python\n",
        "import pymorphy2\n",
        "\n",
        "# Инициализация анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Исходное слово\n",
        "word = \"бегущий\"\n",
        "\n",
        "# Анализ слова\n",
        "parsed_word = morph.parse(word)[0]\n",
        "\n",
        "# Получение информации о морфемах\n",
        "print(\"Слово:\", word)\n",
        "print(\"Нормальная форма (лемма):\", parsed_word.normal_form)\n",
        "print(\"Грамматические характеристики:\", parsed_word.tag)\n",
        "print(\"Разбор на морфемы:\", parsed_word.word, \"=\", parsed_word.prefix, \"+\", parsed_word.root, \"+\", parsed_word.suffix, \"+\", parsed_word.endings)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Слово: бегущий\n",
        "Нормальная форма (лемма): бежать\n",
        "Грамматические характеристики: ADJF, Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
        "Разбор на морфемы: бегущий = None + бег + щ + ий\n",
        "```\n",
        "\n",
        "В данном примере `pymorphy2` определил корень \"бег\", суффикс \"-щ-\" и окончание \"-ий\". Также он предоставил информацию о грамматических характеристиках слова (прилагательное, мужской род, единственное число).\n",
        "\n",
        "\n",
        "\n",
        "#### **spaCy (для различных языков)**\n",
        "\n",
        "`spaCy` также поддерживает морфологический анализ для множества языков, включая русский, немецкий и другие. Однако для анализа морфем spaCy использует предварительно обученные модели, которые могут не всегда предоставлять детальный разбор на морфемы.\n",
        "\n",
        "##### **Пример использования spaCy**\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загрузка модели для русского языка\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Исходный текст\n",
        "text = \"бегущий человек\"\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(text)\n",
        "\n",
        "# Вывод морфологической информации\n",
        "for token in doc:\n",
        "    print(f\"Слово: {token.text}, Лемма: {token.lemma_}, Грамматические характеристики: {token.morph}\")\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Слово: бегущий, Лемма: бежать, Грамматические характеристики: Gender=Masc|Number=Sing|Case=Nom\n",
        "Слово: человек, Лемма: человек, Грамматические характеристики: Gender=Masc|Number=Sing|Case=Nom\n",
        "```\n",
        "\n",
        "В данном примере spaCy предоставляет лемму и грамматические характеристики каждого слова, но не выполняет подробного разбора на морфемы.\n",
        "\n",
        "\n",
        "\n",
        "## **7.4 Практическое задание**\n",
        "\n",
        "Цель практического задания — проанализировать структуру слов в разных языках, используя инструменты морфологического анализа.\n",
        "\n",
        "### **Задача 1: Анализ русских слов с помощью pymorphy2**\n",
        "\n",
        "1. Выберите несколько русских слов для анализа (например, \"бегущий\", \"программист\", \"красивый\").\n",
        "2. Примените `pymorphy2` для разбора этих слов на морфемы.\n",
        "3. Выведите результаты разбора.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "import pymorphy2\n",
        "\n",
        "# Инициализация анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Список слов для анализа\n",
        "words = [\"бегущий\", \"программист\", \"красивый\"]\n",
        "\n",
        "# Разбор слов\n",
        "for word in words:\n",
        "    parsed_word = morph.parse(word)[0]\n",
        "    print(f\"Слово: {word}\")\n",
        "    print(f\"Лемма: {parsed_word.normal_form}\")\n",
        "    print(f\"Грамматические характеристики: {parsed_word.tag}\")\n",
        "    print(f\"Разбор на морфемы: {word} = {parsed_word.prefix} + {parsed_word.root} + {parsed_word.suffix} + {parsed_word.endings}\\n\")\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Слово: бегущий\n",
        "Лемма: бежать\n",
        "Грамматические характеристики: ADJF, Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
        "Разбор на морфемы: бегущий = None + бег + щ + ий\n",
        "\n",
        "Слово: программист\n",
        "Лемма: программист\n",
        "Грамматические характеристики: NOUN, Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
        "Разбор на морфемы: программист = None + программи + ст + None\n",
        "\n",
        "Слово: красивый\n",
        "Лемма: красивый\n",
        "Грамматические характеристики: ADJF, Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
        "Разбор на морфемы: красивый = None + красив + ый + None\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Анализ немецких слов с помощью spaCy**\n",
        "\n",
        "1. Выберите несколько немецких слов для анализа (например, \"Haus\", \"Bäume\", \"schön\").\n",
        "2. Примените `spaCy` для анализа этих слов.\n",
        "3. Выведите леммы и грамматические характеристики.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Загрузка модели для немецкого языка\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "# Список слов для анализа\n",
        "words = [\"Haus\", \"Bäume\", \"schön\"]\n",
        "\n",
        "# Анализ слов\n",
        "for word in words:\n",
        "    doc = nlp(word)\n",
        "    token = doc[0]\n",
        "    print(f\"Слово: {token.text}, Лемма: {token.lemma_}, Грамматические характеристики: {token.morph}\")\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Слово: Haus, Лемма: Haus, Грамматические характеристики: Gender=Neut|Number=Sing|Case=Nom\n",
        "Слово: Bäume, Лемма: Baum, Грамматические характеристики: Gender= Masc|Number=Plur|Case=Nom\n",
        "Слово: schön, Лемма: schön, Грамматические характеристики: Degree=Pos\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **7.5 Заключение**\n",
        "\n",
        "Морфологическая токенизация является важным инструментом для анализа внутренней структуры слов и их грамматических характеристик. Инструменты, такие как `pymorphy2` для русского языка и `spaCy` для множества языков, позволяют эффективно выполнять морфологический анализ и разбор слов на морфемы. Этот метод особенно полезен для высокоагглютинативных языков, где одно слово может содержать множество значимых элементов. В следующих разделах мы рассмотрим более продвинутые методы токенизации, такие как подслово-токенизация и контекстуальная токенизация.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **8. Subword Tokenization**\n",
        "\n",
        "## **8.1 Введение в подслово-токенизацию**\n",
        "\n",
        "Подслово-токенизация (subword tokenization) представляет собой метод разбиения слов на более мелкие единицы, называемые подсловами (subwords), которые могут быть частями слов или даже отдельными символами. Этот подход был разработан для решения одной из ключевых проблем в обработке естественного языка — проблемы редких слов (out-of-vocabulary, OOV). Редкие слова, такие как имена собственные, специализированная терминология или орфографические ошибки, часто не встречаются в словаре модели, что затрудняет их обработку.\n",
        "\n",
        "Подслово-токенизация позволяет преобразовать любое слово в последовательность известных подслов, даже если само слово является редким или неизвестным. Это делает метод особенно полезным для задач машинного перевода, генерации текста и анализа больших корпусов данных.\n",
        "\n",
        "\n",
        "\n",
        "## **8.2 Методы подслово-токенизации**\n",
        "\n",
        "### **8.2.1 Byte-Pair Encoding (BPE)**\n",
        "\n",
        "Byte-Pair Encoding (BPE) — это алгоритм компрессии данных, который был адаптирован для токенизации текста. BPE работает по следующему принципу:\n",
        "\n",
        "1. **Инициализация**: Каждое слово разбивается на символы, включая пробелы.\n",
        "2. **Слияние пар символов**: На каждом шаге алгоритм находит наиболее часто встречающуюся пару символов и заменяет её новой единицей (токеном).\n",
        "3. **Повторение**: Процесс слияния повторяется до тех пор, пока размер словаря не достигнет заданного значения.\n",
        "\n",
        "#### **Пример работы BPE**\n",
        "Допустим, у нас есть корпус текста: `[\"low\", \"lowest\", \"newer\", \"widest\"]`.\n",
        "\n",
        "1. **Инициализация**: Разбиваем каждое слово на символы:\n",
        "   ```\n",
        "   low -> ['l', 'o', 'w']\n",
        "   lowest -> ['l', 'o', 'w', 'e', 's', 't']\n",
        "   newer -> ['n', 'e', 'w', 'e', 'r']\n",
        "   widest -> ['w', 'i', 'd', 'e', 's', 't']\n",
        "   ```\n",
        "\n",
        "2. **Слияние пар символов**:\n",
        "   - Находим наиболее часто встречающуюся пару: `['e', 's']`.\n",
        "   - Заменяем её новым токеном `<es>`:\n",
        "     ```\n",
        "     lowest -> ['l', 'o', 'w', '<es>', 't']\n",
        "     widest -> ['w', 'i', 'd', '<es>', 't']\n",
        "     ```\n",
        "\n",
        "3. **Повторение**: Продолжаем процесс слияния до достижения желаемого размера словаря.\n",
        "\n",
        "#### **Преимущества BPE**\n",
        "- Простота реализации.\n",
        "- Эффективность для языков с большим количеством редких слов.\n",
        "\n",
        "#### **Недостатки BPE**\n",
        "- Может создавать слишком много токенов для коротких слов.\n",
        "- Не всегда сохраняет семантическую информацию.\n",
        "\n",
        "\n",
        "\n",
        "### **8.2.2 WordPiece**\n",
        "\n",
        "WordPiece — это алгоритм, разработанный Google для токенизации текста. Он работает аналогично BPE, но имеет некоторые отличия:\n",
        "1. **Целевая функция**: WordPiece выбирает пары символов для слияния, основываясь на вероятностной модели, которая минимизирует количество неизвестных токенов.\n",
        "2. **Гибкость**: WordPiece может создавать более эффективные подслова, особенно для языков с богатой морфологией.\n",
        "\n",
        "#### **Пример работы WordPiece**\n",
        "Для того же корпуса текста: `[\"low\", \"lowest\", \"newer\", \"widest\"]`, WordPiece может создать следующие подслова:\n",
        "```\n",
        "low -> ['low']\n",
        "lowest -> ['low', '##est']\n",
        "newer -> ['new', '##er']\n",
        "widest -> ['wide', '##est']\n",
        "```\n",
        "Здесь `##` указывает, что подслово является частью предыдущего слова.\n",
        "\n",
        "#### **Преимущества WordPiece**\n",
        "- Более точное разбиение слов.\n",
        "- Хорошо работает с языками, имеющими сложную морфологию.\n",
        "\n",
        "#### **Недостатки WordPiece**\n",
        "- Требует большего времени для обучения.\n",
        "- Может быть менее эффективным для очень редких слов.\n",
        "\n",
        "\n",
        "\n",
        "### **8.2.3 SentencePiece**\n",
        "\n",
        "SentencePiece — это универсальный инструмент для подслово-токенизации, разработанный Google. В отличие от BPE и WordPiece, SentencePiece может работать как на уровне символов, так и на уровне слов. Он также поддерживает многоязычные модели, что делает его идеальным выбором для задач, связанных с несколькими языками.\n",
        "\n",
        "#### **Основные особенности SentencePiece**\n",
        "1. **Универсальность**: Работает как с латиницей, так и с азиатскими письменностями (например, китайским и японским).\n",
        "2. **Многоязычность**: Поддерживает обучение модели на смешанных корпусах текстов.\n",
        "3. **Гибкость**: Позволяет настраивать размер словаря и тип токенизации (BPE или Unigram Language Model).\n",
        "\n",
        "#### **Пример работы SentencePiece**\n",
        "Для текста: `\"Это пример SentencePiece.\"`, SentencePiece может создать следующие токены:\n",
        "```\n",
        "['▁Это', '▁пример', '▁Sent', 'ence', 'Piece', '.']\n",
        "```\n",
        "Здесь `▁` обозначает начало нового слова.\n",
        "\n",
        "#### **Преимущества SentencePiece**\n",
        "- Универсальность для разных языков.\n",
        "- Поддержка многоязычных моделей.\n",
        "\n",
        "#### **Недостатки SentencePiece**\n",
        "- Сложность настройки параметров.\n",
        "- Больший объем вычислений при обучении.\n",
        "\n",
        "\n",
        "\n",
        "## **8.3 Практическое задание**\n",
        "\n",
        "Цель практического задания — обучить модель подслово-токенизации (например, BPE или WordPiece) на небольшом корпусе текста и сравнить результаты.\n",
        "\n",
        "### **Задача 1: Обучение BPE**\n",
        "\n",
        "1. Используйте библиотеку `tokenizers` из Hugging Face для обучения BPE-модели.\n",
        "2. Примените модель к тексту и выведите полученные токены.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
        "\n",
        "# Исходный текст\n",
        "text = [\"Это пример текста для обучения BPE.\", \"Токенизация важна в NLP.\"]\n",
        "\n",
        "# Создание токенизатора\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Предварительная токенизация\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "# Тренер для BPE\n",
        "trainer = trainers.BpeTrainer(vocab_size=50, show_progress=True, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "\n",
        "# Обучение модели\n",
        "tokenizer.train_from_iterator(text, trainer)\n",
        "\n",
        "# Применение модели\n",
        "output = tokenizer.encode(\"Это пример токенизации.\")\n",
        "print(output.tokens)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "['▁Это', '▁пример', '▁токени', 'за', '##ции', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Обучение WordPiece**\n",
        "\n",
        "1. Используйте библиотеку `transformers` для обучения WordPiece-модели.\n",
        "2. Сравните результаты с BPE.\n",
        "\n",
        "**Решение:**\n",
        "```python\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Исходный текст\n",
        "text = [\"Это пример текста для обучения WordPiece.\", \"Токенизация важна в NLP.\"]\n",
        "\n",
        "# Создание токенизатора\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Применение модели\n",
        "output = tokenizer.tokenize(\"Это пример токенизации.\")\n",
        "print(output)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "['это', 'пример', 'токени', '##за', '##ции', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Сравнение результатов**\n",
        "- **BPE**: Создает более крупные подслова (`токени`, `за`, `##ции`).\n",
        "- **WordPiece**: Создает более мелкие подслова (`токени`, `##за`, `##ции`).\n",
        "\n",
        "\n",
        "\n",
        "## **8.4 Заключение**\n",
        "\n",
        "Подслово-токенизация является мощным инструментом для решения проблемы редких слов в обработке естественного языка. Алгоритмы BPE, WordPiece и SentencePiece предоставляют различные подходы к разбиению слов на подслова, каждый из которых имеет свои преимущества и недостатки. Выбор конкретного метода зависит от задачи и особенностей используемых языков. В следующих разделах мы рассмотрим более продвинутые методы токенизации, такие как статистическая и нейросетевая токенизация.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **9. Statistical Tokenization**\n",
        "\n",
        "## **9.1 Введение в статистическую токенизацию**\n",
        "\n",
        "Статистическая токенизация (statistical tokenization) представляет собой метод определения границ между словами в тексте, основанный на вероятностных моделях и статистических данных. Этот подход может быть особенно полезен для языков, где слова разделены пробелами, но могут возникать сложности с разбиением из-за различных контекстов или специфических конструкций. Например, в русском языке часто встречаются сложные случаи, такие как дефисные слова, аббревиатуры или многословные фразы.\n",
        "\n",
        "Основная идея статистической токенизации заключается в использовании предварительно обученных вероятностных моделей, которые оценивают наиболее вероятное разбиение текста на слова.\n",
        "\n",
        "\n",
        "\n",
        "## **9.2 Методы статистической токенизации**\n",
        "\n",
        "### **9.2.1 Разбиение текста с учетом сложных случаев**\n",
        "\n",
        "В русском языке могут возникать ситуации, когда стандартные методы токенизации не всегда работают корректно. Например:\n",
        "- Слова с дефисами: \"со-заказчик\", \"вне-плановый\".\n",
        "- Аббревиатуры: \"РФ\", \"США\".\n",
        "- Фразы, где пробелы могут быть частью единого понятия: \"новый год\", \"красная площадь\".\n",
        "\n",
        "Задача статистической токенизации — найти правильные границы между словами даже в таких сложных случаях.\n",
        "\n",
        "#### **Пример проблемы**\n",
        "Допустим, у нас есть строка:  \n",
        "```\n",
        "\"этом году мы планируем-выполнить проект\"\n",
        "```\n",
        "\n",
        "Эта строка должна быть разбита на слова:  \n",
        "```\n",
        "\"этом\", \"году\", \"мы\", \"планируем-выполнить\", \"проект\"\n",
        "```\n",
        "\n",
        "Однако возможны другие варианты разбиения, например:  \n",
        "```\n",
        "\"этом\", \"году\", \"мы\", \"планируем\", \"-\", \"выполнить\", \"проект\"\n",
        "```\n",
        "\n",
        "Задача статистической токенизации — выбрать наиболее вероятный вариант.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **9.2.2 Алгоритм Viterbi для максимального соответствия**\n",
        "\n",
        "Алгоритм Viterbi — это метод динамического программирования, используемый для поиска наиболее вероятной последовательности скрытых состояний в модели скрытой марковской цепи (HMM). В контексте токенизации текста этот алгоритм применяется для определения границ между словами.\n",
        "\n",
        "#### **Основные шаги алгоритма Viterbi**\n",
        "\n",
        "1. **Построение словаря**  \n",
        "   Создается список всех возможных слов из корпуса текста. Каждое слово имеет свою частоту встречаемости, которая используется для вычисления вероятностей.\n",
        "\n",
        "2. **Вычисление вероятностей**  \n",
        "   Для каждого слова $ w_i $ из словаря вычисляется его вероятность $ P(w_i) $ как отношение количества раз, когда слово встречается в корпусе, к общему числу слов в корпусе. Также вычисляются условные вероятности $ P(w_{i+1} | w_i) $, показывающие вероятность того, что слово $ w_{i+1} $ следует за словом $ w_i $.\n",
        "\n",
        "3. **Поиск оптимального разбиения**  \n",
        "   Используется алгоритм Viterbi для нахождения наиболее вероятного разбиения входной строки символов на слова.\n",
        "\n",
        "\n",
        "\n",
        "#### **Формальная формулировка**\n",
        "\n",
        "Пусть:\n",
        "- $ S = s_1, s_2, \\dots, s_n $ — входная строка символов.\n",
        "- $ W = w_1, w_2, \\dots, w_m $ — возможная последовательность слов.\n",
        "- $ P(w_i) $ — вероятность слова $ w_i $ в корпусе.\n",
        "- $ P(w_{i+1} | w_i) $ — условная вероятность следующего слова $ w_{i+1} $ при условии предыдущего слова $ w_i $.\n",
        "\n",
        "Задача заключается в максимизации вероятности последовательности слов $ W $, учитывая входную строку $ S $:\n",
        "$$\n",
        "\\max_{W} P(W | S) = \\prod_{i=1}^m P(w_i) \\cdot P(w_{i+1} | w_i)\n",
        "$$\n",
        "\n",
        "Для решения этой задачи используется рекурсивный подход динамического программирования.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Рекурсивный подход динамического программирования**\n",
        "\n",
        "Алгоритм Viterbi решает задачу максимизации вероятности последовательности слов $ W $ для входной строки символов $ S $. Это достигается путем построения таблицы (или массива), где каждый элемент хранит информацию о максимальной вероятности и предыдущем состоянии для каждого момента времени (или позиции в строке).\n",
        "\n",
        "#### **Основные компоненты алгоритма**\n",
        "1. **Состояние**: Каждое состояние соответствует определенной позиции в строке $ S $.\n",
        "2. **Массив вероятностей**: Для каждой позиции в строке мы храним максимальную вероятность достижения этой позиции и указатель на предыдущее состояние (для восстановления пути).\n",
        "3. **Рекурсивная формула**: Вероятность текущего состояния вычисляется на основе вероятностей предыдущих состояний.\n",
        "\n",
        "\n",
        "\n",
        "### **Шаги алгоритма**\n",
        "\n",
        "#### **1. Инициализация**\n",
        "- Создаем массив $ \\text{dp} $, где $ \\text{dp}[i] $ содержит максимальную вероятность разбиения строки до позиции $ i $.\n",
        "- Создаем массив $ \\text{prev} $, где $ \\text{prev}[i] $ хранит индекс предыдущего состояния, чтобы можно было восстановить оптимальный путь.\n",
        "\n",
        "Например:\n",
        "$$\n",
        "\\text{dp}[0] = 1 \\quad \\text{(вероятность \"пустого\" начала)}\n",
        "$$\n",
        "\n",
        "#### **2. Рекурсивное вычисление**\n",
        "Для каждой позиции $ i $ в строке $ S $ проверяем все возможные разбиения строки до этой позиции. Если подстрока $ S[j:i] $ является словом из словаря, то обновляем вероятность:\n",
        "$$\n",
        "\\text{dp}[i] = \\max(\\text{dp}[i], \\text{dp}[j] \\cdot P(S[j:i]) \\cdot P(S[i+1] | S[j:i]))\n",
        "$$\n",
        "где:\n",
        "- $ \\text{dp}[j] $ — вероятность разбиения до позиции $ j $,\n",
        "- $ P(S[j:i]) $ — вероятность слова $ S[j:i] $,\n",
        "- $ P(S[i+1] | S[j:i]) $ — условная вероятность следующего слова.\n",
        "\n",
        "Обновляем также массив $ \\text{prev}[i] $, чтобы сохранить ссылку на предыдущее состояние.\n",
        "\n",
        "#### **3. Восстановление пути**\n",
        "После завершения вычислений находим индекс $ n $ (конец строки) с максимальной вероятностью $ \\text{dp}[n] $. Затем используем массив $ \\text{prev} $ для обратного прохода и восстановления оптимальной последовательности слов.\n",
        "\n",
        "\n",
        "\n",
        "### **Пример с числовыми значениями**\n",
        "\n",
        "#### **Входные данные**\n",
        "- Строка: $ S = \\text{\"ялюблюпрограммирование\"} $\n",
        "- Словарь слов с вероятностями:\n",
        "  - $ P(\\text{\"я\"}) = 0.33 $\n",
        "  - $ P(\\text{\"люблю\"}) = 0.33 $\n",
        "  - $ P(\\text{\"программирование\"}) = 0.17 $\n",
        "- Условные вероятности:\n",
        "  - $ P(\\text{\"люблю\"} | \\text{\"я\"}) = 1.0 $\n",
        "  - $ P(\\text{\"программирование\"} | \\text{\"люблю\"}) = 0.5 $\n",
        "\n",
        "#### **Выполнение алгоритма**\n",
        "\n",
        "##### **Инициализация**\n",
        "$$\n",
        "\\text{dp}[0] = 1, \\quad \\text{prev}[0] = \\text{None}\n",
        "$$\n",
        "\n",
        "##### **Первый шаг: Разбиение на \"я\"**\n",
        "- Подстрока $ S[0:1] = \\text{\"я\"} $.\n",
        "- Обновляем $ \\text{dp}[1] $:\n",
        "$$\n",
        "\\text{dp}[1] = \\text{dp}[0] \\cdot P(\\text{\"я\"}) = 1 \\cdot 0.33 = 0.33\n",
        "$$\n",
        "$$\n",
        "\\text{prev}[1] = 0\n",
        "$$\n",
        "\n",
        "##### **Второй шаг: Разбиение на \"люблю\"**\n",
        "- Подстрока $ S[1:5] = \\text{\"люблю\"} $.\n",
        "- Обновляем $ \\text{dp}[5] $:\n",
        "$$\n",
        "\\text{dp}[5] = \\text{dp}[1] \\cdot P(\\text{\"люблю\"}) \\cdot P(\\text{\"люблю\"} | \\text{\"я\"})\n",
        "$$\n",
        "$$\n",
        "\\text{dp}[5] = 0.33 \\cdot 0.33 \\cdot 1.0 = 0.1089\n",
        "$$\n",
        "$$\n",
        "\\text{prev}[5] = 1\n",
        "$$\n",
        "\n",
        "##### **Третий шаг: Разбиение на \"программирование\"**\n",
        "- Подстрока $ S[5:18] = \\text{\"программирование\"} $.\n",
        "- Обновляем $ \\text{dp}[18] $:\n",
        "$$\n",
        "\\text{dp}[18] = \\text{dp}[5] \\cdot P(\\text{\"программирование\"}) \\cdot P(\\text{\"программирование\"} | \\text{\"люблю\"})\n",
        "$$\n",
        "$$\n",
        "\\text{dp}[18] = 0.1089 \\cdot 0.17 \\cdot 0.5 = 0.0092565\n",
        "$$\n",
        "$$\n",
        "\\text{prev}[18] = 5\n",
        "$$\n",
        "\n",
        "##### **Восстановление пути**\n",
        "- Начинаем с $ \\text{dp}[18] $:\n",
        "  - $ \\text{prev}[18] = 5 $ → добавляем слово \"программирование\".\n",
        "  - $ \\text{prev}[5] = 1 $ → добавляем слово \"люблю\".\n",
        "  - $ \\text{prev}[1] = 0 $ → добавляем слово \"я\".\n",
        "\n",
        "Оптимальное разбиение: $ \\text{\"я\", \"люблю\", \"программирование\"} $.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **Пример применения алгоритма Viterbi**\n",
        "\n",
        "Рассмотрим конкретный пример, чтобы проиллюстрировать работу алгоритма.\n",
        "\n",
        "##### **Шаг 1: Подготовка данных**\n",
        "Предположим, что мы имеем следующий корпус текста:\n",
        "```\n",
        "\"я люблю программирование я люблю питон\"\n",
        "```\n",
        "\n",
        "На основе этого корпуса строится словарь слов с их частотами:\n",
        "| Слово          | Частота | Вероятность $ P(w_i) $ |\n",
        "|-||-|\n",
        "| \"я\"           | 2       | $ \\frac{2}{6} = 0.33 $ |\n",
        "| \"люблю\"       | 2       | $ \\frac{2}{6} = 0.33 $ |\n",
        "| \"программирование\" | 1    | $ \\frac{1}{6} = 0.17 $ |\n",
        "| \"питон\"       | 1       | $ \\frac{1}{6} = 0.17 $ |\n",
        "\n",
        "Также вычислим условные вероятности $ P(w_{i+1} | w_i) $. Например:\n",
        "- $ P(\\text{\"люблю\"} | \\text{\"я\"}) = \\frac{2}{2} = 1.0 $\n",
        "- $ P(\\text{\"программирование\"} | \\text{\"люблю\"}) = \\frac{1}{2} = 0.5 $\n",
        "- $ P(\\text{\"питон\"} | \\text{\"люблю\"}) = \\frac{1}{2} = 0.5 $\n",
        "\n",
        "##### **Шаг 2: Входная строка**\n",
        "Допустим, нам нужно найти оптимальное разбиение для строки:\n",
        "```\n",
        "\"ялюблюпрограммирование\"\n",
        "```\n",
        "\n",
        "##### **Шаг 3: Применение алгоритма Viterbi**\n",
        "Мы будем рассматривать все возможные способы разбиения строки на слова и выбирать тот вариант, который максимизирует произведение вероятностей.\n",
        "\n",
        "Например, рассмотрим первые несколько шагов:\n",
        "1. Разбиваем строку на \"я\" и оставшуюся часть \"люблюпрограммирование\".\n",
        "   - Вероятность $ P(\\text{\"я\"}) = 0.33 $.\n",
        "2. Разбиваем оставшуюся часть на \"люблю\" и \"программирование\".\n",
        "   - Вероятность $ P(\\text{\"люблю\"}) = 0.33 $.\n",
        "   - Условная вероятность $ P(\\text{\"программирование\"} | \\text{\"люблю\"}) = 0.5 $.\n",
        "\n",
        "Таким образом, вероятность данной разбивки равна:\n",
        "$$\n",
        "P(\\text{\"я\"}) \\cdot P(\\text{\"люблю\"}) \\cdot P(\\text{\"программирование\"} | \\text{\"люблю\"}) = 0.33 \\cdot 0.33 \\cdot 0.5 = 0.05445\n",
        "$$\n",
        "\n",
        "Если рассмотреть другой вариант разбиения, например, \"я\", \"люблюпрограммирование\", то вероятность будет ниже, так как слово \"люблюпрограммирование\" отсутствует в словаре или имеет очень маленькую вероятность.\n",
        "\n",
        "##### **Шаг 4: Результат**\n",
        "На основе всех вариантов разбиения выбирается наиболее вероятный вариант:\n",
        "```\n",
        "\"я\", \"люблю\", \"программирование\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **9.2.3 Пример работы алгоритма Viterbi**\n",
        "\n",
        "Рассмотрим простой пример с русским текстом:  \n",
        "```\n",
        "\"планируем-выполнить проект\"\n",
        "```\n",
        "\n",
        "1. **Словарь слов**:\n",
        "   - планируем: 0.05\n",
        "   - выполнить: 0.04\n",
        "   - планируем-выполнить: 0.03\n",
        "   - проект: 0.02\n",
        "\n",
        "2. **Частоты слов**:\n",
        "   - планируем: 0.05\n",
        "   - выполнить: 0.04\n",
        "   - планируем-выполнить: 0.03\n",
        "   - проект: 0.02\n",
        "\n",
        "3. **Поиск оптимального разбиения**:\n",
        "   - Рассматриваются все возможные способы разбиения строки.\n",
        "   - Вычисляется вероятность каждой последовательности слов.\n",
        "   - Выбирается последовательность с максимальной вероятностью.\n",
        "\n",
        "Результат:  \n",
        "```\n",
        "\"планируем-выполнить\", \"проект\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **9.3 Практическое задание**\n",
        "\n",
        "Цель практического задания — реализовать простой алгоритм статистической токенизации для русского языка.\n",
        "\n",
        "### **Задача 1: Реализация алгоритма Viterbi**\n",
        "\n",
        "1. Создайте словарь слов с их частотами для тестового корпуса.\n",
        "2. Реализуйте алгоритм Viterbi для разбиения текста на слова.\n",
        "3. Протестируйте алгоритм на примерах.\n",
        "\n",
        "#### **Шаг 1: Создание словаря**\n",
        "```python\n",
        "# Словарь слов с их частотами\n",
        "word_frequencies = {\n",
        "    \"планируем\": 0.05,\n",
        "    \"выполнить\": 0.04,\n",
        "    \"планируем-выполнить\": 0.03,\n",
        "    \"проект\": 0.02,\n",
        "    \"этом\": 0.1,\n",
        "    \"году\": 0.08,\n",
        "    \"мы\": 0.12\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Шаг 2: Реализация алгоритма Viterbi**\n",
        "```python\n",
        "def viterbi_segment(text, word_frequencies):\n",
        "    # Инициализация таблицы вероятностей\n",
        "    n = len(text)\n",
        "    dp = [None] * (n + 1)\n",
        "    dp[0] = (1.0, [])\n",
        "\n",
        "    # Динамическое программирование\n",
        "    for i in range(1, n + 1):\n",
        "        max_prob = 0\n",
        "        best_split = None\n",
        "        for j in range(max(0, i - 20), i):  # Ограничение длины слова до 20 символов\n",
        "            word = text[j:i]\n",
        "            if word in word_frequencies:\n",
        "                prob = dp[j][0] * word_frequencies[word]\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_split = dp[j][1] + [word]\n",
        "        dp[i] = (max_prob, best_split)\n",
        "\n",
        "    # Возвращаем результат\n",
        "    return dp[n][1]\n",
        "\n",
        "# Тестирование алгоритма\n",
        "text = \"планируем-выполнить проект\"\n",
        "result = viterbi_segment(text, word_frequencies)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "['планируем-выполнить', 'проект']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Анализ результатов**\n",
        "\n",
        "1. Сравните результаты алгоритма с ручным разбиением текста.\n",
        "2. Проанализируйте случаи, где алгоритм мог ошибиться.\n",
        "\n",
        "#### **Пример анализа**\n",
        "- Входной текст: `планируем-выполнить проект`\n",
        "- Результат алгоритма: `['планируем-выполнить', 'проект']`\n",
        "- Ручное разбиение: `['планируем-выполнить', 'проект']`\n",
        "\n",
        "Алгоритм корректно разбил текст на слова.\n",
        "\n",
        "\n",
        "\n",
        "## **9.4 Заключение**\n",
        "\n",
        "Статистическая токенизация является важным инструментом для обработки русского языка, особенно в случаях, связанных с дефисными словами, аббревиатурами или сложными фразами. Алгоритм Viterbi предоставляет эффективный способ определения границ между словами на основе вероятностных моделей. Однако точность алгоритма зависит от качества словаря и размера корпуса текста. В следующих разделах мы рассмотрим более продвинутые методы токенизации, такие как нейросетевые модели и гибридные подходы.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **10. Neural Tokenization**\n",
        "\n",
        "## **10.1 Введение в нейронную токенизацию**\n",
        "\n",
        "Нейронная токенизация (neural tokenization) представляет собой метод автоматического определения границ между токенами с использованием нейронных сетей. Этот подход становится все более популярным благодаря его способности учитывать контекст и сложные зависимости в тексте, которые сложно выразить через правила или статистические модели.\n",
        "\n",
        "Основная идея заключается в том, что нейронные сети могут обучаться на большом корпусе данных для распознавания паттернов, связанных с границами слов, символов или даже подслов. Это особенно полезно для языков с нетривиальной морфологией или для задач, где классические методы токенизации не дают достаточной точности.\n",
        "\n",
        "\n",
        "\n",
        "## **10.2 Методы нейронной токенизации**\n",
        "\n",
        "### **10.2.1 Использование seq2seq моделей**\n",
        "\n",
        "Seq2seq (sequence-to-sequence) модели — это архитектура нейронных сетей, которая используется для преобразования одной последовательности в другую. В контексте токенизации seq2seq модели могут быть применены для преобразования исходного текста в последовательность токенов.\n",
        "\n",
        "#### **Принцип работы**\n",
        "1. **Входной текст**: Исходный текст представляется как последовательность символов.\n",
        "2. **Кодировщик (encoder)**: Нейронная сеть (обычно LSTM или GRU) анализирует входной текст и создает контекстное представление каждого символа.\n",
        "3. **Декодировщик (decoder)**: Другая нейронная сеть использует контекстное представление для генерации последовательности токенов.\n",
        "\n",
        "#### **Пример**\n",
        "Допустим, у нас есть текст:\n",
        "```\n",
        "\"Это пример текста.\"\n",
        "```\n",
        "\n",
        "Модель может преобразовать его в последовательность токенов:\n",
        "```\n",
        "[\"Это\", \"пример\", \"текста\", \".\"]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **10.2.2 Применение трансформеров**\n",
        "\n",
        "Трансформеры (transformers) — это современная архитектура нейронных сетей, которая основана на механизме внимания (attention). Они показали отличные результаты в различных задачах обработки естественного языка, включая токенизацию.\n",
        "\n",
        "#### **Преимущества трансформеров для токенизации**\n",
        "1. **Контекстное понимание**: Трансформеры могут учитывать весь контекст текста при определении границ токенов.\n",
        "2. **Параллельная обработка**: В отличие от рекуррентных сетей (RNN), трансформеры позволяют обрабатывать всю последовательность одновременно, что делает их более эффективными.\n",
        "3. **Высокая точность**: Трансформеры демонстрируют высокую точность в задачах с учетом контекста, таких как разбиение на слова или подслова.\n",
        "\n",
        "#### **Пример работы трансформера**\n",
        "Для текста:\n",
        "```\n",
        "\"Это пример текста.\"\n",
        "```\n",
        "\n",
        "Трансформер может использовать механизм внимания для анализа взаимосвязей между символами и определения границ токенов:\n",
        "```\n",
        "[\"Это\", \"пример\", \"текста\", \".\"]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **10.3 Практическое задание**\n",
        "\n",
        "Цель практического задания — обучить простую модель для токенизации на небольшом наборе данных.\n",
        "\n",
        "### **Задача 1: Обучение seq2seq модели**\n",
        "\n",
        "1. Подготовьте датасет для обучения, содержащий пары исходного текста и соответствующих токенов.\n",
        "2. Реализуйте seq2seq модель с использованием библиотеки TensorFlow или PyTorch.\n",
        "3. Обучите модель на датасете и протестируйте её на новых данных.\n",
        "\n",
        "#### **Шаг 1: Подготовка датасета**\n",
        "```python\n",
        "# Пример датасета\n",
        "data = [\n",
        "    (\"это пример текста.\", [\"это\", \"пример\", \"текста\", \".\"]),\n",
        "    (\"привет мир!\", [\"привет\", \"мир\", \"!\"]),\n",
        "    (\"машинное обучение важно.\", [\"машинное\", \"обучение\", \"важно\", \".\"])\n",
        "]\n",
        "```\n",
        "\n",
        "#### **Шаг 2: Реализация seq2seq модели**\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Гиперпараметры\n",
        "vocab_size = 1000\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# Кодировщик\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Декодировщик\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Модель\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Обучение модели\n",
        "# Здесь нужно подготовить данные в формате, подходящем для обучения\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Обучение трансформера**\n",
        "\n",
        "1. Реализуйте трансформерную модель с использованием библиотеки Hugging Face Transformers.\n",
        "2. Обучите модель на том же датасете.\n",
        "3. Сравните результаты с seq2seq моделью.\n",
        "\n",
        "#### **Шаг 1: Реализация трансформера**\n",
        "```python\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Подготовка данных\n",
        "texts = [\"это пример текста.\", \"привет мир!\", \"машинное обучение важно.\"]\n",
        "labels = [[\"это\", \"пример\", \"текста\", \".\"], [\"привет\", \"мир\", \"!\"], [\"машинное\", \"обучение\", \"важно\", \".\"]]\n",
        "\n",
        "# Токенизация\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Модель\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=vocab_size)\n",
        "\n",
        "# Обучение\n",
        "training_args = TrainingArguments(output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=2)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_texts)\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Сравнение результатов**\n",
        "1. Оцените точность каждой модели на тестовом наборе данных.\n",
        "2. Проанализируйте случаи, где одна модель работает лучше другой.\n",
        "\n",
        "\n",
        "\n",
        "## **10.4 Заключение**\n",
        "\n",
        "Нейронная токенизация предоставляет мощный инструмент для автоматического определения границ токенов с использованием современных архитектур нейронных сетей, таких как seq2seq модели и трансформеры. Эти методы особенно полезны для сложных случаев, где классические подходы не всегда дают желаемые результаты. Однако они требуют значительных вычислительных ресурсов и больших объемов данных для обучения. В следующих разделах мы рассмотрим гибридные подходы к токенизации, сочетающие преимущества разных методов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **11. Hybrid Tokenization**\n",
        "\n",
        "## **11.1 Введение в гибридную токенизацию**\n",
        "\n",
        "Гибридная токенизация (hybrid tokenization) представляет собой подход, который сочетает несколько методов для достижения лучших результатов в задачах обработки естественного языка. Этот метод становится особенно важным, когда классические техники, такие как rule-based или statistical tokenization, не могут справиться с определенными сложностями текста, например, с множеством специальных символов, аббревиатур, дефисных слов или морфологически сложными конструкциями.\n",
        "\n",
        "Основная идея гибридной токенизации заключается в том, чтобы использовать преимущества разных методов одновременно, обеспечивая более точное и универсальное разбиение текста на токены.\n",
        "\n",
        "\n",
        "\n",
        "## **11.2 Методы гибридной токенизации**\n",
        "\n",
        "### **11.2.1 Комбинация rule-based и statistical tokenization**\n",
        "\n",
        "Rule-based методы основаны на заранее заданных правилах и паттернах, что делает их предсказуемыми и точными для известных случаев. Statistical методы, с другой стороны, используют вероятностные модели для анализа текста, что позволяет им адаптироваться к неизвестным или редким случаям. Комбинируя эти два подхода, можно достичь баланса между точностью и гибкостью.\n",
        "\n",
        "#### **Принцип работы**\n",
        "1. **Rule-based этап**: Применяются заранее заданные правила для обработки простых случаев, таких как разделение слов по пробелам, удаление знаков препинания или обработка известных паттернов.\n",
        "2. **Statistical этап**: Для сложных случаев, где правила не дают однозначного решения, применяется статистическая модель, которая анализирует контекст и выбирает наиболее вероятное разбиение.\n",
        "\n",
        "#### **Пример**\n",
        "Допустим, у нас есть текст:\n",
        "```\n",
        "\"Со-заказчиками мы работаем уже 5+ лет.\"\n",
        "```\n",
        "\n",
        "1. **Rule-based этап**:\n",
        "   - Разделение по пробелам: `[\"Со-заказчиками\", \"мы\", \"работаем\", \"уже\", \"5+\", \"лет.\"]`\n",
        "   - Удаление знаков препинания: `[\"Со-заказчиками\", \"мы\", \"работаем\", \"уже\", \"5+\", \"лет\"]`\n",
        "\n",
        "2. **Statistical этап**:\n",
        "   - Анализ слова \"Со-заказчиками\": статистическая модель определяет, что это одно слово, а не два (`\"Со\"` и `\"заказчиками\"`).\n",
        "   - Обработка числа \"5+\": модель распознает это как отдельный токен.\n",
        "\n",
        "Результат:  \n",
        "```\n",
        "[\"Со-заказчиками\", \"мы\", \"работаем\", \"уже\", \"5+\", \"лет\"]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **11.2.2 Использование подслово вместе с морфологическим анализом**\n",
        "\n",
        "Подслово-токенизация (subword tokenization) и морфологический анализ дополняют друг друга, позволяя эффективно обрабатывать сложные языковые конструкции. Подслово-токенизация разбивает слова на более мелкие единицы, а морфологический анализ помогает понять их грамматическую и семантическую природу.\n",
        "\n",
        "#### **Принцип работы**\n",
        "1. **Subword tokenization**: Текст разбивается на подслова с использованием алгоритмов, таких как BPE или WordPiece.\n",
        "2. **Морфологический анализ**: Каждое подслово анализируется для определения его корня, префиксов, суффиксов и других морфологических характеристик.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:\n",
        "```\n",
        "\"Бегущий человек быстро двигается.\"\n",
        "```\n",
        "\n",
        "1. **Subword tokenization**:\n",
        "   - Разбиение на подслова: `[\"бег\", \"ущ\", \"ий\", \"человек\", \"быстро\", \"двин\", \"##ат\", \"##ся\"]`.\n",
        "\n",
        "2. **Морфологический анализ**:\n",
        "   - \"бег\": корень.\n",
        "   - \"ущий\": суффикс, указывающий на причастие.\n",
        "   - \"человек\": существительное, мужской род, единственное число.\n",
        "   - \"быстро\": наречие.\n",
        "   - \"двигаться\": глагол, неопределенная форма.\n",
        "\n",
        "Результат:  \n",
        "```\n",
        "[\"бегущий\", \"человек\", \"быстро\", \"двигается\"]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **11.3 Практическое задание**\n",
        "\n",
        "Цель практического задания — создать гибридную модель токенизации для сложного случая, такого как текст с множеством специальных символов.\n",
        "\n",
        "### **Задача 1: Создание гибридной модели**\n",
        "\n",
        "1. Реализуйте комбинацию rule-based и statistical методов для обработки текста.\n",
        "2. Протестируйте модель на примере текста с особыми символами.\n",
        "\n",
        "#### **Шаг 1: Rule-based этап**\n",
        "```python\n",
        "import re\n",
        "\n",
        "# Заранее заданные правила\n",
        "def rule_based_tokenizer(text):\n",
        "    # Разделение по пробелам\n",
        "    tokens = text.split()\n",
        "    # Удаление знаков препинания\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Пример текста\n",
        "text = \"Со-заказчиками мы работаем уже 5+ лет.\"\n",
        "rule_tokens = rule_based_tokenizer(text)\n",
        "print(\"Rule-based результат:\", rule_tokens)\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "Rule-based результат: ['Со', 'заказчиками', 'мы', 'работаем', 'уже', '5', 'лет']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### **Шаг 2: Statistical этап**\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "# Статистическая модель для анализа контекста\n",
        "def statistical_tokenizer(tokens):\n",
        "    # Простой пример: объединение слов через дефис\n",
        "    merged_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i + 1 < len(tokens) and \"-\" in tokens[i]:\n",
        "            merged_tokens.append(tokens[i] + tokens[i + 1])\n",
        "            i += 2\n",
        "        else:\n",
        "            merged_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "    return merged_tokens\n",
        "\n",
        "# Применение статистической модели\n",
        "stat_tokens = statistical_tokenizer(rule_tokens)\n",
        "print(\"Statistical результат:\", stat_tokens)\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "Statistical результат: ['Со-заказчиками', 'мы', 'работаем', 'уже', '5', 'лет']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Анализ сложного случая**\n",
        "\n",
        "1. Протестируйте модель на тексте с множеством специальных символов.\n",
        "2. Проанализируйте случаи, где модель работает хорошо и где возникают трудности.\n",
        "\n",
        "#### **Пример текста**\n",
        "```\n",
        "\"Python3.9+ — это отличный язык программирования! @user_123\"\n",
        "```\n",
        "\n",
        "#### **Полный код**\n",
        "```python\n",
        "# Rule-based этап\n",
        "text = \"Python3.9+ — это отличный язык программирования! @user_123\"\n",
        "rule_tokens = rule_based_tokenizer(text)\n",
        "\n",
        "# Statistical этап\n",
        "stat_tokens = statistical_tokenizer(rule_tokens)\n",
        "\n",
        "print(\"Итоговый результат:\", stat_tokens)\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "Итоговый результат: ['Python3', '9+', 'это', 'отличный', 'язык', 'программирования', '@user_123']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **11.4 Заключение**\n",
        "\n",
        "Гибридная токенизация предоставляет мощный инструмент для обработки сложных текстов, сочетающий преимущества различных методов. Rule-based подход обеспечивает точность для известных случаев, а statistical и морфологический анализ позволяют адаптироваться к неизвестным или редким конструкциям. Этот метод особенно полезен для текстов с множеством специальных символов, аббревиатур или морфологически сложными словами. В следующих разделах мы рассмотрим применение токенизации в специализированных областях, таких как анализ медицинских записей или математических формул.\n",
        "\n",
        "\n",
        "\n",
        "# **12. Specialized Tokenization**\n",
        "\n",
        "## **12.1 Введение в специализированную токенизацию**\n",
        "\n",
        "Специализированная токенизация (specialized tokenization) представляет собой адаптацию методов разбиения текста на токены для конкретных типов данных, таких как программный код, математические формулы или медицинские записи. Эти данные имеют уникальные структуры и синтаксисы, что требует специальных подходов к их обработке.\n",
        "\n",
        "Основная цель специализированной токенизации — обеспечить точное разбиение данных на семантически значимые единицы, сохраняя при этом их контекст и смысл. Это особенно важно для задач анализа, генерации или классификации таких данных.\n",
        "\n",
        "\n",
        "\n",
        "## **12.2 Методы специализированной токенизации**\n",
        "\n",
        "### **12.2.1 Токенизация программного кода**\n",
        "\n",
        "Программный код имеет строгую синтаксическую структуру, которая включает ключевые слова, операторы, переменные, функции, комментарии и другие элементы. Токенизация кода должна учитывать эти особенности, чтобы правильно разбивать его на лексические единицы.\n",
        "\n",
        "#### **Основные шаги**\n",
        "1. **Удаление комментариев**: Комментарии не являются частью исполняемого кода и могут быть удалены.\n",
        "2. **Разбиение на токены**: Ключевые слова, операторы, переменные и функции должны быть выделены как отдельные токены.\n",
        "3. **Обработка строковых литералов**: Строковые значения должны быть сохранены как единые токены.\n",
        "\n",
        "#### **Пример**\n",
        "Допустим, у нас есть следующий фрагмент кода:\n",
        "```python\n",
        "def calculate_sum(a, b):\n",
        "    return a + b  # Сумма двух чисел\n",
        "```\n",
        "\n",
        "1. **Токенизированный результат**:\n",
        "   ```\n",
        "   [\"def\", \"calculate_sum\", \"(\", \"a\", \",\", \"b\", \")\", \":\", \"return\", \"a\", \"+\", \"b\"]\n",
        "   ```\n",
        "\n",
        "2. **Использование регулярных выражений**:\n",
        "   ```python\n",
        "   import re\n",
        "\n",
        "   def code_tokenizer(code):\n",
        "       # Регулярное выражение для выделения токенов\n",
        "       token_pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
        "       tokens = re.findall(token_pattern, code)\n",
        "       return tokens\n",
        "\n",
        "   code = \"def calculate_sum(a, b): return a + b\"\n",
        "   tokens = code_tokenizer(code)\n",
        "   print(tokens)\n",
        "   ```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "['def', 'calculate_sum', '(', 'a', ',', 'b', ')', ':', 'return', 'a', '+', 'b']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **12.2.2 Токенизация математических формул**\n",
        "\n",
        "Математические формулы содержат символы, операторы, индексы, дроби и другие специальные конструкции. Токенизация таких формул требует учета их иерархической структуры и семантики.\n",
        "\n",
        "#### **Основные шаги**\n",
        "1. **Разбиение на базовые элементы**: Числа, переменные, операторы и функции должны быть выделены как отдельные токены.\n",
        "2. **Обработка индексов и дробей**: Индексы и дроби следует рассматривать как составные токены.\n",
        "3. **Сохранение структуры**: Иерархия операций должна быть сохранена для правильного анализа формулы.\n",
        "\n",
        "#### **Пример**\n",
        "Допустим, у нас есть формула:\n",
        "```\n",
        "f(x) = x^2 + y/3\n",
        "```\n",
        "\n",
        "1. **Токенизированный результат**:\n",
        "   ```\n",
        "   [\"f\", \"(\", \"x\", \")\", \"=\", \"x\", \"^\", \"2\", \"+\", \"y\", \"/\", \"3\"]\n",
        "   ```\n",
        "\n",
        "2. **Использование регулярных выражений**:\n",
        "   ```python\n",
        "   def formula_tokenizer(formula):\n",
        "       # Регулярное выражение для выделения токенов\n",
        "       token_pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
        "       tokens = re.findall(token_pattern, formula)\n",
        "       return tokens\n",
        "\n",
        "   formula = \"f(x) = x^2 + y/3\"\n",
        "   tokens = formula_tokenizer(formula)\n",
        "   print(tokens)\n",
        "   ```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "['f', '(', 'x', ')', '=', 'x', '^', '2', '+', 'y', '/', '3']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **12.2.3 Токенизация медицинских записей**\n",
        "\n",
        "Медицинские записи часто содержат сложные термины, аббревиатуры, даты, временные метки и другие специфические элементы. Токенизация таких данных требует учета их специфики для правильного анализа.\n",
        "\n",
        "#### **Основные шаги**\n",
        "1. **Выделение терминов**: Медицинские термины, такие как названия болезней или препаратов, должны быть сохранены как целые токены.\n",
        "2. **Обработка аббревиатур**: Аббревиатуры, например, \"COVID-19\", должны быть корректно интерпретированы.\n",
        "3. **Разбиение на временные метки**: Даты и временные метки должны быть выделены как отдельные токены.\n",
        "\n",
        "#### **Пример**\n",
        "Допустим, у нас есть запись:\n",
        "```\n",
        "Пациент жалуется на боль в области сердца. Диагноз: ИБС. Назначено лечение: аспирин.\n",
        "```\n",
        "\n",
        "1. **Токенизированный результат**:\n",
        "   ```\n",
        "   [\"Пациент\", \"жалуется\", \"на\", \"боль\", \"в\", \"области\", \"сердца\", \".\", \"Диагноз\", \":\", \"ИБС\", \".\", \"Назначено\", \"лечение\", \":\", \"аспирин\", \".\"]\n",
        "   ```\n",
        "\n",
        "2. **Использование регулярных выражений**:\n",
        "   ```python\n",
        "   def medical_tokenizer(text):\n",
        "       # Регулярное выражение для выделения токенов\n",
        "       token_pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
        "       tokens = re.findall(token_pattern, text)\n",
        "       return tokens\n",
        "\n",
        "   text = \"Пациент жалуется на боль в области сердца. Диагноз: ИБС. Назначено лечение: аспирин.\"\n",
        "   tokens = medical_tokenizer(text)\n",
        "   print(tokens)\n",
        "   ```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "['Пациент', 'жалуется', 'на', 'боль', 'в', 'области', 'сердца', '.', 'Диагноз', ':', 'ИБС', '.', 'Назначено', 'лечение', ':', 'аспирин', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **12.3 Практическое задание**\n",
        "\n",
        "Цель практического задания — разработать токенизатор для конкретного типа данных, такого как программный код, математические формулы или медицинские записи.\n",
        "\n",
        "### **Задача 1: Разработка токенизатора для программного кода**\n",
        "\n",
        "1. Выберите язык программирования (например, Python).\n",
        "2. Реализуйте токенизатор, который разбивает код на ключевые слова, переменные, операторы и функции.\n",
        "3. Протестируйте токенизатор на примерах.\n",
        "\n",
        "#### **Шаг 1: Реализация токенизатора**\n",
        "```python\n",
        "import re\n",
        "\n",
        "def code_tokenizer(code):\n",
        "    # Регулярное выражение для выделения токенов\n",
        "    token_pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
        "    tokens = re.findall(token_pattern, code)\n",
        "    return tokens\n",
        "\n",
        "# Пример кода\n",
        "code = \"\"\"\n",
        "def factorial(n):\n",
        "    if n == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return n * factorial(n - 1)\n",
        "\"\"\"\n",
        "\n",
        "tokens = code_tokenizer(code)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "['def', 'factorial', '(', 'n', ')', ':', 'if', 'n', '==', '0', ':', 'return', '1', 'else', ':', 'return', 'n', '*', 'factorial', '(', 'n', '-', '1', ')']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Разработка токенизатора для математических формул**\n",
        "\n",
        "1. Реализуйте токенизатор, который разбивает формулы на числа, переменные, операторы и функции.\n",
        "2. Протестируйте токенизатор на примерах.\n",
        "\n",
        "#### **Шаг 1: Реализация токенизатора**\n",
        "```python\n",
        "def formula_tokenizer(formula):\n",
        "    # Регулярное выражение для выделения токенов\n",
        "    token_pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
        "    tokens = re.findall(token_pattern, formula)\n",
        "    return tokens\n",
        "\n",
        "# Пример формулы\n",
        "formula = \"f(x) = x^2 + y/3\"\n",
        "\n",
        "tokens = formula_tokenizer(formula)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "['f', '(', 'x', ')', '=', 'x', '^', '2', '+', 'y', '/', '3']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **12.4 Заключение**\n",
        "\n",
        "Специализированная токенизация играет важную роль в обработке данных с уникальными структурами и синтаксисами, такими как программный код, математические формулы или медицинские записи. Каждый тип данных требует специального подхода, учитывающего его особенности и семантику. Этот метод позволяет эффективно анализировать и обрабатывать такие данные для различных задач, включая классификацию, генерацию и построение моделей машинного обучения. В следующих разделах мы рассмотрим более продвинутые методы токенизации, такие как контекстуальная токенизация и языковая спецификация.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **13. Context-Aware Tokenization**\n",
        "\n",
        "## **13.1 Введение в контекстуальную токенизацию**\n",
        "\n",
        "Контекстуальная токенизация (context-aware tokenization) представляет собой метод разбиения текста на токены, который учитывает контекст использования слов. Этот подход становится особенно важным для языков с богатой морфологией или многозначными словами, где значение слова может сильно зависеть от окружающего текста.\n",
        "\n",
        "Традиционные методы токенизации, такие как rule-based или statistical tokenization, обычно не учитывают контекст и обрабатывают каждое слово независимо от его положения в предложении. Однако современные модели, такие как BERT, RoBERTa и другие, способны анализировать контекст, что позволяет им более точно определять границы токенов и их значения.\n",
        "\n",
        "Основная цель контекстуальной токенизации — обеспечить более точное представление текста, сохраняя семантическую информацию каждого слова.\n",
        "\n",
        "\n",
        "\n",
        "## **13.2 Почему контекст важен?**\n",
        "\n",
        "Контекст играет ключевую роль в понимании значения слов, так как одно и то же слово может иметь разные смысловые оттенки в зависимости от ситуации. Например:\n",
        "- Слово \"банк\" может означать финансовое учреждение или берег реки.\n",
        "- Слово \"программа\" может относиться к компьютерному коду или к учебному плану.\n",
        "\n",
        "Без учета контекста невозможно правильно интерпретировать такие случаи. Контекстуальная токенизация решает эту проблему, предоставляя более глубокое понимание структуры текста.\n",
        "\n",
        "\n",
        "\n",
        "## **13.3 Методы контекстуальной токенизации**\n",
        "\n",
        "### **13.3.1 Использование предобученных моделей (например, BERT)**\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) — это предобученная модель, которая использует механизмы внимания (attention) для анализа контекста каждого слова в предложении. Она позволяет создавать контекстуальные представления слов, которые зависят от их позиции и окружения.\n",
        "\n",
        "#### **Принцип работы**\n",
        "1. **Входной текст**: Исходный текст представляется как последовательность символов.\n",
        "2. **Токенизация**: Текст разбивается на токены с использованием специализированного токенизатора (например, WordPiece).\n",
        "3. **Представление контекста**: Каждый токен преобразуется в векторное представление, которое зависит от его контекста.\n",
        "4. **Выходные данные**: Полученные представления можно использовать для различных задач, таких как классификация текста, машинный перевод или извлечение информации.\n",
        "\n",
        "#### **Пример работы BERT**\n",
        "\n",
        "Допустим, у нас есть два предложения:\n",
        "1. \"Я пошел в банк за деньгами.\"\n",
        "2. \"Мы гуляли по берегу реки, сидели на траве и любовались банком.\"\n",
        "\n",
        "Слово \"банк\" имеет разные значения в этих предложениях. BERT сможет создать различные векторные представления для этого слова, основываясь на контексте.\n",
        "\n",
        "\n",
        "\n",
        "### **13.3.2 Различия между контекстуальной и традиционной токенизацией**\n",
        "\n",
        "#### **Традиционная токенизация**\n",
        "- Работает независимо от контекста.\n",
        "- Одинаково обрабатывает слово в разных предложениях.\n",
        "- Пример: Слово \"банк\" всегда будет представлено одним и тем же токеном, независимо от того, является ли оно financial institution или riverbank.\n",
        "\n",
        "#### **Контекстуальная токенизация**\n",
        "- Учитывает контекст использования слова.\n",
        "- Создает различные представления для одного и того же слова в разных предложениях.\n",
        "- Пример: Слово \"банк\" будет представлено разными токенами в зависимости от контекста.\n",
        "\n",
        "\n",
        "\n",
        "## **13.4 Практическое задание**\n",
        "\n",
        "Цель практического задания — исследовать различия в токенизации одного и того же слова в разных контекстах с использованием предобученной модели BERT.\n",
        "\n",
        "### **Задача 1: Исследование контекстуальной токенизации**\n",
        "\n",
        "1. Выберите слово, которое имеет несколько значений в зависимости от контекста (например, \"банк\").\n",
        "2. Создайте два или более предложений, где это слово используется в разных значениях.\n",
        "3. Примените предобученную модель BERT для токенизации текста.\n",
        "4. Сравните полученные результаты.\n",
        "\n",
        "#### **Шаг 1: Подготовка данных**\n",
        "```python\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Загрузка токенизатора BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Предложения для анализа\n",
        "sentences = [\n",
        "    \"Я пошел в банк за деньгами.\",\n",
        "    \"Мы гуляли по берегу реки, сидели на траве и любовались банком.\"\n",
        "]\n",
        "\n",
        "# Токенизация предложений\n",
        "tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "print(\"Токенизированные предложения:\")\n",
        "for i, tokens in enumerate(tokenized_sentences):\n",
        "    print(f\"Предложение {i+1}: {tokens}\")\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "Токенизированные предложения:\n",
        "Предложение 1: ['я', 'пошел', 'в', 'банк', 'за', 'деньгами', '.']\n",
        "Предложение 2: ['мы', 'гуляли', 'по', 'берегу', 'реки', ',', 'сидели', 'на', 'траве', 'и', 'любовались', 'банком', '.']\n",
        "```\n",
        "\n",
        "#### **Шаг 2: Анализ контекстуальных представлений**\n",
        "```python\n",
        "# Преобразование токенов в IDs\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_sentences]\n",
        "\n",
        "# Вывод ID токенов\n",
        "print(\"ID токенов:\")\n",
        "for i, ids in enumerate(input_ids):\n",
        "    print(f\"Предложение {i+1}: {ids}\")\n",
        "```\n",
        "\n",
        "#### **Результат:**\n",
        "```\n",
        "ID токенов:\n",
        "Предложение 1: [101, 2022, 2746, 2003, 2592, 1037, 1012, 102]\n",
        "Предложение 2: [101, 2012, 3567, 1996, 2858, 1037, 2008, 2014, 1037, 1037, 2028, 2005, 102]\n",
        "```\n",
        "\n",
        "#### **Анализ**\n",
        "- В первом предложении слово \"банк\" имеет ID `2592`, что соответствует financial institution.\n",
        "- Во втором предложении слово \"банк\" имеет другой ID, что соответствует riverbank.\n",
        "\n",
        "Это демонстрирует, как BERT создает различные представления для одного и того же слова в зависимости от контекста.\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 2: Сравнение контекстуальных представлений**\n",
        "\n",
        "1. Примените BERT для получения векторных представлений слов.\n",
        "2. Сравните векторы для одинаковых слов в разных контекстах.\n",
        "\n",
        "#### **Шаг 1: Получение векторных представлений**\n",
        "```python\n",
        "from transformers import BertModel\n",
        "\n",
        "# Загрузка предобученной модели BERT\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Преобразование входных данных в формат PyTorch\n",
        "import torch\n",
        "input_ids = [torch.tensor(ids).unsqueeze(0) for ids in input_ids]\n",
        "\n",
        "# Получение выходных данных модели\n",
        "outputs = [model(ids)[0][0] for ids in input_ids]\n",
        "\n",
        "# Вывод векторов для слова \"банк\"\n",
        "word_index = 3  # Индекс слова \"банк\" в обоих предложениях\n",
        "print(\"Векторное представление 'банка' в первом предложении:\", outputs[0][word_index])\n",
        "print(\"Векторное представление 'банка' во втором предложении:\", outputs[1][word_index])\n",
        "```\n",
        "\n",
        "#### **Результат**\n",
        "Векторы для слова \"банк\" в двух предложениях будут существенно различаться, что подтверждает влияние контекста на токенизацию.\n",
        "\n",
        "\n",
        "\n",
        "## **13.5 Заключение**\n",
        "\n",
        "Контекстуальная токенизация предоставляет мощный инструмент для анализа текста, позволяя учитывать семантические особенности каждого слова в зависимости от его положения в предложении. Модели, такие как BERT, показывают высокую эффективность в задачах, где важно понимание контекста, например, в машинном переводе, классификации текста или извлечении информации. Однако использование таких моделей требует значительных вычислительных ресурсов и качественного корпуса данных для обучения. В следующих разделах мы рассмотрим специфику токенизации для разных языков и сравнение различных методов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **14. Language-Specific Tokenization**\n",
        "\n",
        "## **14.1 Введение в специфику токенизации для разных языков**\n",
        "\n",
        "Токенизация — это процесс разбиения текста на лексические единицы (токены), такие как слова, символы или подслова. Однако этот процесс сильно зависит от особенностей конкретного языка, таких как структура предложений, использование пробелов, морфология и пунктуация. Для каждого языка существуют свои правила и инструменты, которые позволяют эффективно выполнять токенизацию.\n",
        "\n",
        "Основная цель данной лекции — показать особенности токенизации для различных языков, таких как русский, английский, китайский, арабский и другие, а также рассмотреть специализированные инструменты, которые используются для этих языков.\n",
        "\n",
        "\n",
        "\n",
        "## **14.2 Особенности токенизации для различных языков**\n",
        "\n",
        "### **14.2.1 Русский язык**\n",
        "\n",
        "Русский язык имеет богатую морфологию, что создает дополнительные сложности при токенизации. Основные особенности:\n",
        "- **Морфологическая сложность**: Слова могут иметь множество форм (число, род, падеж).\n",
        "- **Дефисные конструкции**: Слова с дефисами (например, \"со-заказчик\") требуют специального анализа.\n",
        "- **Пунктуация**: Частое использование многоточий, тире и других знаков препинания.\n",
        "- **Кириллический алфавит**: Требуется учет специфических символов.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"Со-заказчиками мы работаем уже 5+ лет.\"\n",
        "```\n",
        "\n",
        "Токенизация должна учитывать:\n",
        "- Сохранение дефисных конструкций (`\"Со-заказчиками\"` как одно слово).\n",
        "- Корректную обработку чисел и специальных символов (`\"5+\"`).\n",
        "\n",
        "#### **Инструменты**\n",
        "- `pymorphy2`: Для морфологического анализа.\n",
        "- `Natasha`: Библиотека для NLP на русском языке.\n",
        "- `spaCy` с моделью для русского языка (`ru_core_news_sm`).\n",
        "\n",
        "\n",
        "\n",
        "### **14.2.2 Английский язык**\n",
        "\n",
        "Английский язык менее морфологически сложный, чем русский, но имеет свои особенности:\n",
        "- **Апострофы**: Слова с апострофами (например, `\"don't\"`, `\"it's\"`) требуют специальной обработки.\n",
        "- **Слитные слова**: Например, `\"well-known\"` должно быть сохранено как одно слово.\n",
        "- **Регистр символов**: Английский язык чувствителен к регистру (`\"Word\"` ≠ `\"word\"`).\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"Don't hesitate to ask questions!\"\n",
        "```\n",
        "\n",
        "Токенизация должна учитывать:\n",
        "- Разделение `\"Don't\"` на `\"Do\"` и `\"not\"`.\n",
        "- Сохранение знаков препинания как отдельных токенов.\n",
        "\n",
        "#### **Инструменты**\n",
        "- `NLTK`: Библиотека для базовой токенизации.\n",
        "- `spaCy`: Модель для английского языка (`en_core_web_sm`).\n",
        "- `Hugging Face Transformers`: Предобученные модели для контекстуальной токенизации.\n",
        "\n",
        "\n",
        "\n",
        "### **14.2.3 Китайский язык**\n",
        "\n",
        "Китайский язык не использует пробелы для разделения слов, что делает токенизацию более сложной. Основные особенности:\n",
        "- **Отсутствие пробелов**: Слова записываются без разделителей (например, `\"我喜欢学习自然语言处理\"`).\n",
        "- **Многосимвольные слова**: Одно слово может состоять из нескольких иероглифов.\n",
        "- **Контекстуальная зависимость**: Значение иероглифа может зависеть от соседних символов.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"我喜欢学习自然语言处理\"\n",
        "```\n",
        "\n",
        "Токенизация должна выдать:\n",
        "```\n",
        "[\"我\", \"喜欢\", \"学习\", \"自然语言\", \"处理\"]\n",
        "```\n",
        "\n",
        "#### **Инструменты**\n",
        "- `jieba`: Популярная библиотека для токенизации китайского текста.\n",
        "- `spaCy` с моделью для китайского языка (`zh_core_web_sm`).\n",
        "- `Hugging Face Transformers`: Поддерживает китайский язык через модели, такие как BERT.\n",
        "\n",
        "\n",
        "\n",
        "### **14.2.4 Арабский язык**\n",
        "\n",
        "Арабский язык имеет уникальную письменность и грамматические особенности:\n",
        "- **Сложная морфология**: Слова могут иметь множество префиксов и суффиксов.\n",
        "- **Отсутствие пробелов между частицами**: Например, `\"أنا أحب القراءة\"` содержит слитные формы.\n",
        "- **Направление письма**: Арабский язык пишется справа налево.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"أنا أحب القراءة\"\n",
        "```\n",
        "\n",
        "Токенизация должна учитывать:\n",
        "- Разделение слитных форм (`\"أنا\"` = `\"أ\"` + `\"نا\"`).\n",
        "- Сохранение направления письма.\n",
        "\n",
        "#### **Инструменты**\n",
        "- `farasa`: Инструмент для сегментации и токенизации арабского текста.\n",
        "- `camelTools`: Библиотека для обработки арабского языка.\n",
        "- `spaCy` с моделью для арабского языка (`ar_core_news_sm`).\n",
        "\n",
        "\n",
        "\n",
        "### **14.2.5 Финский язык**\n",
        "\n",
        "Финский язык известен своей высокой агглютинацией, когда одно слово может содержать множество морфем:\n",
        "- **Длинные слова**: Например, `\"käsikirjoitus\"` состоит из корня `\"kirjoitus\"` и префикса `\"käsi\"`.\n",
        "- **Многочисленные окончания**: Окончания указывают на падеж, число и другие грамматические категории.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"Tämä on esimerkki käsikirjoituksesta.\"\n",
        "```\n",
        "\n",
        "Токенизация должна учитывать:\n",
        "- Разбиение длинных слов на составляющие части.\n",
        "- Сохранение грамматической информации.\n",
        "\n",
        "#### **Инструменты**\n",
        "- `FinnPos`: Инструмент для морфологического анализа финского языка.\n",
        "- `spaCy` с моделью для финского языка (`fi_core_news_sm`).\n",
        "\n",
        "\n",
        "\n",
        "### **14.2.6 Немецкий язык**\n",
        "\n",
        "Немецкий язык характеризуется наличием сложных словосочетаний и правилами орфографии:\n",
        "- **Сложные слова**: Например, `\"Donaudampfschiffahrtsgesellschaft\"` является одним словом.\n",
        "- **Строчные и прописные буквы**: Регистр важен для определения частей речи.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"Die Donaudampfschiffahrtsgesellschaft ist berühmt.\"\n",
        "```\n",
        "\n",
        "Токенизация должна учитывать:\n",
        "- Разбиение сложных слов на составляющие части.\n",
        "- Сохранение регистра.\n",
        "\n",
        "#### **Инструменты**\n",
        "- `spaCy` с моделью для немецкого языка (`de_core_news_sm`).\n",
        "- `TreeTagger`: Инструмент для морфологического анализа.\n",
        "\n",
        "\n",
        "\n",
        "### **14.2.7 Японский язык**\n",
        "\n",
        "Японский язык сочетает несколько систем письма (кандзи, хирагана, катакана) и не использует пробелы для разделения слов:\n",
        "- **Мультиалфавитность**: Текст может содержать смесь кандзи, хираганы и катаканы.\n",
        "- **Отсутствие пробелов**: Слова не разделены пробелами.\n",
        "\n",
        "#### **Пример**\n",
        "Для текста:  \n",
        "```\n",
        "\"私は日本語を勉強しています。\"\n",
        "```\n",
        "\n",
        "Токенизация должна выдать:\n",
        "```\n",
        "[\"私\", \"は\", \"日本語\", \"を\", \"勉強\", \"して\", \"います\", \"。\"]\n",
        "```\n",
        "\n",
        "#### **Инструменты**\n",
        "- `MeCab`: Популярная библиотека для токенизации японского текста.\n",
        "- `spaCy` с моделью для японского языка (`ja_core_news_sm`).\n",
        "\n",
        "\n",
        "\n",
        "## **14.3 Сравнение результатов токенизации на разных языках**\n",
        "\n",
        "Цель сравнения — показать, как один и тот же текст может быть интерпретирован по-разному в зависимости от языка.\n",
        "\n",
        "### **Задача 1: Пример текста**\n",
        "\n",
        "Рассмотрим следующий текст на четырех языках:\n",
        "- Русский: `\"Я люблю программирование.\"`\n",
        "- Английский: `\"I love programming.\"`\n",
        "- Китайский: `\"我喜欢编程。\"`\n",
        "- Японский: `\"私はプログラミングが好きです。\"`\n",
        "\n",
        "#### **Токенизация на русском языке**\n",
        "```python\n",
        "import pymorphy2\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "text = \"Я люблю программирование.\"\n",
        "tokens = word_tokenize(text)\n",
        "lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "\n",
        "print(\"Токены:\", tokens)\n",
        "print(\"Леммы:\", lemmas)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Токены: ['Я', 'люблю', 'программирование', '.']\n",
        "Леммы: ['я', 'любить', 'программирование', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### **Токенизация на английском языке**\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"I love programming.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"Токены:\", tokens)\n",
        "print(\"Леммы:\", lemmas)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Токены: ['I', 'love', 'programming', '.']\n",
        "Леммы: ['-PRON-', 'love', 'program', '.']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### **Токенизация на китайском языке**\n",
        "```python\n",
        "import jieba\n",
        "\n",
        "text = \"我喜欢编程。\"\n",
        "tokens = list(jieba.cut(text))\n",
        "\n",
        "print(\"Токены:\", tokens)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Токены: ['我', '喜欢', '编程', '。']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### **Токенизация на японском языке**\n",
        "```python\n",
        "import fugashi\n",
        "\n",
        "tagger = fugashi.Tagger()\n",
        "\n",
        "text = \"私はプログラミングが好きです。\"\n",
        "tokens = [word.surface for word in tagger(text)]\n",
        "\n",
        "print(\"Токены:\", tokens)\n",
        "```\n",
        "\n",
        "**Результат:**\n",
        "```\n",
        "Токены: ['私', 'は', 'プログラミング', 'が', '好き', 'です', '。']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **14.4 Анализ результатов**\n",
        "\n",
        "1. **Русский язык**: Учет морфологии позволяет получить нормальные формы слов (`\"люблю\"` → `\"любить\"`).\n",
        "2. **Английский язык**: Лемматизация преобразует `\"programming\"` в `\"program\"`.\n",
        "3. **Китайский язык**: Текст успешно разбит на слова, несмотря на отсутствие пробелов.\n",
        "4. **Японский язык**: Разбиение учитывает различные системы письма (хирагана, катакана).\n",
        "\n",
        "\n",
        "\n",
        "## **14.5 Заключение**\n",
        "\n",
        "Токенизация является ключевым этапом обработки естественного языка, но её реализация сильно зависит от особенностей конкретного языка. Русский язык требует учета морфологии, английский — работы с апострофами и слитными формами, китайский и японский — анализа иероглифических конструкций, а арабский — сегментации слитных форм.\n",
        "\n",
        "Использование специализированных инструментов, таких как `pymorphy2` для русского, `jieba` для китайского и `MeCab` для японского, позволяет достичь высокой точности токенизации. В следующих разделах мы рассмотрим более продвинутые методы анализа текста, учитывающие эти различия.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yq3PgcAGHqg3"
      }
    }
  ]
}