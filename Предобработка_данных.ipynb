{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOxSqtHQYLtAjszJaxcxls5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/%D0%9F%D1%80%D0%B5%D0%B4%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Предобработка данных\n",
        "\n",
        "Предобработка данных — это важный этап в анализе и подготовке данных перед их использованием в машинном обучении, визуализации или других задачах. Она направлена на улучшение качества данных, чтобы последующие модели были более точными и надежными."
      ],
      "metadata": {
        "id": "NFCXKWnsz7KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Изучение данных (Data Understanding)\n",
        "\n",
        "## Введение\n",
        "\n",
        "Изучение данных (Data Understanding) является ключевым этапом в процессе анализа данных и разработки моделей машинного обучения. На данном этапе исследователь проводит первичный анализ структуры и содержания набора данных, выявляет потенциальные проблемы, формирует гипотезы о взаимосвязях между переменными и готовится к последующему этапу предварительной обработки данных. Целью данного этапа является глубокое понимание характеристик исходных данных, что позволяет принимать более обоснованные решения на дальнейших этапах проекта.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Загрузка данных\n",
        "\n",
        "### Определение\n",
        "\n",
        "Загрузка данных — это процесс импорта информации из внешних источников в рабочую среду для последующего анализа. Данный шаг является технической основой всего исследования.\n",
        "\n",
        "### Форматы файлов\n",
        "\n",
        "Наиболее распространённые форматы хранения табличных данных:\n",
        "\n",
        "- **CSV** (Comma-Separated Values) — текстовый формат с разделителями.\n",
        "- **Excel (.xlsx)** — формат электронных таблиц Microsoft Excel.\n",
        "- **JSON** — структурированный формат для представления данных.\n",
        "- **SQL Базы данных** — реляционные базы данных.\n",
        "- **Parquet, HDF5, Pickle** — специализированные форматы для работы с большими объемами данных.\n",
        "\n",
        "### Пример реализации на Python\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Загрузка CSV файла\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Загрузка Excel файла\n",
        "# df = pd.read_excel('data.xlsx')\n",
        "```\n",
        "\n",
        "> Примечание: перед загрузкой необходимо убедиться, что файл находится в правильной директории или указан корректный путь к нему.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Осмотр структуры данных\n",
        "\n",
        "Цель данного подэтапа — получить общее представление о размерности, типах данных и наличии пропущенных значений.\n",
        "\n",
        "### a) Размерность датасета\n",
        "\n",
        "Для определения количества строк и столбцов используется атрибут `.shape` объекта DataFrame.\n",
        "\n",
        "```python\n",
        "print(df.shape)\n",
        "```\n",
        "\n",
        "Результатом будет кортеж `(количество_строк, количество_столбцов)`.\n",
        "\n",
        "Пример:\n",
        "```\n",
        "(1000, 10)\n",
        "```\n",
        "означает, что датасет содержит 1000 строк и 10 столбцов.\n",
        "\n",
        "### b) Информация о типах данных\n",
        "\n",
        "Метод `.info()` выводит информацию о структуре DataFrame, включая:\n",
        "\n",
        "- Имена колонок;\n",
        "- Количество непропущенных значений;\n",
        "- Типы данных (`int64`, `float64`, `object` и другие).\n",
        "\n",
        "```python\n",
        "print(df.info())\n",
        "```\n",
        "\n",
        "Особое внимание следует обратить на столбцы типа `object`, которые могут быть категориальными признаками или ошибками парсинга числовых значений.\n",
        "\n",
        "### c) Статистическое описание числовых признаков\n",
        "\n",
        "Функция `.describe()` предоставляет статистическую сводку по числовым столбцам:\n",
        "\n",
        "```python\n",
        "print(df.describe())\n",
        "```\n",
        "\n",
        "Выводимые метрики:\n",
        "- Count — количество заполненных значений;\n",
        "- Mean — среднее значение;\n",
        "- Std — стандартное отклонение;\n",
        "- Min/Max — минимальное и максимальное значения;\n",
        "- Перцентили: 25%, 50% (медиана), 75%.\n",
        "\n",
        "Для получения аналогичной статистики по категориальным признакам используется вызов метода с параметром `include=['O']`:\n",
        "\n",
        "```python\n",
        "print(df.describe(include=['O']))\n",
        "```\n",
        "\n",
        "\n",
        "## 3. Первичный осмотр первых N строк\n",
        "\n",
        "Для визуального анализа структуры и содержания данных рекомендуется просмотреть первые несколько строк.\n",
        "\n",
        "```python\n",
        "print(df.head(5))  # Отображение первых 5 строк\n",
        "```\n",
        "\n",
        "Этот шаг позволяет:\n",
        "\n",
        "- Понять, как представлены данные в каждой колонке;\n",
        "- Обнаружить возможные ошибки ввода, несоответствия форматов;\n",
        "- Получить интуитивное представление о смысле каждого столбца.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Понимание значений признаков и целевой переменной\n",
        "\n",
        "### a) Анализ названий и значений признаков\n",
        "\n",
        "На этом этапе важно интерпретировать смысл каждого столбца. Например:\n",
        "\n",
        "- Колонка `Age` может означать возраст клиента;\n",
        "- `X1`, `Var_2`, `feature_17` — требуют дополнительного контекста или документации.\n",
        "\n",
        "Если доступна информация о словаре данных, её использование существенно ускоряет и углубляет процесс анализа.\n",
        "\n",
        "### b) Наличие целевой переменной\n",
        "\n",
        "Если задача относится к классификации или регрессии, то в датасете должна присутствовать **целевая переменная (label)**.\n",
        "\n",
        "Примеры:\n",
        "\n",
        "- `Churn` (значения: \"Yes\"/\"No\") — задача бинарной классификации;\n",
        "- `Price` — задача линейной регрессии.\n",
        "\n",
        "Целевая переменная обычно выделяется отдельно от остальных признаков:\n",
        "\n",
        "```python\n",
        "y = df['Churn']  # Целевая переменная\n",
        "X = df.drop('Churn', axis=1)  # Матрица признаков\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 5. Выявление категориальных и числовых признаков\n",
        "\n",
        "Классификация признаков по типам необходима для выбора подходящих методов анализа и моделирования.\n",
        "\n",
        "### a) Числовые признаки (Numerical Features)\n",
        "\n",
        "Числовые признаки делятся на два подтипа:\n",
        "\n",
        "- **Дискретные** — принимают только целочисленные значения (например, `Number of Children`);\n",
        "- **Непрерывные** — принимают любые вещественные значения (например, `Height`, `Weight`).\n",
        "\n",
        "Для их выделения в Pandas используется следующий код:\n",
        "\n",
        "```python\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "print(numerical_cols)\n",
        "```\n",
        "\n",
        "### b) Категориальные признаки (Categorical Features)\n",
        "\n",
        "Категориальные признаки представляют собой качественные характеристики и часто имеют тип `object`.\n",
        "\n",
        "```python\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(categorical_cols)\n",
        "```\n",
        "\n",
        "> Важно! Иногда категориальные признаки могут быть закодированы числами (например, `0`, `1`). Такие случаи требуют дополнительной проверки.\n",
        "\n",
        "\n",
        "\n",
        "## Дополнительные рекомендации\n",
        "\n",
        "- **Использование словаря данных** — если он предоставлен, его применение позволяет значительно повысить точность интерпретации признаков.\n",
        "- **Автоматизация анализа** — для автоматического создания отчётов можно использовать библиотеки `pandas-profiling` или `sweetviz`.\n",
        "\n",
        "Пример использования `pandas-profiling`:\n",
        "\n",
        "```bash\n",
        "pip install pandas-profiling\n",
        "```\n",
        "\n",
        "```python\n",
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
        "profile.to_file(\"report.html\")\n",
        "```\n",
        "\n",
        "- **Формулирование вопросов** — задавайте вопросы: «Что мы хотим предсказать?», «Как связаны признаки между собой?», «Какие закономерности можно наблюдать?».\n",
        "\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Этап изучения данных является фундаментальной частью любого аналитического или ML-проекта. Ниже приведена таблица с кратким описанием каждого этапа:\n",
        "\n",
        "| Этап | Описание |\n",
        "|------|----------|\n",
        "| Загрузка данных | Чтение данных из внешнего источника |\n",
        "| Осмотр структуры | Определение размерности, типов данных, наличия пропусков |\n",
        "| Первичный просмотр | Анализ первых строк датасета |\n",
        "| Интерпретация признаков | Определение смысла каждого столбца и наличие целевой переменной |\n",
        "| Разделение признаков | Выделение числовых и категориальных признаков |\n",
        "\n",
        "\n",
        "\n",
        "## Домашнее задание\n",
        "\n",
        "1. Загрузите открытый набор данных (например, [Titanic](https://www.kaggle.com/c/titanic/data), [Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).\n",
        "2. Выполните все этапы, рассмотренные выше: загрузите данные, изучите структуру, просмотрите первые строки, интерпретируйте признаки, разделите их на числовые и категориальные.\n",
        "3. Создайте сводной отчёт с использованием библиотеки `pandas-profiling`.\n"
      ],
      "metadata": {
        "id": "O4z54gcpOIXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 2. Очистка данных (Data Cleaning)  \n",
        "## a. Обработка пропущенных значений\n",
        "\n",
        "Очистка данных — это один из ключевых этапов предварительной обработки, который позволяет улучшить качество набора данных и повысить точность последующего анализа или моделирования. Одним из самых распространённых аспектов очистки является **обработка пропущенных значений** (missing data). Пропуски могут возникать по различным причинам: ошибки ввода, неполные данные, технические сбои и т.д.\n",
        "\n",
        "\n",
        "\n",
        "## 📌 Цели обработки пропущенных значений\n",
        "\n",
        "1. Устранение или корректная замена отсутствующих данных.\n",
        "2. Сохранение максимального объёма информации.\n",
        "3. Предотвращение систематических ошибок (bias), которые могут возникнуть при игнорировании пропусков.\n",
        "4. Подготовка данных к дальнейшему анализу или обучению моделей.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Выявление пропущенных значений\n",
        "\n",
        "Перед тем как обрабатывать пропуски, необходимо их **обнаружить**. В Python библиотека `pandas` предоставляет удобные инструменты для этого.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Показывает True там, где есть пропуск\n",
        "df.isnull()\n",
        "\n",
        "# Количество пропусков по столбцам\n",
        "df.isnull().sum()\n",
        "\n",
        "# Процент пропусков по столбцам\n",
        "(df.isnull().sum() / len(df)) * 100\n",
        "```\n",
        "\n",
        "Также можно использовать визуализацию (например, `missingno`, `seaborn`) для более наглядного представления.\n",
        "\n",
        "\n",
        "## 2. Удаление строк/столбцов с большим количеством пропусков\n",
        "\n",
        "Если пропуски составляют очень большую долю данных (например, >70%), то может быть целесообразно удалить такие строки или столбцы.\n",
        "\n",
        "### a) Удаление строк (axis=0)\n",
        "\n",
        "```python\n",
        "# Удаляем все строки, где есть хотя бы один пропуск\n",
        "df_clean = df.dropna(axis=0)\n",
        "\n",
        "# Удаляем только те строки, где пропущено больше N значений\n",
        "df_clean = df.dropna(thresh=5)\n",
        "```\n",
        "\n",
        "> **Плюсы**: простота, скорость.  \n",
        "> **Минусы**: потеря информации, возможное нарушение структуры данных.\n",
        "\n",
        "### b) Удаление столбцов (axis=1)\n",
        "\n",
        "```python\n",
        "# Удаляем столбцы, где больше 70% пропусков\n",
        "threshold = len(df) * 0.7\n",
        "df_clean = df.dropna(axis=1, thresh=threshold)\n",
        "```\n",
        "\n",
        "> Используется редко, если столбец не важен или восстановим из других.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Заполнение пропущенных значений\n",
        "\n",
        "В отличие от удаления, заполнение позволяет сохранить структуру данных и использовать всю информацию. Существует несколько подходов:\n",
        "\n",
        "### a) Заполнение средним (для числовых признаков)\n",
        "\n",
        "Используется, когда распределение данных примерно нормальное.\n",
        "\n",
        "```python\n",
        "mean_value = df['column_name'].mean()\n",
        "df['column_name'].fillna(mean_value, inplace=True)\n",
        "```\n",
        "\n",
        "> **Плюсы**: простота, устойчивость к выбросам (при использовании медианы).  \n",
        "> **Минусы**: может занижать дисперсию, если много пропусков.\n",
        "\n",
        "### b) Заполнение медианой (для числовых признаков)\n",
        "\n",
        "Рекомендуется при наличии выбросов.\n",
        "\n",
        "```python\n",
        "median_value = df['column_name'].median()\n",
        "df['column_name'].fillna(median_value, inplace=True)\n",
        "```\n",
        "\n",
        "### c) Заполнение модой (для категориальных признаков)\n",
        "\n",
        "Для категориальных переменных наиболее логичным способом заполнения является использование **моды** — наиболее часто встречающегося значения.\n",
        "\n",
        "```python\n",
        "mode_value = df['category_column'].mode()[0]\n",
        "df['category_column'].fillna(mode_value, inplace=True)\n",
        "```\n",
        "\n",
        "> Также можно использовать \"Unknown\" или \"Missing\" как отдельную категорию.\n",
        "\n",
        "#### 🆕 Альтернатива: заполнение специальным значением (например `'Unknown'`)\n",
        "\n",
        "Если мода может вносить смещение или пропуски являются информативными, лучше заполнить их специальным значением:\n",
        "\n",
        "```python\n",
        "df['category_column'].fillna('Unknown', inplace=True)\n",
        "```\n",
        "\n",
        "Это особенно актуально, если пропуски свидетельствуют о скрытой категории.\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Сохраняет информацию о том, что значение было пропущено;\n",
        "- Может быть полезно для модели, если пропуски закономерны.\n",
        "\n",
        "\n",
        "\n",
        "### d) Интерполяция (временные ряды)\n",
        "\n",
        "Для временных рядов можно применять интерполяцию.\n",
        "\n",
        "```python\n",
        "df['column_name'].interpolate(method='linear', inplace=True)\n",
        "```\n",
        "\n",
        "Методы: `linear`, `polynomial`, `spline`, `time`.\n",
        "\n",
        "\n",
        "\n",
        "### e) Использование моделей для предсказания пропущенных значений\n",
        "\n",
        "Более сложный, но потенциально более точный метод — прогнозирование пропущенных значений с помощью регрессии или классификации.\n",
        "\n",
        "#### Пример:\n",
        "Если пропущены значения в числовом столбце `Age`, можно обучить модель на остальных признаках, чтобы предсказать недостающие значения.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Разделение на известные и неизвестные\n",
        "known = df[df['Age'].notnull()]\n",
        "unknown = df[df['Age'].isnull()]\n",
        "\n",
        "X_train = known.drop('Age', axis=1)\n",
        "y_train = known['Age']\n",
        "X_test = unknown.drop('Age', axis=1)\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "predicted = model.predict(X_test)\n",
        "\n",
        "# Заполняем пропуски\n",
        "df.loc[df['Age'].isnull(), 'Age'] = predicted\n",
        "```\n",
        "\n",
        "> **Плюсы**: высокая точность.  \n",
        "> **Минусы**: требует времени, ресурсов, знаний, может привести к переобучению.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Флагирование пропусков (Создание новых признаков)\n",
        "\n",
        "Этот подход заключается в том, что вместо простого заполнения пропусков мы создаём **новый бинарный признак**, указывающий, было ли значение пропущено.\n",
        "\n",
        "### Пример:\n",
        "\n",
        "```python\n",
        "df['Age_missing'] = df['Age'].isnull().astype(int)\n",
        "```\n",
        "\n",
        "Затем можно заполнить пропуски, например, медианой:\n",
        "\n",
        "```python\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "```\n",
        "\n",
        "> **Плюсы**: модель получает дополнительную информацию о том, были ли данные пропущены. Это может быть важно, если пропуски не случайны.  \n",
        "> **Минусы**: увеличение размерности данных.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Рекомендации по выбору метода\n",
        "\n",
        "| Метод | Когда использовать | Преимущества | Недостатки |\n",
        "|------|---------------------|---------------|-------------|\n",
        "| Удаление строк/столбцов | Если пропусков очень много (>70%) или они не критичны | Просто и быстро | Потеря информации |\n",
        "| Заполнение средним/медианой | Для числовых признаков с нормальным распределением или наличием выбросов | Быстро, легко реализовать | Может снижать дисперсию |\n",
        "| Заполнение модой | Для категориальных признаков | Логично и просто | Может создавать смещение |\n",
        "| Заполнение `'Unknown'` | Для категориальных признаков, если пропуск информативен | Сохраняет информацию о пропуске | Требует корректной интерпретации |\n",
        "| Интерполяция | Для временных рядов | Учитывает порядок | Не подходит для неупорядоченных данных |\n",
        "| Модели машинного обучения | При высокой цене за неточность данных | Точнее других методов | Сложно, долго, требует опыта |\n",
        "| Флагирование пропусков | Если пропуски информативны | Добавляет полезную информацию | Увеличивает размерность |\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Итоги\n",
        "\n",
        "Обработка пропущенных значений — это ответственный процесс, требующий понимания данных и задачи. Ниже представлен общий алгоритм действий:\n",
        "\n",
        "1. **Выявить** наличие пропусков.\n",
        "2. **Проанализировать** характер пропусков: случайны они или закономерны.\n",
        "3. **Выбрать** метод обработки в зависимости от типа признака и объёма пропусков.\n",
        "4. **Применить** выбранный метод.\n",
        "5. **Зафиксировать** изменения и, при необходимости, добавить новые признаки (флаги).\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Возьмите любой открытый датасет с пропущенными значениями (например, [Titanic](https://www.kaggle.com/c/titanic/data)).\n",
        "2. Проверьте наличие пропусков.\n",
        "3. Примените следующие методы:\n",
        "   - Удаление строк или столбцов;\n",
        "   - Заполнение средним/медианой и модой;\n",
        "   - Создайте флаговые признаки;\n",
        "   - По желанию: попробуйте заполнить пропуски моделью.\n",
        "4. Сравните, как разные методы влияют на статистики признаков.\n"
      ],
      "metadata": {
        "id": "OBqgotDdOWpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 2. Очистка данных (Data Cleaning)  \n",
        "## b. Удаление дубликатов\n",
        "\n",
        "Дубликаты — это повторяющиеся записи в наборе данных, которые могут искажать результаты анализа или обучения модели. Они могут быть **полными** (повторяются все значения в строке) или **частичными** (повторяются только ключевые поля). Их удаление — важный этап подготовки данных.\n",
        "\n",
        "\n",
        "\n",
        "### 📌 Цели удаления дубликатов\n",
        "\n",
        "1. **Устранение избыточности данных**, которая может привести к смещению статистических оценок.\n",
        "2. **Повышение точности анализа** и качества моделей машинного обучения.\n",
        "3. **Сохранение чистоты и логической целостности данных.**\n",
        "\n",
        "\n",
        "\n",
        "### 1. Полные дубликаты (строки)\n",
        "\n",
        "Это строки, где все значения во всех столбцах совпадают. Такие дубликаты обычно легко обнаружить и удалить.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "# Показывает дубликаты (кроме первой встречаемой)\n",
        "df.duplicated()\n",
        "\n",
        "# Подсчёт количества полных дубликатов\n",
        "df.duplicated().sum()\n",
        "\n",
        "# Удаление дубликатов\n",
        "df_clean = df.drop_duplicates()\n",
        "```\n",
        "\n",
        "> ⚠️ По умолчанию `drop_duplicates()` сохраняет первую встреченную строку и удаляет последующие. Чтобы сохранять последнюю, используйте параметр:  \n",
        "> ```python\n",
        "> df.drop_duplicates(keep='last')\n",
        "> ```\n",
        "\n",
        "\n",
        "\n",
        "### 2. Частичные дубликаты (по ключевым полям)\n",
        "\n",
        "Иногда важно проверить дублирование только по определённым признакам — например, по ID клиента, номеру транзакции, дате события и т.д.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "# Удаление дубликатов только по ключевому столбцу 'customer_id'\n",
        "df_clean = df.drop_duplicates(subset=['customer_id'])\n",
        "\n",
        "# Можно указать несколько полей:\n",
        "df_clean = df.drop_duplicates(subset=['customer_id', 'transaction_date'])\n",
        "```\n",
        "\n",
        "> ✅ Это особенно актуально при работе с таблицами, где уникальность определяется несколькими полями.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Рекомендации\n",
        "\n",
        "| Ситуация | Что делать |\n",
        "|---------|------------|\n",
        "| Полные дубликаты | Удалить с помощью `drop_duplicates()` |\n",
        "| Частичные дубликаты | Указать `subset` с ключевыми столбцами |\n",
        "| Нужно оставить все дубликаты для анализа | Не удалять, а анализировать отдельно |\n",
        "\n",
        "\n",
        "\n",
        "## c. Исправление ошибок\n",
        "\n",
        "Ошибки в данных — ещё один распространённый источник проблем. Они могут быть как техническими (например, некорректный тип), так и смысловыми (логические несоответствия).\n",
        "\n",
        "\n",
        "\n",
        "### 1. Типографические ошибки\n",
        "\n",
        "Часто встречаются в категориальных переменных. Например:\n",
        "\n",
        "- `'Yees'`, `'No'` вместо `'Yes'`;\n",
        "- `'Malee'`, `'M'` вместо `'Male'`.\n",
        "\n",
        "#### Пример исправления:\n",
        "\n",
        "```python\n",
        "# Используем replace для замены значений\n",
        "df['Gender'] = df['Gender'].replace({'Malee': 'Male', 'M': 'Male', 'Femal': 'Female'})\n",
        "\n",
        "# Для множества ошибок можно использовать регулярные выражения\n",
        "import pandas as pd\n",
        "\n",
        "df['Answer'] = df['Answer'].str.replace(r'(?i)y.*s', 'Yes', regex=True)\n",
        "df['Answer'] = df['Answer'].str.replace(r'(?i)n.*', 'No', regex=True)\n",
        "```\n",
        "\n",
        "> 💡 Использование библиотеки `fuzzywuzzy` позволяет автоматически находить и исправлять неточности на основе \"расстояния Левенштейна\".\n",
        "\n",
        "\n",
        "\n",
        "### 2. Логические ошибки\n",
        "\n",
        "Эти ошибки связаны с нарушением здравого смысла или бизнес-правил. Например:\n",
        "\n",
        "- Отрицательный возраст;\n",
        "- Дата рождения в будущем;\n",
        "- Зарплата меньше прожиточного минимума (при явном заведомо неверном значении);\n",
        "- Возраст ребёнка старше родителя и т.д.\n",
        "\n",
        "#### Примеры обработки:\n",
        "\n",
        "##### a) Отрицательный возраст\n",
        "\n",
        "```python\n",
        "# Установим минимальный возраст равным 0\n",
        "df = df[df['Age'] >= 0]\n",
        "```\n",
        "\n",
        "или заполним неверные значения медианой:\n",
        "\n",
        "```python\n",
        "median_age = df['Age'][df['Age'] > 0].median()\n",
        "df.loc[df['Age'] < 0, 'Age'] = median_age\n",
        "```\n",
        "\n",
        "##### b) Дата рождения в будущем\n",
        "\n",
        "```python\n",
        "from datetime import datetime\n",
        "\n",
        "current_year = datetime.now().year\n",
        "df = df[df['Birth_Date'].dt.year <= current_year]\n",
        "```\n",
        "\n",
        "##### c) Использование условий для фильтрации\n",
        "\n",
        "```python\n",
        "# Если зарплата меньше 1000 — возможно, ошибка\n",
        "df = df[df['Salary'] >= 1000]\n",
        "```\n",
        "\n",
        "> ⚠️ Важно понимать контекст: иногда такие значения могут быть допустимыми (например, тестовые данные, дети и т.п.).\n",
        "\n",
        "\n",
        "\n",
        "## d. Приведение к правильным типам данных\n",
        "\n",
        "Правильное определение типа данных — ключевой шаг, влияющий на корректность вычислений, визуализаций и моделирования.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Изменение типа столбцов\n",
        "\n",
        "Иногда при загрузке данных Pandas неправильно определяет типы (например, числа интерпретируются как строки, даты как объекты и т.д.).\n",
        "\n",
        "#### Примеры:\n",
        "\n",
        "##### a) Преобразование в числовой тип\n",
        "\n",
        "```python\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
        "```\n",
        "\n",
        "> `errors='coerce'` преобразует некорректные значения в `NaN`.\n",
        "\n",
        "##### b) Преобразование в дату\n",
        "\n",
        "```python\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "```\n",
        "\n",
        "##### c) Преобразование в категориальный тип\n",
        "\n",
        "```python\n",
        "df['Category'] = df['Category'].astype('category')\n",
        "```\n",
        "\n",
        "> Это уменьшает объём памяти и ускоряет операции.\n",
        "\n",
        "##### d) Явное указание типов при загрузке\n",
        "\n",
        "```python\n",
        "dtypes = {\n",
        "    'ID': 'int64',\n",
        "    'Name': 'object',\n",
        "    'Birth_Date': 'datetime64[ns]',\n",
        "    'Gender': 'category'\n",
        "}\n",
        "\n",
        "df = pd.read_csv('data.csv', dtype=dtypes)\n",
        "```\n",
        "\n",
        "\n",
        "### 2. Автоматическое определение типов\n",
        "\n",
        "Если типы неизвестны, можно воспользоваться функцией `infer_objects()`:\n",
        "\n",
        "```python\n",
        "df = df.infer_objects()\n",
        "```\n",
        "\n",
        "> Эта функция пытается автоматически преобразовать столбцы в более подходящие типы.\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Общий алгоритм очистки данных\n",
        "\n",
        "1. **Поиск и удаление дубликатов**:\n",
        "   - Полные и частичные дубликаты.\n",
        "2. **Исправление ошибок**:\n",
        "   - Типографические и логические ошибки.\n",
        "3. **Приведение к правильным типам данных**:\n",
        "   - Числовые, даты, категориальные значения.\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой открытый датасет (например, [Titanic](https://www.kaggle.com/c/titanic/data), [Adult Income](https://archive.ics.uci.edu/ml/datasets/Adult)).\n",
        "2. Выполните следующее:\n",
        "   - Найдите и удалите полные и частичные дубликаты.\n",
        "   - Проверьте наличие типографических и логических ошибок — исправьте их.\n",
        "   - Приведите все столбцы к правильным типам данных.\n",
        "3. Сохраните очищенный датасет и сравните его с исходным по размеру и структуре.\n",
        "\n"
      ],
      "metadata": {
        "id": "WxJHWS6-SZyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 3. Работа с выбросами (Outliers)\n",
        "\n",
        "## 📌 Введение\n",
        "\n",
        "**Выбросы (outliers)** — это аномальные значения в наборе данных, которые существенно отличаются от остальных наблюдений. Они могут возникать вследствие ошибок измерения, сбора данных или быть результатом редких, но закономерных событий.\n",
        "\n",
        "### Возможные причины появления выбросов:\n",
        "\n",
        "- Технические ошибки при вводе/сборе данных;\n",
        "- Аномалии в системе (например, неисправность оборудования);\n",
        "- Естественные отклонения от среднего (например, высокий доход клиента в выборке со средним уровнем достатка);\n",
        "- Сбои программного обеспечения.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Значение работы с выбросами\n",
        "\n",
        "Учет и корректная обработка выбросов важны по следующим причинам:\n",
        "\n",
        "1. **Влияние на статистические показатели**: выбросы могут сильно исказить такие метрики, как среднее значение, дисперсия, стандартное отклонение.\n",
        "2. **Снижение качества моделей машинного обучения**, особенно тех, которые чувствительны к расстояниям между точками (линейная регрессия, KNN, SVM).\n",
        "3. **Искажение графиков и визуализаций**, что затрудняет восприятие основной тенденции.\n",
        "4. **Потенциальная полезность информации**: в задачах детектирования мошенничества, анализа аномалий, прогнозирования отказов оборудование выбросы могут содержать ключевые данные.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Визуализация распределения\n",
        "\n",
        "Первым этапом в работе с выбросами является их **визуальное обнаружение**. Ниже приведены наиболее распространённые методы визуализации для выявления аномалий.\n",
        "\n",
        "### a) Boxplot (ящик с усами)\n",
        "\n",
        "Boxplot — один из самых популярных способов визуального определения выбросов. Он отображает медиану, квартили, а также точки, выходящие за пределы \"усов\" графика.\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "sns.boxplot(x=df['Age'])\n",
        "```\n",
        "\n",
        "> ✅ Преимущества: наглядно, простая интерпретация.  \n",
        "> ⚠️ Недостатки: может давать ложные сигналы при ненормальном распределении.\n",
        "\n",
        "\n",
        "\n",
        "### b) Scatter plot (диаграмма рассеивания)\n",
        "\n",
        "Диаграмма рассеивания позволяет анализировать выбросы в двумерном пространстве.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(df['Age'], df['Salary'])\n",
        "```\n",
        "\n",
        "> ✅ Полезно для выявления аномалий в зависимости между двумя числовыми переменными.\n",
        "\n",
        "---\n",
        "\n",
        "### c) Гистограммы и KDE (ядерная оценка плотности)\n",
        "\n",
        "Гистограммы и графики плотности позволяют понять форму распределения и заметить хвосты, указывающие на наличие выбросов.\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "sns.histplot(df['Age'], kde=True)\n",
        "```\n",
        "\n",
        "> ✅ Хорошо подходит для анализа формы распределения и оценки его симметрии.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Статистические методы обнаружения выбросов\n",
        "\n",
        "Кроме визуального анализа, можно использовать **количественные методы**, позволяющие формально определить, какие значения следует считать выбросами.\n",
        "\n",
        "### a) Z-score (стандартизированное отклонение)\n",
        "\n",
        "Z-score используется для нормально распределённых данных и показывает, насколько стандартных отклонений значение отклоняется от среднего.\n",
        "\n",
        "#### Формула:\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "где:\n",
        "- $ x $ — текущее значение,\n",
        "- $ \\mu $ — среднее,\n",
        "- $ \\sigma $ — стандартное отклонение.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "z_scores = np.abs(stats.zscore(df['Age']))\n",
        "threshold = 3\n",
        "df_clean = df[z_scores < threshold]\n",
        "```\n",
        "\n",
        "> ✅ Подходит для нормального распределения.  \n",
        "> ❌ Может быть неприменим при наличии сильной асимметрии.\n",
        "\n",
        "\n",
        "\n",
        "### b) IQR — Межквартильный размах\n",
        "\n",
        "IQR (Interquartile Range) — универсальный метод, который применяется даже при ненормальном распределении данных.\n",
        "\n",
        "#### Формула:\n",
        "$$\n",
        "IQR = Q3 - Q1\n",
        "$$\n",
        "Значения считаются выбросами, если они:\n",
        "- меньше $ Q1 - 1.5 \\cdot IQR $\n",
        "- больше $ Q3 + 1.5 \\cdot IQR $\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "Q1 = df['Age'].quantile(0.25)\n",
        "Q3 = df['Age'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df_clean = df[(df['Age'] >= lower_bound) & (df['Age'] <= upper_bound)]\n",
        "```\n",
        "\n",
        "> ✅ Универсальный подход, работает с любым типом распределения.  \n",
        "> ⚠️ Чувствителен к размеру выборки.\n",
        "\n",
        "\n",
        "\n",
        "## 📘 Что такое скошенное (асимметричное) распределение?\n",
        "\n",
        "**Скошенное распределение (skewed distribution)** — это несимметричное распределение, в котором хвост значений смещён либо влево, либо вправо относительно центральной части.\n",
        "\n",
        "### Два типа асимметрии:\n",
        "\n",
        "- **Правосторонняя (положительная) асимметрия** — длинный правый хвост. Среднее > медианы.\n",
        "- **Левосторонняя (отрицательная) асимметрия** — длинный левый хвост. Среднее < медианы.\n",
        "\n",
        "#### Причины скошенности:\n",
        "- Наличие очень больших значений (например, зарплаты, стоимость недвижимости).\n",
        "- Ограничения на минимальные/максимальные значения.\n",
        "\n",
        "#### Пример скошенного распределения:\n",
        "Рассмотрим столбец `Income` — большинство людей имеют средний доход, но есть несколько с очень высоким доходом, что создаёт длинный правый хвост.\n",
        "\n",
        "```python\n",
        "sns.histplot(df['Income'], kde=True)\n",
        "```\n",
        "\n",
        "#### Как исправить?\n",
        "Для работы с скошенным распределением часто используются **преобразования**, например:\n",
        "- Логарифмирование: `np.log(x)`\n",
        "- Квадратный корень: `np.sqrt(x)`\n",
        "- Box-Cox преобразование (только для положительных значений)\n",
        "\n",
        "\n",
        "\n",
        "## 3. Решения по работе с выбросами\n",
        "\n",
        "После обнаружения выбросов необходимо принять решение, как с ними поступить. Ниже представлены основные стратегии.\n",
        "\n",
        "\n",
        "\n",
        "### a) Удаление записей\n",
        "\n",
        "Если выбросы являются явной ошибкой или их количество невелико, можно удалить соответствующие строки.\n",
        "\n",
        "```python\n",
        "df_clean = df[(df['Age'] >= lower_bound) & (df['Age'] <= upper_bound)]\n",
        "```\n",
        "\n",
        "> ✅ Простой и быстрый способ.  \n",
        "> ❌ Может привести к потере важной информации.\n",
        "\n",
        "\n",
        "\n",
        "### b) Каппинг (ограничение значений)\n",
        "\n",
        "Можно ограничить значения до определённого порога вместо удаления.\n",
        "\n",
        "```python\n",
        "df['Age'] = np.where(df['Age'] > upper_bound, upper_bound, df['Age'])\n",
        "df['Age'] = np.where(df['Age'] < lower_bound, lower_bound, df['Age'])\n",
        "```\n",
        "\n",
        "> ✅ Сохраняет объём выборки.  \n",
        "> ❌ Может исказить реальное распределение.\n",
        "\n",
        "\n",
        "\n",
        "### c) Трансформация (логарифмирование, степенные преобразования)\n",
        "\n",
        "Для уменьшения влияния выбросов и устранения скошенности применяются различные математические преобразования.\n",
        "\n",
        "#### Логарифмическое преобразование:\n",
        "\n",
        "```python\n",
        "df['Log_Age'] = np.log(df['Age'] + 1)  # +1 чтобы избежать log(0)\n",
        "```\n",
        "\n",
        "#### Box-Cox преобразование:\n",
        "\n",
        "```python\n",
        "from scipy.stats import boxcox\n",
        "df['Transformed_Age'], lambda_val = boxcox(df['Age'] + 1)\n",
        "```\n",
        "\n",
        "> ✅ Полезно для моделей, требующих нормального распределения.  \n",
        "> ❌ Требует интерпретации преобразованных данных.\n",
        "\n",
        "\n",
        "\n",
        "### d) Игнорирование, если модель нечувствительна к выбросам\n",
        "\n",
        "Некоторые модели машинного обучения менее чувствительны к выбросам:\n",
        "\n",
        "- **Деревья решений**\n",
        "- **Случайный лес**\n",
        "- **Градиентный бустинг (XGBoost, LightGBM, CatBoost)**\n",
        "\n",
        "> ✅ Не теряем данные, экономим время.  \n",
        "> ❌ Выбросы всё ещё могут влиять на обучение, особенно в ансамблях.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Как выбрать метод работы с выбросами?\n",
        "\n",
        "| Метод | Когда использовать | Преимущества | Недостатки |\n",
        "|------|---------------------|---------------|-------------|\n",
        "| Удаление | Если выбросы — ошибки и их мало | Простота | Потеря информации |\n",
        "| Каппинг | Если выбросы возможны, но не критичны | Сохраняет размерность | Искажает распределение |\n",
        "| Логарифмирование | При скошенных данных и необходимости нормализации | Уменьшает влияние больших значений | Требует интерпретации |\n",
        "| Игнорирование | Если модель нечувствительна к выбросам | Экономия времени | Риск недообучения или переобучения |\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Общий алгоритм работы с выбросами\n",
        "\n",
        "1. **Визуализируйте** данные (boxplot, гистограмма, scatter plot).\n",
        "2. **Обнаружьте** выбросы с помощью статистических методов (IQR, Z-score).\n",
        "3. **Проанализируйте** контекст: ошибка или закономерность?\n",
        "4. **Примените** подходящий метод:\n",
        "   - Удаление,\n",
        "   - Каппинг,\n",
        "   - Трансформация,\n",
        "   - Игнорирование.\n",
        "5. **Оцените** влияние изменений на модель или анализ.\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой числовой датасет (например, [Boston House Prices](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)).\n",
        "2. Постройте визуализации (boxplot, гистограмма, scatter plot) для нескольких числовых столбцов.\n",
        "3. Обнаружьте выбросы с помощью IQR и Z-score.\n",
        "4. Примените разные методы:\n",
        "   - Удаление;\n",
        "   - Каппинг;\n",
        "   - Логарифмирование.\n",
        "5. Сравните, как эти методы повлияли на распределение и статистику признаков.\n"
      ],
      "metadata": {
        "id": "4ynBvxwMS-cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 4. Кодирование категориальных признаков (Categorical Encoding)\n",
        "\n",
        "## 📌 Введение\n",
        "\n",
        "Категориальные признаки — это переменные, которые принимают значения из ограниченного набора возможных вариантов (например: `['Male', 'Female']`, `['Red', 'Green', 'Blue']`).\n",
        "\n",
        "Большинство алгоритмов машинного обучения не могут напрямую обрабатывать текстовые или категориальные данные, поэтому их необходимо **закодировать** в числовом виде.\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Типы категориальных признаков\n",
        "\n",
        "| Тип | Описание | Пример |\n",
        "|-----|----------|--------|\n",
        "| **Номинальные (Nominal)** | Нет естественного порядка | Пол (`Male`, `Female`), Цвет (`Red`, `Blue`) |\n",
        "| **Ординальные (Ordinal)** | Есть упорядоченность | Уровень удовлетворённости (`Low`, `Medium`, `High`), Образование (`High School`, `Bachelor`, `Master`, `PhD`) |\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Задача кодирования\n",
        "\n",
        "Цель кодирования:\n",
        "- Перевести категориальные значения в числа.\n",
        "- Сохранить информацию о различиях и взаимосвязях между категориями.\n",
        "- Избежать ложной интерпретации порядка (например, чтобы модель не думала, что `1 < 2 < 3` для номинальных данных).\n",
        "\n",
        "\n",
        "\n",
        "## 🔢 Подробный разбор методов кодирования\n",
        "\n",
        "### 1. **One-Hot Encoding**\n",
        "\n",
        "Преобразует каждую категорию в отдельный бинарный столбец (0 или 1), указывающий наличие этой категории.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "| Original | A | B | C |\n",
        "|----------|---|---|---|\n",
        "| A        | 1 | 0 | 0 |\n",
        "| B        | 0 | 1 | 0 |\n",
        "| C        | 0 | 0 | 1 |\n",
        "\n",
        "```python\n",
        "pd.get_dummies(df, columns=['Category'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Не предполагает порядок;\n",
        "- Просто реализуется.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Создаёт много новых признаков (curse of dimensionality);\n",
        "- Не подходит для категорий с высокой кардинальностью.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Label Encoding**\n",
        "\n",
        "Присваивает каждой уникальной категории целое число от 0 до n_classes - 1.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "| Original | Encoded |\n",
        "|----------|---------|\n",
        "| Red      | 0       |\n",
        "| Green    | 1       |\n",
        "| Blue     | 2       |\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Encoded'] = le.fit_transform(df['Color'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Простота реализации;\n",
        "- Экономия памяти.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Модель может ошибочно интерпретировать числа как порядковые значения;\n",
        "- Не рекомендуется использовать для номинальных признаков.\n",
        "\n",
        "\n",
        "### 3. **Ordinal Encoding**\n",
        "\n",
        "Аналогичен Label Encoding, но позволяет явно задать порядок категорий.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "| Original | Encoded |\n",
        "|----------|---------|\n",
        "| Low      | 0       |\n",
        "| Medium   | 1       |\n",
        "| High     | 2       |\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "df['Encoded'] = encoder.fit_transform(df[['Rating']])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Учитывает порядок;\n",
        "- Подходит для ординальных данных.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Не подходит для номинальных категорий.\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Target Encoding / Mean Encoding**\n",
        "\n",
        "Заменяет категорию на среднее значение целевой переменной для этой категории.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "Для задачи классификации с целевой переменной `Churn` (0/1):\n",
        "\n",
        "| City       | Churn_mean |\n",
        "|------------|------------|\n",
        "| Moscow     | 0.35       |\n",
        "| Saint-Petersburg | 0.28 |\n",
        "| Kazan      | 0.42       |\n",
        "\n",
        "```python\n",
        "from category_encoders import TargetEncoder\n",
        "te = TargetEncoder()\n",
        "df['City_encoded'] = te.fit_transform(df['City'], df['Churn'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Работает с высокой кардинальностью;\n",
        "- Использует информацию о целевой переменной.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Может вызвать переобучение (особенно на малых выборках);\n",
        "- Требует защиты от data leakage (например, кросс-валидационное кодирование).\n",
        "\n",
        "\n",
        "\n",
        "### 5. **Frequency Encoding**\n",
        "\n",
        "Каждая категория заменяется частотой её появления в датасете.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "| City       | Frequency |\n",
        "|------------|-----------|\n",
        "| Moscow     | 0.4       |\n",
        "| Saint-Petersburg | 0.3 |\n",
        "| Kazan      | 0.3       |\n",
        "\n",
        "```python\n",
        "freq = df['City'].value_counts(normalize=True)\n",
        "df['City_freq'] = df['City'].map(freq)\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Простота;\n",
        "- Хорошо работает, когда частота связана с целевой переменной.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Может быть шумным;\n",
        "- Не учитывает связь с целевой переменной напрямую.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Binary Encoding**\n",
        "\n",
        "Сначала категории кодируются числами (как в Label Encoding), затем эти числа переводятся в двоичную систему и разбиваются на биты.\n",
        "\n",
        "```python\n",
        "from category_encoders import BinaryEncoder\n",
        "be = BinaryEncoder()\n",
        "df_binary = be.fit_transform(df['City'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Меньше колонок, чем One-Hot;\n",
        "- Сохраняет часть информации.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Сложно интерпретируемый;\n",
        "- Менее точный, чем Target Encoding.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Hashing Encoding (Feature Hashing)**\n",
        "\n",
        "Использует хэш-функцию для преобразования категорий в фиксированное количество чисел (обычно меньше исходного числа категорий).\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "hasher = FeatureHasher(n_features=4, input_type='string')\n",
        "hashed_features = hasher.transform(df['City'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Подходит для больших категорий;\n",
        "- Фиксированная размерность.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Возможны коллизии (разные категории получают один и тот же хэш);\n",
        "- Потеря интерпретируемости.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Leave-One-Out Encoding**\n",
        "\n",
        "Похож на Target Encoding, но при кодировании конкретной строки не учитывается сама эта строка (минимизация утечки данных).\n",
        "\n",
        "```python\n",
        "from category_encoders import LeaveOneOutEncoder\n",
        "looe = LeaveOneOutEncoder()\n",
        "df_loo = looe.fit_transform(df['City'], df['Churn'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Защищено от data leakage;\n",
        "- Высокая эффективность.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Требует больше вычислений.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **James-Stein Encoder**\n",
        "\n",
        "Основан на статистическом подходе Штейна — сглаживает средние значения целевой переменной по группам, \"схлопывая\" их к общему среднему.\n",
        "\n",
        "```python\n",
        "from category_encoders import JamesSteinEncoder\n",
        "jse = JamesSteinEncoder()\n",
        "df_jse = jse.fit_transform(df['City'], df['Churn'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Снижает влияние шума в редких категориях;\n",
        "- Хорошо работает с малыми выборками.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Сложнее в интерпретации.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **M-Estimator Encoding**\n",
        "\n",
        "Упрощённая версия Target Encoding, где категория кодируется средним значением целевой переменной, скорректированным с учетом априорного распределения.\n",
        "\n",
        "```python\n",
        "from category_encoders import MEstimateEncoder\n",
        "mee = MEstimateEncoder(m=10)  # m — вес априорного среднего\n",
        "df_mee = mee.fit_transform(df['City'], df['Churn'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Баланс между общей средней и средней по категории;\n",
        "- Устойчив к выбросам.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Требует настройки параметра `m`.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. **Helmert Encoding**\n",
        "\n",
        "Сравнивает уровни фактора с **средним всех предыдущих уровней** (Reverse Helmert — наоборот).\n",
        "\n",
        "```python\n",
        "from category_encoders import HelmertEncoder\n",
        "he = HelmertEncoder()\n",
        "df_he = he.fit_transform(df['Education'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Используется в ANOVA и регрессионном анализе.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Требует понимания порядка категорий;\n",
        "- Редко используется в ML.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. **Polynomial Encoding**\n",
        "\n",
        "Используется, если категория имеет **количественную природу** и можно привязать её к полиномиальным функциям (линейный, квадратичный тренд и т.д.).\n",
        "\n",
        "```python\n",
        "from category_encoders import PolynomialEncoder\n",
        "pe = PolynomialEncoder()\n",
        "df_pe = pe.fit_transform(df['Education'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Учитывает числовую природу категорий.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Редко применяется в практике ML.\n",
        "\n",
        "---\n",
        "\n",
        "### 13. **Sum (Deviation) Encoding**\n",
        "\n",
        "Каждый уровень кодируется отклонением от общего среднего значения целевой переменной.\n",
        "\n",
        "```python\n",
        "from category_encoders import SumEncoder\n",
        "se = SumEncoder()\n",
        "df_se = se.fit_transform(df['City'], df['Churn'])\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Подходит для анализа отклонений.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Сложно интерпретируемый;\n",
        "- Редко используется.\n",
        "\n",
        "---\n",
        "\n",
        "### 14. **CatBoost Encoding**\n",
        "\n",
        "Реализация Target Encoding с защитой от утечки данных. Используется внутри самого CatBoost.\n",
        "\n",
        "```python\n",
        "from catboost import CatBoostClassifier\n",
        "model = CatBoostClassifier()\n",
        "model.fit(X_train, y_train, cat_features=[0, 1, 2])  # Индексы категориальных признаков\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Встроенная защита от утечки;\n",
        "- Высокая производительность.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Работает только с CatBoost.\n",
        "\n",
        "---\n",
        "\n",
        "### 15. **Entity Embeddings (для нейронных сетей)**\n",
        "\n",
        "Используется в глубоком обучении: каждая категория кодируется вектором непрерывных чисел (эмбеддингом), который обучается вместе с моделью.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(1,))\n",
        "embedding = Embedding(input_dim=len(unique_categories), output_dim=embedding_dim)(inputs)\n",
        "flattened = Flatten()(embedding)\n",
        "output = Dense(1)(flattened)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "```\n",
        "\n",
        "> ✅ Плюсы:\n",
        "- Ловит сложные зависимости;\n",
        "- Подходит для больших категорий.\n",
        "\n",
        "> ❌ Минусы:\n",
        "- Требует больших данных;\n",
        "- Сложно интерпретируемый;\n",
        "- Работает только в нейросетях.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Сводная таблица методов\n",
        "\n",
        "| Метод | Для каких данных | Порядок | Учет целевой | Преимущества | Недостатки |\n",
        "|-------|------------------|---------|--------------|---------------|-------------|\n",
        "| One-Hot | Номинальные | ❌ | ❌ | Простота | Большая размерность |\n",
        "| Label | Номинальные | ❌ | ❌ | Простота | Ложный порядок |\n",
        "| Ordinal | Ординальные | ✅ | ❌ | Учитывает порядок | Только для ординальных |\n",
        "| Target | Любые | ❌ | ✅ | Высокая мощность | Risk overfitting |\n",
        "| Frequency | Любые | ❌ | ❌ | Простота | Слабая связь с целевой |\n",
        "| Binary | Любые | ❌ | ❌ | Меньше колонок | Менее точный |\n",
        "| Hashing | Любые | ❌ | ❌ | Фиксированная размерность | Коллизии |\n",
        "| Leave-One-Out | Любые | ❌ | ✅ | Без утечки | Вычислительно затратно |\n",
        "| James-Stein | Любые | ❌ | ✅ | Сглаживание | Сложность |\n",
        "| M-Estimator | Любые | ❌ | ✅ | Баланс | Настройка параметра |\n",
        "| Helmert | Ординальные | ✅ | ❌ | Для ANOVA | Редко используется |\n",
        "| Polynomial | Ординальные | ✅ | ❌ | Учитывает численность | Редко используется |\n",
        "| Sum | Любые | ❌ | ✅ | Отклонения | Сложно интерпретировать |\n",
        "| CatBoost | Любые | ❌ | ✅ | Встроенный | Только в CatBoost |\n",
        "| Entity Embedding | Любые | ❌ | ✅ | Глубокие связи | Только в DL |\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Рекомендации по выбору метода\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Сводная таблица с формулами и обозначениями\n",
        "\n",
        "| Метод | Формула | Обозначения |\n",
        "|-------|---------|-------------|\n",
        "| **One-Hot Encoding** | $ x_i^{(k)} = \\begin{cases} 1, & x_i = c_k \\\\ 0, & \\text{иначе} \\end{cases} $ | $ x_i $ — значение признака у i-го объекта; $ c_k $ — k-я категория; $ x_i^{(k)} $ — новое бинарное значение |\n",
        "| **Label Encoding** | $ x_{\\text{encoded}} = \\text{rank}(x_i) $ | Каждой категории присваивается уникальный номер (целое число) |\n",
        "| **Ordinal Encoding** | $ x_{\\text{encoded}} = f(c_1) < f(c_2) < \\dots < f(c_K) $ | $ f(\\cdot) $ — заданное числовое отображение для упорядоченных категорий |\n",
        "| **Target / Mean Encoding** | $ x_{\\text{encoded}}(c_k) = \\frac{\\sum y_i \\cdot \\mathbb{I}(x_i = c_k)}{\\sum \\mathbb{I}(x_i = c_k)} $ | $ y_i $ — целевая переменная; $ \\mathbb{I}(\\cdot) $ — индикатор равенства категории |\n",
        "| **Frequency Encoding** | $ x_{\\text{encoded}}(c_k) = \\frac{\\#\\{x_i = c_k\\}}{n} $ | $ n $ — общее количество наблюдений |\n",
        "| **Binary Encoding** | $ x_{\\text{binary}} = \\text{binarize}(\\text{label\\_encode}(x_i)) $ | Сначала Label Encoding, затем перевод в бинарную форму и разделение на биты |\n",
        "| **Hashing Encoding** | $ x_{\\text{hashed}} = h(x_i) \\mod N $ | $ h(\\cdot) $ — хэш-функция; $ N $ — размер выходного пространства |\n",
        "| **Leave-One-Out Encoding** | $ x_{\\text{loo}}(c_k) = \\frac{\\sum_{j \\neq i} y_j \\cdot \\mathbb{I}(x_j = c_k)}{\\sum_{j \\neq i} \\mathbb{I}(x_j = c_k)} $ | Для каждого объекта среднее считается без него |\n",
        "| **James-Stein Encoder** | $ x_{\\text{JS}}(c_k) = w_k \\cdot \\mu_k + (1 - w_k) \\cdot \\mu_{\\text{global}} $ | $ \\mu_k $ — среднее таргета по категории; $ \\mu_{\\text{global}} $ — общее среднее; $ w_k = \\frac{n_k}{n_k + m} $ |\n",
        "| **M-Estimator Encoding** | $ x_{\\text{ME}}(c_k) = \\frac{n_k \\cdot \\mu_k + m \\cdot \\mu_{\\text{global}}}{n_k + m} $ | $ n_k $ — кол-во объектов в категории; $ m $ — параметр регуляризации |\n",
        "| **Helmert Encoding** | $ x_{\\text{helmert}}(c_k) = \\bar{y}_k - \\frac{1}{k - 1} \\sum_{j=1}^{k - 1} \\bar{y}_j $ | Сравнение текущей категории со средним предыдущих |\n",
        "| **Polynomial Encoding** | $ x_{\\text{poly}} = [P_1(z_k), ..., P_d(z_k)] $ | $ z_k $ — числовое представление категории (например, ранг); $ P_d $ — ортогональные полиномы |\n",
        "| **Sum Encoding (Deviation)** | $ x_{\\text{sum}}(c_k) = \\mu_k - \\mu_{\\text{global}} $ | Отклонение среднего категории от общего среднего значения |\n",
        "| **CatBoost Encoding** | $ x_{\\text{catboost}}(c_k) = \\frac{\\sum_{i \\in S_{<t}} y_i}{|S_{<t}|} $ | $ S_{<t} $ — данные, наблюдавшиеся до текущего шага обучения (для избежания утечки) |\n",
        "| **Entity Embedding** | $ x_{\\text{embedding}}(c_k) = v_k \\in \\mathbb{R}^d $ | $ v_k $ — обучаемый d-мерный вектор для категории $ c_k $ |\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Рекомендации по выбору метода\n",
        "\n",
        "| Задача / Контекст | Подходящие методы |\n",
        "|-------------------|------------------|\n",
        "| Научная работа / статистика | M-Estimator, James-Stein, Target Encoding |\n",
        "| Интерпретируемость важна | One-Hot, Ordinal, Frequency Encoding |\n",
        "| Глубокое обучение / нейросети | Entity Embeddings |\n",
        "| Большие данные / много категорий | Target Encoding, Hashing Encoding, CatBoost Encoding |\n",
        "| Деревья / ансамбли | Label/Ordinal Encoding, Target Encoding, CatBoost Encoding |\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой открытый датасет с категориальными признаками (например, [Titanic](https://www.kaggle.com/c/titanic/data)).\n",
        "2. Примените следующие методы кодирования:\n",
        "   - One-Hot Encoding\n",
        "   - Label Encoding\n",
        "   - Target Encoding\n",
        "   - CatBoost Encoding\n",
        "   - Binary Encoding\n",
        "3. Обучите простую модель (например, LogisticRegression или RandomForest).\n",
        "4. Сравните качество моделей после применения разных кодирований.\n"
      ],
      "metadata": {
        "id": "l6fZAJ-HVfwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 5. Масштабирование признаков (Feature Scaling)\n",
        "\n",
        "## 📌 Введение\n",
        "\n",
        "**Масштабирование признаков (Feature Scaling)** — это процесс приведения числовых значений различных признаков к одному масштабу. Это важно, потому что **многие алгоритмы машинного обучения чувствительны к разнице в диапазонах между признаками**, особенно те, которые используют **расстояние или градиенты**.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Почему важно масштабировать признаки?\n",
        "\n",
        "1. **Ускоряет обучение**: особенно для методов на основе градиентного спуска.\n",
        "2. **Повышает точность модели**: предотвращает доминирование признака с большим масштабом.\n",
        "3. **Улучшает интерпретируемость**: все признаки находятся в одинаковых условиях.\n",
        "4. **Необходимо для некоторых алгоритмов**, например:\n",
        "   - KNN\n",
        "   - SVM\n",
        "   - Логистическая регрессия с регуляризацией\n",
        "   - Градиентный бустинг (частично)\n",
        "   - PCA\n",
        "   - Нейронные сети\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Основные методы масштабирования\n",
        "\n",
        "| Метод | Чувствителен к выбросам | Диапазон | Формула |\n",
        "|-------|--------------------------|----------|---------|\n",
        "| Standardization (Z-score) | ✅ Да | $ (-\\infty, +\\infty) $ | $ z = \\frac{x - \\mu}{\\sigma} $ |\n",
        "| Min-Max Normalization | ✅ Да | $ [0, 1] $ | $ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} $ |\n",
        "| Robust Scaler | ❌ Нет | $ [-1, 1] $ (по умолчанию) | $ x' = \\frac{x - Q_2}{Q_3 - Q_1} $ |\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Подробное описание методов\n",
        "\n",
        "\n",
        "\n",
        "### 1. **Standardization (Z-score scaling)**\n",
        "\n",
        "Преобразует данные так, чтобы они имели **нулевое среднее и единичное стандартное отклонение**.\n",
        "\n",
        "### Формула:\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ x $ — исходное значение;\n",
        "- $ \\mu $ — среднее значение по признаку:  \n",
        "  $$ \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n",
        "- $ \\sigma $ — стандартное отклонение:  \n",
        "  $$ \\sigma = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\mu)^2} $$\n",
        "\n",
        "> ✅ Результат: среднее = 0, дисперсия = 1.\n",
        "\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "> ✅ Преимущества:\n",
        "- Хорошо работает, если данные нормально распределены;\n",
        "- Подходит для моделей, использующих нормальное распределение (например, линейная регрессия).\n",
        "\n",
        "> ❌ Недостатки:\n",
        "- Чувствителен к выбросам (из-за использования среднего и дисперсии).\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Min-Max Normalization**\n",
        "\n",
        "Приводит значения к диапазону **[0, 1]**.\n",
        "\n",
        "#### Формула:\n",
        "$$\n",
        "x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ x_{min} $ — минимальное значение по признаку;\n",
        "- $ x_{max} $ — максимальное значение по признаку.\n",
        "\n",
        "> ✅ Результат: все значения лежат между 0 и 1.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "> ✅ Преимущества:\n",
        "- Все значения легко интерпретируются;\n",
        "- Полезно, когда известны границы данных (например, измерения температуры от 0 до 100°C).\n",
        "\n",
        "> ❌ Недостатки:\n",
        "- Также чувствителен к выбросам;\n",
        "- Не сохраняет формы распределения.\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Robust Scaling (устойчивый к выбросам)**\n",
        "\n",
        "Использует медиану и межквартильный размах (IQR), поэтому устойчив к выбросам.\n",
        "\n",
        "#### Формула:\n",
        "$$\n",
        "x' = \\frac{x - Q_2}{Q_3 - Q_1}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ Q_2 $ — медиана (50-й перцентиль);\n",
        "- $ Q_3 $ — 75-й перцентиль;\n",
        "- $ Q_1 $ — 25-й перцентиль;\n",
        "- $ IQR = Q_3 - Q_1 $\n",
        "\n",
        "> ✅ Результат: данные центрированы около медианы и нормализованы по IQR.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "> ✅ Преимущества:\n",
        "- Устойчив к выбросам;\n",
        "- Хорошо подходит, если данные имеют тяжёлые хвосты или аномалии.\n",
        "\n",
        "> ❌ Недостатки:\n",
        "- Результат зависит от распределения;\n",
        "- Может быть неудобен, если требуется фиксированный диапазон, например [0, 1].\n",
        "\n",
        "\n",
        "\n",
        "## 🧩 Другие полезные методы\n",
        "\n",
        "Хотя они не всегда относятся к классическому \"масштабированию\", иногда применяются для преобразования признаков:\n",
        "\n",
        "### a) **Max Absolute Scaling**\n",
        "\n",
        "Приводит значения к диапазону **[-1, 1]**, деля каждое значение на максимум по модулю.\n",
        "\n",
        "#### Формула:\n",
        "$$\n",
        "x' = \\frac{x}{|x_{max}|}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ x_{max} $ — максимальное значение по модулю:  \n",
        "  $$ |x_{max}| = \\max(|x_1|, |x_2|, ..., |x_n|) $$\n",
        "\n",
        "> ✅ Результат: сохраняет структуру распределения и знак.\n",
        "\n",
        "Пример\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### b) **Quantile Transformer / Rank Transformation**\n",
        "\n",
        "Преобразует значения в равномерное или нормальное распределение, основываясь на рангах.\n",
        "\n",
        "#### Формула (равномерное распределение):\n",
        "$$\n",
        "x'_i = \\frac{\\text{rank}(x_i)}{n}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ \\text{rank}(x_i) $ — позиция $ x_i $ в отсортированном списке значений;\n",
        "- $ n $ — общее количество наблюдений.\n",
        "\n",
        "Можно также преобразовать в нормальное с помощью обратной функции ошибок:\n",
        "$$\n",
        "x'_i = \\Phi^{-1}\\left(\\frac{\\text{rank}(x_i)}{n + 1}\\right)\n",
        "$$\n",
        "\n",
        "Где $ \\Phi^{-1} $ — обратная функция стандартного нормального распределения.\n",
        "\n",
        "> ✅ Результат: значения следуют заданному закону распределения.\n",
        "\n",
        "Пример\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "scaler = QuantileTransformer(output_distribution='normal')\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "> ✅ Полезно для негладких распределений и нелинейных зависимостей.\n",
        "\n",
        "\n",
        "\n",
        "### c) **Power Transformer (Yeo-Johnson, Box-Cox)**\n",
        "\n",
        "\n",
        "Применяется для приведения распределения к нормальному виду.\n",
        "\n",
        "#### a) **Box-Cox Transformation** (работает только для положительных значений):\n",
        "\n",
        "##### Формула:\n",
        "$$\n",
        "x' =\n",
        "\\begin{cases}\n",
        "\\frac{x^\\lambda - 1}{\\lambda}, & \\text{если } \\lambda \\neq 0 \\\\\n",
        "\\ln(x), & \\text{если } \\lambda = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "#### b) **Yeo-Johnson Transformation** (работает для любых значений, включая отрицательные):\n",
        "\n",
        "##### Формула:\n",
        "\n",
        "$$\n",
        "x' =\n",
        "\\begin{cases}\n",
        "\\frac{(x + 1)^\\lambda - 1}{\\lambda}, & \\text{если } x \\geq 0, \\lambda \\neq 0 \\\\\n",
        "\\ln(x + 1), & \\text{если } x \\geq 0, \\lambda = 0 \\\\\n",
        "\\frac{(-x + 1)^{2 - \\lambda} - 1}{2 - \\lambda}, & \\text{если } x < 0, \\lambda \\neq 2 \\\\\n",
        "-\\ln(-x + 1), & \\text{если } x < 0, \\lambda = 2\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "> Параметр $ \\lambda $ подбирается так, чтобы минимизировать асимметрию (например, через MLE).\n",
        "\n",
        "\n",
        "\n",
        "## ✅ 1. Использование `PowerTransformer`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(42)\n",
        "data = np.random.exponential(scale=2.0, size=(1000, 1))\n",
        "\n",
        "# Создаем трансформер\n",
        "pt = PowerTransformer(method='yeo-johnson')  # или 'box-cox'\n",
        "pt.fit(data)\n",
        "\n",
        "# Применяем преобразование\n",
        "transformed_data = pt.transform(data)\n",
        "\n",
        "print(\"Оптимальный λ:\", pt.lambdas_)\n",
        "```\n",
        "\n",
        "> ⚠️ `method='box-cox'` требует, чтобы данные были **строго положительными**, иначе будет ошибка.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 2. Использование `scipy.stats.boxcox`\n",
        "\n",
        "Если тебе нужно именно **Box-Cox** преобразование:\n",
        "\n",
        "```python\n",
        "from scipy.stats import boxcox\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(42)\n",
        "data = np.random.exponential(scale=2.0, size=1000)\n",
        "\n",
        "# Применяем Box-Cox\n",
        "transformed_data, lambda_ = boxcox(data)\n",
        "\n",
        "print(\"Оптимальный λ:\", lambda_)\n",
        "```\n",
        "\n",
        "> 📌 Этот метод работает только с **положительными числами**.\n",
        "\n",
        "\n",
        "\n",
        "## 🧪 Визуализация до/после\n",
        "\n",
        "Можно добавить график распределения до и после преобразования:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('До преобразования')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(transformed_data, bins=30, color='salmon', edgecolor='black')\n",
        "plt.title('После Yeo-Johnson')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## ⚠️ Ограничения:\n",
        "\n",
        "- Box-Cox работает **только с положительными числами**.\n",
        "- Yeo-Johnson — **работает с любыми числами**, включая отрицательные и ноль.\n",
        "\n",
        "\n",
        "## 📊 Сравнение методов масштабирования\n",
        "\n",
        "| Метод | Диапазон | Устойчивость к выбросам | Когда использовать |\n",
        "|------|----------|--------------------------|---------------------|\n",
        "| Standardization | $ (-\\infty, +\\infty) $ | ❌ Нет | Нормальное распределение, PCA, градиентный спуск |\n",
        "| Min-Max | $ [0, 1] $ | ❌ Нет | Обучение нейросетей, визуализация |\n",
        "| Robust | $ [-1, 1] $ | ✅ Да | Выбросы, IQR |\n",
        "| Max Abs | $ [-1, 1] $ | ❌ Нет | Разреженные данные |\n",
        "| Quantile | Задаётся пользователем | ✅ Да | Нелинейные зависимости |\n",
        "| Power Transformer | Зависит от метода | ✅ Да | Скошенные распределения |\n",
        "\n",
        "\n",
        "\n",
        "## 🤖 Какие алгоритмы требуют масштабирования?\n",
        "\n",
        "| Алгоритм | Требуется масштабирование? | Причина |\n",
        "|----------|-----------------------------|---------|\n",
        "| KNN | ✅ | Использует евклидово расстояние |\n",
        "| SVM | ✅ | На основе расстояний |\n",
        "| Logistic Regression (с L1/L2) | ✅ | Регуляризация зависит от масштаба |\n",
        "| Linear Regression (OLS) | ❌ | Коэффициенты адаптируются автоматически |\n",
        "| Decision Trees | ❌ | Не использует расстояния |\n",
        "| Random Forest | ❌ | На основе деревьев |\n",
        "| Gradient Boosting | ❌ | На основе деревьев |\n",
        "| Neural Networks | ✅ | Градиентный спуск |\n",
        "| PCA | ✅ | Основан на дисперсии |\n",
        "| Clustering (K-means и др.) | ✅ | На основе расстояний |\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Общий алгоритм применения масштабирования\n",
        "\n",
        "1. **Разделите данные на обучающую и тестовую выборки.**\n",
        "2. **Обучите scaler только на обучающей выборке** (`fit_transform`).\n",
        "3. **Примените его к тестовой выборке** (`transform`).\n",
        "4. **Обучите модель на масштабированных данных.**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "> ⚠️ Никогда не делайте `fit_transform` на тестовой выборке — это вызовет **утечку данных**.\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой числовой набор данных (например, `Boston`, `Iris`, `Wine`).\n",
        "2. Постройте графики распределения нескольких признаков.\n",
        "3. Примените разные методы масштабирования: Standardization, Min-Max, Robust Scaling.\n"
      ],
      "metadata": {
        "id": "WvYi6NJeX6MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 7. Работа с датами и временем (Date and Time Feature Engineering)\n",
        "\n",
        "## 📌 Введение\n",
        "\n",
        "В реальных задачах анализа данных и машинного обучения часто встречаются **признаки, связанные с датой и временем**. Это может быть:\n",
        "- Дата рождения клиента;\n",
        "- Дата регистрации;\n",
        "- Время последнего посещения;\n",
        "- Момент совершения транзакции и т.д.\n",
        "\n",
        "Эти данные изначально могут быть представлены в виде строк (`str`) или других форматов, поэтому их необходимо **преобразовать в `datetime`-объект**, а затем — **извлечь полезные признаки**, которые помогут улучшить модель.\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Этапы работы с датами и временем\n",
        "\n",
        "1. **Преобразование в datetime-объект**\n",
        "2. **Извлечение временных признаков**\n",
        "3. **Работа с разницей между датами**\n",
        "4. **Обогащение данными: выходные, праздники и т.д.**\n",
        "\n",
        "\n",
        "\n",
        "## 1. Преобразование в datetime-объект\n",
        "\n",
        "Часто даты хранятся как строки (`str`). Для корректной обработки нужно преобразовать их в объект типа `datetime`.\n",
        "\n",
        "### Пример:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'date_str': ['2025-01-01', '2025-01-02', '2025-01-03']})\n",
        "df['date'] = pd.to_datetime(df['date_str'])\n",
        "```\n",
        "\n",
        "Можно указать формат, если он не определяется автоматически:\n",
        "\n",
        "```python\n",
        "df['date'] = pd.to_datetime(df['date_str'], format='%Y-%m-%d')\n",
        "```\n",
        "\n",
        "> ⚠️ Если дата содержит ошибки, можно использовать параметр `errors='coerce'`, чтобы создать `NaT` (Not a Time) вместо ошибки:\n",
        "```python\n",
        "df['date'] = pd.to_datetime(df['date_str'], errors='coerce')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 2. Извлечение признаков из даты и времени\n",
        "\n",
        "После преобразования в `datetime` можно **извлекать информативные признаки**, такие как год, месяц, день, час и другие.\n",
        "\n",
        "### Основные компоненты даты и времени:\n",
        "\n",
        "| Компонент | Пример кода |\n",
        "|----------|-------------|\n",
        "| Год      | `df['date'].dt.year` |\n",
        "| Месяц    | `df['date'].dt.month` |\n",
        "| День     | `df['date'].dt.day` |\n",
        "| Час      | `df['date'].dt.hour` |\n",
        "| Минута   | `df['date'].dt.minute` |\n",
        "| Секунда  | `df['date'].dt.second` |\n",
        "\n",
        "#### Пример:\n",
        "```python\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "df['hour'] = df['date'].dt.hour\n",
        "df['weekday'] = df['date'].dt.weekday  # 0=Пн, ..., 4=Пт, 5=Сб, 6=Вс\n",
        "```\n",
        "\n",
        "\n",
        "### День недели и выходные\n",
        "\n",
        "День недели может быть важным признаком, особенно для задач прогнозирования спроса, продаж, активности пользователей и т.д.\n",
        "\n",
        "#### Пример: флаг выходного дня\n",
        "\n",
        "```python\n",
        "df['is_weekend'] = df['date'].dt.weekday >= 5  # True - выходной\n",
        "```\n",
        "\n",
        "Если нужны названия дней:\n",
        "\n",
        "```python\n",
        "df['day_name'] = df['date'].dt.strftime('%A')  # Monday, Tuesday...\n",
        "```\n",
        "\n",
        "\n",
        "### Временные интервалы\n",
        "\n",
        "Можно выделить:\n",
        "- Квартал года;\n",
        "- Номер недели;\n",
        "- Утро/день/вечер;\n",
        "- Сезонность (весна, лето...).\n",
        "\n",
        "#### Примеры:\n",
        "\n",
        "```python\n",
        "df['quarter'] = df['date'].dt.quarter\n",
        "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
        "```\n",
        "\n",
        "#### Утро/день/вечер:\n",
        "\n",
        "```python\n",
        "def time_of_day(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "df['time_of_day'] = df['hour'].apply(time_of_day)\n",
        "```\n",
        "\n",
        "\n",
        "## 3. Разница между датами (временные метрики)\n",
        "\n",
        "Одним из самых популярных приёмов является **вычисление временной разницы** между двумя датами.\n",
        "\n",
        "### Пример 1: Возраст клиента\n",
        "\n",
        "```python\n",
        "from datetime import datetime\n",
        "\n",
        "df['birthdate'] = pd.to_datetime(df['birthdate'])\n",
        "current_date = datetime.now()\n",
        "df['age_days'] = (current_date - df['birthdate']).dt.days\n",
        "df['age_years'] = df['age_days'] / 365.25\n",
        "```\n",
        "\n",
        "### Пример 2: Время с момента регистрации до последней активности\n",
        "\n",
        "```python\n",
        "df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
        "df['last_login'] = pd.to_datetime(df['last_login'])\n",
        "\n",
        "df['days_since_registration'] = (df['last_login'] - df['registration_date']).dt.days\n",
        "```\n",
        "\n",
        "> 💡 Разницу можно получать в днях, часах, минутах, секундах и даже в микросекундах.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Обогащение данными: праздники, события и т.п.\n",
        "\n",
        "Для некоторых задач полезно знать, была ли дата:\n",
        "- Праздником?\n",
        "- Выходным?\n",
        "- Днем особого события?\n",
        "\n",
        "Можно использовать библиотеки, такие как `holidays`:\n",
        "\n",
        "```bash\n",
        "pip install holidays\n",
        "```\n",
        "\n",
        "```python\n",
        "import holidays\n",
        "\n",
        "ru_holidays = holidays.RU(years=[2025])\n",
        "df['is_holiday'] = df['date'].isin(ru_holidays)\n",
        "```\n",
        "\n",
        "Теперь столбец `is_holiday` будет содержать `True`, если дата совпадает с государственным праздником.\n",
        "\n",
        "\n",
        "## ✅ Общий алгоритм работы с датами\n",
        "\n",
        "1. **Преобразуйте** исходные значения в `datetime`.\n",
        "2. **Извлеките** нужные компоненты: год, месяц, день, день недели, время суток.\n",
        "3. **Вычислите** разницу между датами (например, возраст).\n",
        "4. **Добавьте** дополнительные фичи: выходной, праздник, квартал и т.д.\n",
        "5. **Удалите** оригинальный столбец с датой, если он больше не нужен.\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Пример конечного набора признаков на основе даты\n",
        "\n",
        "| Исходная дата | year | month | day | weekday | is_weekend | time_of_day | days_since_registration | is_holiday |\n",
        "|---------------|------|-------|-----|----------|------------|--------------|------------------------|-------------|\n",
        "| 2025-01-01    | 2025 | 1     | 1   | 2        | False      | Morning      | 365                    | True        |\n",
        "| 2025-01-05    | 2025 | 1     | 5   | 6        | True       | Evening      | 369                    | False       |\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой датасет, содержащий даты (например, [Air Passenger](https://www.kaggle.com/rakannimer/air-passengers), [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)).\n",
        "2. Преобразуйте даты в `datetime`-объекты.\n",
        "3. Извлеките следующие признаки:\n",
        "   - Год, месяц, день, час\n",
        "   - День недели\n",
        "   - Признак \"выходной\"\n",
        "   - Время суток\n",
        "   - Квартал\n",
        "4. Вычислите разницу между двумя датами (например, сколько прошло времени с первой записи до текущей).\n",
        "5. Добавьте информацию о праздниках (через библиотеку `holidays`).\n",
        "6. Сохраните новый датасет и сравните его с исходным.\n"
      ],
      "metadata": {
        "id": "wblQAe86bGdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 8. Обработка текстовых данных (Text Data Processing)\n",
        "\n",
        "## 📌 Введение\n",
        "\n",
        "Текстовые данные — это один из самых распространённых типов неструктурированной информации: отзывы, сообщения, статьи, твиты, описания товаров и т.д.\n",
        "\n",
        "Для работы с текстами в машинном обучении необходима **предварительная обработка**, которая превращает текст в числовые признаки, понятные модели. Этот процесс называется **текстовой инженерией (text feature engineering)**.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Почему важна предобработка текста?\n",
        "\n",
        "1. **Модели не понимают слова** — им нужны числа.\n",
        "2. **Одинаковые слова могут быть записаны по-разному** (например, `«бегать»` и `«бежал»`).\n",
        "3. **Шум снижает качество моделей** (мусорные слова, пунктуация, стоп-слова).\n",
        "4. **Контекст и семантика важны** — современные методы учитывают значение слов (`Word2Vec`, `BERT`).\n",
        "\n",
        "\n",
        "\n",
        "## 🔢 Этапы обработки текстовых данных\n",
        "\n",
        "| Этап | Цель |\n",
        "|------|------|\n",
        "| Токенизация | Разбить текст на отдельные слова или фразы |\n",
        "| Очистка | Удаление лишних символов, стоп-слов, знаков препинания |\n",
        "| Нормализация | Привести слова к базовой форме (лемматизация / стемминг) |\n",
        "| Векторизация | Перевести слова в числовое представление |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Токенизация\n",
        "\n",
        "Разбиение текста на **токены** — минимальные единицы анализа: слова, символы, n-граммы.\n",
        "\n",
        "### Пример:\n",
        "```\n",
        "\"Я люблю машинное обучение\"\n",
        "→ [\"Я\", \"люблю\", \"машинное\", \"обучение\"]\n",
        "```\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Я люблю машинное обучение!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "# ['Я', 'люблю', 'машинное', 'обучение', '!']\n",
        "```\n",
        "\n",
        "> ✅ Также можно использовать `RegexpTokenizer`, `TweetTokenizer` (для соцсетей), `sent_tokenize` для разбиения на предложения.\n",
        "\n",
        "\n",
        "## 2. Удаление стоп-слов и мусора\n",
        "\n",
        "Стоп-слова — часто встречающиеся, но малоинформативные слова: `«и»`, `«в»`, `«на»`, `«я»`.\n",
        "\n",
        "### Пример:\n",
        "\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))  # или 'english'\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "```\n",
        "\n",
        "Также удаляем:\n",
        "- Знаки препинания\n",
        "- Числа\n",
        "- HTML-теги\n",
        "- Ссылки\n",
        "- Эмодзи и спецсимволы\n",
        "\n",
        "\n",
        "\n",
        "## 3. Нормализация: лемматизация и стемминг\n",
        "\n",
        "Цель — привести слова к их базовой форме, чтобы объединить варианты одного и того же слова.\n",
        "\n",
        "### a) **Стемминг (Stemming)**  \n",
        "Обрезает окончания, оставляя корень. Быстрый, но менее точный.\n",
        "\n",
        "```python\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "stems = [stemmer.stem(word) for word in filtered_tokens]\n",
        "```\n",
        "\n",
        "Пример:  \n",
        "`машинное → машинн, обучение → обучен`\n",
        "\n",
        "\n",
        "\n",
        "### b) **Лемматизация (Lemmatization)**  \n",
        "Использует словарь, чтобы привести слово к начальной форме (лемме). Более точный, чем стемминг.\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ru_core_news_sm\")  # модель для русского языка\n",
        "doc = nlp(\"Машина быстро бежала\")\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "```\n",
        "\n",
        "Пример:  \n",
        "`бежала → бежать`\n",
        "\n",
        "> ✅ Лемматизация рекомендуется для большинства задач NLP, особенно где важна семантика.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Векторизация текста\n",
        "\n",
        "После очистки и нормализации нужно перевести текст в числа — это этап **векторизации**. Ниже основные подходы:\n",
        "\n",
        "---\n",
        "\n",
        "### a) **Bag of Words (BoW)**\n",
        "\n",
        "Создаёт матрицу, где каждая строка — документ, столбцы — уникальные слова, значения — частота встречаемости.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "```\n",
        "\n",
        "> ✅ Простой и быстрый метод.  \n",
        "> ❌ Не учитывает порядок слов и семантику.\n",
        "\n",
        "\n",
        "\n",
        "### b) **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
        "\n",
        "Учитывает, насколько часто слово встречается в документе и насколько оно редко в корпусе.\n",
        "\n",
        "#### Формула:\n",
        "$$\n",
        "\\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\times \\text{IDF}(t,D)\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ \\text{TF}(t,d) = \\frac{\\text{кол-во вхождений } t \\text{ в } d}{\\text{общее кол-во слов в } d} $\n",
        "- $ \\text{IDF}(t,D) = \\log\\left(\\frac{|D|}{|\\{d \\in D : t \\in d\\}|}\\right) $\n",
        "\n",
        "#### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "```\n",
        "\n",
        "> ✅ Улучшает BoW за счёт учёта важности слов.  \n",
        "> ❌ Требует больше памяти, всё ещё не учитывает контекст.\n",
        "\n",
        "\n",
        "\n",
        "### c) **Word Embeddings**\n",
        "\n",
        "Векторизация через **встраивания слов (word embeddings)** — это переход от простых частот к плотным векторам, которые несут в себе **семантическую информацию**.\n",
        "\n",
        "#### Популярные методы:\n",
        "\n",
        "| Метод | Краткое описание | Реализация |\n",
        "|-------|------------------|------------|\n",
        "| **Word2Vec** | Обучается на больших корпусах, кодирует семантику слов | Gensim, spaCy |\n",
        "| **GloVe** | Глобальная матрица ко-окуррентностей | Stanford NLP |\n",
        "| **FastText** | Учитывает подслова, хорошо работает с редкими словами | Facebook AI |\n",
        "| **BERT, RoBERTa, RuBERT** | Контекстуальные эмбеддинги | HuggingFace Transformers |\n",
        "\n",
        "#### Пример с Word2Vec (на английском):\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
        "vector = model.wv['king']  # Получаем вектор для слова \"king\"\n",
        "```\n",
        "\n",
        "> ✅ Учитывает смысл слов.  \n",
        "> ❌ Требует больших вычислений и данных.\n",
        "\n",
        "\n",
        "### d) **Sentence Embeddings**\n",
        "\n",
        "Если нужен вектор для целого предложения или документа, можно использовать:\n",
        "\n",
        "- **Sentence-BERT (SBERT)**\n",
        "- **Universal Sentence Encoder (USE)**\n",
        "- **LaBSE (языковые BERT-эмбеддинги)**\n",
        "- **MPNet, DistilBERT и др.**\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM')\n",
        "embeddings = model.encode([\"Это пример текста\", \"И еще один\"])\n",
        "```\n",
        "\n",
        "> ✅ Подходит для классификации, кластеризации, поиска схожих документов.  \n",
        "> ❌ Высокие требования к памяти и времени.\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Сравнение методов векторизации\n",
        "\n",
        "| Метод | Учитывает порядок слов | Учитывает семантику | Размерность | Когда использовать |\n",
        "|-------|------------------------|----------------------|-------------|--------------------|\n",
        "| Bag of Words | ❌ | ❌ | Высокая | Простые задачи, небольшие выборки |\n",
        "| TF-IDF | ❌ | ❌ | Высокая | То же + лучшая точность |\n",
        "| Word2Vec | ❌ | ✅ | Низкая–средняя | Классификация, поиск синонимов |\n",
        "| FastText | ❌ | ✅ | Низкая–средняя | Работа с редкими словами |\n",
        "| BERT / SBERT | ❌ / ✅ | ✅ | Средняя | Контекстуальные задачи, перевод, вопросы-ответы |\n",
        "\n",
        "\n",
        "\n",
        "## 🧪 Пример полного пайплайна обработки текста\n",
        "\n",
        "```python\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^а-яА-ЯёЁ ]', '', text.lower())\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('russian')]\n",
        "    stemmer = SnowballStemmer('russian')\n",
        "    return ' '.join([stemmer.stem(word) for word in tokens])\n",
        "\n",
        "cleaned_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_texts)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 🤖 Глубокие методы (Deep Learning)\n",
        "\n",
        "Если вы используете **нейронные сети**, то лучше работать с **токенизацией через BPE или WordPiece** и передавать последовательности в модели типа:\n",
        "\n",
        "- **BERT**, **RoBERTa**, **DistilBERT**, **XLM-RoBERTa**\n",
        "- **CNN**, **RNN**, **LSTM**, **GRU**\n",
        "- **Transformer-based модели**\n",
        "\n",
        "### Пример использования Hugging Face Transformers:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "inputs = tokenizer(\"Пример текста\", return_tensors=\"pt\", padding=True, truncation=True)\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "> ✅ Учитывает контекст и семантику.  \n",
        "> ❌ Требует GPU и много памяти.\n",
        "\n",
        "\n",
        "\n",
        "## ✅ Общий алгоритм обработки текстовых данных\n",
        "\n",
        "1. **Токенизация**: разделение на слова или фразы.\n",
        "2. **Очистка**: удаление стоп-слов, пунктуации, HTML-тегов и т.д.\n",
        "3. **Нормализация**: стемминг или лемматизация.\n",
        "4. **Векторизация**:\n",
        "   - BoW / TF-IDF (для классических ML),\n",
        "   - Word2Vec, FastText (для глубокого обучения),\n",
        "   - BERT и др. (для контекстуальных задач).\n",
        "5. **Обучение модели**:\n",
        "   - Logistic Regression, SVM, Random Forest,\n",
        "   - LSTM, GRU, Transformer (в зависимости от задачи).\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Возьмите датасет с текстовыми данными (например, [IMDB Movie Reviews](https://www.kaggle.com/lakshmi25n/tagline-dataset), [RuSentiment](https://github.com/text-machine-lab/rusentiment)).\n",
        "2. Примените полную предобработку:\n",
        "   - Токенизация\n",
        "   - Удаление стоп-слов\n",
        "   - Стемминг / лемматизация\n",
        "3. Примените разные методы векторизации:\n",
        "   - Bag of Words\n",
        "   - TF-IDF\n",
        "   - Word2Vec (или FastText)\n",
        "   - BERT\n",
        "4. Обучите модель классификации (например, LogisticRegression) и сравните метрики.\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Полезные библиотеки\n",
        "\n",
        "| Библиотека | Описание |\n",
        "|------------|----------|\n",
        "| **NLTK** | Базовая обработка текста, токенизация, стоп-слова |\n",
        "| **spaCy** | Продвинутая обработка на Python, лемматизация |\n",
        "| **scikit-learn** | BoW, TF-IDF |\n",
        "| **Gensim** | Word2Vec, FastText, LDA |\n",
        "| **Transformers (HuggingFace)** | BERT, RoBERTa, XLM и другие |\n",
        "| **Sentence Transformers** | Контекстуальные эмбеддинги предложений |\n",
        "| **langdetect**, **fasttext-langdetect** | Определение языка текста |\n",
        "\n"
      ],
      "metadata": {
        "id": "bqFo5Wd9j9aJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 9. Создание новых признаков (Feature Engineering)\n",
        "\n",
        "## 📌 Введение\n",
        "\n",
        "**Создание новых признаков (Feature Engineering)** — это процесс **генерации полезных переменных из существующих данных**, чтобы улучшить качество модели машинного обучения. Это один из самых важных этапов в работе с данными, поскольку даже самая мощная модель не сможет хорошо работать на плохих или плохо подготовленных данных.\n",
        "\n",
        "> 💡 По словам Эндрю Нг:  \n",
        "> *\"Фичи определяют результат, всё остальное — детали.\"*\n",
        "\n",
        "\n",
        "## 🧠 Почему важно создавать новые признаки?\n",
        "\n",
        "1. **Увеличивает информативность данных**.\n",
        "2. **Позволяет выявлять скрытые закономерности**.\n",
        "3. **Повышает точность и обобщающую способность модели**.\n",
        "4. **Упрощает интерпретацию модели**.\n",
        "5. **Может компенсировать недостаток данных**.\n",
        "\n",
        "\n",
        "\n",
        "## 🧪 Основные подходы к созданию новых признаков\n",
        "\n",
        "| Категория | Описание |\n",
        "|----------|----------|\n",
        "| Комбинирование признаков | Сложение, умножение, деление, разность между признаками |\n",
        "| Индикаторы (флаги) | Бинарные признаки, отмечающие наличие/отсутствие условия |\n",
        "| Агрегированные признаки | Среднее, медиана, сумма по группам |\n",
        "| Временные признаки | Год, месяц, день, временные окна |\n",
        "| Доменная экспертиза | Признаки, основанные на знании предметной области |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Комбинирование существующих признаков\n",
        "\n",
        "Часто полезные признаки можно получить путём простых математических операций над уже имеющимися.\n",
        "\n",
        "### Примеры:\n",
        "\n",
        "- `Income / Expenses` → отношение дохода к расходам;\n",
        "- `TotalSpent = ProductA + ProductB + ProductC` → общий объем трат;\n",
        "- `Price per Unit = TotalCost / Quantity` → цена за единицу товара;\n",
        "- `BMI = Weight / Height²` → индекс массы тела.\n",
        "\n",
        "```python\n",
        "df['bmi'] = df['weight'] / (df['height'] ** 2)\n",
        "df['price_per_unit'] = df['total_cost'] / df['quantity']\n",
        "```\n",
        "\n",
        "> ✅ Полезно для моделей, которые не умеют \"видеть\" такие зависимости самостоятельно.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Создание индикаторов (бинарные флаги)\n",
        "\n",
        "Индикаторы — это **бинарные признаки (0/1)**, которые показывают выполнение какого-либо условия.\n",
        "\n",
        "### Примеры:\n",
        "\n",
        "- `IsSenior = Age >= 60`\n",
        "- `IsVIP = TotalPurchases > 10000`\n",
        "- `HasMissingValues = pd.isnull(df['Age'])`\n",
        "- `IsWeekend = df['date'].dt.weekday >= 5`\n",
        "\n",
        "```python\n",
        "df['is_senior'] = (df['age'] >= 60).astype(int)\n",
        "df['is_vip'] = (df['total_purchases'] > 10000).astype(int)\n",
        "```\n",
        "\n",
        "> ✅ Такие признаки могут быть очень информативными, особенно в задачах классификации.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Агрегированные признаки\n",
        "\n",
        "Агрегация — это вычисление статистик по группам. Например, среднее значение по категории, количество событий на пользователя и т.д.\n",
        "\n",
        "### Примеры:\n",
        "\n",
        "- `avg_salary_by_dept` — средняя зарплата по отделу;\n",
        "- `user_activity_count` — количество действий пользователя за период;\n",
        "- `avg_order_value_per_customer` — средний чек клиента.\n",
        "\n",
        "```python\n",
        "# Пример агрегации с помощью groupby\n",
        "df['avg_salary_by_dept'] = df.groupby('department')['salary'].transform('mean')\n",
        "df['total_orders_per_user'] = df.groupby('user_id')['order_id'].transform('count')\n",
        "```\n",
        "\n",
        "> ✅ Особенно эффективны в задачах с категориальными группами (клиенты, продукты, регионы).\n",
        "> ⚠️ Не забывайте использовать `.transform()` вместо `.agg()`, если хотите сохранить размерность DataFrame.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Временные признаки и оконные функции (для временных рядов)\n",
        "\n",
        "Для дат и времени можно строить **временные признаки** и использовать **оконные функции**, чтобы выявить сезонность и тренды.\n",
        "\n",
        "### a) Временные признаки\n",
        "\n",
        "```python\n",
        "df['year'] = df['datetime'].dt.year\n",
        "df['month'] = df['datetime'].dt.month\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['is_weekend'] = (df['datetime'].dt.dayofweek >= 5).astype(int)\n",
        "```\n",
        "\n",
        "### b) Оконные функции\n",
        "\n",
        "Оконные функции позволяют вычислять скользящие средние, накопительные суммы и другие метрики.\n",
        "\n",
        "#### Пример: скользящее среднее по клиенту\n",
        "\n",
        "```python\n",
        "df.sort_values(['customer_id', 'date'], inplace=True)\n",
        "df['rolling_avg_7d'] = df.groupby('customer_id')['amount'].transform(\n",
        "    lambda x: x.rolling(window=7).mean()\n",
        ")\n",
        "```\n",
        "\n",
        "#### Пример: накопительная сумма\n",
        "\n",
        "```python\n",
        "df['cumulative_spent'] = df.groupby('customer_id')['purchase_amount'].cumsum()\n",
        "```\n",
        "\n",
        "> ✅ Эти признаки особенно важны в задачах прогнозирования и анализа поведения пользователей.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Использование доменной экспертизы\n",
        "\n",
        "**Доменная экспертиза (domain knowledge)** — это использование знаний о предметной области для создания более осмысленных признаков.\n",
        "\n",
        "### Примеры:\n",
        "\n",
        "| Задача | Признак | Обоснование |\n",
        "|--------|---------|-------------|\n",
        "| Финансы | `Debt to Income Ratio = Debt / Income` | Показывает платежеспособность |\n",
        "| Маркетинг | `Customer Lifetime Value = AvgOrderValue * PurchaseFrequency` | Оценка ценности клиента |\n",
        "| Ритейл | `Stockout Flag = Stock == 0` | Индикатор того, что товар закончился |\n",
        "| Здравоохранение | `BMI = Weight / Height²` | Индикатор риска заболеваний |\n",
        "| E-commerce | `Days Since Last Purchase` | Мера активности клиента |\n",
        "\n",
        "### Пример:\n",
        "\n",
        "```python\n",
        "# Доля просроченных платежей\n",
        "df['late_payment_ratio'] = df['num_late_payments'] / df['total_payments']\n",
        "\n",
        "# Расстояние до ближайшего магазина\n",
        "df['distance_to_nearest_store_km'] = haversine_distance(lat1, lon1, lat2, lon2)\n",
        "```\n",
        "\n",
        "> ✅ Это самый ценный тип признаков — он может кардинально улучшить качество модели, если вы понимаете бизнес-задачу.\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Типы агрегаций и преобразований\n",
        "\n",
        "| Тип | Пример | Когда использовать |\n",
        "|-----|--------|---------------------|\n",
        "| Арифметические | `x + y`, `x / y` | Когда есть смысл комбинировать признаки |\n",
        "| Логические | `x > y`, `x.isin([...])` | Для создания индикаторов |\n",
        "| Условные | `np.where(x > threshold, 1, 0)` | Для пороговых флагов |\n",
        "| Статистики по группам | `groupby().mean(), .std(), .nunique()` | Для работы с категориальными группами |\n",
        "| Временные | `diff()`, `shift()`, `rolling()` | Для временных рядов |\n",
        "| Сложные формулы | `BMI`, `CLV`, `Churn Score` | На основе доменной экспертизы |\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Примеры Feature Engineering\n",
        "\n",
        "### a) **Клиентская аналитика**\n",
        "\n",
        "Допустим, у нас есть данные о покупках клиентов:\n",
        "\n",
        "| CustomerID | Date       | Amount |\n",
        "|------------|------------|--------|\n",
        "| 1          | 2024-01-01 | 100    |\n",
        "| 1          | 2024-01-15 | 200    |\n",
        "| 2          | 2024-01-05 | 50     |\n",
        "\n",
        "#### Возможные признаки:\n",
        "- `TotalSpentPerCustomer` — общий доход от клиента;\n",
        "- `AvgPurchaseValue` — средний чек;\n",
        "- `DaysSinceLastPurchase` — сколько дней прошло с последней покупки;\n",
        "- `IsHighSpender` — флаг, если клиент потратил больше X рублей.\n",
        "\n",
        "```python\n",
        "# Общая сумма по клиенту\n",
        "df['total_spent'] = df.groupby('CustomerID')['Amount'].transform('sum')\n",
        "\n",
        "# Средний чек\n",
        "df['avg_purchase'] = df.groupby('CustomerID')['Amount'].transform('mean')\n",
        "\n",
        "# Последняя дата покупки\n",
        "last_purchase = df.groupby('CustomerID')['Date'].transform('max')\n",
        "df['days_since_last_purchase'] = (pd.to_datetime('today') - last_purchase).dt.days\n",
        "\n",
        "# Флаг VIP\n",
        "df['is_vip'] = (df['total_spent'] > 10000).astype(int)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### b) **Текстовые признаки**\n",
        "\n",
        "Если у вас есть текстовые поля (например, описание продукта), можно извлекать числовые метрики:\n",
        "\n",
        "- `text_length = len(text)`\n",
        "- `word_count = text.split().len()`\n",
        "- `has_discount = 'скидка' in text`\n",
        "- `sentiment_score = sentiment_model.predict(text)`\n",
        "\n",
        "```python\n",
        "df['text_len'] = df['description'].str.len()\n",
        "df['word_count'] = df['description'].str.split().str.len()\n",
        "df['has_promo'] = df['description'].str.contains('распродажа|скидка|акция', case=False).astype(int)\n",
        "```\n",
        "\n",
        "\n",
        "## 📈 Примеры использования оконных функций\n",
        "\n",
        "### a) Скользящее среднее (Moving Average)\n",
        "\n",
        "```python\n",
        "df['ma_7_days'] = df.groupby('product_id')['sales'].transform(lambda x: x.rolling(7).mean())\n",
        "```\n",
        "\n",
        "### b) Накопительная сумма (Cumulative Sum)\n",
        "\n",
        "```python\n",
        "df['cum_sales'] = df.groupby('product_id')['sales'].cumsum()\n",
        "```\n",
        "\n",
        "### c) Разница между текущим и предыдущим значением\n",
        "\n",
        "```python\n",
        "df['delta_prev_day'] = df.groupby('product_id')['sales'].diff()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 🎯 Как придумать хорошие признаки?\n",
        "\n",
        "1. **Знай свою задачу**: чем лучше ты понимаешь бизнес-объект, тем точнее признаки можешь создать.\n",
        "2. **Изучи данные**: анализируй корреляции, распределения, выбросы.\n",
        "3. **Проверяй гипотезы**: например, \"пользователи с высокой частотой покупок чаще уходят\".\n",
        "4. **Используй визуализации**: графики помогают находить скрытые связи.\n",
        "5. **Тестируй признаки**: добавь их в модель и проверь, улучшилось ли качество.\n",
        "\n",
        "\n",
        "\n",
        "## 🧰 Инструменты для Feature Engineering\n",
        "\n",
        "| Инструмент | Возможности |\n",
        "|-----------|-------------|\n",
        "| **Pandas** | groupby, rolling, transform, apply |\n",
        "| **NumPy** | np.where, np.select, np.clip |\n",
        "| **Scikit-learn** | FunctionTransformer, ColumnTransformer |\n",
        "| **Feature-engine** | Автоматическое создание признаков |\n",
        "| **tsfresh / featuretools** | Автоматическая генерация признаков (для временных рядов и общих задач) |\n",
        "| **Category Encoders** | Преобразование категориальных признаков с учетом целевой переменной |\n",
        "\n",
        "\n",
        "\n",
        "## 📦 Пример набора новых признаков\n",
        "\n",
        "| Исходные признаки | Новые признаки |\n",
        "|------------------|----------------|\n",
        "| `income`, `expenses` | `savings = income - expenses` |\n",
        "| `latency`, `response_time` | `is_slow = latency > mean_latency` |\n",
        "| `start_date`, `end_date` | `duration_days = end_date - start_date` |\n",
        "| `clicks`, `views` | `CTR = clicks / views` |\n",
        "| `latitude`, `longitude` | `distance_from_home` (если известны координаты дома) |\n",
        "| `order_date`, `delivery_date` | `delivery_delay_days = delivery_date - order_date` |\n",
        "| `temperature`, `humidity` | `heat_index = f(temperature, humidity)` |\n",
        "| `transaction_amount`, `time` | `rolling_avg_30d` — скользящее среднее за 30 дней |\n",
        "\n",
        "\n",
        "\n",
        "## 🧪 Пример кода: автоматизация Feature Engineering\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "def engineer_features(df):\n",
        "    # Комбинирование\n",
        "    df['income_expense_diff'] = df['income'] - df['expenses']\n",
        "    df['ratio_income_expenses'] = df['income'] / df['expenses']\n",
        "\n",
        "    # Флаги\n",
        "    df['is_high_risk'] = (df['debt_ratio'] > 0.8).astype(int)\n",
        "    df['is_new_customer'] = (df['first_purchase_date'] > '2024-01-01').astype(int)\n",
        "\n",
        "    # Агрегации\n",
        "    df['avg_purchase_by_city'] = df.groupby('city')['purchase_amount'].transform('mean')\n",
        "    df['total_purchases_by_user'] = df.groupby('user_id')['amount'].cumsum()\n",
        "\n",
        "    # Временные признаки\n",
        "    df['signup_year'] = df['signup_date'].dt.year\n",
        "    df['is_weekend'] = df['visit_date'].dt.weekday >= 5\n",
        "\n",
        "    return df\n",
        "\n",
        "df = engineer_features(df)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Сводная таблица\n",
        "\n",
        "| Тип признака | Что это | Примеры | Инструменты |\n",
        "|--------------|---------|----------|-------------|\n",
        "| Комбинированные | Новые признаки из нескольких старых | `income / expenses`, `price per unit` | Pandas |\n",
        "| Индикаторы | Бинарные признаки | `is_weekend`, `is_vip` | np.where, isin |\n",
        "| Агрегации | Статистики по группам | `avg_salary_by_dept`, `total_orders_per_user` | groupby, transform |\n",
        "| Временные | Признаки из дат | `year`, `day_of_week`, `days_since_last_visit` | dt, diff, shift |\n",
        "| Доменная экспертиза | Признаки из контекста | `CLV`, `Churn Risk`, `LTV` | Pandas, NumPy, domain logic |\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Возьми любой датасет (например, [Titanic](https://www.kaggle.com/c/titanic/data), [Telco Churn](https://www.kaggle.com/blastchar/telco-customer-churn)).\n",
        "2. Выполни следующие шаги:\n",
        "   - Создай 2–3 новых признака через комбинирование (например, отношение, разность, произведение);\n",
        "   - Добавь 2–3 индикатора (например, `is_vip`, `has_missing`);\n",
        "   - Добавь 1–2 агрегированных признака (например, среднее по группе);\n",
        "   - Если есть даты — сделай 2–3 временных признака;\n",
        "   - По желанию: используй доменную экспертизу и создай 1–2 признака, основанных на логике задачи.\n",
        "3. Обучи простую модель (например, LogisticRegression) до и после добавления признаков — сравни качество.\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Дополнительные библиотеки\n",
        "\n",
        "| Библиотека | Описание |\n",
        "|-------------|----------|\n",
        "| **FeatureTools** | Автоматический Feature Engineering |\n",
        "| **tsfresh** | Автоматическая генерация признаков для временных рядов |\n",
        "| **AutoFeat** | Библиотека для автоматической генерации фичей |\n",
        "| **sklearn.preprocessing.FunctionTransformer** | Встраивание собственных функций в пайплайн |\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Заключение\n",
        "\n",
        "Feature Engineering — это искусство и наука одновременно. Он требует как технических навыков, так и понимания предметной области. Хорошо спроектированные признаки могут:\n",
        "- **Сэкономить время на подбор модели**;\n",
        "- **Снизить риск переобучения**;\n",
        "- **Увеличить точность модели без сложных алгоритмов**;\n",
        "- **Ускорить обучение и сделать его более устойчивым**.\n"
      ],
      "metadata": {
        "id": "JhyVfnWpnc8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 10. Уменьшение размерности (Dimensionality Reduction)  \n",
        "## 📌 Введение\n",
        "\n",
        "**Уменьшение размерности** — это процесс упрощения набора данных за счёт **уменьшения количества признаков**, сохраняя при этом как можно больше информации.\n",
        "\n",
        "Это особенно важно, когда:\n",
        "- Признаков слишком много (curse of dimensionality).\n",
        "- Есть коррелирующие или шумные признаки.\n",
        "- Нужно ускорить обучение модели или улучшить интерпретируемость.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Почему нужно уменьшать размерность?\n",
        "\n",
        "| Проблема | Решение |\n",
        "|---------|---------|\n",
        "| Много признаков → переобучение | Отбор важных признаков |\n",
        "| Высокая вычислительная сложность | Снижение размерности |\n",
        "| Корреляции между признаками → мультиколлинеарность | PCA, удаление зависимых признаков |\n",
        "| Шум в данных → снижение качества | Фильтрация по дисперсии |\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Основные методы уменьшения размерности\n",
        "\n",
        "| Категория | Метод | Тип | Когда использовать |\n",
        "|-----------|-------|-----|---------------------|\n",
        "| Удаление нерелевантных/коррелирующих признаков | Дисперсионный отбор, корреляционный анализ | Фильтры | Быстрая очистка |\n",
        "| Линейные проекции | PCA, LDA | Преобразование | Для числовых признаков, нормализованных данных |\n",
        "| Отбор признаков | Filter, Wrapper, Embedded | Отбор | Общие задачи ML |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Удаление нерелевантных и коррелирующих признаков\n",
        "\n",
        "### a) Удаление признаков с низкой дисперсией\n",
        "\n",
        "Признаки, которые почти не меняются, не несут полезной информации.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_reduced = selector.fit_transform(X)\n",
        "```\n",
        "\n",
        "> ✅ Полезен для разреженных или бинарных признаков.\n",
        "\n",
        "\n",
        "\n",
        "### b) Удаление сильно коррелированных признаков\n",
        "\n",
        "Мультиколлинеарность может ухудшить модель и её интерпретацию.\n",
        "\n",
        "#### Пример:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "corr_matrix = df.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "\n",
        "df.drop(to_drop, axis=1, inplace=True)\n",
        "```\n",
        "\n",
        "> ✅ Уменьшает избыточность и повышает стабильность модели.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Метод главных компонент (PCA)\n",
        "\n",
        "**Principal Component Analysis (PCA)** — это линейный метод уменьшения размерности, который находит ортогональные направления (главные компоненты), объясняющие максимальную дисперсию данных.\n",
        "\n",
        "### Алгоритм работы:\n",
        "1. Центрирование данных (Standardization).\n",
        "2. Вычисление ковариационной матрицы.\n",
        "3. Нахождение собственных векторов и значений.\n",
        "4. Проекция данных на первые `k` собственных векторов.\n",
        "\n",
        "#### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "pca = PCA(n_components=0.95)  # Сохраняем 95% дисперсии\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "```\n",
        "\n",
        "> ✅ Полезно при большом числе признаков и их корреляции.  \n",
        "> ❌ Новые признаки трудно интерпретировать.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Линейный дискриминантный анализ (LDA)\n",
        "\n",
        "**LDA (Linear Discriminant Analysis)** — метод, аналогичный PCA, но учитывающий **целевую переменную**. Используется только в задачах **классификации**.\n",
        "\n",
        "### Цель:\n",
        "Найти проекцию, которая **максимизирует разделение классов**.\n",
        "\n",
        "```python\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "```\n",
        "\n",
        "> ✅ Хорошо работает, если данные линейно разделимы.  \n",
        "> ❌ Только для задач классификации.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Отбор признаков\n",
        "\n",
        "Отличается от преобразования: мы **выбираем подмножество исходных признаков**, а не создаём новые.\n",
        "\n",
        "### a) **Filter Methods (Фильтрационные методы)**\n",
        "\n",
        "Выбор признаков на основе статистик без участия модели.\n",
        "\n",
        "#### Примеры:\n",
        "- Коэффициент корреляции Пирсона\n",
        "- Мера информативности (Mutual Information)\n",
        "- ANOVA F-value\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "```\n",
        "\n",
        "> ✅ Простота, быстродействие.  \n",
        "> ❌ Не учитывает взаимосвязь между признаками.\n",
        "\n",
        "\n",
        "\n",
        "### b) **Wrapper Methods (Обёрточные методы)**\n",
        "\n",
        "Используют модель для оценки важности признаков.\n",
        "\n",
        "#### Пример: Recursive Feature Elimination (RFE)\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "rfe = RFE(estimator=model, n_features_to_select=10)\n",
        "X_rfe = rfe.fit_transform(X, y)\n",
        "```\n",
        "\n",
        "> ✅ Учитывает взаимодействие между признаками.  \n",
        "> ❌ Вычислительно затратно.\n",
        "\n",
        "\n",
        "\n",
        "### c) **Embedded Methods (Встроенные методы)**\n",
        "\n",
        "Совмещают отбор признаков и обучение модели.\n",
        "\n",
        "#### Примеры:\n",
        "- **Lasso (L1-регуляризация)** — обнуляет коэффициенты малозначимых признаков.\n",
        "- **Деревья решений, Random Forest, XGBoost** — используют feature importance.\n",
        "\n",
        "##### Lasso (линейная регрессия с L1):\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "lasso = LassoCV(cv=5)\n",
        "lasso.fit(X_scaled, y)\n",
        "\n",
        "# Признаки с нулевым коэффициентом можно удалить\n",
        "mask = lasso.coef_ != 0\n",
        "X_lasso = X[:, mask]\n",
        "```\n",
        "\n",
        "##### Feature Importance через деревья:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "top_k = indices[:10]  # топ-10 признаков\n",
        "X_top = X[:, top_k]\n",
        "```\n",
        "\n",
        "> ✅ Эффективно, интегрировано в обучение.  \n",
        "> ❌ Зависит от модели.\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Сравнение методов\n",
        "\n",
        "| Метод | Учитывает целевую переменную | Требует обучения модели | Интерпретируемость | Подходит для |\n",
        "|--------|------------------------------|--------------------------|--------------------|---------------|\n",
        "| Дисперсионный отбор | ❌ | ❌ | ✅ | Все задачи |\n",
        "| Корреляционный отбор | ✅ | ❌ | ✅ | Числовые признаки |\n",
        "| PCA | ❌ | ❌ | ❌ | Все задачи |\n",
        "| LDA | ✅ | ✅ | ❌ | Классификация |\n",
        "| Filter Methods | ✅ | ❌ | ✅ | Все задачи |\n",
        "| Wrapper Methods (RFE) | ✅ | ✅ | ✅ | Все задачи |\n",
        "| Embedded Methods (Lasso, Tree-based) | ✅ | ✅ | ✅ | Все задачи |\n",
        "\n",
        "\n",
        "\n",
        "## 📈 Как выбрать количество компонент / признаков?\n",
        "\n",
        "### a) PCA\n",
        "\n",
        "```python\n",
        "pca = PCA().fit(X_scaled)\n",
        "explained_variance = pca.explained_variance_ratio_.cumsum()\n",
        "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
        "```\n",
        "\n",
        "### b) Feature Importance (деревья)\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(range(len(importances)), importances[indices])\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### c) RFE\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'n_features_to_select': range(5, 20)}\n",
        "grid = GridSearchCV(rfe, param_grid, scoring='accuracy', cv=5)\n",
        "grid.fit(X, y)\n",
        "best_n = grid.best_params_['n_features_to_select']\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 🧪 Когда использовать каждый метод?\n",
        "\n",
        "| Задача | Рекомендуемый метод |\n",
        "|--------|----------------------|\n",
        "| Много числовых признаков, нет цели интерпретации | PCA |\n",
        "| Классификация, есть метки | LDA |\n",
        "| Нужны интерпретируемые признаки | Filter, Embedded |\n",
        "| Есть время и ресурсы, нужна точность | Wrapper (RFE) |\n",
        "| Мало данных, нужны важные признаки | Embedded (Lasso, Random Forest) |\n",
        "\n",
        "\n",
        "\n",
        "## 📉 Пример конвейера отбора признаков\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
        "    ('dimensionality_reduction', PCA(n_components=5)),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой датасет со многими признаками (например, [Breast Cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), [Digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)).\n",
        "2. Примените следующие методы:\n",
        "   - Удаление признаков с низкой дисперсией;\n",
        "   - Удаление коррелированных признаков;\n",
        "   - PCA;\n",
        "   - LDA (если классификация);\n",
        "   - Filter / Embedded / Wrapper методы отбора признаков.\n",
        "3. Обучите модель до и после уменьшения размерности — сравните качество.\n",
        "4. Постройте графики важности признаков и доли объяснённой дисперсии.\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Дополнительные библиотеки\n",
        "\n",
        "| Библиотека | Описание |\n",
        "|------------|----------|\n",
        "| **Scikit-learn** | PCA, LDA, RFE, SelectKBest |\n",
        "| **Feature-engine** | Автоматический отбор |\n",
        "| **Boruta** | Расширение Random Forest для отбора признаков |\n",
        "| **SHAP / LIME** | Анализ важности признаков с точки зрения модели |\n",
        "| **eli5** | Интерпретация и оценка важности признаков |\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Заключение\n",
        "\n",
        "Уменьшение размерности — мощный инструмент, который позволяет:\n",
        "- **Снизить риск переобучения**,\n",
        "- **Ускорить обучение модели**,\n",
        "- **Упростить интерпретацию**,\n",
        "- **Сохранить важную информацию**.\n",
        "\n",
        "Правильно применённое уменьшение размерности может значительно повысить производительность моделей, особенно в условиях высокой размерности данных.\n"
      ],
      "metadata": {
        "id": "RwgQsWKRoYJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 11. Разделение выборки  \n",
        "## 📌 Введение\n",
        "\n",
        "**Разделение выборки** — это ключевой этап в подготовке данных для машинного обучения. Оно позволяет:\n",
        "- **Обучить модель** на части данных,\n",
        "- **Оценить её качество** на невидимых примерах (валидация),\n",
        "- **Проверить финальную модель** на независимой тестовой выборке.\n",
        "\n",
        "Правильное разделение данных помогает избежать **утечки информации**, обеспечивает **объективную оценку модели** и снижает риск **переобучения**.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Почему важно правильно разделять данные?\n",
        "\n",
        "| Проблема | Решение |\n",
        "|---------|----------|\n",
        "| Утечка данных | Разделение до обработки |\n",
        "| Нестабильная оценка качества | Кросс-валидация |\n",
        "| Смещённая оценка качества | Стратифицированное разделение |\n",
        "| Нарушение временного порядка | TimeSeriesSplit |\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Основные типы разделения\n",
        "\n",
        "| Метод | Когда использовать | Особенности |\n",
        "|-------|---------------------|-------------|\n",
        "| `train_test_split()` | Быстрое разделение на train и test | Просто, но может быть нестабильно |\n",
        "| K-Fold | Для более надёжной оценки | Повторяется несколько раз |\n",
        "| Stratified K-Fold | Для задач классификации с несбалансированными классами | Сохраняет распределение целевой переменной |\n",
        "| TimeSeriesSplit | Для временных рядов | Учитывает хронологический порядок |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Обычное разделение: `train_test_split`\n",
        "\n",
        "Это самый простой и часто используемый способ разделить данные на **обучающую** и **тестовую** выборки.\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "Где:\n",
        "- `test_size` — доля данных, выделяемых под тест;\n",
        "- `random_state` — фиксирует случайность для воспроизводимости;\n",
        "- `stratify=y` — сохраняет пропорции классов (для классификации).\n",
        "\n",
        "#### Пример с `stratify`:\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "```\n",
        "\n",
        "> ✅ Просто и быстро.  \n",
        "> ❌ Не устойчив к вариации данных.\n",
        "\n",
        "\n",
        "\n",
        "## 2. K-Fold Cross Validation\n",
        "\n",
        "**K-Fold** — метод, при котором данные делятся на `k` частей (фолдов), и модель обучается `k` раз, каждый раз на `k-1` фолдах и тестируется на оставшемся.\n",
        "\n",
        "### Этапы:\n",
        "1. Данные делятся на `k` равных частей.\n",
        "2. На каждой итерации одна часть используется как валидационная, остальные — как обучающие.\n",
        "3. Итоговая метрика — среднее по всем `k` итерациям.\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "print(\"Средняя точность:\", scores.mean())\n",
        "```\n",
        "\n",
        "> ✅ Более стабильная оценка качества.  \n",
        "> ❌ Требует больше времени на обучение.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Stratified K-Fold (для классификации)\n",
        "\n",
        "Аналогичен K-Fold, но **сохраняет соотношение классов** в каждом фолде. Это особенно важно, если классы несбалансированы.\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, val_index in skf.split(X, y):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_val, y_val)\n",
        "    print(\"Валидационная точность:\", score)\n",
        "```\n",
        "\n",
        "> ✅ Подходит для задач с несбалансированными классами.  \n",
        "> ❌ Не подходит для временных рядов.\n",
        "\n",
        "\n",
        "\n",
        "## 4. TimeSeriesSplit (для временных рядов)\n",
        "\n",
        "Для временных данных нельзя просто перемешивать данные — это нарушит временную зависимость. Поэтому используется специальный метод **TimeSeriesSplit**, где данные делятся в порядке возрастания времени.\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "for train_index, val_index in tscv.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_val, y_val)\n",
        "    print(\"Временная валидация — точность:\", score)\n",
        "```\n",
        "\n",
        "> ✅ Учитывает временной порядок.  \n",
        "> ❌ Нельзя перемешивать данные.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Отложенная тестовая выборка (Hold-out)\n",
        "\n",
        "Иногда используется **трехчастное разделение**:  \n",
        "- `train` — обучение модели,  \n",
        "- `validation` — настройка гиперпараметров,  \n",
        "- `test` — финальная оценка.\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "# Сначала делим на train и test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Затем делим train на train и validation\n",
        "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=42)\n",
        "```\n",
        "\n",
        "> ✅ Подходит для ранней оценки и сравнения моделей.  \n",
        "> ❌ Требует достаточного объёма данных.\n",
        "\n",
        "\n",
        "\n",
        "## 6. Групповое разделение (Group-based Splitting)\n",
        "\n",
        "Если данные содержат **группы** (например, пользователи, пациенты, клиенты), то важно **не допустить попадания одной группы в train и test**.\n",
        "\n",
        "### Пример: GroupKFold\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "for train_idx, val_idx in gkf.split(X, y, groups=groups):\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_val, y_val)\n",
        "    print(\"Точность на валидации:\", score)\n",
        "```\n",
        "\n",
        "> ✅ Избегаем утечки между группами.  \n",
        "> ❌ Требуется знать групповые метки.\n",
        "\n",
        "\n",
        "\n",
        "## 7. Leave-One-Out (LOO) — редко используется\n",
        "\n",
        "Каждая точка используется как валидационная один раз, остальные — обучающие.\n",
        "\n",
        "### Реализация:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "scores = cross_val_score(model, X, y, cv=loo)\n",
        "```\n",
        "\n",
        "> ⚠️ Очень медленный метод.  \n",
        "> ❌ Используется редко из-за высоких вычислительных затрат.\n",
        "\n",
        "\n",
        "\n",
        "## 🧮 Как выбрать количество фолдов?\n",
        "\n",
        "| Метод | Рекомендуемое число фолдов |\n",
        "|--------|-----------------------------|\n",
        "| K-Fold / Stratified K-Fold | 5 или 10 |\n",
        "| TimeSeriesSplit | 3–5 |\n",
        "| Leave-One-Out | Все точки (очень долго) |\n",
        "| Hold-out | 80% / 20% |\n",
        "\n",
        "Чем меньше данных — тем больше фолдов нужно, чтобы оценка была устойчивой.\n",
        "\n",
        "\n",
        "\n",
        "## 📈 Пример: сравнение разных стратегий\n",
        "\n",
        "| Метод | Размер train | Размер test | Особенности |\n",
        "|--------|--------------|-------------|-------------|\n",
        "| `train_test_split(0.2)` | 80% | 20% | Одно разбиение |\n",
        "| K-Fold (n=5) | ~80% | ~20% | 5 разбиений, усреднение |\n",
        "| Stratified K-Fold | ~80% | ~20% | То же + сохранение баланса классов |\n",
        "| TimeSeriesSplit (n=5) | По возрастанию времени | Последние наблюдения | Учет временного тренда |\n",
        "| GroupKFold (n=5) | Без пересечения групп | Без пересечения групп | Устранение утечки между группами |\n",
        "\n",
        "\n",
        "\n",
        "## 📦 Пример полного пайплайна предобработки и разделения\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Предобработка\n",
        "df = pd.read_csv('data.csv')\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Разделение на train и test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Создание пайплайна\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Кросс-валидация\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "    pipeline.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
        "    score = pipeline.score(X_train.iloc[val_idx], y_train.iloc[val_idx])\n",
        "    scores.append(score)\n",
        "\n",
        "print(\"Средняя точность на валидации:\", np.mean(scores))\n",
        "\n",
        "# Финальное обучение на всех train данных\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Оценка на тесте\n",
        "final_score = pipeline.score(X_test, y_test)\n",
        "print(\"Финальная точность на тесте:\", final_score)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Советы по разделению данных\n",
        "\n",
        "| Ситуация | Что делать |\n",
        "|----------|-------------|\n",
        "| Мало данных | Использовать K-Fold (5 или 10 фолдов) |\n",
        "| Несбалансированные классы | Stratified K-Fold |\n",
        "| Временные ряды | TimeSeriesSplit |\n",
        "| Группы в данных | GroupKFold |\n",
        "| Контроль утечки данных | Всегда делить до предобработки и фиче-инженерии |\n",
        "| Выбор числа фолдов | 5 или 10 — оптимальный компромисс между качеством и скоростью |\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите любой датасет (например, [Breast Cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)).\n",
        "2. Разделите данные следующими способами:\n",
        "   - `train_test_split`\n",
        "   - `K-Fold`\n",
        "   - `StratifiedKFold`\n",
        "   - `TimeSeriesSplit` (если есть временной фактор)\n",
        "3. Обучите модель (например, `RandomForestClassifier`) на каждом из них.\n",
        "4. Сравните полученные метрики (accuracy, F1-score).\n",
        "5. Постройте графики распределения оценок качества.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Заключение\n",
        "\n",
        "Правильное разделение выборки — основа корректной оценки модели. Вот основные рекомендации:\n",
        "\n",
        "- **Используйте `train_test_split()`** для быстрого эксперимента.\n",
        "- **Применяйте `K-Fold` или `Stratified K-Fold`**, если нужна стабильная оценка.\n",
        "- **Используйте `TimeSeriesSplit`**, если данные имеют временной фактор.\n",
        "- **Учитывайте групповую структуру**, если есть повторяющиеся объекты (например, пользователи).\n",
        "- **Не забывайте про стратификацию**, особенно если классы несбалансированы.\n",
        "\n",
        "Хочешь, чтобы я показал реализацию для конкретной задачи? Например, как разделить данные для временных рядов или с учётом пользовательских ID — пиши, сделаю!"
      ],
      "metadata": {
        "id": "umXdXl51qGzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 12. Балансировка классов (Class Imbalance Handling)  \n",
        "## 📌 Введение\n",
        "\n",
        "**Балансировка классов** — это важный этап при работе с задачами **классификации**, в которых **классы распределены неравномерно**. Например, в задаче детектирования мошенничества нормальные транзакции могут составлять 99.9% данных, а мошеннические — лишь 0.1%. Такие данные называются **несбалансированными (imbalanced)**.\n",
        "\n",
        "Если не учитывать дисбаланс, модель может:\n",
        "- **игнорировать редкий класс**,\n",
        "- давать высокую метрику (например, `accuracy`), но быть бесполезной на практике,\n",
        "- **переобучаться на частом классе** и плохо работать на редком.\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Почему балансировка важна?\n",
        "\n",
        "| Проблема | Решение |\n",
        "|---------|----------|\n",
        "| Низкое качество на редких классах | Балансировка |\n",
        "| Завышенная оценка качества (`accuracy`) | Изменение метрики |\n",
        "| Утечка данных из train во время обучения | Корректное разделение выборки |\n",
        "| Смещённая оценка модели | Oversampling / Undersampling |\n",
        "\n",
        "\n",
        "\n",
        "## 🔢 Типы подходов к балансировке классов\n",
        "\n",
        "| Метод | Описание | Когда использовать |\n",
        "|--------|----------|---------------------|\n",
        "| **Undersampling** | Уменьшение числа объектов большого класса | Много данных, малая цена потери информации |\n",
        "| **Oversampling** | Увеличение числа объектов малого класса | Мало данных, важно сохранить все примеры |\n",
        "| **Синтетические методы (SMOTE, ADASYN)** | Генерация новых примеров для малого класса | Мало данных, нужно увеличение |\n",
        "| **Взвешивание классов (`class_weight`)** | Учет весов классов в обучении | Когда нельзя менять данные |\n",
        "| **Изменение метрики оценки** | Использование F1, ROC AUC вместо accuracy | При оценке качества модели |\n",
        "\n",
        "\n",
        "\n",
        "## 1. Oversampling: увеличение редкого класса\n",
        "\n",
        "### a) **Random Oversampling**\n",
        "\n",
        "Просто **дублирует случайные образцы** из редкого класса.\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler()\n",
        "X_res, y_res = ros.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Простой способ уравнять классы.  \n",
        "> ❌ Может вызвать переобучение.\n",
        "\n",
        "\n",
        "\n",
        "### b) **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
        "\n",
        "Генерирует **синтетические примеры** между соседями редкого класса.\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "#### Плюсы:\n",
        "- Не просто копирует данные.\n",
        "- Сохраняет структуру признакового пространства.\n",
        "\n",
        "#### Минусы:\n",
        "- Может создать шумовые образцы.\n",
        "- Требует корректного определения соседей.\n",
        "\n",
        "\n",
        "\n",
        "### c) **ADASYN (Adaptive Synthetic Sampling)**\n",
        "\n",
        "Аналог SMOTE, но делает больше синтетических примеров в сложных областях.\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "adasyn = ADASYN()\n",
        "X_res, y_res = adasyn.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Подходит, если редкий класс сложно разделить.  \n",
        "> ❌ Требует больше вычислений.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Undersampling: уменьшение большого класса\n",
        "\n",
        "### a) **Random Undersampling**\n",
        "\n",
        "Случайным образом **удаляет объекты из большого класса**.\n",
        "\n",
        "```python\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler()\n",
        "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Ускоряет обучение.  \n",
        "> ❌ Может привести к потере важной информации.\n",
        "\n",
        "\n",
        "\n",
        "### b) **Tomek Links**\n",
        "\n",
        "Находит **\"противоречивые\" точки** между классами и удаляет их.\n",
        "\n",
        "```python\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "tl = TomekLinks()\n",
        "X_res, y_res = tl.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Убирает \"шум\" между классами.  \n",
        "> ❌ Требует анализа границы разделения.\n",
        "\n",
        "\n",
        "\n",
        "### c) **NearMiss**\n",
        "\n",
        "Выбирает представительные образцы из большого класса, основываясь на расстоянии до малого класса.\n",
        "\n",
        "```python\n",
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "nm = NearMiss(version=1)\n",
        "X_res, y_res = nm.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Сохраняет информативные примеры.  \n",
        "> ❌ Требует больше вычислений.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Комбинированный подход\n",
        "\n",
        "Можно комбинировать oversampling и undersampling:\n",
        "\n",
        "```python\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "over = SMOTE(sampling_strategy=0.5)  # Увеличиваем до 50% от большого класса\n",
        "under = RandomUnderSampler(sampling_strategy=0.7)  # Уменьшаем до 70%\n",
        "\n",
        "pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
        "X_res, y_res = pipeline.fit_resample(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Баланс между качеством и количеством данных.  \n",
        "> ❌ Требует подбора параметров.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Взвешивание классов (class_weight)\n",
        "\n",
        "Некоторые алгоритмы позволяют задать **веса классов**, чтобы модель уделяла больше внимания редкому классу.\n",
        "\n",
        "### Пример: RandomForestClassifier\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "### Пример: XGBoost\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(scale_pos_weight=weight_for_positive_class)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "> ✅ Простой и эффективный способ.  \n",
        "> ❌ Не всегда помогает при сильном дисбалансе.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Изменение метрики оценки\n",
        "\n",
        "Accuracy — плохая метрика для несбалансированных данных. Лучше использовать:\n",
        "\n",
        "| Метрика | Когда использовать |\n",
        "|--------|---------------------|\n",
        "| **F1-score** | Когда важны и precision, и recall |\n",
        "| **Precision / Recall** | Если важна точность или полнота |\n",
        "| **ROC AUC** | Для двоичной классификации |\n",
        "| **PR AUC (Precision-Recall AUC)** | Если положительный класс редкий |\n",
        "| **Balanced Accuracy** | Если классы сильно несбалансированы |\n",
        "\n",
        "### Пример:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 📊 Сводная таблица методов\n",
        "\n",
        "| Метод | Подходит для | Преимущества | Недостатки |\n",
        "|--------|---------------|---------------|-------------|\n",
        "| Random Oversampling | Мало данных | Просто реализуется | Может вызвать переобучение |\n",
        "| SMOTE | Мало данных | Создаёт новые образцы | Может генерировать шум |\n",
        "| ADASYN | Сложные области редкого класса | Адаптивно генерирует | Высокие затраты |\n",
        "| Random Undersampling | Много данных | Ускоряет обучение | Потеря информации |\n",
        "| Tomek Links | Чистка данных | Убирает шум между классами | Уменьшает размер выборки |\n",
        "| NearMiss | Экономия памяти | Сохраняет важные примеры | Может потерять информацию |\n",
        "| Class Weight | Все случаи | Без изменения данных | Может быть недостаточно |\n",
        "| Изменение метрики | Все случаи | Объективная оценка | Не влияет на обучение |\n",
        "\n",
        "\n",
        "\n",
        "## 📈 Как выбрать метод балансировки?\n",
        "\n",
        "| Задача | Рекомендация |\n",
        "|--------|---------------|\n",
        "| Очень мало данных по редкому классу | SMOTE / ADASYN |\n",
        "| Есть много данных, можно терять часть | Undersampling |\n",
        "| Нельзя менять данные | class_weight |\n",
        "| Нужно учесть сложные зоны редкого класса | ADASYN |\n",
        "| Нужно убрать шум между классами | Tomek Links |\n",
        "| Нужно уменьшить больший класс | NearMiss / Random Under |\n",
        "| Нужно улучшить обобщающую способность | SMOTE + Tomek Links (гибрид) |\n",
        "\n",
        "\n",
        "\n",
        "## 🧪 Пример полного пайплайна с балансировкой\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Создание пайплайна\n",
        "pipeline = imbpipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Обучение\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Оценка\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 📉 Что делать, если классы сильно несбалансированы?\n",
        "\n",
        "### a) Взвешенная метрика\n",
        "\n",
        "Используйте `f1_weighted`, `precision_weighted`, `recall_weighted`.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "score = f1_score(y_test, y_pred, average='weighted')\n",
        "```\n",
        "\n",
        "### b) Кросс-валидация с учетом дисбаланса\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for train_idx, val_idx in skf.split(X, y):\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    score = pipeline.score(X_val, y_val)\n",
        "    print(\"Validation Score:\", score)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## 📦 Пример: сравнение метрик до и после балансировки\n",
        "\n",
        "| Метрика | До балансировки | После SMOTE |\n",
        "|--------|------------------|--------------|\n",
        "| Accuracy | 0.98 | 0.95 |\n",
        "| Precision (positive) | 0.35 | 0.72 |\n",
        "| Recall (positive) | 0.10 | 0.68 |\n",
        "| F1-score | 0.15 | 0.70 |\n",
        "\n",
        "> 💡 Даже если accuracy упал, другие метрики показывают улучшение!\n",
        "\n",
        "\n",
        "\n",
        "## 🧰 Полезные библиотеки\n",
        "\n",
        "| Библиотека | Возможности |\n",
        "|------------|-------------|\n",
        "| **scikit-learn** | class_weight, метрики |\n",
        "| **imbalanced-learn (imblearn)** | SMOTE, ADASYN, RandomOverSampler, TomekLinks и др. |\n",
        "| **xgboost / lightgbm / catboost** | scale_pos_weight |\n",
        "| **sklearn.metrics** | weighted, macro, micro метрики |\n",
        "| **eli5 / shap** | Анализ важности признаков с учётом дисбаланса |\n",
        "\n",
        "\n",
        "\n",
        "## 📚 Домашнее задание\n",
        "\n",
        "1. Загрузите датасет с несбалансированными классами (например, [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)).\n",
        "2. Примените следующие методы:\n",
        "   - Random Oversampling\n",
        "   - SMOTE\n",
        "   - Random Undersampling\n",
        "   - Tomek Links\n",
        "   - ADASYN\n",
        "3. Обучите модель (например, `RandomForestClassifier`) на каждом из них.\n",
        "4. Сравните результаты по метрикам:\n",
        "   - Accuracy\n",
        "   - F1-score\n",
        "   - ROC AUC\n",
        "   - Precision / Recall\n",
        "5. Сделайте вывод: какой метод дал лучшие результаты и почему?\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Заключение\n",
        "\n",
        "Балансировка классов — один из ключевых шагов в подготовке данных для задач классификации. Она позволяет:\n",
        "- **улучшить качество модели на редких классах**;\n",
        "- **избежать завышенной оценки** через `accuracy`;\n",
        "- **сохранить ценную информацию** о редких событиях (мошенничество, отказ оборудования, болезни и т.д.).\n",
        "\n",
        "Правильно подобранный метод балансировки может существенно повысить производительность модели, особенно если редкий класс имеет большое значение.\n"
      ],
      "metadata": {
        "id": "Z65e1QTByPSp"
      }
    }
  ]
}