{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNRcHO1vQi32n5paOyJVsmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/%D0%A0%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Простая линейная регрессия: Математическая модель и метод наименьших квадратов\n",
        "\n",
        "#### 1. Введение\n",
        "\n",
        "Простая линейная регрессия является одним из базовых инструментов статистического анализа, позволяющим моделировать линейную зависимость между одной независимой переменной $x$ (предиктором) и зависимой переменной $y$ (откликом). Модель простой линейной регрессии описывается следующим уравнением:\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1 x + \\varepsilon,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w_0$ — свободный член (intercept), представляющий значение $y$, когда $x = 0$;\n",
        "- $w_1$ — коэффициент наклона (slope), характеризующий изменение $y$ при единичном изменении $x$;\n",
        "- $\\varepsilon$ — случайная ошибка (residual), отражающая отклонение фактических значений $y$ от предсказанных моделью.\n",
        "\n",
        "Цель простой линейной регрессии заключается в определении параметров $w_0$ и $w_1$, которые минимизируют отклонение предсказанных значений от фактических данных. Для этого используется метод наименьших квадратов (Least Squares Method).\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Функция потерь (Mean Squared Error)\n",
        "\n",
        "Для оценки качества модели применяется функция потерь, которая измеряет среднеквадратичное отклонение предсказанных значений от фактических. В случае простой линейной регрессии функция потерь имеет вид:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{n} \\left( y_i - (w_0 + w_1 x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $E(w)$ — функция потерь;\n",
        "- $n$ — количество наблюдений в выборке;\n",
        "- $y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения;\n",
        "- $x_i$ — значение независимой переменной для $i$-го наблюдения.\n",
        "\n",
        "Коэффициент $\\frac{1}{2}$ добавлен для удобства дифференцирования при последующей оптимизации.\n",
        "\n",
        "Функция потерь $E(w)$ характеризует общую ошибку модели и стремится к минимуму при подборе параметров $w_0$ и $w_1$. Это достигается путем вычисления частных производных функции потерь по каждому параметру и их приравнивания к нулю.\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Оптимизация методом наименьших квадратов\n",
        "\n",
        "Метод наименьших квадратов позволяет найти такие значения параметров $w_0$ и $w_1$, которые минимизируют функцию потерь. Для этого вычисляются частные производные функции $E(w)$ по $w_0$ и $w_1$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_0} = -\\sum_{i=1}^{n} \\left( y_i - (w_0 + w_1 x_i) \\right),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_1} = -\\sum_{i=1}^{n} x_i \\left( y_i - (w_0 + w_1 x_i) \\right).\n",
        "$$\n",
        "\n",
        "Приравнивая эти производные к нулю, получаем систему нормальных уравнений:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} y_i = n w_0 + w_1 \\sum_{i=1}^{n} x_i,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} x_i y_i = w_0 \\sum_{i=1}^{n} x_i + w_1 \\sum_{i=1}^{n} x_i^2.\n",
        "$$\n",
        "\n",
        "Решение данной системы позволяет найти оптимальные значения параметров $w_0$ и $w_1$:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{n \\sum_{i=1}^{n} x_i y_i - \\left( \\sum_{i=1}^{n} x_i \\right) \\left( \\sum_{i=1}^{n} y_i \\right)}{n \\sum_{i=1}^{n} x_i^2 - \\left( \\sum_{i=1}^{n} x_i \\right)^2},\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_0 = \\frac{\\sum_{i=1}^{n} y_i - w_1 \\left( \\sum_{i=1}^{n} x_i \\right)}{n}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 4. Упрощение через ковариацию и дисперсию\n",
        "\n",
        "Для более компактного представления формулы для $w_1$ и $w_0$ могут быть выражены через ковариацию и дисперсию. Ковариация $ \\text{Cov}(x, y) $ и дисперсия $ \\text{Var}(x) $ определяются как:\n",
        "\n",
        "$$\n",
        "\\text{Cov}(x, y) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}(x) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2,\n",
        "$$\n",
        "\n",
        "где $\\bar{x}$ и $\\bar{y}$ — выборочные средние:\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i.\n",
        "$$\n",
        "\n",
        "Тогда коэффициент наклона $w_1$ выражается через ковариацию и дисперсию:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}.\n",
        "$$\n",
        "\n",
        "Свободный член $w_0$ вычисляется как:\n",
        "\n",
        "$$\n",
        "w_0 = \\bar{y} - w_1 \\bar{x}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 5. Интерпретация формул\n",
        "\n",
        "1. **Наклон ($w_1$):**\n",
        "   - Коэффициент $w_1$ характеризует силу и направление линейной зависимости между переменными $x$ и $y$. Он пропорционален ковариации между $x$ и $y$, что отражает их совместную вариацию.\n",
        "   - Деление на дисперсию $x$ нормализует эту зависимость, обеспечивая корректное масштабирование.\n",
        "\n",
        "2. **Свободный член ($w_0$):**\n",
        "   - Свободный член гарантирует, что линия регрессии проходит через центр масс данных $(\\bar{x}, \\bar{y})$.\n",
        "\n"
      ],
      "metadata": {
        "id": "_-B3swD2HqPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Построение функции $ y(x, w) $\n",
        "\n",
        "Задача линейной регрессии заключается в построении модели, описывающей зависимость целевой переменной $ y $ от входных признаков $ x $. Модель представляется в виде функции:\n",
        "\n",
        "$$\n",
        "y(x, w) \\to \\mathbb{R},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ w = (w_0, w_1, \\ldots, w_D)^T $ — вектор параметров модели,\n",
        "- $ x = (x_1, x_2, \\ldots, x_D)^T $ — вектор входных признаков.\n",
        "\n",
        "Наиболее распространённая форма линейной модели имеет вид:\n",
        "\n",
        "$$\n",
        "y(x, w) = w_0 + \\sum_{i=1}^D w_i x_i,\n",
        "$$\n",
        "\n",
        "или в векторной форме:\n",
        "\n",
        "$$\n",
        "y(x, w) = w^T \\phi(x),\n",
        "$$\n",
        "\n",
        "где $ \\phi(x) = (1, x_1, x_2, \\ldots, x_D)^T $ — вектор базисных функций, включающий свободный член $ w_0 $.\n",
        "\n",
        "\n",
        "\n",
        "### Базисные функции (Basis Functions)\n",
        "\n",
        "Базисные функции позволяют расширить пространство признаков и улучшить способность модели описывать сложные зависимости между входными данными и целевой переменной. Введение базисных функций особенно полезно, когда исходные данные не подчиняются простой линейной зависимости.\n",
        "\n",
        "Модель линейной регрессии с базисными функциями записывается как:\n",
        "\n",
        "$$\n",
        "y(x, w) = w_0 + \\sum_{j=1}^{M-1} w_j \\phi_j(x),\n",
        "$$\n",
        "\n",
        "или в векторной форме:\n",
        "\n",
        "$$\n",
        "y(x, w) = w^T \\phi(x),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi(x) = (\\phi_0(x), \\phi_1(x), \\ldots, \\phi_{M-1}(x))^T $ — вектор базисных функций,\n",
        "- $ w = (w_0, w_1, \\ldots, w_{M-1})^T $ — вектор параметров модели.\n",
        "\n",
        "\n",
        "\n",
        "#### Зачем нужны базисные функции?\n",
        "\n",
        "Использование базисных функций решает следующие задачи:\n",
        "1. **Моделирование сложных зависимостей.** Базисные функции позволяют учитывать нелинейные взаимосвязи между входными признаками и целевой переменной.\n",
        "2. **Повышение точности предсказаний.** Расширение пространства признаков увеличивает гибкость модели, что улучшает её способность аппроксимировать данные.\n",
        "3. **Снижение ошибки модели.** Базисные функции помогают минимизировать ошибку за счёт более точного описания закономерностей в данных.\n",
        "4. **Улучшение обобщающей способности.** Модель становится более устойчивой к изменениям в данных, особенно если зависимости носят нелинейный характер.\n",
        "\n",
        "\n",
        "\n",
        "#### Примеры базисных функций\n",
        "\n",
        "Рассмотрим несколько типов базисных функций, которые могут быть использованы в линейной регрессии:\n",
        "\n",
        "1. **Полиномиальные базисные функции:**\n",
        "   Полиномиальные функции добавляют степени исходных признаков для моделирования нелинейных зависимостей:\n",
        "   $$\n",
        "   \\phi(x) = (1, x, x^2, x^3, \\ldots, x^K).\n",
        "   $$\n",
        "\n",
        "2. **Радиально-базисные функции (RBF):**\n",
        "   RBF сосредоточены вокруг определённых центров $ \\mu_j $ и имеют форму Гауссовых функций:\n",
        "   $$\n",
        "   \\phi(x) = \\left( e^{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}}, e^{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}}, \\ldots \\right).\n",
        "   $$\n",
        "\n",
        "3. **Сплайн-функции:**\n",
        "   Сплайны разделяют пространство признаков на интервалы и строят полиномы для каждого интервала, плавно соединяя их:\n",
        "   $$\n",
        "   \\phi(x) =\n",
        "   \\begin{cases}\n",
        "      p_1(x), & x < k_1, \\\\\n",
        "      p_2(x), & k_1 \\leq x < k_2, \\\\\n",
        "      \\dots, \\\\\n",
        "      p_n(x), & x \\geq k_{n-1}.\n",
        "   \\end{cases}\n",
        "   $$\n",
        "\n",
        "4. **Синусоидальные базисные функции:**\n",
        "   Эти функции полезны для моделирования периодических процессов:\n",
        "   $$\n",
        "   \\phi(x) = (\\sin(\\omega_1 x), \\cos(\\omega_1 x), \\sin(\\omega_2 x), \\cos(\\omega_2 x), \\ldots).\n",
        "   $$\n",
        "\n",
        "5. **Логистические базисные функции:**\n",
        "   Логистические функции подходят для задач с пороговыми эффектами:\n",
        "   $$\n",
        "   \\phi(x) = \\frac{1}{1 + e^{-(x - \\mu)/\\sigma}}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Матрица плана (Design Matrix)\n",
        "\n",
        "Матрица плана, обозначаемая как $ X $, является ключевым элементом в построении линейной регрессии. Она представляет собой структурированную таблицу значений всех независимых переменных (признаков) для каждого наблюдения в данных. Каждая строка матрицы соответствует одному наблюдению, а каждый столбец — значению конкретного признака или его преобразования.\n",
        "\n",
        "\n",
        "\n",
        "### Структура матрицы плана\n",
        "\n",
        "Для классической линейной регрессии матрица плана $ X $ имеет размерность $ n \\times (p+1) $, где:\n",
        "- $ n $ — количество наблюдений,\n",
        "- $ p $ — количество исходных признаков (без учёта свободного члена).\n",
        "\n",
        "Стандартная форма матрицы плана выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- Первый столбец состоит из единиц и соответствует свободному члену $ w_0 $ (bias term),\n",
        "- Остальные столбцы содержат значения исходных признаков для каждого наблюдения.\n",
        "\n",
        "\n",
        "\n",
        "### Функции и задачи матрицы плана\n",
        "\n",
        "Матрица плана играет центральную роль в линейной регрессии, обеспечивая удобство вычислений с использованием методов линейной алгебры. Основные её функции включают:\n",
        "\n",
        "1. **Оценка параметров модели:**\n",
        "   Параметры модели $ w $ вычисляются с помощью метода наименьших квадратов (Least Squares Method). Формула для оценки параметров имеет вид:\n",
        "   $$\n",
        "   \\hat{w} = (X^T X)^{-1} X^T y,\n",
        "   $$\n",
        "   где:\n",
        "   - $ X^T $ — транспонированная матрица плана,\n",
        "   - $ y $ — вектор значений целевой переменной,\n",
        "   - $ (X^T X)^{-1} $ — обратная матрица произведения $ X^T X $.\n",
        "\n",
        "2. **Учёт множества факторов:**\n",
        "   Матрица плана позволяет модели учитывать несколько факторов одновременно, что увеличивает её гибкость и способность анализировать влияние различных переменных на целевую величину.\n",
        "\n",
        "3. **Проведение статистических тестов:**\n",
        "   Через матрицу плана можно проводить статистические тесты для оценки значимости коэффициентов модели. Это помогает определить, какие факторы оказывают существенное влияние на целевую переменную.\n",
        "\n",
        "\n",
        "\n",
        "### Матрица плана с базисными функциями\n",
        "\n",
        "При использовании базисных функций структура матрицы плана изменяется. В этом случае каждый признак может быть представлен нелинейным преобразованием, что позволяет модели захватывать более сложные зависимости между признаками и целевой переменной. Матрица плана с базисными функциями обозначается как $ \\Phi(x) $ и имеет вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\dots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\dots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\dots & \\phi_p(x_n)\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi_j(x_i) $ — значение базисной функции $ j $ для наблюдения $ i $,\n",
        "- $ \\phi_0(x_i) = 1 $ — свободный член модели.\n",
        "\n",
        "\n",
        "\n",
        "### Примеры матрицы плана с базисными функциями\n",
        "\n",
        "#### 1. Полиномиальные базисные функции\n",
        "\n",
        "Для полиномиальной регрессии базисные функции представляют собой степени исходной переменной $ x $. Матрица плана принимает вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\dots & x_1^p \\\\\n",
        "1 & x_2 & x_2^2 & \\dots & x_2^p \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & \\dots & x_n^p\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь каждая строка содержит значения степеней переменной $ x $ для соответствующего наблюдения.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Радиально-базисные функции (RBF)\n",
        "\n",
        "Радиально-базисные функции (Radial Basis Functions, RBF) используются для моделирования локальных зависимостей. Каждая базисная функция фокусируется на определённом центре $ \\mu_j $, и матрица плана принимает вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "e^{-\\frac{(x_1 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_1 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_1 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "e^{-\\frac{(x_2 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_2 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_2 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "e^{-\\frac{(x_n - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_n - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_n - \\mu_p)^2}{2\\sigma^2}}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ \\mu_j $ — центры базисных функций,\n",
        "- $ \\sigma $ — параметр ширины, регулирующий \"разброс\" функций.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HpSNPVehSJLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Функция правдоподобия для одного наблюдения**\n",
        "\n",
        "Функция правдоподобия $\\mathcal{L}(w, \\sigma^2 | t_i, x_i)$ для одного наблюдения $t_i$ задается через плотность вероятности нормального распределения:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t_i, x_i) = p(t_i | x_i, w, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}\\right),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $p(t_i | x_i, w, \\sigma^2)$ — условная плотность вероятности,\n",
        "- $\\mathcal{L}(w, \\sigma^2 | t_i, x_i)$ — функция правдоподобия, рассматриваемая как функция параметров $w$ и $\\sigma^2$.\n",
        "\n",
        "Здесь:\n",
        "- $t_i$ — наблюдаемое значение целевой переменной для объекта $x_i$,\n",
        "- $y(x_i, w) = w^T \\phi(x_i)$ — предсказанное значение модели,\n",
        "- $\\phi(x_i)$ — вектор значений базисных функций для объекта $x_i$,\n",
        "- $\\sigma^2$ — дисперсия шума.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **2. Полная функция правдоподобия**\n",
        "\n",
        "Для выборки из $n$ независимых наблюдений полная функция правдоподобия определяется как произведение правдоподобий для каждого наблюдения:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = \\prod_{i=1}^{n} p(t_i | x_i, w, \\sigma^2).\n",
        "$$\n",
        "\n",
        "Подставляя выражение для $p(t_i | x_i, w, \\sigma^2)$, получаем:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "Выражение можно переписать в компактной форме:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2\\right).\n",
        "$$\n",
        "\n",
        "Обозначим сумму квадратов ошибок через $E(w)$:\n",
        "\n",
        "$$\n",
        "E(w) = \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2 = \\| t - \\Phi w \\|^2,\n",
        "$$\n",
        "\n",
        "где $\\Phi$ — матрица базисных функций размерности $n \\times (p+1)$. Тогда функция правдоподобия принимает вид:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{E(w)}{2\\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **3. Логарифмическая функция правдоподобия**\n",
        "\n",
        "Для упрощения анализа и оптимизации параметров модели максимизируется логарифм функции правдоподобия. Логарифмируем $\\mathcal{L}(w, \\sigma^2 | t, X)$:\n",
        "\n",
        "$$\n",
        "\\ln \\mathcal{L}(w, \\sigma^2 | t, X) = \\ln \\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{E(w)}{2\\sigma^2}\\right) \\right).\n",
        "$$\n",
        "\n",
        "Разложим логарифм на слагаемые:\n",
        "\n",
        "$$\n",
        "\\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{E(w)}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "Таким образом, логарифмическая функция правдоподобия имеет вид:\n",
        "\n",
        "$$\n",
        "\\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{E(w)}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **4. Оптимизация параметров модели**\n",
        "\n",
        "#### **4.1. Оптимальные веса $w$**\n",
        "\n",
        "Первые два слагаемых ($-\\frac{n}{2} \\ln(2\\pi)$ и $-\\frac{n}{2} \\ln(\\sigma^2)$) не зависят от $w$, поэтому задача максимизации логарифмической функции правдоподобия эквивалентна минимизации третьего слагаемого:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{E(w)}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "Если $\\sigma^2$ фиксировано, то множитель $\\frac{1}{2\\sigma^2}$ можно игнорировать, и задача сводится к минимизации функции потерь:\n",
        "\n",
        "$$\n",
        "E(w) = \\| t - \\Phi w \\|^2 = \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "Аналитическое решение для $w$ находится из условия $\\nabla_w E(w) = 0$:\n",
        "\n",
        "$$\n",
        "\\hat{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T t.\n",
        "$$\n",
        "\n",
        "#### **4.2. Оптимальная дисперсия $\\sigma^2$**\n",
        "\n",
        "Для нахождения оптимального значения $\\sigma^2$ максимизируем логарифмическую функцию правдоподобия по $\\sigma^2$. Берем производную по $\\sigma^2$ и приравниваем её к нулю:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\sigma^2} \\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2\\sigma^2} + \\frac{E(w)}{2(\\sigma^2)^2} = 0.\n",
        "$$\n",
        "\n",
        "Упрощая:\n",
        "\n",
        "$$\n",
        "\\frac{n}{\\sigma^2} = \\frac{E(w)}{(\\sigma^2)^2}.\n",
        "$$\n",
        "\n",
        "Отсюда:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{E(\\hat{w})}{n} = \\frac{1}{n} \\sum_{i=1}^{n} (t_i - \\hat{w}^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **5. Итоговые результаты**\n",
        "\n",
        "1. **Функция правдоподобия для одного наблюдения**:\n",
        "   $$\n",
        "   \\mathcal{L}(w, \\sigma^2 | t_i, x_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}\\right).\n",
        "   $$\n",
        "\n",
        "2. **Полная функция правдоподобия**:\n",
        "   $$\n",
        "   \\mathcal{L}(w, \\sigma^2 | t, X) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{\\| t - \\Phi w \\|^2}{2\\sigma^2}\\right).\n",
        "   $$\n",
        "\n",
        "3. **Логарифмическая функция правдоподобия**:\n",
        "   $$\n",
        "   \\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{\\| t - \\Phi w \\|^2}{2\\sigma^2}.\n",
        "   $$\n",
        "\n",
        "4. **Оптимальные параметры модели**:\n",
        "   - Вектор параметров $w$:\n",
        "$$\n",
        "     \\hat{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T t.\n",
        "$$\n",
        "   - Дисперсия шума $\\sigma^2$:\n",
        "$$\n",
        "     \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (t_i - \\hat{w}^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "c7uWB3qYCCKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Градиент функции потерь в задаче линейной регрессии с базисными функциями**\n",
        "\n",
        "#### **1. Постановка задачи регрессии**\n",
        "\n",
        "Пусть целевая переменная $ t $ для входного вектора $ x $ описывается детерминированной функцией $ y(x, w) $ с добавлением гауссовского шума $ \\varepsilon $:\n",
        "\n",
        "$$\n",
        "t = y(x, w) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ y(x, w) $ — детерминированная функция, зависящая от параметров модели $ w $,\n",
        "- $ \\varepsilon $ — случайный шум, распределённый нормально с нулевым средним и дисперсией $ \\sigma^2 $.\n",
        "\n",
        "Модель предполагает линейную зависимость между параметрами $ w $ и значениями базисных функций $ \\phi_j(x) $. Таким образом, функция $ y(x, w) $ записывается как:\n",
        "\n",
        "$$\n",
        "y(x, w) = w^T \\phi(x),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi(x) = [\\phi_0(x), \\phi_1(x), \\dots, \\phi_p(x)]^T $ — вектор значений базисных функций для объекта $ x $,\n",
        "- $ w = [w_0, w_1, \\dots, w_p]^T $ — вектор параметров модели.\n",
        "\n",
        "Следовательно, модель принимает вид:\n",
        "\n",
        "$$\n",
        "t = w^T \\phi(x) + \\varepsilon.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **2. Матрица базисных функций**\n",
        "\n",
        "Для набора данных, состоящего из $ N $ объектов $ \\{x_1, x_2, \\dots, x_N\\} $, вводится матрица базисных функций $ \\mathbf{\\Phi} $ размерности $ N \\times (p+1) $:\n",
        "\n",
        "$$\n",
        "\\mathbf{\\Phi} =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_N) & \\phi_1(x_N) & \\phi_2(x_N) & \\ldots & \\phi_p(x_N)\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь строки соответствуют объектам данных ($ x_1, x_2, \\dots, x_N $), а столбцы — значениям базисных функций ($ \\phi_0, \\phi_1, \\dots, \\phi_p $).\n",
        "\n",
        "Для каждого объекта $ x_i $ вектор значений базисных функций обозначается как $ \\phi(x_i) $:\n",
        "\n",
        "$$\n",
        "\\phi(x_i) = [\\phi_0(x_i), \\phi_1(x_i), \\dots, \\phi_p(x_i)]^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **3. Функция потерь**\n",
        "\n",
        "Целевая функция $ E(w) $ минимизирует квадратичную ошибку между истинными значениями $ t_i $ и предсказанными значениями $ w^T \\phi(x_i) $ для всех объектов данных:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ t_i $ — истинное значение целевой переменной для объекта $ i $,\n",
        "- $ w^T \\phi(x_i) $ — предсказанное значение для объекта $ x_i $,\n",
        "- коэффициент $ \\frac{1}{2} $ добавлен для удобства дифференцирования.\n",
        "\n",
        "\n",
        "\n",
        "#### **4. Градиент функции потерь**\n",
        "\n",
        "Градиент функции потерь $ E(w) $ по вектору параметров $ w $ определяется как вектор частных производных:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\begin{bmatrix}\n",
        "\\frac{\\partial E(w)}{\\partial w_0} \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_1} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_p}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Вычислим каждую компоненту $ \\frac{\\partial E(w)}{\\partial w_k} $.\n",
        "\n",
        "\n",
        "\n",
        "#### **5. Расчет производной $ \\frac{\\partial E(w)}{\\partial w_k} $**\n",
        "\n",
        "##### a) Исходное выражение для $ E(w) $:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2.\n",
        "$$\n",
        "\n",
        "Производная по $ w_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\left( \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2 \\right).\n",
        "$$\n",
        "\n",
        "Применяем правило дифференцирования сложной функции:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\cdot \\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right).\n",
        "$$\n",
        "\n",
        "Упрощаем коэффициент $ \\frac{1}{2} \\cdot 2 = 1 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\cdot \\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right).\n",
        "$$\n",
        "\n",
        "##### b) Производная внутреннего выражения:\n",
        "\n",
        "Внутреннее выражение:\n",
        "\n",
        "$$\n",
        "t_i - w^T \\phi(x_i) = t_i - \\sum_{j=0}^{p} w_j \\phi_j(x_i).\n",
        "$$\n",
        "\n",
        "Производная по $ w_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right) = \\frac{\\partial}{\\partial w_k} \\left( t_i - \\sum_{j=0}^{p} w_j \\phi_j(x_i) \\right).\n",
        "$$\n",
        "\n",
        "Так как $ t_i $ не зависит от $ w_k $, а производная суммы $ \\sum_{j=0}^{p} w_j \\phi_j(x_i) $ по $ w_k $ равна $ \\phi_k(x_i) $, получаем:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right) = -\\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Подставляем это в выражение для $ \\frac{\\partial E(w)}{\\partial w_k} $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\cdot (-\\phi_k(x_i)).\n",
        "$$\n",
        "\n",
        "Выносим минус за сумму:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = -\\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Перепишем разность $ t_i - w^T \\phi(x_i) $ как $ -(w^T \\phi(x_i) - t_i) $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **6. Переход к векторной форме**\n",
        "\n",
        "Собираем все частные производные $ \\frac{\\partial E(w)}{\\partial w_k} $ в один вектор $ \\nabla_w E $. Заметим, что:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\phi_k(x_i),\n",
        "$$\n",
        "\n",
        "где $ \\phi_k(x_i) $ — это $ k $-й элемент вектора $ \\phi(x_i) $. Таким образом, можно записать:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i).\n",
        "$$\n",
        "\n",
        "Здесь $ \\phi(x_i) $ — вектор значений базисных функций для объекта $ x_i $, а $ w^T \\phi(x_i) - t_i $ — скалярное значение ошибки для этого объекта. Умножение на $ \\phi(x_i) $ даёт вклад в градиент для каждого параметра $ w_k $.\n",
        "\n",
        "\n",
        "\n",
        "#### **7. Итоговый результат**\n",
        "\n",
        "Градиент функции потерь $ E(w) $ имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i).}\n",
        "$$\n",
        "\n",
        "Это выражение позволяет эффективно вычислять градиент для задачи многомерной линейной регрессии с использованием базисных функций."
      ],
      "metadata": {
        "id": "9UxypVDgBv3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Условие минимума функции потерь\n",
        "\n",
        "Для нахождения вектора параметров $ w $, минимизирующего функцию потерь $ E(w) $, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = 0,\n",
        "$$\n",
        "\n",
        "где $ \\nabla_w E $ — градиент функции потерь по параметрам $ w $. Транспонирование обеих частей уравнения не изменяет его смысла, так как нулевой вектор остается инвариантным относительно операции транспонирования:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = 0^T.\n",
        "$$\n",
        "\n",
        "Таким образом, условие минимума может быть записано в эквивалентной форме:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = 0.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Развернутая форма градиента\n",
        "\n",
        "Градиент функции потерь $ E(w) $ выражается через базисные функции $ \\phi(x_i) $ и целевые значения $ t_i $ следующим образом:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi(x_i) $ — вектор значений базисных функций для объекта $ x_i $,\n",
        "- $ w^T \\phi(x_i) $ — предсказанное значение модели для объекта $ x_i $,\n",
        "- $ t_i $ — истинное значение целевой переменной для объекта $ x_i $.\n",
        "\n",
        "При транспонировании градиента получаем:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i)^T.\n",
        "$$\n",
        "\n",
        "Разложим это выражение на два слагаемых:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = w^T \\cdot \\sum_{i=1}^{N} \\phi(x_i) \\cdot \\phi(x_i)^T - \\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Анализ первого слагаемого: $\\sum_{i=1}^{N} \\phi(x_i) \\cdot \\phi(x_i)^T$\n",
        "\n",
        "Произведение вектора-столбца $ \\phi(x_i) $ на вектор-строку $ \\phi(x_i)^T $ дает квадратную матрицу размерности $ (p+1) \\times (p+1) $, где $ p+1 $ — количество базисных функций. Элементы этой матрицы определяются как:\n",
        "\n",
        "$$\n",
        "\\phi(x_i) \\cdot \\phi(x_i)^T =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i)^2 & \\phi_0(x_i) \\phi_1(x_i) & \\ldots & \\phi_0(x_i) \\phi_p(x_i) \\\\\n",
        "\\phi_1(x_i) \\phi_0(x_i) & \\phi_1(x_i)^2 & \\ldots & \\phi_1(x_i) \\phi_p(x_i) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_p(x_i) \\phi_0(x_i) & \\phi_p(x_i) \\phi_1(x_i) & \\ldots & \\phi_p(x_i)^2\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Суммирование данных матриц по всем объектам $ i $ дает матрицу $ \\mathbf{\\Phi}^T \\mathbf{\\Phi} $, где $ \\mathbf{\\Phi} $ — матрица базисных функций размерности $ N \\times (p+1) $.\n",
        "\n",
        "\n",
        "\n",
        "### Анализ второго слагаемого: $\\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T$\n",
        "\n",
        "Второе слагаемое представляет собой линейную комбинацию строк матрицы $ \\mathbf{\\Phi} $ с коэффициентами $ t_i $. В явном виде:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T = (\\sum_{i=1}^{N} t_i \\cdot \\phi_0(x_i), \\sum_{i=1}^{N} t_i \\cdot \\phi_1(x_i), \\ldots).\n",
        "$$\n",
        "\n",
        "Это выражение может быть записано в матричной форме как:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T = \\mathbf{t}^T \\mathbf{\\Phi},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\mathbf{t} $ — вектор целевых значений размерности $ N \\times 1 $,\n",
        "- $ \\mathbf{\\Phi} $ — матрица базисных функций размерности $ N \\times (p+1) $.\n",
        "\n",
        "\n",
        "\n",
        "### Подстановка выражений в градиент\n",
        "\n",
        "Подставляя результаты анализа в выражение для транспонированного градиента, получаем:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = w^T \\cdot \\mathbf{\\Phi}^T \\mathbf{\\Phi} - \\mathbf{t}^T \\mathbf{\\Phi}.\n",
        "$$\n",
        "\n",
        "Приравнивая градиент к нулю:\n",
        "\n",
        "$$\n",
        "w^T \\cdot \\mathbf{\\Phi}^T \\mathbf{\\Phi} = \\mathbf{t}^T \\mathbf{\\Phi}.\n",
        "$$\n",
        "\n",
        "Умножая обе части уравнения справа на $ (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} $, получаем:\n",
        "\n",
        "$$\n",
        "w^T = \\mathbf{t}^T \\mathbf{\\Phi} (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Применение свойств транспонирования\n",
        "\n",
        "Для перехода от транспонированной формы $ w^T $ к исходному вектору $ w $, транспонируем обе части уравнения:\n",
        "\n",
        "$$\n",
        "(w^T)^T = \\left( \\mathbf{t}^T \\mathbf{\\Phi} (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\right)^T.\n",
        "$$\n",
        "\n",
        "Используя свойства транспонирования:\n",
        "1. $(A B)^T = B^T A^T$,\n",
        "2. $(A^{-1})^T = (A^T)^{-1}$,\n",
        "\n",
        "получаем:\n",
        "\n",
        "$$\n",
        "w = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T \\mathbf{t}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итоговое выражение\n",
        "\n",
        "Окончательное выражение для вектора параметров $ w $, минимизирующего функцию потерь $ E(w) $, имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{w = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T \\mathbf{t}.}\n",
        "$$\n",
        "\n",
        "Это выражение известно как **нормальное уравнение** и представляет собой аналитическое решение задачи линейной регрессии с базисными функциями.\n"
      ],
      "metadata": {
        "id": "K9zKCIWfBwBo"
      }
    }
  ]
}