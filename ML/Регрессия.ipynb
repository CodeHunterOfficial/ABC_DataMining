{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO5YKmwRklixMR4Mvq8Cqpq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/%D0%A0%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Простая линейная регрессия: Математическая модель и метод наименьших квадратов\n",
        "\n",
        "#### 1. Введение\n",
        "\n",
        "Простая линейная регрессия является одним из базовых инструментов статистического анализа, позволяющим моделировать линейную зависимость между одной независимой переменной $x$ (предиктором) и зависимой переменной $y$ (откликом). Модель простой линейной регрессии описывается следующим уравнением:\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1 x + \\varepsilon,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w_0$ — свободный член (intercept), представляющий значение $y$, когда $x = 0$;\n",
        "- $w_1$ — коэффициент наклона (slope), характеризующий изменение $y$ при единичном изменении $x$;\n",
        "- $\\varepsilon$ — случайная ошибка (residual), отражающая отклонение фактических значений $y$ от предсказанных моделью.\n",
        "\n",
        "Цель простой линейной регрессии заключается в определении параметров $w_0$ и $w_1$, которые минимизируют отклонение предсказанных значений от фактических данных. Для этого используется метод наименьших квадратов (Least Squares Method).\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Функция потерь (Mean Squared Error)\n",
        "\n",
        "Для оценки качества модели применяется функция потерь, которая измеряет среднеквадратичное отклонение предсказанных значений от фактических. В случае простой линейной регрессии функция потерь имеет вид:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{n} \\left( y_i - (w_0 + w_1 x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $E(w)$ — функция потерь;\n",
        "- $n$ — количество наблюдений в выборке;\n",
        "- $y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения;\n",
        "- $x_i$ — значение независимой переменной для $i$-го наблюдения.\n",
        "\n",
        "Коэффициент $\\frac{1}{2}$ добавлен для удобства дифференцирования при последующей оптимизации.\n",
        "\n",
        "Функция потерь $E(w)$ характеризует общую ошибку модели и стремится к минимуму при подборе параметров $w_0$ и $w_1$. Это достигается путем вычисления частных производных функции потерь по каждому параметру и их приравнивания к нулю.\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Оптимизация методом наименьших квадратов\n",
        "\n",
        "Метод наименьших квадратов позволяет найти такие значения параметров $w_0$ и $w_1$, которые минимизируют функцию потерь. Для этого вычисляются частные производные функции $E(w)$ по $w_0$ и $w_1$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_0} = -\\sum_{i=1}^{n} \\left( y_i - (w_0 + w_1 x_i) \\right),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_1} = -\\sum_{i=1}^{n} x_i \\left( y_i - (w_0 + w_1 x_i) \\right).\n",
        "$$\n",
        "\n",
        "Приравнивая эти производные к нулю, получаем систему нормальных уравнений:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} y_i = n w_0 + w_1 \\sum_{i=1}^{n} x_i,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} x_i y_i = w_0 \\sum_{i=1}^{n} x_i + w_1 \\sum_{i=1}^{n} x_i^2.\n",
        "$$\n",
        "\n",
        "Решение данной системы позволяет найти оптимальные значения параметров $w_0$ и $w_1$:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{n \\sum_{i=1}^{n} x_i y_i - \\left( \\sum_{i=1}^{n} x_i \\right) \\left( \\sum_{i=1}^{n} y_i \\right)}{n \\sum_{i=1}^{n} x_i^2 - \\left( \\sum_{i=1}^{n} x_i \\right)^2},\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_0 = \\frac{\\sum_{i=1}^{n} y_i - w_1 \\left( \\sum_{i=1}^{n} x_i \\right)}{n}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 4. Упрощение через ковариацию и дисперсию\n",
        "\n",
        "Для более компактного представления формулы для $w_1$ и $w_0$ могут быть выражены через ковариацию и дисперсию. Ковариация $ \\text{Cov}(x, y) $ и дисперсия $ \\text{Var}(x) $ определяются как:\n",
        "\n",
        "$$\n",
        "\\text{Cov}(x, y) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}(x) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2,\n",
        "$$\n",
        "\n",
        "где $\\bar{x}$ и $\\bar{y}$ — выборочные средние:\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i.\n",
        "$$\n",
        "\n",
        "Тогда коэффициент наклона $w_1$ выражается через ковариацию и дисперсию:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}.\n",
        "$$\n",
        "\n",
        "Свободный член $w_0$ вычисляется как:\n",
        "\n",
        "$$\n",
        "w_0 = \\bar{y} - w_1 \\bar{x}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 5. Интерпретация формул\n",
        "\n",
        "1. **Наклон ($w_1$):**\n",
        "   - Коэффициент $w_1$ характеризует силу и направление линейной зависимости между переменными $x$ и $y$. Он пропорционален ковариации между $x$ и $y$, что отражает их совместную вариацию.\n",
        "   - Деление на дисперсию $x$ нормализует эту зависимость, обеспечивая корректное масштабирование.\n",
        "\n",
        "2. **Свободный член ($w_0$):**\n",
        "   - Свободный член гарантирует, что линия регрессии проходит через центр масс данных $(\\bar{x}, \\bar{y})$.\n",
        "\n"
      ],
      "metadata": {
        "id": "_-B3swD2HqPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Построение функции $ y(x, w) $\n",
        "\n",
        "Задача линейной регрессии заключается в построении модели, описывающей зависимость целевой переменной $ y $ от входных признаков $ x $. Модель представляется в виде функции:\n",
        "\n",
        "$$\n",
        "y(x, w) \\to \\mathbb{R},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ w = (w_0, w_1, \\ldots, w_D)^T $ — вектор параметров модели,\n",
        "- $ x = (x_1, x_2, \\ldots, x_D)^T $ — вектор входных признаков.\n",
        "\n",
        "Наиболее распространённая форма линейной модели имеет вид:\n",
        "\n",
        "$$\n",
        "y(x, w) = w_0 + \\sum_{i=1}^D w_i x_i,\n",
        "$$\n",
        "\n",
        "или в векторной форме:\n",
        "\n",
        "$$\n",
        "y(x, w) = w^T \\phi(x),\n",
        "$$\n",
        "\n",
        "где $ \\phi(x) = (1, x_1, x_2, \\ldots, x_D)^T $ — вектор базисных функций, включающий свободный член $ w_0 $.\n",
        "\n",
        "\n",
        "\n",
        "### Базисные функции (Basis Functions)\n",
        "\n",
        "Базисные функции позволяют расширить пространство признаков и улучшить способность модели описывать сложные зависимости между входными данными и целевой переменной. Введение базисных функций особенно полезно, когда исходные данные не подчиняются простой линейной зависимости.\n",
        "\n",
        "Модель линейной регрессии с базисными функциями записывается как:\n",
        "\n",
        "$$\n",
        "y(x, w) = w_0 + \\sum_{j=1}^{M-1} w_j \\phi_j(x),\n",
        "$$\n",
        "\n",
        "или в векторной форме:\n",
        "\n",
        "$$\n",
        "y(x, w) = w^T \\phi(x),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi(x) = (\\phi_0(x), \\phi_1(x), \\ldots, \\phi_{M-1}(x))^T $ — вектор базисных функций,\n",
        "- $ w = (w_0, w_1, \\ldots, w_{M-1})^T $ — вектор параметров модели.\n",
        "\n",
        "\n",
        "\n",
        "#### Зачем нужны базисные функции?\n",
        "\n",
        "Использование базисных функций решает следующие задачи:\n",
        "1. **Моделирование сложных зависимостей.** Базисные функции позволяют учитывать нелинейные взаимосвязи между входными признаками и целевой переменной.\n",
        "2. **Повышение точности предсказаний.** Расширение пространства признаков увеличивает гибкость модели, что улучшает её способность аппроксимировать данные.\n",
        "3. **Снижение ошибки модели.** Базисные функции помогают минимизировать ошибку за счёт более точного описания закономерностей в данных.\n",
        "4. **Улучшение обобщающей способности.** Модель становится более устойчивой к изменениям в данных, особенно если зависимости носят нелинейный характер.\n",
        "\n",
        "\n",
        "\n",
        "#### Примеры базисных функций\n",
        "\n",
        "Рассмотрим несколько типов базисных функций, которые могут быть использованы в линейной регрессии:\n",
        "\n",
        "1. **Полиномиальные базисные функции:**\n",
        "   Полиномиальные функции добавляют степени исходных признаков для моделирования нелинейных зависимостей:\n",
        "   $$\n",
        "   \\phi(x) = (1, x, x^2, x^3, \\ldots, x^K).\n",
        "   $$\n",
        "\n",
        "2. **Радиально-базисные функции (RBF):**\n",
        "   RBF сосредоточены вокруг определённых центров $ \\mu_j $ и имеют форму Гауссовых функций:\n",
        "   $$\n",
        "   \\phi(x) = \\left( e^{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}}, e^{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}}, \\ldots \\right).\n",
        "   $$\n",
        "\n",
        "3. **Сплайн-функции:**\n",
        "   Сплайны разделяют пространство признаков на интервалы и строят полиномы для каждого интервала, плавно соединяя их:\n",
        "   $$\n",
        "   \\phi(x) =\n",
        "   \\begin{cases}\n",
        "      p_1(x), & x < k_1, \\\\\n",
        "      p_2(x), & k_1 \\leq x < k_2, \\\\\n",
        "      \\dots, \\\\\n",
        "      p_n(x), & x \\geq k_{n-1}.\n",
        "   \\end{cases}\n",
        "   $$\n",
        "\n",
        "4. **Синусоидальные базисные функции:**\n",
        "   Эти функции полезны для моделирования периодических процессов:\n",
        "   $$\n",
        "   \\phi(x) = (\\sin(\\omega_1 x), \\cos(\\omega_1 x), \\sin(\\omega_2 x), \\cos(\\omega_2 x), \\ldots).\n",
        "   $$\n",
        "\n",
        "5. **Логистические базисные функции:**\n",
        "   Логистические функции подходят для задач с пороговыми эффектами:\n",
        "   $$\n",
        "   \\phi(x) = \\frac{1}{1 + e^{-(x - \\mu)/\\sigma}}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Матрица плана (Design Matrix)\n",
        "\n",
        "Матрица плана, обозначаемая как $ X $, является ключевым элементом в построении линейной регрессии. Она представляет собой структурированную таблицу значений всех независимых переменных (признаков) для каждого наблюдения в данных. Каждая строка матрицы соответствует одному наблюдению, а каждый столбец — значению конкретного признака или его преобразования.\n",
        "\n",
        "\n",
        "\n",
        "### Структура матрицы плана\n",
        "\n",
        "Для классической линейной регрессии матрица плана $ X $ имеет размерность $ n \\times (p+1) $, где:\n",
        "- $ n $ — количество наблюдений,\n",
        "- $ p $ — количество исходных признаков (без учёта свободного члена).\n",
        "\n",
        "Стандартная форма матрицы плана выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- Первый столбец состоит из единиц и соответствует свободному члену $ w_0 $ (bias term),\n",
        "- Остальные столбцы содержат значения исходных признаков для каждого наблюдения.\n",
        "\n",
        "\n",
        "\n",
        "### Функции и задачи матрицы плана\n",
        "\n",
        "Матрица плана играет центральную роль в линейной регрессии, обеспечивая удобство вычислений с использованием методов линейной алгебры. Основные её функции включают:\n",
        "\n",
        "1. **Оценка параметров модели:**\n",
        "   Параметры модели $ w $ вычисляются с помощью метода наименьших квадратов (Least Squares Method). Формула для оценки параметров имеет вид:\n",
        "   $$\n",
        "   \\hat{w} = (X^T X)^{-1} X^T y,\n",
        "   $$\n",
        "   где:\n",
        "   - $ X^T $ — транспонированная матрица плана,\n",
        "   - $ y $ — вектор значений целевой переменной,\n",
        "   - $ (X^T X)^{-1} $ — обратная матрица произведения $ X^T X $.\n",
        "\n",
        "2. **Учёт множества факторов:**\n",
        "   Матрица плана позволяет модели учитывать несколько факторов одновременно, что увеличивает её гибкость и способность анализировать влияние различных переменных на целевую величину.\n",
        "\n",
        "3. **Проведение статистических тестов:**\n",
        "   Через матрицу плана можно проводить статистические тесты для оценки значимости коэффициентов модели. Это помогает определить, какие факторы оказывают существенное влияние на целевую переменную.\n",
        "\n",
        "\n",
        "\n",
        "### Матрица плана с базисными функциями\n",
        "\n",
        "При использовании базисных функций структура матрицы плана изменяется. В этом случае каждый признак может быть представлен нелинейным преобразованием, что позволяет модели захватывать более сложные зависимости между признаками и целевой переменной. Матрица плана с базисными функциями обозначается как $ \\Phi(x) $ и имеет вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\dots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\dots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\dots & \\phi_p(x_n)\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi_j(x_i) $ — значение базисной функции $ j $ для наблюдения $ i $,\n",
        "- $ \\phi_0(x_i) = 1 $ — свободный член модели.\n",
        "\n",
        "\n",
        "\n",
        "### Примеры матрицы плана с базисными функциями\n",
        "\n",
        "#### 1. Полиномиальные базисные функции\n",
        "\n",
        "Для полиномиальной регрессии базисные функции представляют собой степени исходной переменной $ x $. Матрица плана принимает вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\dots & x_1^p \\\\\n",
        "1 & x_2 & x_2^2 & \\dots & x_2^p \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & \\dots & x_n^p\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь каждая строка содержит значения степеней переменной $ x $ для соответствующего наблюдения.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Радиально-базисные функции (RBF)\n",
        "\n",
        "Радиально-базисные функции (Radial Basis Functions, RBF) используются для моделирования локальных зависимостей. Каждая базисная функция фокусируется на определённом центре $ \\mu_j $, и матрица плана принимает вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "e^{-\\frac{(x_1 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_1 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_1 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "e^{-\\frac{(x_2 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_2 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_2 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "e^{-\\frac{(x_n - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_n - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_n - \\mu_p)^2}{2\\sigma^2}}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ \\mu_j $ — центры базисных функций,\n",
        "- $ \\sigma $ — параметр ширины, регулирующий \"разброс\" функций.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества матрицы плана с базисными функциями\n",
        "\n",
        "Использование матрицы плана с базисными функциями даёт несколько преимуществ:\n",
        "\n",
        "1. **Моделирование нелинейных зависимостей.** Преобразованные признаки позволяют регрессионной модели захватывать нелинейные зависимости между факторами и целевой переменной, делая её более гибкой и подходящей для моделирования сложных данных.\n",
        "\n",
        "2. **Расширение возможностей анализа.** Базисные функции позволяют строить более сложные и точные модели, которые можно адаптировать для задач с различными типами данных и зависимостей.\n",
        "\n",
        "3. **Повышение точности модели.** С добавлением базисных функций модель способна лучше подстраиваться под закономерности в данных, улучшая точность прогнозирования и обобщающую способность.\n",
        "\n",
        "Таким образом, матрица плана с базисными функциями является мощным инструментом при построении моделей линейной регрессии, позволяя учитывать различные нелинейные преобразования исходных данных и повышать качество модели за счёт лучшего описания сложных зависимостей.\n"
      ],
      "metadata": {
        "id": "HpSNPVehSJLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Функция правдоподобия для одного наблюдения**\n",
        "\n",
        "Функция правдоподобия $\\mathcal{L}(w, \\sigma^2 | t_i, x_i)$ для одного наблюдения $t_i$ задается через плотность вероятности нормального распределения:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t_i, x_i) = p(t_i | x_i, w, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}\\right),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $p(t_i | x_i, w, \\sigma^2)$ — условная плотность вероятности,\n",
        "- $\\mathcal{L}(w, \\sigma^2 | t_i, x_i)$ — функция правдоподобия, рассматриваемая как функция параметров $w$ и $\\sigma^2$.\n",
        "\n",
        "Здесь:\n",
        "- $t_i$ — наблюдаемое значение целевой переменной для объекта $x_i$,\n",
        "- $y(x_i, w) = w^T \\phi(x_i)$ — предсказанное значение модели,\n",
        "- $\\phi(x_i)$ — вектор значений базисных функций для объекта $x_i$,\n",
        "- $\\sigma^2$ — дисперсия шума.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **2. Полная функция правдоподобия**\n",
        "\n",
        "Для выборки из $n$ независимых наблюдений полная функция правдоподобия определяется как произведение правдоподобий для каждого наблюдения:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = \\prod_{i=1}^{n} p(t_i | x_i, w, \\sigma^2).\n",
        "$$\n",
        "\n",
        "Подставляя выражение для $p(t_i | x_i, w, \\sigma^2)$, получаем:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "Выражение можно переписать в компактной форме:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2\\right).\n",
        "$$\n",
        "\n",
        "Обозначим сумму квадратов ошибок через $E(w)$:\n",
        "\n",
        "$$\n",
        "E(w) = \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2 = \\| t - \\Phi w \\|^2,\n",
        "$$\n",
        "\n",
        "где $\\Phi$ — матрица базисных функций размерности $n \\times (p+1)$. Тогда функция правдоподобия принимает вид:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w, \\sigma^2 | t, X) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{E(w)}{2\\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **3. Логарифмическая функция правдоподобия**\n",
        "\n",
        "Для упрощения анализа и оптимизации параметров модели максимизируется логарифм функции правдоподобия. Логарифмируем $\\mathcal{L}(w, \\sigma^2 | t, X)$:\n",
        "\n",
        "$$\n",
        "\\ln \\mathcal{L}(w, \\sigma^2 | t, X) = \\ln \\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{E(w)}{2\\sigma^2}\\right) \\right).\n",
        "$$\n",
        "\n",
        "Разложим логарифм на слагаемые:\n",
        "\n",
        "$$\n",
        "\\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{E(w)}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "Таким образом, логарифмическая функция правдоподобия имеет вид:\n",
        "\n",
        "$$\n",
        "\\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{E(w)}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **4. Оптимизация параметров модели**\n",
        "\n",
        "#### **4.1. Оптимальные веса $w$**\n",
        "\n",
        "Первые два слагаемых ($-\\frac{n}{2} \\ln(2\\pi)$ и $-\\frac{n}{2} \\ln(\\sigma^2)$) не зависят от $w$, поэтому задача максимизации логарифмической функции правдоподобия эквивалентна минимизации третьего слагаемого:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{E(w)}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "Если $\\sigma^2$ фиксировано, то множитель $\\frac{1}{2\\sigma^2}$ можно игнорировать, и задача сводится к минимизации функции потерь:\n",
        "\n",
        "$$\n",
        "E(w) = \\| t - \\Phi w \\|^2 = \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "Аналитическое решение для $w$ находится из условия $\\nabla_w E(w) = 0$:\n",
        "\n",
        "$$\n",
        "\\hat{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T t.\n",
        "$$\n",
        "\n",
        "#### **4.2. Оптимальная дисперсия $\\sigma^2$**\n",
        "\n",
        "Для нахождения оптимального значения $\\sigma^2$ максимизируем логарифмическую функцию правдоподобия по $\\sigma^2$. Берем производную по $\\sigma^2$ и приравниваем её к нулю:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\sigma^2} \\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2\\sigma^2} + \\frac{E(w)}{2(\\sigma^2)^2} = 0.\n",
        "$$\n",
        "\n",
        "Упрощая:\n",
        "\n",
        "$$\n",
        "\\frac{n}{\\sigma^2} = \\frac{E(w)}{(\\sigma^2)^2}.\n",
        "$$\n",
        "\n",
        "Отсюда:\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{E(\\hat{w})}{n} = \\frac{1}{n} \\sum_{i=1}^{n} (t_i - \\hat{w}^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **5. Итоговые результаты**\n",
        "\n",
        "1. **Функция правдоподобия для одного наблюдения**:\n",
        "   $$\n",
        "   \\mathcal{L}(w, \\sigma^2 | t_i, x_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}\\right).\n",
        "   $$\n",
        "\n",
        "2. **Полная функция правдоподобия**:\n",
        "   $$\n",
        "   \\mathcal{L}(w, \\sigma^2 | t, X) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{\\| t - \\Phi w \\|^2}{2\\sigma^2}\\right).\n",
        "   $$\n",
        "\n",
        "3. **Логарифмическая функция правдоподобия**:\n",
        "   $$\n",
        "   \\ln \\mathcal{L}(w, \\sigma^2 | t, X) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{\\| t - \\Phi w \\|^2}{2\\sigma^2}.\n",
        "   $$\n",
        "\n",
        "4. **Оптимальные параметры модели**:\n",
        "   - Вектор параметров $w$:\n",
        "$$\n",
        "     \\hat{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T t.\n",
        "$$\n",
        "   - Дисперсия шума $\\sigma^2$:\n",
        "$$\n",
        "     \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (t_i - \\hat{w}^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "c7uWB3qYCCKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Градиент функции потерь в задаче линейной регрессии с базисными функциями**\n",
        "\n",
        "#### **1. Постановка задачи регрессии**\n",
        "\n",
        "Пусть целевая переменная $ t $ для входного вектора $ x $ описывается детерминированной функцией $ y(x, w) $ с добавлением гауссовского шума $ \\varepsilon $:\n",
        "\n",
        "$$\n",
        "t = y(x, w) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ y(x, w) $ — детерминированная функция, зависящая от параметров модели $ w $,\n",
        "- $ \\varepsilon $ — случайный шум, распределённый нормально с нулевым средним и дисперсией $ \\sigma^2 $.\n",
        "\n",
        "Модель предполагает линейную зависимость между параметрами $ w $ и значениями базисных функций $ \\phi_j(x) $. Таким образом, функция $ y(x, w) $ записывается как:\n",
        "\n",
        "$$\n",
        "y(x, w) = w^T \\phi(x),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi(x) = [\\phi_0(x), \\phi_1(x), \\dots, \\phi_p(x)]^T $ — вектор значений базисных функций для объекта $ x $,\n",
        "- $ w = [w_0, w_1, \\dots, w_p]^T $ — вектор параметров модели.\n",
        "\n",
        "Следовательно, модель принимает вид:\n",
        "\n",
        "$$\n",
        "t = w^T \\phi(x) + \\varepsilon.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **2. Матрица базисных функций**\n",
        "\n",
        "Для набора данных, состоящего из $ N $ объектов $ \\{x_1, x_2, \\dots, x_N\\} $, вводится матрица базисных функций $ \\mathbf{\\Phi} $ размерности $ N \\times (p+1) $:\n",
        "\n",
        "$$\n",
        "\\mathbf{\\Phi} =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_N) & \\phi_1(x_N) & \\phi_2(x_N) & \\ldots & \\phi_p(x_N)\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь строки соответствуют объектам данных ($ x_1, x_2, \\dots, x_N $), а столбцы — значениям базисных функций ($ \\phi_0, \\phi_1, \\dots, \\phi_p $).\n",
        "\n",
        "Для каждого объекта $ x_i $ вектор значений базисных функций обозначается как $ \\phi(x_i) $:\n",
        "\n",
        "$$\n",
        "\\phi(x_i) = [\\phi_0(x_i), \\phi_1(x_i), \\dots, \\phi_p(x_i)]^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **3. Функция потерь**\n",
        "\n",
        "Целевая функция $ E(w) $ минимизирует квадратичную ошибку между истинными значениями $ t_i $ и предсказанными значениями $ w^T \\phi(x_i) $ для всех объектов данных:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ t_i $ — истинное значение целевой переменной для объекта $ i $,\n",
        "- $ w^T \\phi(x_i) $ — предсказанное значение для объекта $ x_i $,\n",
        "- коэффициент $ \\frac{1}{2} $ добавлен для удобства дифференцирования.\n",
        "\n",
        "\n",
        "\n",
        "#### **4. Градиент функции потерь**\n",
        "\n",
        "Градиент функции потерь $ E(w) $ по вектору параметров $ w $ определяется как вектор частных производных:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\begin{bmatrix}\n",
        "\\frac{\\partial E(w)}{\\partial w_0} \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_1} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_p}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Вычислим каждую компоненту $ \\frac{\\partial E(w)}{\\partial w_k} $.\n",
        "\n",
        "\n",
        "\n",
        "#### **5. Расчет производной $ \\frac{\\partial E(w)}{\\partial w_k} $**\n",
        "\n",
        "##### a) Исходное выражение для $ E(w) $:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2.\n",
        "$$\n",
        "\n",
        "Производная по $ w_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\left( \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2 \\right).\n",
        "$$\n",
        "\n",
        "Применяем правило дифференцирования сложной функции:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\cdot \\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right).\n",
        "$$\n",
        "\n",
        "Упрощаем коэффициент $ \\frac{1}{2} \\cdot 2 = 1 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\cdot \\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right).\n",
        "$$\n",
        "\n",
        "##### b) Производная внутреннего выражения:\n",
        "\n",
        "Внутреннее выражение:\n",
        "\n",
        "$$\n",
        "t_i - w^T \\phi(x_i) = t_i - \\sum_{j=0}^{p} w_j \\phi_j(x_i).\n",
        "$$\n",
        "\n",
        "Производная по $ w_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right) = \\frac{\\partial}{\\partial w_k} \\left( t_i - \\sum_{j=0}^{p} w_j \\phi_j(x_i) \\right).\n",
        "$$\n",
        "\n",
        "Так как $ t_i $ не зависит от $ w_k $, а производная суммы $ \\sum_{j=0}^{p} w_j \\phi_j(x_i) $ по $ w_k $ равна $ \\phi_k(x_i) $, получаем:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\phi(x_i) \\right) = -\\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Подставляем это в выражение для $ \\frac{\\partial E(w)}{\\partial w_k} $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\cdot (-\\phi_k(x_i)).\n",
        "$$\n",
        "\n",
        "Выносим минус за сумму:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = -\\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right) \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Перепишем разность $ t_i - w^T \\phi(x_i) $ как $ -(w^T \\phi(x_i) - t_i) $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### **6. Переход к векторной форме**\n",
        "\n",
        "Собираем все частные производные $ \\frac{\\partial E(w)}{\\partial w_k} $ в один вектор $ \\nabla_w E $. Заметим, что:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\phi_k(x_i),\n",
        "$$\n",
        "\n",
        "где $ \\phi_k(x_i) $ — это $ k $-й элемент вектора $ \\phi(x_i) $. Таким образом, можно записать:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i).\n",
        "$$\n",
        "\n",
        "Здесь $ \\phi(x_i) $ — вектор значений базисных функций для объекта $ x_i $, а $ w^T \\phi(x_i) - t_i $ — скалярное значение ошибки для этого объекта. Умножение на $ \\phi(x_i) $ даёт вклад в градиент для каждого параметра $ w_k $.\n",
        "\n",
        "\n",
        "\n",
        "#### **7. Итоговый результат**\n",
        "\n",
        "Градиент функции потерь $ E(w) $ имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i).}\n",
        "$$\n",
        "\n",
        "Это выражение позволяет эффективно вычислять градиент для задачи многомерной линейной регрессии с использованием базисных функций."
      ],
      "metadata": {
        "id": "9UxypVDgBv3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Условие минимума функции потерь\n",
        "\n",
        "Для нахождения вектора параметров $ w $, минимизирующего функцию потерь $ E(w) $, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = 0,\n",
        "$$\n",
        "\n",
        "где $ \\nabla_w E $ — градиент функции потерь по параметрам $ w $. Транспонирование обеих частей уравнения не изменяет его смысла, так как нулевой вектор остается инвариантным относительно операции транспонирования:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = 0^T.\n",
        "$$\n",
        "\n",
        "Таким образом, условие минимума может быть записано в эквивалентной форме:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = 0.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Развернутая форма градиента\n",
        "\n",
        "Градиент функции потерь $ E(w) $ выражается через базисные функции $ \\phi(x_i) $ и целевые значения $ t_i $ следующим образом:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\phi(x_i) $ — вектор значений базисных функций для объекта $ x_i $,\n",
        "- $ w^T \\phi(x_i) $ — предсказанное значение модели для объекта $ x_i $,\n",
        "- $ t_i $ — истинное значение целевой переменной для объекта $ x_i $.\n",
        "\n",
        "При транспонировании градиента получаем:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = \\sum_{i=1}^{N} \\left( w^T \\phi(x_i) - t_i \\right) \\cdot \\phi(x_i)^T.\n",
        "$$\n",
        "\n",
        "Разложим это выражение на два слагаемых:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = w^T \\cdot \\sum_{i=1}^{N} \\phi(x_i) \\cdot \\phi(x_i)^T - \\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Анализ первого слагаемого: $\\sum_{i=1}^{N} \\phi(x_i) \\cdot \\phi(x_i)^T$\n",
        "\n",
        "Произведение вектора-столбца $ \\phi(x_i) $ на вектор-строку $ \\phi(x_i)^T $ дает квадратную матрицу размерности $ (p+1) \\times (p+1) $, где $ p+1 $ — количество базисных функций. Элементы этой матрицы определяются как:\n",
        "\n",
        "$$\n",
        "\\phi(x_i) \\cdot \\phi(x_i)^T =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i)^2 & \\phi_0(x_i) \\phi_1(x_i) & \\ldots & \\phi_0(x_i) \\phi_p(x_i) \\\\\n",
        "\\phi_1(x_i) \\phi_0(x_i) & \\phi_1(x_i)^2 & \\ldots & \\phi_1(x_i) \\phi_p(x_i) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_p(x_i) \\phi_0(x_i) & \\phi_p(x_i) \\phi_1(x_i) & \\ldots & \\phi_p(x_i)^2\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Суммирование данных матриц по всем объектам $ i $ дает матрицу $ \\mathbf{\\Phi}^T \\mathbf{\\Phi} $, где $ \\mathbf{\\Phi} $ — матрица базисных функций размерности $ N \\times (p+1) $.\n",
        "\n",
        "\n",
        "\n",
        "### Анализ второго слагаемого: $\\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T$\n",
        "\n",
        "Второе слагаемое представляет собой линейную комбинацию строк матрицы $ \\mathbf{\\Phi} $ с коэффициентами $ t_i $. В явном виде:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T = (\\sum_{i=1}^{N} t_i \\cdot \\phi_0(x_i), \\sum_{i=1}^{N} t_i \\cdot \\phi_1(x_i), \\ldots).\n",
        "$$\n",
        "\n",
        "Это выражение может быть записано в матричной форме как:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\phi(x_i)^T = \\mathbf{t}^T \\mathbf{\\Phi},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\mathbf{t} $ — вектор целевых значений размерности $ N \\times 1 $,\n",
        "- $ \\mathbf{\\Phi} $ — матрица базисных функций размерности $ N \\times (p+1) $.\n",
        "\n",
        "\n",
        "\n",
        "### Подстановка выражений в градиент\n",
        "\n",
        "Подставляя результаты анализа в выражение для транспонированного градиента, получаем:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = w^T \\cdot \\mathbf{\\Phi}^T \\mathbf{\\Phi} - \\mathbf{t}^T \\mathbf{\\Phi}.\n",
        "$$\n",
        "\n",
        "Приравнивая градиент к нулю:\n",
        "\n",
        "$$\n",
        "w^T \\cdot \\mathbf{\\Phi}^T \\mathbf{\\Phi} = \\mathbf{t}^T \\mathbf{\\Phi}.\n",
        "$$\n",
        "\n",
        "Умножая обе части уравнения справа на $ (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} $, получаем:\n",
        "\n",
        "$$\n",
        "w^T = \\mathbf{t}^T \\mathbf{\\Phi} (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Применение свойств транспонирования\n",
        "\n",
        "Для перехода от транспонированной формы $ w^T $ к исходному вектору $ w $, транспонируем обе части уравнения:\n",
        "\n",
        "$$\n",
        "(w^T)^T = \\left( \\mathbf{t}^T \\mathbf{\\Phi} (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\right)^T.\n",
        "$$\n",
        "\n",
        "Используя свойства транспонирования:\n",
        "1. $(A B)^T = B^T A^T$,\n",
        "2. $(A^{-1})^T = (A^T)^{-1}$,\n",
        "\n",
        "получаем:\n",
        "\n",
        "$$\n",
        "w = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T \\mathbf{t}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итоговое выражение\n",
        "\n",
        "Окончательное выражение для вектора параметров $ w $, минимизирующего функцию потерь $ E(w) $, имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{w = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T \\mathbf{t}.}\n",
        "$$\n",
        "\n",
        "Это выражение известно как **нормальное уравнение** и представляет собой аналитическое решение задачи линейной регрессии с базисными функциями.\n"
      ],
      "metadata": {
        "id": "K9zKCIWfBwBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Вопрос на подумать\n",
        "\n",
        "*Для вычисления $w$ нам приходится обращать (квадратную) матрицу $\\Phi^T \\Phi$, что возможно только если она невырожденна. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?*\n",
        "\n",
        "### Ответ\n",
        "\n",
        "С точки зрения линейной алгебры, для вещественной матрицы $\\Phi$ ранги матриц $\\Phi$ и $\\Phi^T \\Phi$ совпадают. Матрица $\\Phi^T \\Phi$ будет невырожденной, то есть обратимой, тогда и только тогда, когда её ранг равен числу её столбцов, что также равно числу столбцов матрицы $\\Phi$. Иными словами, формула регрессии поломается, только если столбцы матрицы $\\Phi$ линейно зависимы.\n",
        "\n",
        "В данной ситуации столбцы матрицы $\\Phi$ представляют собой признаки (фичи) нашего набора данных. Если признаки линейно зависимы, это значит, что один или несколько признаков могут быть выражены как линейные комбинации других признаков. Это приводит к проблеме избыточности информации. Когда признаки линейно зависимы, необходимо оставить только линейно независимые признаки, так как зависимые признаки не добавляют новой информации и лишь усложняют вычисления. Таким образом, чтобы избежать вырождения, мы обычно следим за тем, чтобы признаки были линейно независимыми.\n",
        "\n",
        "Однако на практике часто встречается ситуация, когда признаки приближённо линейно зависимы, особенно если их много. В таких случаях матрица $\\Phi^T \\Phi$ будет близка к вырожденной, что приводит к ряду вычислительных проблем. Как мы увидим далее, такие случаи плохо сказываются на численной устойчивости решения и могут потребовать дополнительных методов для его стабилизации.\n",
        "\n",
        "## Вычислительная сложность аналитического решения\n",
        "\n",
        "Аналитическое решение задачи линейной регрессии имеет вычислительную сложность $O(D^2 N + D^3)$, где:\n",
        "- $N$ — это количество объектов в выборке,\n",
        "- $D$ — это количество признаков у каждого объекта.\n",
        "\n",
        "### Разбор компонентов вычислительной сложности\n",
        "\n",
        "1. **Слагаемое $O(D^2 N)$** отвечает за сложность перемножения матриц $\\Phi^T$ и $\\Phi$. Для нахождения матрицы $\\Phi^T \\Phi$ требуется выполнить $N$ умножений для каждого из $D \\times D$ элементов.\n",
        "\n",
        "2. **Слагаемое $O(D^3)$** обусловлено сложностью обращения матрицы $\\Phi^T \\Phi$. Поскольку эта матрица размерности $D \\times D$, стандартные методы обращения, такие как метод Гаусса или метод разложения Холецкого, требуют $O(D^3)$ операций.\n",
        "\n",
        "Чтобы минимизировать вычислительные затраты, не рекомендуется перемножать матрицы в выражении $(\\Phi^T \\Phi)^{-1} \\cdot \\Phi^T$ целиком. Вместо этого целесообразно сначала умножить вектор $t$ на $\\Phi^T$, а затем результат умножить на $(\\Phi^T \\Phi)^{-1}$. Такой подход позволяет значительно снизить объём вычислений и уменьшает необходимость хранения матрицы $(\\Phi^T \\Phi)^{-1} \\cdot \\Phi^T$ в памяти, что особенно важно при больших размерах данных.\n",
        "\n",
        "### Ускорение вычислений\n",
        "\n",
        "Вычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц, такие как метод Штрассена или метод Винограда, которые снижают степень вычислительной сложности. Кроме того, для больших матриц эффективны итерационные методы обращения, такие как метод Якоби или метод сопряжённых градиентов, которые позволяют обойтись без явного обращения матрицы, что также повышает стабильность численных решений.\n",
        "\n",
        "## Проблемы «точного» решения\n",
        "\n",
        "Для нахождения коэффициентов регрессии требуется обращение матрицы $\\Phi^T \\Phi$, что порождает несколько серьёзных проблем:\n",
        "\n",
        "1. **Сложность обращения больших матриц.**  \n",
        "   Обращение больших матриц вычислительно затратно, а в задачах машинного обучения часто необходимо работать с датасетами, содержащими миллионы точек. В таких случаях обращение может потребовать значительных вычислительных ресурсов, и для оптимизации приходится применять приближённые или итерационные методы.\n",
        "\n",
        "2. **Плохая обусловленность матрицы $\\Phi^T \\Phi$.**  \n",
        "   Даже если матрица $\\Phi^T \\Phi$ является обратимой, она может быть плохо обусловлена. Это происходит особенно часто, когда количество признаков велико. В такой ситуации некоторые признаки могут быть почти линейно зависимы от других, что приводит к тому, что $\\Phi^T \\Phi$ становится близкой к вырожденной. Плохая обусловленность означает, что малые изменения в данных могут привести к большим изменениям в итоговом решении $w$, что делает его численно неустойчивым.\n",
        "\n",
        "   Например, малое изменение в целевом векторе $t$ может привести к значительным изменениям в коэффициентах $w$. Это явление возникает из-за того, что погрешность результата будет зависеть от квадрата обусловленности матрицы $\\Phi$. В итоге решение становится ненадёжным: незначительные погрешности данных могут вызвать серьёзные ошибки в предсказаниях модели.\n",
        "\n",
        "3. **Численные ошибки при вычислении.**  \n",
        "   Плохая обусловленность ведет к накоплению численных ошибок. Даже при использовании высокоточных вычислений, ошибки округления при обращении матрицы $\\Phi^T \\Phi$ могут существенно исказить решение. Эта проблема особенно критична для систем с большой размерностью, где погрешности округления накапливаются в каждой операции и могут искажать конечный результат.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, обращение матрицы $\\Phi^T \\Phi$ при решении задачи линейной регрессии связано с рядом вычислительных и численных трудностей, особенно в случае больших данных и многомерных признаков. Для их преодоления разработаны методы регуляризации, такие как метод ридж-регрессии (регуляризация Тихонова), позволяющие стабилизировать решение путем добавления регуляризующего члена к матрице $\\Phi^T \\Phi$. Этот подход помогает улучшить обусловленность матрицы и снизить зависимость от линейных зависимостей между признаками, что в свою очередь делает модель более устойчивой к численным ошибкам и повышает её обобщающую способность.\n",
        "\n",
        "\n",
        "\n",
        "### Пара слов про число обусловленности\n",
        "\n",
        "С математической точки зрения, число обусловленности матрицы $\\Phi$ — это показатель, отражающий, насколько различаются масштабы её собственных значений. Упрощая, число обусловленности матрицы $\\Phi$ можно рассматривать как корень из отношения наибольшего и наименьшего собственных значений матрицы $\\Phi^T \\Phi$. Иными словами, оно показывает, насколько разного масштаба бывают собственные значения этой матрицы.\n",
        "\n",
        "Если рассмотреть $L^2$-норму ошибки предсказания как функцию от коэффициентов, то линии уровня этой функции представляют собой эллипсоиды. Форма этих эллипсоидов определяется квадратичной формой, заданной матрицей $\\Phi^T \\Phi$. Вытянутость этих эллипсоидов говорит о том, насколько сильно отличаются величины собственных значений $\\Phi^T \\Phi$, что и выражает число обусловленности. Высокое число обусловленности может сигнализировать о том, что матрица плохо обусловлена, что, в свою очередь, может привести к численным проблемам при вычислении.\n",
        "\n",
        "### Подробнее\n",
        "\n",
        "Проблемы с численной устойчивостью и высокой сложностью вычислений не означают, что «точное» решение необходимо отбросить. Существуют несколько методов для улучшения численных свойств решения. Однако для их полного понимания необходимо знание сингулярного разложения. Если оно вам не знакомо, рекомендуется сначала изучить этот материал и затем вернуться к данной теме.\n",
        "\n",
        "#### Метод 1: Использование QR-разложения\n",
        "\n",
        "Первый способ улучшения решения заключается в применении QR-разложения матрицы $X$. QR-разложение — это представление матрицы $X$ в виде произведения $X = QR$, где:\n",
        "- $Q$ — матрица с ортогональными столбцами (то есть $Q^T Q = E$),\n",
        "- $R$ — квадратная верхнетреугольная матрица.\n",
        "\n",
        "Подставив это разложение в исходное уравнение для $w$, получим:\n",
        "\n",
        "$$\n",
        "w = ((QR)^T QR)^{-1} (QR)^T y\n",
        "$$\n",
        "\n",
        "Раскроем выражение, используя свойства ортогональной матрицы $Q$:\n",
        "\n",
        "$$\n",
        "w = (R^T Q^T Q R)^{-1} R^T Q^T y = (R^T R)^{-1} R^T Q^T y = R^{-1} R^{-T} R^T Q^T y = R^{-1} Q^T y\n",
        "$$\n",
        "\n",
        "Здесь мы использовали тот факт, что $(R^T R)^{-1} = R^{-1} R^{-T}$. Полученное выражение значительно проще, поскольку обращение верхнетреугольной матрицы $R$ сводится к решению системы уравнений с верхнетреугольной левой частью, что позволяет выполнять вычисления быстрее и с меньшей численной погрешностью. Погрешность вычисления $w$ при этом будет зависеть от числа обусловленности матрицы $\\Phi$, а QR-разложение, как правило, обладает хорошей численной устойчивостью, что делает его применение предпочтительным для улучшения численных свойств решения.\n",
        "\n",
        "#### Метод 2: Использование псевдообратной матрицы через сингулярное разложение\n",
        "\n",
        "Другим подходом является применение псевдообратной матрицы, построенной с помощью сингулярного разложения. Пусть\n",
        "\n",
        "$$\n",
        "A = U \\underbrace{\\mathrm{diag}(\\sigma_1, \\ldots, \\sigma_r)}_{=\\Sigma} V^T\n",
        "$$\n",
        "\n",
        "— это усечённое сингулярное разложение матрицы $A$, где $r$ — ранг $A$. Здесь:\n",
        "- $\\Sigma$ — диагональная матрица, содержащая ненулевые сингулярные значения $\\sigma_i$,\n",
        "- $U$ и $V$ — ортогональные матрицы, такие что $U^T U = E$ и $V^T V = E$.\n",
        "\n",
        "В таком случае $w$ можно выразить следующим образом:\n",
        "\n",
        "$$\n",
        "w = (V \\Sigma U^T U \\Sigma V^T)^{-1} V \\Sigma U^T y\n",
        "$$\n",
        "\n",
        "Используем свойства ортогональных матриц и диагональной матрицы $\\Sigma$. Поскольку $V \\Sigma^{-2} V^T \\cdot V \\Sigma^2 V^T = E$, верно, что\n",
        "\n",
        "$$\n",
        "(V \\Sigma^2 V^T)^{-1} = V \\Sigma^{-2} V^T,\n",
        "$$\n",
        "\n",
        "что позволяет упростить выражение:\n",
        "\n",
        "$$\n",
        "w = V \\Sigma^{-2} V^T \\Sigma U^T y = V \\Sigma^{-1} U^T y\n",
        "$$\n",
        "\n",
        "Применение сингулярного разложения гарантирует численную устойчивость решения, так как сингулярное разложение хорошо себя ведёт при вычислениях, даже если матрица плохо обусловлена. Это улучшает численные свойства решения и делает его более надёжным.\n",
        "\n",
        "Тем не менее, несмотря на преимущества сингулярного разложения, вычислительная сложность остаётся значительной, особенно для больших матриц. Плохая обусловленность матрицы $X$ всё равно окажет влияние, хоть и в меньшей степени, на точность вычислений.\n",
        "\n",
        "Хотя эти методы помогают улучшить численную устойчивость и стабилизировать решение, они не могут полностью устранить проблемы, связанные с вырожденностью или плохой обусловленностью матрицы $X$. Поэтому нет необходимости останавливаться на «точных» решениях, которые всё равно никогда не будут полностью точными в условиях реальных данных. В следующих разделах мы рассмотрим альтернативные подходы к решению задачи, которые не полагаются на обращение матрицы и могут дать более надёжные результаты в условиях плохой обусловленности.\n",
        "\n",
        "\n",
        "\n",
        "### Почему градиент указывает направление наибольшего возрастания функции?\n",
        "\n",
        "**Что такое градиент функции?**\n",
        "\n",
        "Градиент функции многих переменных — это вектор, составленный из частных производных этой функции по каждой переменной. Он показывает направление наибольшего возрастания функции в данной точке и его величина указывает на скорость изменения функции вдоль этого направления. Градиент используется в задачах оптимизации для нахождения экстремумов, так как именно он указывает направление наискорейшего увеличения функции.\n",
        "\n",
        "Для функции $f(x_1, x_2, \\dots, x_n)$ градиент обозначается символом $\\nabla f$ и формируется следующим образом:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
        "$$\n",
        "\n",
        "Этот вектор указывает направление наибольшего возрастания функции в данной точке.\n",
        "\n",
        "**Что такое производная по направлению?**\n",
        "\n",
        "Производная по направлению — это производная функции вдоль заданного вектора. Пусть $\\vec{u} = (u_1, u_2, \\dots, u_n)$ — нормированный вектор (то есть $||\\vec{u}|| = 1$), вдоль которого берется производная функции $f(x_1, x_2, \\dots, x_n)$. Норма вектора $\\vec{u}$ определяется как:\n",
        "\n",
        "$$\n",
        "||\\vec{u}|| = \\sqrt{u_1^2 + u_2^2 + \\dots + u_n^2}\n",
        "$$\n",
        "\n",
        "Производная функции $f$ по направлению вектора $\\vec{u}$ определяется как предел:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\frac{f(\\vec{x} + h \\cdot \\vec{u}) - f(\\vec{x})}{h}\n",
        "$$\n",
        "\n",
        "где $\\vec{x} + h \\cdot \\vec{u} = (x_1 + h \\cdot u_1, x_2 + h \\cdot u_2, \\dots, x_n + h \\cdot u_n)$. Эта формула показывает изменение функции вдоль направления, заданного вектором $\\vec{u}$.\n",
        "\n",
        "### Доказательство, что градиент указывает направление наибольшего возрастания функции\n",
        "\n",
        "Рассмотрим производную функции $f$ по направлению вектора $\\vec{u}$:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\frac{f(x_1 + h \\cdot u_1, x_2 + h \\cdot u_2, \\dots, x_n + h \\cdot u_n) - f(x_1, x_2, \\dots, x_n)}{h}\n",
        "$$\n",
        "\n",
        "Мы можем разложить эту производную, выделяя отдельные слагаемые для каждого индекса:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\sum_{i=1}^{n} \\frac{f(x_1, \\dots, x_i + h \\cdot u_i, \\dots, x_n) - f(x_1, \\dots, x_n)}{h}\n",
        "$$\n",
        "\n",
        "Эта сумма почти представляет собой выражение для частной производной функции. Если умножить каждый член на $u_i$ и затем разделить на $u_i$, мы получим следующее:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\sum_{i=1}^{n} \\frac{f(x_1, \\dots, x_i + h \\cdot u_i, \\dots, x_n) - f(x_1, \\dots, x_n)}{h \\cdot u_i} \\cdot u_i = \\nabla f^T \\cdot \\vec{u}\n",
        "$$\n",
        "\n",
        "Скалярное произведение двух векторов $\\vec{a}$ и $\\vec{b}$ можно выразить как:\n",
        "\n",
        "$$\n",
        "\\vec{a}^T \\cdot \\vec{b} = ||\\vec{a}|| \\cdot ||\\vec{b}|| \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "где $\\theta$ — угол между векторами. Таким образом, для градиента мы имеем:\n",
        "\n",
        "$$\n",
        "\\nabla f^T \\cdot \\vec{u} = ||\\nabla f|| \\cdot \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "Производная по направлению отражает скорость возрастания функции в заданном направлении $\\vec{u}$. Чтобы максимизировать скорость роста функции, вектор $\\vec{u}$ должен быть направлен так, чтобы угол $\\theta = 0$, то есть векторы $\\nabla f$ и $\\vec{u}$ должны совпадать по направлению. Таким образом, направление градиента действительно является направлением наибольшего возрастания функции.\n",
        "\n",
        "### Ортогональность градиента к линиям уровня\n",
        "\n",
        "Градиент также обладает свойством ортогональности к линиям уровня функции, что важно для визуализации процесса оптимизации. Доказательство этого свойства выглядит следующим образом.\n",
        "\n",
        "Пусть $x_0$ — некоторая точка, и $S(x_0) = \\{ x \\in \\mathbb{R}^d \\mid f(x) = f(x_0) \\}$ — линия уровня функции $f$, соответствующая этой точке. Разложим функцию в ряд Тейлора на линии уровня в окрестности точки $x_0$:\n",
        "\n",
        "$$\n",
        "f(x_0 + \\varepsilon) = f(x_0) + \\langle \\nabla f, \\varepsilon \\rangle + o(||\\varepsilon||),\n",
        "$$\n",
        "\n",
        "где $x_0 + \\varepsilon \\in S(x_0)$. Поскольку $f(x_0 + \\varepsilon) = f(x_0)$ (это свойство линии уровня), получаем:\n",
        "\n",
        "$$\n",
        "\\langle \\nabla f, \\varepsilon \\rangle = o(||\\varepsilon||).\n",
        "$$\n",
        "\n",
        "Разделив обе части на $||\\varepsilon||$, имеем:\n",
        "\n",
        "$$\n",
        "\\left\\langle \\nabla f, \\frac{\\varepsilon}{||\\varepsilon||} \\right\\rangle = o(1).\n",
        "$$\n",
        "\n",
        "При стремлении $||\\varepsilon||$ к нулю вектор $\\frac{\\varepsilon}{||\\varepsilon||}$ будет стремиться к касательной к линии уровня в точке $x_0$. В пределе мы получаем, что градиент ортогонален этой касательной.\n",
        "\n",
        "Таким образом, градиент функции является мощным инструментом в математике и оптимизации, указывая направление наибольшего возрастания функции и демонстрируя важные свойства, такие как ортогональность к линиям уровня. Понимание этих концепций позволяет более эффективно решать задачи оптимизации и анализировать поведение многомерных функций.\n",
        "\n",
        "\n",
        "Чтобы лучше понять концепцию градиента и его применение, давайте рассмотрим несколько конкретных примеров.\n",
        "\n",
        "### Пример 1: Градиент функции двух переменных\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "f(x, y) = x^2 + y^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Градиент функции $f$ будет вычисляться как:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
        "$$\n",
        "\n",
        "Найдём частные производные:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 2y\n",
        "$$\n",
        "\n",
        "Таким образом, градиент равен:\n",
        "\n",
        "$$\n",
        "\\nabla f = (2x, 2y)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент показывает направление наибольшего возрастания функции. Например, в точке $(1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla f(1, 1) = (2 \\cdot 1, 2 \\cdot 1) = (2, 2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1)$ функция $f$ возрастает быстрее всего в направлении вектора $(2, 2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для данной функции представляют собой круги, заданные уравнением $x^2 + y^2 = c$. Градиент будет направлен от центра этих кругов к их границе, что подтверждает, что он ортогонален линиям уровня.\n",
        "\n",
        "### Пример 2: Градиент функции с ограничениями\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "g(x, y) = 4 - x^2 - y^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Найдём градиент функции $g$:\n",
        "\n",
        "$$\n",
        "\\nabla g = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right) = (-2x, -2y)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент в точке $(1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla g(1, 1) = (-2 \\cdot 1, -2 \\cdot 1) = (-2, -2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1)$ функция $g$ убывает быстрее всего в направлении вектора $(-2, -2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для этой функции будут представлять собой окружности, заданные уравнением $x^2 + y^2 = c$, что также подтверждает, что градиент направлен внутрь окружностей, оставаясь ортогональным им.\n",
        "\n",
        "### Пример 3: Градиент в многомерной функции\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "h(x, y, z) = x^2 + y^2 + z^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Найдём градиент функции $h$:\n",
        "\n",
        "$$\n",
        "\\nabla h = \\left( \\frac{\\partial h}{\\partial x}, \\frac{\\partial h}{\\partial y}, \\frac{\\partial h}{\\partial z} \\right) = (2x, 2y, 2z)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент в точке $(1, 1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla h(1, 1, 1) = (2 \\cdot 1, 2 \\cdot 1, 2 \\cdot 1) = (2, 2, 2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1, 1)$ функция $h$ возрастает быстрее всего в направлении вектора $(2, 2, 2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для функции $h$ будут представлять собой сферы, заданные уравнением $x^2 + y^2 + z^2 = c$. Градиент будет направлен от центра сфер к их поверхности, что также подтверждает его ортогональность к линиям уровня.\n",
        "\n",
        "### Пример 4: Градиент в задачах оптимизации\n",
        "\n",
        "Предположим, мы хотим минимизировать функцию потерь в регрессионной модели, заданной следующей функцией:\n",
        "\n",
        "$$\n",
        "L(w) = \\sum_{i=1}^{n} (y_i - (w_1 x_{1i} + w_2 x_{2i}))^2\n",
        "$$\n",
        "\n",
        "где $w = (w_1, w_2)$ — вектор весов, $x_{1i}, x_{2i}$ — входные данные, а $y_i$ — целевые значения.\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Градиент функции потерь будет иметь вид:\n",
        "\n",
        "$$\n",
        "\\nabla L(w) = \\left( \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2} \\right)\n",
        "$$\n",
        "\n",
        "При нахождении производных получаем:\n",
        "\n",
        "$$\n",
        "\\nabla L(w) = \\left( -2 \\sum_{i=1}^{n} x_{1i} (y_i - (w_1 x_{1i} + w_2 x_{2i})), -2 \\sum_{i=1}^{n} x_{2i} (y_i - (w_1 x_{1i} + w_2 x_{2i})) \\right)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент указывает на то, как изменить веса $w_1$ и $w_2$, чтобы минимизировать функцию потерь $L(w)$. Направление, указанное градиентом, показывает, как нужно корректировать веса, чтобы достигнуть оптимального результата.\n",
        "\n",
        "Эти примеры демонстрируют, как градиент помогает понять поведение многомерных функций, указывая направление наибольшего возрастания и предоставляя полезные инструменты для задач оптимизации. Понимание этих концепций позволяет более эффективно решать практические задачи в математике и машинном обучении. Если у вас есть вопросы или нужны дополнительные примеры, дайте знать!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Пример применения\n",
        "\n",
        "Рассмотрим простой пример с использованием метода наименьших квадратов для линейной регрессии, основанный на наборе данных, где мы исследуем влияние цены на объем продаж.\n",
        "\n",
        "#### Данные\n",
        "\n",
        "Предположим, у нас есть следующие данные о ценах и объемах продаж:\n",
        "\n",
        "| Цена (X) | Объем продаж (Y) |\n",
        "|----------|------------------|\n",
        "| 10       | 100              |\n",
        "| 20       | 150              |\n",
        "| 30       | 200              |\n",
        "| 40       | 300              |\n",
        "| 50       | 400              |\n",
        "\n",
        "#### 1. Определим функции базиса\n",
        "\n",
        "Мы будем использовать линейные функции базиса:\n",
        "\n",
        "- $\\phi_0(x) = 1$ (константа)\n",
        "- $\\phi_1(x) = x$ (линейная функция)\n",
        "\n",
        "#### 2. Сформируем матрицу функций базиса\n",
        "\n",
        "Для нашей выборки получаем матрицу:\n",
        "\n",
        "$$\n",
        "\\Phi =\n",
        "\\begin{bmatrix}\n",
        "1 & 10 \\\\\n",
        "1 & 20 \\\\\n",
        "1 & 30 \\\\\n",
        "1 & 40 \\\\\n",
        "1 & 50\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### 3. Определим вектор целевой переменной\n",
        "\n",
        "Вектор целевой переменной:\n",
        "\n",
        "$$\n",
        "t =\n",
        "\\begin{bmatrix}\n",
        "100 \\\\\n",
        "150 \\\\\n",
        "200 \\\\\n",
        "300 \\\\\n",
        "400\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### 4. Подставим данные в формулу\n",
        "\n",
        "Теперь можем найти коэффициенты $w$:\n",
        "\n",
        "1. Вычислим $\\Phi^T \\Phi$:\n",
        "\n",
        "$$\n",
        "\\Phi^T \\Phi =\n",
        "\\begin{bmatrix}\n",
        "5 & 150 \\\\\n",
        "150 & 3850\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Найдем обратную матрицу $(\\Phi^T \\Phi)^{-1}$:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} = \\frac{1}{(5)(3850) - (150)(150)}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "После вычислений получаем:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} = \\frac{1}{(19250 - 22500)}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "= \\frac{1}{-3250}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "-1.1846 & 0.04615 \\\\\n",
        "0.04615 & -0.0015385\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. Вычислим $\\Phi^T t$:\n",
        "\n",
        "$$\n",
        "\\Phi^T t =\n",
        "\\begin{bmatrix}\n",
        "1300 \\\\\n",
        "28500\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Теперь можем вычислить $w$:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
        "$$\n",
        "\n",
        "#### 5. Получение коэффициентов\n",
        "\n",
        "После подстановки значений мы получаем:\n",
        "\n",
        "$$\n",
        "w \\approx\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Интерпретация коэффициентов\n",
        "\n",
        "Полученные коэффициенты означают следующее:\n",
        "\n",
        "- $w_0 = 0$: При цене 0 объем продаж также равен 0, что логично.\n",
        "- $w_1 = 8$: Это значит, что на каждый доллар увеличения цены объем продаж увеличивается на 8 единиц.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kxzoe8YVS3JT"
      }
    }
  ]
}