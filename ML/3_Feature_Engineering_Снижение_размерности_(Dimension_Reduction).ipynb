{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlkYA6450eM6Myxel01jQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_Feature_Engineering_%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8_(Dimension_Reduction).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "_ZiknElU3C2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "\n",
        "Снижение размерности (Dimension Reduction) — это процесс уменьшения количества признаков (фичей) в наборе данных, при этом сохраняя как можно больше полезной информации. Это важный этап в feature engineering, который помогает улучшить производительность моделей машинного обучения, сократить время обучения, уменьшить риск переобучения и упростить интерпретацию данных.\n",
        "\n",
        "#### Зачем нужно снижение размерности?\n",
        "\n",
        "1. **Устранение избыточности данных**: Многие признаки могут быть коррелированы между собой, что приводит к избыточности информации. Снижение размерности помогает устранить эту проблему.\n",
        "2. **Ускорение обучения моделей**: Меньшее количество признаков означает меньше вычислений, что ускоряет процесс обучения.\n",
        "3. **Уменьшение переобучения**: Модели, обученные на данных с меньшим количеством признаков, менее склонны к переобучению, особенно если исходный набор данных был очень большим.\n",
        "4. **Визуализация данных**: Снижение размерности до 2 или 3 измерений позволяет визуализировать данные, что помогает лучше понять их структуру.\n"
      ],
      "metadata": {
        "id": "0UN4OfqS7-Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Неконтролируемые методы (Unsupervised Methods)\n",
        "\n",
        "Неконтролируемые методы снижения размерности — это подходы, которые не используют информацию о целевой переменной (если она есть). Они работают исключительно на основе структуры данных, пытаясь сохранить важные закономерности или отношения между объектами. Эти методы особенно полезны, когда у нас нет размеченных данных или когда мы хотим исследовать структуру данных без явного целевого признака.\n"
      ],
      "metadata": {
        "id": "vTyRDLl57-n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод главных компонент (PCA, Principal Components Analysis)\n",
        "\n",
        "Метод главных компонент (PCA) — это метод снижения размерности данных, который позволяет выделить наиболее важные направления (компоненты) в данных и спроецировать данные на эти направления. PCA широко используется для визуализации данных, сжатия информации и удаления шума.\n",
        "\n",
        "#### Основные шаги PCA:\n",
        "\n",
        "1. **Стандартизация данных**  \n",
        "   Если признаки имеют разные масштабы, их необходимо стандартизировать, чтобы каждый признак вносил равный вклад в анализ.  \n",
        "   Формула стандартизации:  \n",
        "$$\n",
        "   z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$  \n",
        "   где:  \n",
        "   - $x_{ij}$ — значение $i$-го объекта по $j$-му признаку,  \n",
        "   - $\\mu_j$ — среднее значение $j$-го признака,  \n",
        "   - $\\sigma_j$ — стандартное отклонение $j$-го признака.\n",
        "\n",
        "2. **Вычисление ковариационной матрицы**  \n",
        "   Ковариационная матрица показывает, как признаки коррелируют друг с другом.  \n",
        "   Формула ковариационной матрицы $\\Sigma$:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$  \n",
        "   где:  \n",
        "   - $Z$ — матрица стандартизированных данных,  \n",
        "   - $n$ — количество объектов.\n",
        "\n",
        "3. **Вычисление собственных значений и собственных векторов**  \n",
        "   Собственные значения ($\\lambda$) и собственные векторы ($v$) ковариационной матрицы находятся из уравнения:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$  \n",
        "   Собственные векторы определяют направления главных компонент, а собственные значения — их важность (дисперсию).\n",
        "\n",
        "4. **Сортировка собственных значений и векторов**  \n",
        "   Собственные значения сортируются в порядке убывания. Соответствующие собственные векторы также переупорядочиваются. Первые $k$ собственных векторов соответствуют $k$ главным компонентам.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на выбранные главные компоненты для снижения размерности.  \n",
        "   Формула проекции:  \n",
        "$$\n",
        "   Y = Z W\n",
        "$$  \n",
        "   где:  \n",
        "   - $Y$ — матрица данных в новом пространстве,  \n",
        "   - $W$ — матрица, содержащая $k$ собственных векторов.\n",
        "\n",
        "#### Пример алгоритма PCA:\n",
        "\n",
        "1. Стандартизируем данные:  \n",
        "$$\n",
        "   Z = \\begin{bmatrix}\n",
        "   z_{11} & z_{12} & \\dots & z_{1p} \\\\\n",
        "   z_{21} & z_{22} & \\dots & z_{2p} \\\\\n",
        "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   z_{n1} & z_{n2} & \\dots & z_{np}\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычисляем ковариационную матрицу:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "3. Находим собственные значения и векторы:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$\n",
        "\n",
        "4. Сортируем собственные значения и векторы:  \n",
        "$$\n",
        "   \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\n",
        "$$\n",
        "\n",
        "5. Выбираем $k$ главных компонент и проецируем данные:  \n",
        "$$\n",
        "   Y = Z W_k\n",
        "$$  \n",
        "   где $W_k$ — матрица из $k$ собственных векторов.\n",
        "\n",
        "#### Интерпретация результатов:\n",
        "\n",
        "- **Главные компоненты**: Новые оси, вдоль которых данные имеют наибольшую дисперсию.  \n",
        "- **Доля объясненной дисперсии**: Доля дисперсии, объясняемая каждой компонентой:  \n",
        "$$\n",
        "   \\text{Доля} = \\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n",
        "$$  \n",
        "- **Снижение размерности**: Выбор $k$ компонент, которые объясняют большую часть дисперсии.\n",
        "\n",
        "#### Преимущества PCA:\n",
        "- Уменьшает размерность данных.  \n",
        "- Удаляет корреляции между признаками.  \n",
        "- Упрощает визуализацию данных.\n",
        "\n",
        "#### Недостатки PCA:\n",
        "- Линейность: PCA предполагает линейные зависимости между признаками.  \n",
        "- Интерпретируемость: Главные компоненты могут быть трудны для интерпретации.  \n",
        "- Потеря информации: При сильном снижении размерности часть информации может быть потеряна.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения метода главных компонент (PCA) для снижения размерности данных.\n",
        "\n",
        "### Исходные данные\n",
        "Пусть у нас есть следующие данные с двумя признаками $X_1$ и $X_2$:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь 4 объекта и 2 признака.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "Сначала стандартизируем данные, чтобы каждый признак имел среднее значение 0 и стандартное отклонение 1.\n",
        "\n",
        "Средние значения признаков:\n",
        "$$\n",
        "\\mu_1 = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\mu_2 = \\frac{2 + 3 + 4 + 5}{4} = 3.5\n",
        "$$\n",
        "\n",
        "Стандартные отклонения признаков:\n",
        "$$\n",
        "\\sigma_1 = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4}} = 1.118, \\quad \\sigma_2 = 1.118\n",
        "$$\n",
        "\n",
        "Стандартизированные данные:\n",
        "$$\n",
        "Z = \\begin{bmatrix}\n",
        "\\frac{1-2.5}{1.118} & \\frac{2-3.5}{1.118} \\\\\n",
        "\\frac{2-2.5}{1.118} & \\frac{3-3.5}{1.118} \\\\\n",
        "\\frac{3-2.5}{1.118} & \\frac{4-3.5}{1.118} \\\\\n",
        "\\frac{4-2.5}{1.118} & \\frac{5-3.5}{1.118}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "Ковариационная матрица $\\Sigma$ вычисляется по формуле:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "Подставляем данные:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "-1.342 & -0.447 & 0.447 & 1.342 \\\\\n",
        "-1.342 & -0.447 & 0.447 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "4 & 4 \\\\\n",
        "4 & 4\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение $\\Sigma v = \\lambda v$.\n",
        "\n",
        "Характеристическое уравнение:\n",
        "$$\n",
        "\\text{det}(\\Sigma - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Подставляем:\n",
        "$$\n",
        "\\text{det}\\left(\\begin{bmatrix}\n",
        "1.333 - \\lambda & 1.333 \\\\\n",
        "1.333 & 1.333 - \\lambda\n",
        "\\end{bmatrix}\\right) = 0\n",
        "$$\n",
        "\n",
        "Раскрываем определитель:\n",
        "$$\n",
        "(1.333 - \\lambda)^2 - (1.333)^2 = 0\n",
        "$$\n",
        "\n",
        "Решаем:\n",
        "$$\n",
        "\\lambda^2 - 2.666\\lambda = 0 \\implies \\lambda(\\lambda - 2.666) = 0\n",
        "$$\n",
        "\n",
        "Собственные значения:\n",
        "$$\n",
        "\\lambda_1 = 2.666, \\quad \\lambda_2 = 0\n",
        "$$\n",
        "\n",
        "Собственные векторы:\n",
        "Для $\\lambda_1 = 2.666$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 - 2.666 & 1.333 \\\\\n",
        "1.333 & 1.333 - 2.666\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1.333 & 1.333 \\\\\n",
        "1.333 & -1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "-1.333 v_{11} + 1.333 v_{12} = 0 \\implies v_{11} = v_{12}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_1 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Для $\\lambda_2 = 0$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{21} \\\\\n",
        "v_{22}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "1.333 v_{21} + 1.333 v_{22} = 0 \\implies v_{21} = -v_{22}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_2 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "-\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "-0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Сортировка собственных значений и векторов\n",
        "Собственные значения уже отсортированы: $\\lambda_1 = 2.666$, $\\lambda_2 = 0$.  \n",
        "Соответствующие собственные векторы: $v_1 = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}$, $v_2 = \\begin{bmatrix} 0.707 \\\\ -0.707 \\end{bmatrix}$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё.\n",
        "\n",
        "Матрица проекции:\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Проекция данных:\n",
        "$$\n",
        "Y = Z W = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 \\cdot 0.707 + (-1.342) \\cdot 0.707 \\\\\n",
        "-0.447 \\cdot 0.707 + (-0.447) \\cdot 0.707 \\\\\n",
        "0.447 \\cdot 0.707 + 0.447 \\cdot 0.707 \\\\\n",
        "1.342 \\cdot 0.707 + 1.342 \\cdot 0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как PCA может быть использован для снижения размерности данных.\n",
        "\n",
        "\n",
        "Реализуем метод главных компонент (PCA) на Python для приведенного числового примера. Мы будем использовать библиотеку `numpy` для выполнения всех вычислений.\n",
        "\n",
        "### Код Python:\n"
      ],
      "metadata": {
        "id": "3EvC8uww9erl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Шаг 1: Стандартизация данных\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "Z = (X - mean) / std\n",
        "\n",
        "print(\"Стандартизированные данные:\\n\", Z)\n",
        "\n",
        "# Шаг 2: Вычисление ковариационной матрицы\n",
        "cov_matrix = np.cov(Z, rowvar=False)\n",
        "\n",
        "print(\"Ковариационная матрица:\\n\", cov_matrix)\n",
        "\n",
        "# Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Собственные значения:\\n\", eigenvalues)\n",
        "print(\"Собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 4: Сортировка собственных значений и векторов\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "print(\"Отсортированные собственные значения:\\n\", eigenvalues)\n",
        "print(\"Отсортированные собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 5: Проецирование данных на главные компоненты\n",
        "# Выберем первую главную компоненту (с наибольшим собственным значением)\n",
        "k = 1\n",
        "W = eigenvectors[:, :k]\n",
        "\n",
        "Y = Z.dot(W)\n",
        "\n",
        "print(\"Проекция данных на главные компоненты:\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H1Nb2mf9gj8",
        "outputId": "d783edf8-93ce-417c-b52d-48f6f67970a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стандартизированные данные:\n",
            " [[-1.34164079 -1.34164079]\n",
            " [-0.4472136  -0.4472136 ]\n",
            " [ 0.4472136   0.4472136 ]\n",
            " [ 1.34164079  1.34164079]]\n",
            "Ковариационная матрица:\n",
            " [[1.33333333 1.33333333]\n",
            " [1.33333333 1.33333333]]\n",
            "Собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Отсортированные собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Отсортированные собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Проекция данных на главные компоненты:\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Проверка с использованием `sklearn`:\n",
        "Для проверки воспользуемся библиотекой `sklearn`, которая предоставляет встроенную реализацию PCA.\n",
        "\n"
      ],
      "metadata": {
        "id": "LP2xWNCJ9g9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Применение PCA\n",
        "pca = PCA(n_components=1)\n",
        "Y_sklearn = pca.fit_transform(Z)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2GPg0N-9iVz",
        "outputId": "12807f79-8ac1-4e87-f46a-fbcd6a22ac02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Результаты совпадают, что подтверждает правильность нашей реализации."
      ],
      "metadata": {
        "id": "pUmCqFIm7tx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "\n",
        "Ядерный метод главных компонент (Kernel PCA) — это расширение классического PCA, которое позволяет работать с нелинейными данными. В отличие от PCA, который находит линейные комбинации признаков, Kernel PCA использует ядерные функции для отображения данных в пространство более высокой размерности, где они могут быть линейно разделимы.\n",
        "\n",
        "#### Основные шаги Kernel PCA:\n",
        "\n",
        "1. **Выбор ядерной функции**  \n",
        "   Ядерная функция $K(x_i, x_j)$ вычисляет скалярное произведение в пространстве более высокой размерности без явного вычисления этого пространства.  \n",
        "   Примеры ядерных функций:  \n",
        "   - Линейное ядро: $K(x_i, x_j) = x_i^T x_j$  \n",
        "   - Полиномиальное ядро: $K(x_i, x_j) = (x_i^T x_j + c)^d$  \n",
        "   - Гауссово (RBF) ядро: $K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)$  \n",
        "\n",
        "2. **Вычисление ядерной матрицы**  \n",
        "   Ядерная матрица $K$ вычисляется для всех пар объектов:  \n",
        "$$\n",
        "   K_{ij} = K(x_i, x_j)\n",
        "$$  \n",
        "   Матрица $K$ является симметричной и положительно полуопределенной.\n",
        "\n",
        "3. **Центрирование ядерной матрицы**  \n",
        "   Для того чтобы ядерная матрица соответствовала центрированным данным, выполняется центрирование:  \n",
        "$$\n",
        "   K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$  \n",
        "   где $1_N$ — матрица, состоящая из $\\frac{1}{N}$, и $N$ — количество объектов.\n",
        "\n",
        "4. **Вычисление собственных значений и собственных векторов**  \n",
        "   Находятся собственные значения $\\lambda$ и собственные векторы $\\alpha$ центрированной ядерной матрицы:  \n",
        "$$\n",
        "   K_{\\text{центр}} \\alpha = \\lambda \\alpha\n",
        "$$  \n",
        "   Собственные векторы нормируются так, чтобы $\\alpha^T \\alpha = \\frac{1}{\\lambda}$.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на главные компоненты с использованием собственных векторов:  \n",
        "$$\n",
        "   Y_i = \\sum_{j=1}^N \\alpha_j K(x_j, x_i)\n",
        "$$  \n",
        "   где $Y_i$ — проекция $i$-го объекта на главные компоненты.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим пошаговое решение задачи с использованием ядерного метода главных компонент (Kernel PCA) на примере данных:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Выбор ядерной функции\n",
        "Мы используем **гауссово (RBF) ядро** с параметром $\\sigma = 1$. Формула гауссова ядра:\n",
        "$$\n",
        "K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2}\n",
        "\\right)\n",
        "$$\n",
        "Здесь:\n",
        "- $x_i$ и $x_j$ — два объекта (строки матрицы $X$),\n",
        "- $\\|x_i - x_j\\|^2$ — квадрат евклидова расстояния между объектами.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление ядерной матрицы\n",
        "Ядерная матрица $K$ вычисляется для всех пар объектов. Её размерность $4 \\times 4$, так как у нас 4 объекта.\n",
        "\n",
        "Формула для вычисления элементов матрицы:\n",
        "$$\n",
        "K_{ij} = K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "Вычислим каждый элемент матрицы $K$:\n",
        "\n",
        "1. $K(x_1, x_1) = \\exp\\left(-\\frac{\\|x_1 - x_1\\|^2}{2}\\right) = \\exp(0) = 1$\n",
        "2. $K(x_1, x_2) = \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-2)^2 + (2-3)^2}{2}\\right) = \\exp(-1) \\approx 0.3679$\n",
        "3. $K(x_1, x_3) = \\exp\\left(-\\frac{\\|x_1 - x_3\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-3)^2 + (2-4)^2}{2}\\right) = \\exp(-4) \\approx 0.0183$\n",
        "4. $K(x_1, x_4) = \\exp\\left(-\\frac{\\|x_1 - x_4\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-4)^2 + (2-5)^2}{2}\\right) = \\exp(-9) \\approx 0.0001$\n",
        "\n",
        "Аналогично вычисляем остальные элементы матрицы. В итоге получаем:\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "1 & 0.3679 & 0.0183 & 0.0001 \\\\\n",
        "0.3679 & 1 & 0.3679 & 0.0183 \\\\\n",
        "0.0183 & 0.3679 & 1 & 0.3679 \\\\\n",
        "0.0001 & 0.0183 & 0.3679 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Центрирование ядерной матрицы\n",
        "Центрирование ядерной матрицы необходимо для того, чтобы данные в новом пространстве имели нулевое среднее. Формула центрирования:\n",
        "$$\n",
        "K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$\n",
        "где $1_N$ — матрица $4 \\times 4$, состоящая из $\\frac{1}{4}$.\n",
        "\n",
        "1. Вычислим $1_N K$:\n",
        "$$\n",
        "1_N K = \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix} \\cdot K\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "1_N K = \\begin{bmatrix}\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычислим $K 1_N$:\n",
        "$$\n",
        "K 1_N = K \\cdot \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "K 1_N = \\begin{bmatrix}\n",
        "0.3466 & 0.3466 & 0.3466 & 0.3466 \\\\\n",
        "0.4385 & 0.4385 & 0.4385 & 0.4385 \\\\\n",
        "0.4385 & 0.4385 & 0.4385 & 0.4385 \\\\\n",
        "0.3466 & 0.3466 & 0.3466 & 0.3466\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. Вычислим $1_N K 1_N$:\n",
        "$$\n",
        "1_N K 1_N = \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix} \\cdot K \\cdot \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "1_N K 1_N = \\begin{bmatrix}\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Подставляем в формулу центрирования:\n",
        "$$\n",
        "K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "K_{\\text{центр}} = \\begin{bmatrix}\n",
        "0.2698 & -0.1164 & -0.2698 & -0.1164 \\\\\n",
        "-0.1164 & 0.2698 & -0.1164 & -0.2698 \\\\\n",
        "-0.2698 & -0.1164 & 0.2698 & -0.1164 \\\\\n",
        "-0.1164 & -0.2698 & -0.1164 & 0.2698\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 4: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение:\n",
        "$$\n",
        "K_{\\text{центр}} \\alpha = \\lambda \\alpha\n",
        "$$\n",
        "\n",
        "Находим собственные значения и векторы:\n",
        "$$\n",
        "\\lambda_1 = 0.5397, \\quad \\lambda_2 = 0.5397, \\quad \\lambda_3 = 0, \\quad \\lambda_4 = 0\n",
        "$$\n",
        "Соответствующие собственные векторы:\n",
        "$$\n",
        "\\alpha_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}, \\quad \\alpha_2 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}, \\quad \\alpha_3 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}, \\quad \\alpha_4 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Нормируем собственные векторы:\n",
        "$$\n",
        "\\alpha_1 = \\frac{\\alpha_1}{\\sqrt{\\lambda_1}}, \\quad \\alpha_2 = \\frac{\\alpha_2}{\\sqrt{\\lambda_2}}, \\quad \\alpha_3 = \\frac{\\alpha_3}{\\sqrt{\\lambda_3}}, \\quad \\alpha_4 = \\frac{\\alpha_4}{\\sqrt{\\lambda_4}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё:\n",
        "$$\n",
        "Y_i = \\sum_{j=1}^N \\alpha_j K(x_j, x_i)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.2698 \\\\\n",
        "-0.1164 \\\\\n",
        "-0.2698 \\\\\n",
        "-0.1164\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.2698 \\\\\n",
        "-0.1164 \\\\\n",
        "-0.2698 \\\\\n",
        "-0.1164\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как Kernel PCA может быть использован для снижения размерности нелинейных данных.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uo3PHZAndQea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Реализация Kernel PCA с нуля\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "RaNR3s5edgmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class KernelPCA:\n",
        "    def __init__(self, kernel='rbf', gamma=1):\n",
        "        self.kernel = kernel\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def _rbf_kernel(self, X1, X2):\n",
        "        # Гауссово (RBF) ядро\n",
        "        return np.exp(-self.gamma * np.linalg.norm(X1[:, np.newaxis] - X2[np.newaxis, :], axis=2)**2)\n",
        "\n",
        "    def _center_kernel(self, K):\n",
        "        # Центрирование ядерной матрицы\n",
        "        N = K.shape[0]\n",
        "        one_N = np.ones((N, N)) / N\n",
        "        K_centered = K - one_N.dot(K) - K.dot(one_N) + one_N.dot(K).dot(one_N)\n",
        "        return K_centered\n",
        "\n",
        "    def fit_transform(self, X, n_components=1):\n",
        "        # Вычисление ядерной матрицы\n",
        "        K = self._rbf_kernel(X, X)\n",
        "\n",
        "        # Центрирование ядерной матрицы\n",
        "        K_centered = self._center_kernel(K)\n",
        "\n",
        "        # Вычисление собственных значений и векторов\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(K_centered)\n",
        "\n",
        "        # Сортировка собственных значений и векторов\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[sorted_indices]\n",
        "        eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Нормировка собственных векторов\n",
        "        eigenvectors = eigenvectors / np.sqrt(eigenvalues)\n",
        "\n",
        "        # Выбор первых n_components компонент\n",
        "        alpha = eigenvectors[:, :n_components]\n",
        "\n",
        "        # Проецирование данных\n",
        "        Y = K_centered.dot(alpha)\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Создаем объект KernelPCA\n",
        "kpca = KernelPCA(kernel='rbf', gamma=1)\n",
        "\n",
        "# Применяем Kernel PCA\n",
        "Y = kpca.fit_transform(X, n_components=1)\n",
        "\n",
        "print(\"Проекция данных (реализация с нуля):\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcwx4WlGdi19",
        "outputId": "cb21e789-0dc9-4289-959a-72ac4500e398"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (реализация с нуля):\n",
            " [[ 0.62625661]\n",
            " [ 0.38661812]\n",
            " [-0.38661812]\n",
            " [-0.62625661]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из sklearn"
      ],
      "metadata": {
        "id": "cl_tX18IdjCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Создаем объект KernelPCA\n",
        "kpca = KernelPCA(n_components=1, kernel='rbf', gamma=1)\n",
        "\n",
        "# Применяем Kernel PCA\n",
        "Y_sklearn = kpca.fit_transform(X)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhWIBrS6djha",
        "outputId": "027769d6-eaa6-47c5-8792-93484610f0f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[ 0.62625661]\n",
            " [ 0.38661812]\n",
            " [-0.38661812]\n",
            " [-0.62625661]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "**t-SNE (t-Distributed Stochastic Neighbor Embedding)** — это метод снижения размерности, который используется для визуализации данных в пространстве меньшей размерности (обычно 2D или 3D). Он минимизирует расхождение между распределениями вероятностей, которые описывают попарные сходства объектов в исходном и целевом пространствах.\n",
        "\n",
        "\n",
        "\n",
        "### Основные идеи t-SNE\n",
        "\n",
        "1. **Попарные сходства**:\n",
        "   - В исходном пространстве (высокой размерности) t-SNE вычисляет вероятности, которые отражают сходство между объектами.\n",
        "   - В целевом пространстве (низкой размерности) t-SNE строит аналогичные вероятности и минимизирует разницу между двумя распределениями.\n",
        "\n",
        "2. **t-распределение**:\n",
        "   - В целевом пространстве t-SNE использует t-распределение (распределение Стьюдента) для вычисления сходств. Это позволяет избежать проблемы \"скученности\" (crowding problem), когда точки в низкой размерности располагаются слишком близко друг к другу.\n",
        "\n",
        "3. **Стохастичность**:\n",
        "   - t-SNE использует случайные начальные значения для точек в целевом пространстве, поэтому результаты могут немного отличаться при разных запусках.\n",
        "\n",
        "\n",
        "\n",
        "### Алгоритм t-SNE\n",
        "\n",
        "#### Шаг 1: Вычисление попарных сходств в исходном пространстве\n",
        "Для каждой пары объектов $x_i$ и $x_j$ в исходном пространстве вычисляется вероятность $p_{ij}$, которая отражает сходство между ними. Формула для условной вероятности $p_{j|i}$:\n",
        "\n",
        "$$\n",
        "p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\|x_i - x_j\\|^2$ — квадрат евклидова расстояния между объектами $x_i$ и $x_j$.\n",
        "- $\\sigma_i$ — параметр, который выбирается для каждого объекта $x_i$ так, чтобы распределение вероятностей имело заданную \"перплексию\" (perplexity).\n",
        "\n",
        "Затем симметризуется вероятность:\n",
        "\n",
        "$$\n",
        "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
        "$$\n",
        "\n",
        "где $N$ — количество объектов.\n",
        "\n",
        "#### Шаг 2: Вычисление попарных сходств в целевом пространстве\n",
        "Для каждой пары объектов $y_i$ и $y_j$ в целевом пространстве вычисляется вероятность $q_{ij}$ с использованием t-распределения:\n",
        "\n",
        "$$\n",
        "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\|y_i - y_j\\|^2$ — квадрат евклидова расстояния между объектами $y_i$ и $y_j$ в целевом пространстве.\n",
        "\n",
        "#### Шаг 3: Минимизация расхождения между распределениями\n",
        "t-SNE минимизирует расхождение между распределениями $P$ и $Q$ с использованием функции потерь, которая измеряется как **KL-дивергенция** (Kullback-Leibler divergence):\n",
        "\n",
        "$$\n",
        "KL(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
        "$$\n",
        "\n",
        "Минимизация выполняется с помощью градиентного спуска.\n",
        "\n",
        "#### Шаг 4: Оптимизация\n",
        "Градиент функции потерь по $y_i$ вычисляется как:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial KL}{\\partial y_i} = 4 \\sum_{j \\neq i} (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\n",
        "$$\n",
        "\n",
        "На каждом шаге градиентного спуска координаты $y_i$ обновляются:\n",
        "\n",
        "$$\n",
        "y_i^{(t+1)} = y_i^{(t)} - \\eta \\frac{\\partial KL}{\\partial y_i}\n",
        "$$\n",
        "\n",
        "где $\\eta$ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "\n",
        "### Параметры t-SNE\n",
        "\n",
        "1. **Перплексия (perplexity)**:\n",
        "   - Параметр, который определяет, сколько ближайших соседей учитывается для каждого объекта. Обычно выбирается в диапазоне от 5 до 50.\n",
        "\n",
        "2. **Количество итераций**:\n",
        "   - Количество шагов градиентного спуска. Обычно используется 1000–2000 итераций.\n",
        "\n",
        "3. **Скорость обучения (learning rate)**:\n",
        "   - Параметр, который определяет размер шага градиентного спуска. Обычно выбирается в диапазоне от 10 до 1000.\n",
        "\n",
        "4. **Метрика расстояния**:\n",
        "   - Метрика, используемая для вычисления расстояний между объектами. По умолчанию используется евклидово расстояние.\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества и ограничения t-SNE\n",
        "\n",
        "**Преимущества**:\n",
        "- Эффективен для визуализации сложных нелинейных структур данных.\n",
        "- Хорошо сохраняет локальные кластеры и группы точек.\n",
        "- Позволяет выявлять скрытые закономерности в данных.\n",
        "\n",
        "**Ограничения**:\n",
        "- Результаты могут зависеть от выбора параметров, таких как перплексия и скорость обучения.\n",
        "- t-SNE не сохраняет глобальную структуру данных, что может привести к искажениям на больших масштабах.\n",
        "- Вычислительная сложность алгоритма высока, что делает его менее пригодным для очень больших наборов данных.\n",
        "\n",
        "\n",
        "\n",
        "### Применение t-SNE\n",
        "\n",
        "t-SNE широко используется в различных областях, таких как:\n",
        "- Визуализация данных в машинном обучении и анализе данных.\n",
        "- Исследование структуры данных, включая кластеризацию и классификацию.\n",
        "- Анализ изображений, текстов и других сложных типов данных.\n",
        "\n",
        "Таким образом, t-SNE является мощным инструментом для визуализации и исследования данных, особенно когда необходимо выявить локальные структуры и кластеры. Однако его использование требует понимания параметров и ограничений для получения интерпретируемых результатов.\n",
        "\n",
        "\n",
        "Рассмотрим подробное пошаговое решение задачи с использованием **t-SNE** для набора данных из 4 точек в 3D-пространстве. Мы снизим размерность до 2D. Для простоты будем использовать фиксированные значения параметров и упрощенные вычисления.\n",
        "\n",
        "\n",
        "\n",
        "### Исходные данные\n",
        "Даны 4 точки в 3D-пространстве:\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Цель: снизить размерность до 2D с использованием t-SNE.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Вычисление попарных сходств в исходном пространстве\n",
        "\n",
        "1. **Вычисляем евклидовы расстояния между точками**:\n",
        "$$\n",
        "   \\|x_i - x_j\\|^2 = \\sum_{k=1}^3 (x_{ik} - x_{jk})^2\n",
        "$$\n",
        "   Например:\n",
        "$$\n",
        "   \\|x_1 - x_2\\|^2 = (1-4)^2 + (2-5)^2 + (3-6)^2 = 9 + 9 + 9 = 27\n",
        "$$\n",
        "   Аналогично вычисляем все попарные расстояния:\n",
        "$$\n",
        "   \\begin{bmatrix}\n",
        "   0 & 27 & 108 & 243 \\\\\n",
        "   27 & 0 & 27 & 108 \\\\\n",
        "   108 & 27 & 0 & 27 \\\\\n",
        "   243 & 108 & 27 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Вычисляем условные вероятности $p_{j|i}$**:\n",
        "   Используем формулу:\n",
        "$$\n",
        "   p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
        "$$\n",
        "   Для простоты предположим, что $\\sigma_i = 1$ для всех точек.\n",
        "\n",
        "   Для $x_1$:\n",
        "$$\n",
        "   p_{2|1} = \\frac{\\exp(-27/2)}{\\exp(-27/2) + \\exp(-108/2) + \\exp(-243/2)} \\approx \\frac{e^{-13.5}}{e^{-13.5} + e^{-54} + e^{-121.5}} \\approx 1\n",
        "$$\n",
        "   (Остальные значения $p_{j|1}$ близки к 0 из-за экспоненциального убывания.)\n",
        "\n",
        "   Аналогично вычисляем для всех точек. В результате получаем:\n",
        "$$\n",
        "   P = \\begin{bmatrix}\n",
        "   0 & 1 & 0 & 0 \\\\\n",
        "   1 & 0 & 1 & 0 \\\\\n",
        "   0 & 1 & 0 & 1 \\\\\n",
        "   0 & 0 & 1 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Симметризуем вероятности**:\n",
        "$$\n",
        "   p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
        "$$\n",
        "   где $N = 4$.\n",
        "\n",
        "   Например:\n",
        "$$\n",
        "   p_{12} = \\frac{p_{2|1} + p_{1|2}}{2 \\cdot 4} = \\frac{1 + 1}{8} = 0.25\n",
        "$$\n",
        "   Аналогично вычисляем все $p_{ij}$:\n",
        "$$\n",
        "   P = \\begin{bmatrix}\n",
        "   0 & 0.25 & 0 & 0 \\\\\n",
        "   0.25 & 0 & 0.25 & 0 \\\\\n",
        "   0 & 0.25 & 0 & 0.25 \\\\\n",
        "   0 & 0 & 0.25 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Инициализация точек в целевом пространстве\n",
        "Инициализируем точки в 2D случайным образом:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление попарных сходств в целевом пространстве\n",
        "Используем t-распределение для вычисления $q_{ij}$:\n",
        "\n",
        "$$\n",
        "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
        "$$\n",
        "\n",
        "1. **Вычисляем расстояния в целевом пространстве**:\n",
        "$$\n",
        "   \\|y_i - y_j\\|^2 = (y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2\n",
        "$$\n",
        "   Например:\n",
        "$$\n",
        "   \\|y_1 - y_2\\|^2 = (0.1 - 0.3)^2 + (0.2 - 0.4)^2 = 0.04 + 0.04 = 0.08\n",
        "$$\n",
        "   Аналогично вычисляем все попарные расстояния:\n",
        "$$\n",
        "   \\begin{bmatrix}\n",
        "   0 & 0.08 & 0.32 & 0.72 \\\\\n",
        "   0.08 & 0 & 0.08 & 0.32 \\\\\n",
        "   0.32 & 0.08 & 0 & 0.08 \\\\\n",
        "   0.72 & 0.32 & 0.08 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Вычисляем $q_{ij}$**:\n",
        "   Например:\n",
        "$$\n",
        "   q_{12} = \\frac{(1 + 0.08)^{-1}}{(1 + 0.08)^{-1} + (1 + 0.32)^{-1} + (1 + 0.72)^{-1} + \\dots} \\approx \\frac{0.9259}{0.9259 + 0.7576 + 0.5814 + \\dots}\n",
        "$$\n",
        "   (Продолжаем вычисления для всех $q_{ij}$.)\n",
        "\n",
        "\n",
        "### Шаг 4: Минимизация KL-дивергенции\n",
        "\n",
        "На этом шаге мы минимизируем расхождение между распределениями $P$ (исходное пространство) и $Q$ (целевое пространство) с использованием **KL-дивергенции** (Kullback-Leibler divergence). Формула KL-дивергенции:\n",
        "\n",
        "$$\n",
        "KL(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
        "$$\n",
        "\n",
        "Для минимизации KL-дивергенции используется **градиентный спуск**. Градиент функции потерь по координатам $y_i$ вычисляется следующим образом:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial KL}{\\partial y_i} = 4 \\sum_{j \\neq i} (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Пошаговое вычисление градиента и обновление координат\n",
        "\n",
        "1. **Вычисляем градиент для каждой точки**:\n",
        "   Для каждой точки $y_i$ вычисляем градиент по формуле выше. Например, для $y_1$:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\sum_{j \\neq 1} (p_{1j} - q_{1j})(y_1 - y_j)(1 + \\|y_1 - y_j\\|^2)^{-1}\n",
        "$$\n",
        "   Подставляем значения $p_{1j}$, $q_{1j}$ и расстояния $\\|y_1 - y_j\\|^2$, которые мы вычислили ранее.\n",
        "\n",
        "2. **Обновляем координаты**:\n",
        "   Координаты $y_i$ обновляются по формуле градиентного спуска:\n",
        "$$\n",
        "   y_i^{(t+1)} = y_i^{(t)} - \\eta \\frac{\\partial KL}{\\partial y_i}\n",
        "$$\n",
        "   где $\\eta$ — скорость обучения (learning rate). Обычно $\\eta$ выбирается в диапазоне от 10 до 1000.\n",
        "\n",
        "3. **Повторяем процесс**:\n",
        "   Шаги 1 и 2 повторяются для каждой точки и на каждой итерации градиентного спуска. Процесс продолжается до достижения заданного количества итераций или сходимости.\n",
        "\n",
        "\n",
        "\n",
        "#### Пример вычисления градиента для $y_1$\n",
        "\n",
        "1. **Вычисляем градиент**:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\left[ (p_{12} - q_{12})(y_1 - y_2)(1 + \\|y_1 - y_2\\|^2)^{-1} + (p_{13} - q_{13})(y_1 - y_3)(1 + \\|y_1 - y_3\\|^2)^{-1} + (p_{14} - q_{14})(y_1 - y_4)(1 + \\|y_1 - y_4\\|^2)^{-1} \\right]\n",
        "$$\n",
        "   Подставляем значения:\n",
        "$$\n",
        "   p_{12} = 0.25, \\quad q_{12} \\approx 0.334, \\quad \\|y_1 - y_2\\|^2 = 0.08\n",
        "$$\n",
        "$$\n",
        "   p_{13} = 0, \\quad q_{13} \\approx 0.324, \\quad \\|y_1 - y_3\\|^2 = 0.32\n",
        "$$\n",
        "$$\n",
        "   p_{14} = 0, \\quad q_{14} \\approx 0.334, \\quad \\|y_1 - y_4\\|^2 = 0.72\n",
        "$$\n",
        "   Тогда:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\left[ (0.25 - 0.334)(y_1 - y_2)(1 + 0.08)^{-1} + (0 - 0.324)(y_1 - y_3)(1 + 0.32)^{-1} + (0 - 0.334)(y_1 - y_4)(1 + 0.72)^{-1} \\right]\n",
        "$$\n",
        "   Упрощаем:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\left[ (-0.084)(y_1 - y_2)(0.9259) + (-0.324)(y_1 - y_3)(0.7576) + (-0.334)(y_1 - y_4)(0.5814) \\right]\n",
        "$$\n",
        "\n",
        "2. **Обновляем координаты**:\n",
        "   Предположим, скорость обучения $\\eta = 100$. Тогда:\n",
        "$$\n",
        "   y_1^{(t+1)} = y_1^{(t)} - 100 \\cdot \\frac{\\partial KL}{\\partial y_1}\n",
        "$$\n",
        "\n",
        "Таким образом, после выполнения всех итераций градиентного спуска координаты точек в целевом пространстве $Y$ будут обновлены так, чтобы минимизировать KL-дивергенцию между распределениями $P$ и $Q$. Это позволяет сохранить локальные сходства между точками в низкоразмерном пространстве.\n",
        "\n",
        "\n",
        "1. Реализация t-SNE с нуля\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w9O82CQvduFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class tSNE:\n",
        "    def __init__(self, n_components=2, perplexity=30, learning_rate=200, n_iter=1000):\n",
        "        self.n_components = n_components\n",
        "        self.perplexity = perplexity\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def _compute_pairwise_affinities(self, X):\n",
        "        distances = squareform(pdist(X, 'euclidean'))\n",
        "        affinities = np.zeros((X.shape[0], X.shape[0]))\n",
        "        for i in range(X.shape[0]):\n",
        "            beta = self._binary_search_beta(distances[i], self.perplexity)\n",
        "            affinities[i] = np.exp(-beta * distances[i])\n",
        "            affinities[i, i] = 0  # Исключаем диагональные элементы\n",
        "        affinities /= np.sum(affinities, axis=1, keepdims=True)\n",
        "        return (affinities + affinities.T) / (2 * X.shape[0])\n",
        "\n",
        "    def _binary_search_beta(self, distances, perplexity):\n",
        "        beta = 1.0\n",
        "        beta_min, beta_max = -np.inf, np.inf\n",
        "        tolerance = 1e-5\n",
        "        max_iter = 50\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            affinities = np.exp(-beta * distances)\n",
        "            sum_affinities = np.sum(affinities)\n",
        "            entropy = np.log(sum_affinities) + beta * np.sum(distances * affinities) / sum_affinities\n",
        "            entropy_diff = entropy - np.log(perplexity)\n",
        "\n",
        "            if np.abs(entropy_diff) < tolerance:\n",
        "                break\n",
        "\n",
        "            if entropy_diff > 0:\n",
        "                beta_min = beta\n",
        "                beta = (beta_max + beta) / 2 if beta_max != np.inf else beta * 2\n",
        "            else:\n",
        "                beta_max = beta\n",
        "                beta = (beta_min + beta) / 2 if beta_min != -np.inf else beta / 2\n",
        "\n",
        "        return beta\n",
        "\n",
        "    def _compute_low_dimensional_affinities(self, Y):\n",
        "        distances = squareform(pdist(Y, 'euclidean'))\n",
        "        affinities = 1 / (1 + distances**2)\n",
        "        np.fill_diagonal(affinities, 0)\n",
        "        affinities /= np.sum(affinities)\n",
        "        return affinities\n",
        "\n",
        "    def _compute_gradient(self, P, Q, Y):\n",
        "        n = Y.shape[0]\n",
        "        gradient = np.zeros_like(Y)\n",
        "        for i in range(n):\n",
        "            diff = Y[i] - Y\n",
        "            grad = 4 * np.sum((P[i] - Q[i]) * diff.T * (1 + np.sum(diff**2, axis=1))**(-1), axis=1)\n",
        "            gradient[i] = grad\n",
        "        return gradient\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        Y = np.random.randn(n_samples, self.n_components) * 1e-4\n",
        "        P = self._compute_pairwise_affinities(X)\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            Q = self._compute_low_dimensional_affinities(Y)\n",
        "            gradient = self._compute_gradient(P, Q, Y)\n",
        "            Y -= self.learning_rate * gradient\n",
        "\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "tsne = tSNE(n_components=2, perplexity=2, learning_rate=100, n_iter=1000)\n",
        "Y = tsne.fit_transform(X)\n",
        "\n",
        "print(\"Результат t-SNE (с нуля):\")\n",
        "print(Y)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "plt.title(\"t-SNE (с нуля)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "bkqZN28MgLwp",
        "outputId": "4c14eb66-fc2f-46f8-e542-1067b8038301"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат t-SNE (с нуля):\n",
            "[[ 10.65466838 -19.32408985]\n",
            " [  3.91914037  -7.10808937]\n",
            " [ -3.91924071   7.10814647]\n",
            " [-10.65474846  19.32415812]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn"
      ],
      "metadata": {
        "id": "hXppAc9igNqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=2, learning_rate=100, n_iter=1000)\n",
        "Y = tsne.fit_transform(X)\n",
        "\n",
        "print(\"Результат t-SNE (scikit-learn):\")\n",
        "print(Y)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "plt.title(\"t-SNE (scikit-learn)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "0o0eUVumgRps",
        "outputId": "c14eddeb-eac3-4eca-9420-a7835c9616ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат t-SNE (scikit-learn):\n",
            "[[ 1.6463555e+03  2.9633252e-06]\n",
            " [ 7.4033746e+02 -9.2934698e-07]\n",
            " [-1.2766664e+03 -8.9721962e-06]\n",
            " [-4.8086075e+02  6.8781947e-06]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.4 UMAP (Uniform Manifold Approximation and Projection)\n",
        "\n",
        "**UMAP (Uniform Manifold Approximation and Projection)** — это современный метод снижения размерности, который активно используется для визуализации данных и построения низкоразмерных представлений. В основе UMAP лежат идеи из римановой геометрии и алгебраической топологии, что позволяет ему эффективно работать с большими наборами данных, сохраняя как локальную, так и глобальную структуру.\n",
        "\n",
        "\n",
        "\n",
        "### Основные идеи UMAP\n",
        "\n",
        "1. **Предположение о многообразии**  \n",
        "   UMAP исходит из того, что данные лежат на некотором гладком многообразии в высокоразмерном пространстве. Это позволяет аппроксимировать структуру данных, учитывая их локальные и глобальные свойства.\n",
        "\n",
        "2. **Графовое представление**  \n",
        "   Метод строит взвешенный граф, где вершины соответствуют объектам данных, а ребра отражают степень сходства между ними. Веса ребер вычисляются на основе расстояний между точками.\n",
        "\n",
        "3. **Оптимизация**  \n",
        "   UMAP минимизирует расхождение между графами, построенными в исходном и целевом пространствах, используя стохастический градиентный спуск.\n",
        "\n",
        "\n",
        "\n",
        "### Алгоритм UMAP\n",
        "\n",
        "#### Шаг 1: Построение графа в исходном пространстве\n",
        "\n",
        "1. **Вычисление расстояний**  \n",
        "   Для каждой пары объектов $x_i$ и $x_j$ в исходном пространстве вычисляется расстояние $d(x_i, x_j)$. Чаще всего используется евклидово расстояние:\n",
        "$$\n",
        "   d(x_i, x_j) = \\sqrt{\\sum_{k=1}^n (x_{ik} - x_{jk})^2},\n",
        "$$\n",
        "   где $x_{ik}$ и $x_{jk}$ — $k$-е компоненты векторов $x_i$ и $x_j$.\n",
        "\n",
        "2. **Определение локальной окрестности**  \n",
        "   Для каждого объекта $x_i$ определяется его локальная окрестность на основе параметра $k$ (количество ближайших соседей). Это позволяет учитывать локальную структуру данных.\n",
        "\n",
        "3. **Вычисление весов ребер**  \n",
        "   Веса ребер $w_{ij}$ в графе вычисляются по формуле:\n",
        "$$\n",
        "   w_{ij} = \\exp\\left(-\\frac{d(x_i, x_j) - \\rho_i}{\\sigma_i}\\right),\n",
        "$$\n",
        "   где:\n",
        "   - $\\rho_i$ — расстояние до ближайшего соседа объекта $x_i$,\n",
        "   - $\\sigma_i$ — параметр, который выбирается так, чтобы сумма весов для каждого объекта была равна $\\log_2(k)$.\n",
        "\n",
        "#### Шаг 2: Построение графа в целевом пространстве\n",
        "\n",
        "1. **Инициализация точек**  \n",
        "   Точки в целевом пространстве $y_i$ инициализируются случайным образом или с помощью других методов, таких как PCA.\n",
        "\n",
        "2. **Вычисление весов ребер**  \n",
        "   Веса ребер $v_{ij}$ в целевом пространстве вычисляются по формуле:\n",
        "$$\n",
        "   v_{ij} = \\left(1 + a \\|y_i - y_j\\|^{2b}\\right)^{-1},\n",
        "$$\n",
        "   где $a$ и $b$ — гиперпараметры, определяющие форму кривой.\n",
        "\n",
        "#### Шаг 3: Минимизация расхождения между графами\n",
        "\n",
        "1. **Функция потерь**  \n",
        "   UMAP минимизирует расхождение между графами в исходном и целевом пространствах с помощью функции потерь, которая состоит из двух компонент:\n",
        "   - Притяжение (attraction) для близких точек,\n",
        "   - Отталкивание (repulsion) для далеких точек.  \n",
        "   Формула функции потерь:\n",
        "$$\n",
        "   L = \\sum_{i,j} w_{ij} \\log\\left(\\frac{w_{ij}}{v_{ij}}\\right) + (1 - w_{ij}) \\log\\left(\\frac{1 - w_{ij}}{1 - v_{ij}}\\right).\n",
        "$$\n",
        "\n",
        "2. **Градиентный спуск**  \n",
        "   Для минимизации функции потерь используется стохастический градиентный спуск. Градиент функции потерь по координатам $y_i$ вычисляется аналитически и используется для обновления координат.\n",
        "\n",
        "   - Градиент функции потерь по $y_i$:\n",
        "  $$\n",
        "     \\frac{\\partial L}{\\partial y_i} = \\sum_{j} \\left( w_{ij} - v_{ij} \\right) \\cdot \\frac{\\partial v_{ij}}{\\partial y_i},\n",
        "  $$\n",
        "     где:\n",
        "  $$\n",
        "     \\frac{\\partial v_{ij}}{\\partial y_i} = -2ab \\cdot \\frac{\\|y_i - y_j\\|^{2b-1}}{(1 + a \\|y_i - y_j\\|^{2b})^2} \\cdot (y_i - y_j).\n",
        "  $$\n",
        "\n",
        "   - Обновление координат $y_i$:\n",
        "  $$\n",
        "     y_i^{(t+1)} = y_i^{(t)} - \\eta \\cdot \\frac{\\partial L}{\\partial y_i},\n",
        "  $$\n",
        "     где:\n",
        "     - $\\eta$ — скорость обучения (learning rate),\n",
        "     - $t$ — номер итерации.\n",
        "\n",
        "Таким образом, UMAP — это мощный и гибкий метод снижения размерности, который эффективно сохраняет как локальную, так и глобальную структуру данных. Его алгоритм основан на графовом представлении данных и оптимизации с использованием стохастического градиентного спуска. Формулы и шаги, описанные выше, позволяют глубже понять математическую основу UMAP и процесс его работы.\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Наша цель — снизить размерность данных до 2D-пространства с помощью UMAP. Мы пройдем шаг за шагом через основные этапы алгоритма, включая вычисление расстояний, построение графа и оптимизацию.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Построение графа в исходном пространстве\n",
        "\n",
        "1. **Вычисление расстояний**  \n",
        "   Используем евклидово расстояние для вычисления расстояний между всеми парами точек. Формула для евклидова расстояния между точками $x_i$ и $x_j$:\n",
        "$$\n",
        "   d(x_i, x_j) = \\sqrt{\\sum_{k=1}^3 (x_{ik} - x_{jk})^2}.\n",
        "$$\n",
        "   Вычислим расстояния между всеми парами точек:\n",
        "   - $d(x_1, x_2) = \\sqrt{(1-4)^2 + (2-5)^2 + (3-6)^2} = \\sqrt{9 + 9 + 9} = \\sqrt{27} \\approx 5.196$\n",
        "   - $d(x_1, x_3) = \\sqrt{(1-7)^2 + (2-8)^2 + (3-9)^2} = \\sqrt{36 + 36 + 36} = \\sqrt{108} \\approx 10.392$\n",
        "   - $d(x_1, x_4) = \\sqrt{(1-10)^2 + (2-11)^2 + (3-12)^2} = \\sqrt{81 + 81 + 81} = \\sqrt{243} \\approx 15.588$\n",
        "   - $d(x_2, x_3) = \\sqrt{(4-7)^2 + (5-8)^2 + (6-9)^2} = \\sqrt{9 + 9 + 9} = \\sqrt{27} \\approx 5.196$\n",
        "   - $d(x_2, x_4) = \\sqrt{(4-10)^2 + (5-11)^2 + (6-12)^2} = \\sqrt{36 + 36 + 36} = \\sqrt{108} \\approx 10.392$\n",
        "   - $d(x_3, x_4) = \\sqrt{(7-10)^2 + (8-11)^2 + (9-12)^2} = \\sqrt{9 + 9 + 9} = \\sqrt{27} \\approx 5.196$\n",
        "\n",
        "2. **Определение локальной окрестности**  \n",
        "   Пусть параметр $k = 2$ (каждая точка имеет 2 ближайших соседа).  \n",
        "   Для каждой точки найдем расстояние до второго ближайшего соседа ($\\rho_i$):\n",
        "   - Для $x_1$: ближайшие соседи $x_2$ и $x_3$. Расстояние до второго соседа $\\rho_1 = d(x_1, x_3) \\approx 10.392$.\n",
        "   - Для $x_2$: ближайшие соседи $x_1$ и $x_3$. Расстояние до второго соседа $\\rho_2 = d(x_2, x_3) \\approx 5.196$.\n",
        "   - Для $x_3$: ближайшие соседи $x_2$ и $x_4$. Расстояние до второго соседа $\\rho_3 = d(x_3, x_4) \\approx 5.196$.\n",
        "   - Для $x_4$: ближайшие соседи $x_2$ и $x_3$. Расстояние до второго соседа $\\rho_4 = d(x_4, x_3) \\approx 5.196$.\n",
        "\n",
        "3. **Вычисление весов ребер**  \n",
        "   Используем формулу:\n",
        "$$\n",
        "   w_{ij} = \\exp\\left(-\\frac{d(x_i, x_j) - \\rho_i}{\\sigma_i}\\right).\n",
        "$$\n",
        "   Параметр $\\sigma_i$ выбирается так, чтобы сумма весов для каждой точки была равна $\\log_2(k) = \\log_2(2) = 1$.  \n",
        "   Для простоты предположим, что $\\sigma_i = 1$ для всех точек.  \n",
        "   Тогда:\n",
        "   - $w_{12} = \\exp\\left(-\\frac{5.196 - 10.392}{1}\\right) = \\exp(5.196) \\approx 180.1$\n",
        "   - $w_{13} = \\exp\\left(-\\frac{10.392 - 10.392}{1}\\right) = \\exp(0) = 1$\n",
        "   - $w_{14} = \\exp\\left(-\\frac{15.588 - 10.392}{1}\\right) = \\exp(-5.196) \\approx 0.0056$\n",
        "   - $w_{23} = \\exp\\left(-\\frac{5.196 - 5.196}{1}\\right) = \\exp(0) = 1$\n",
        "   - $w_{24} = \\exp\\left(-\\frac{10.392 - 5.196}{1}\\right) = \\exp(-5.196) \\approx 0.0056$\n",
        "   - $w_{34} = \\exp\\left(-\\frac{5.196 - 5.196}{1}\\right) = \\exp(0) = 1$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Построение графа в целевом пространстве\n",
        "\n",
        "1. **Инициализация точек**  \n",
        "   Инициализируем точки в целевом 2D-пространстве случайным образом:\n",
        "   - $y_1 = (0.5, 0.5)$\n",
        "   - $y_2 = (1.0, 1.0)$\n",
        "   - $y_3 = (1.5, 1.5)$\n",
        "   - $y_4 = (2.0, 2.0)$\n",
        "\n",
        "2. **Вычисление весов ребер**  \n",
        "   Используем формулу:\n",
        "$$\n",
        "   v_{ij} = \\left(1 + a \\|y_i - y_j\\|^{2b}\\right)^{-1}.\n",
        "$$\n",
        "   Пусть $a = 1$ и $b = 1$. Тогда:\n",
        "   - $v_{12} = \\left(1 + 1 \\cdot \\|(0.5, 0.5) - (1.0, 1.0)\\|^2\\right)^{-1} = \\left(1 + 0.5\\right)^{-1} \\approx 0.6667$\n",
        "   - $v_{13} = \\left(1 + 1 \\cdot \\|(0.5, 0.5) - (1.5, 1.5)\\|^2\\right)^{-1} = \\left(1 + 2\\right)^{-1} \\approx 0.3333$\n",
        "   - $v_{14} = \\left(1 + 1 \\cdot \\|(0.5, 0.5) - (2.0, 2.0)\\|^2\\right)^{-1} = \\left(1 + 4.5\\right)^{-1} \\approx 0.1818$\n",
        "   - $v_{23} = \\left(1 + 1 \\cdot \\|(1.0, 1.0) - (1.5, 1.5)\\|^2\\right)^{-1} = \\left(1 + 0.5\\right)^{-1} \\approx 0.6667$\n",
        "   - $v_{24} = \\left(1 + 1 \\cdot \\|(1.0, 1.0) - (2.0, 2.0)\\|^2\\right)^{-1} = \\left(1 + 2\\right)^{-1} \\approx 0.3333$\n",
        "   - $v_{34} = \\left(1 + 1 \\cdot \\|(1.5, 1.5) - (2.0, 2.0)\\|^2\\right)^{-1} = \\left(1 + 0.5\\right)^{-1} \\approx 0.6667$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Минимизация расхождения между графами\n",
        "\n",
        "1. **Функция потерь**  \n",
        "   Используем формулу:\n",
        "$$\n",
        "   L = \\sum_{i,j} w_{ij} \\log\\left(\\frac{w_{ij}}{v_{ij}}\\right) + (1 - w_{ij}) \\log\\left(\\frac{1 - w_{ij}}{1 - v_{ij}}\\right).\n",
        "$$\n",
        "   Вычислим потери для каждой пары точек. Например, для $(x_1, x_2)$:\n",
        "$$\n",
        "   L_{12} = 180.1 \\cdot \\log\\left(\\frac{180.1}{0.6667}\\right) + (1 - 180.1) \\cdot \\log\\left(\\frac{1 - 180.1}{1 - 0.6667}\\right).\n",
        "$$\n",
        "   (Здесь видно, что веса $w_{ij}$ должны быть нормализованы, чтобы избежать отрицательных значений в логарифмах. В реальном алгоритме это учитывается.)\n",
        "\n",
        "2. **Градиентный спуск**  \n",
        "   Градиент функции потерь по $y_i$:\n",
        "$$\n",
        "   \\frac{\\partial L}{\\partial y_i} = \\sum_{j} \\left( w_{ij} - v_{ij} \\right) \\cdot \\frac{\\partial v_{ij}}{\\partial y_i}.\n",
        "$$\n",
        "   Для $y_1$:\n",
        "$$\n",
        "   \\frac{\\partial L}{\\partial y_1} = (w_{12} - v_{12}) \\cdot \\frac{\\partial v_{12}}{\\partial y_1} + (w_{13} - v_{13}) \\cdot \\frac{\\partial v_{13}}{\\partial y_1} + (w_{14} - v_{14}) \\cdot \\frac{\\partial v_{14}}{\\partial y_1}.\n",
        "$$\n",
        "   Аналогично вычисляем для $y_2, y_3, y_4$.\n",
        "\n",
        "3. **Обновление координат**  \n",
        "   Используем формулу:\n",
        "$$\n",
        "   y_i^{(t+1)} = y_i^{(t)} - \\eta \\cdot \\frac{\\partial L}{\\partial y_i}.\n",
        "$$\n",
        "   Пусть $\\eta = 0.1$. После вычисления градиентов обновляем координаты.\n",
        "\n",
        "Таким образом, этот пример демонстрирует основные шаги UMAP для набора данных из 4 точек в 3D-пространстве. В реальных задачах данные более сложные, а вычисления выполняются с использованием библиотек, таких как `umap-learn`. Однако понимание математической основы помогает глубже разобраться в работе алгоритма.\n",
        "\n",
        "\n",
        "Реализация UMAP с нуля\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tYsID0KU7tly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class UMAP:\n",
        "    def __init__(self, n_components=2, n_neighbors=2, learning_rate=0.1, max_iters=200):\n",
        "        self.n_components = n_components\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        # Шаг 1: Построение графа в исходном пространстве\n",
        "        n_samples = X.shape[0]\n",
        "        distances = cdist(X, X, metric='euclidean')  # Вычисляем расстояния между всеми точками\n",
        "\n",
        "        # Находим k ближайших соседей для каждой точки\n",
        "        knn_indices = np.argsort(distances, axis=1)[:, 1:self.n_neighbors + 1]\n",
        "        knn_distances = np.array([distances[i, knn_indices[i]] for i in range(n_samples)])\n",
        "\n",
        "        # Вычисляем веса ребер\n",
        "        rho = knn_distances[:, -1]  # Расстояние до k-го соседа\n",
        "        sigma = np.ones(n_samples)  # Упрощение: sigma = 1 для всех точек\n",
        "        weights = np.exp(-(knn_distances - rho[:, np.newaxis]) / sigma[:, np.newaxis])\n",
        "\n",
        "        # Шаг 2: Инициализация точек в целевом пространстве\n",
        "        Y = np.random.rand(n_samples, self.n_components)  # Случайная инициализация\n",
        "\n",
        "        # Шаг 3: Оптимизация с использованием градиентного спуска\n",
        "        for iter in range(self.max_iters):\n",
        "            # Вычисляем расстояния в целевом пространстве\n",
        "            Y_distances = cdist(Y, Y, metric='euclidean')\n",
        "\n",
        "            # Вычисляем веса в целевом пространстве\n",
        "            a, b = 1, 1  # Гиперпараметры\n",
        "            Y_weights = 1 / (1 + a * Y_distances ** (2 * b))\n",
        "\n",
        "            # Вычисляем градиент\n",
        "            grad = np.zeros_like(Y)\n",
        "            for i in range(n_samples):\n",
        "                for j in knn_indices[i]:\n",
        "                    grad[i] += (weights[i, np.where(knn_indices[i] == j)[0][0]] - Y_weights[i, j]) * \\\n",
        "                               (-2 * a * b * (Y[i] - Y[j]) * Y_distances[i, j] ** (2 * b - 1)) / \\\n",
        "                               (1 + a * Y_distances[i, j] ** (2 * b)) ** 2\n",
        "\n",
        "            # Обновляем координаты\n",
        "            Y -= self.learning_rate * grad\n",
        "\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "umap = UMAP(n_components=2, n_neighbors=2, learning_rate=0.1, max_iters=200)\n",
        "Y = umap.fit_transform(X)\n",
        "\n",
        "print(\"Результат UMAP (с нуля):\")\n",
        "print(Y)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "plt.title(\"UMAP с нуля\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "-9mlJFru7uJh",
        "outputId": "49394466-f488-4bf1-dbff-8856c7ac2aec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат UMAP (с нуля):\n",
            "[[ -2.4089547  -26.3554506 ]\n",
            " [ -1.20598843   2.64720762]\n",
            " [  2.24463693  -1.89918011]\n",
            " [-21.91789049  14.90835387]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Реализация с использованием библиотеки umap-learn"
      ],
      "metadata": {
        "id": "PlX-o6Bw7wpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Устанавливаем библиотеку, если она не установлена\n",
        "!pip install umap-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZpjDKyK78wO",
        "outputId": "d733f575-365b-4649-e5b4-f59355ed4d42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.6.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые модули\n",
        "import umap.umap_ as umap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Создание и обучение модели UMAP\n",
        "reducer = umap.UMAP(n_components=2, n_neighbors=2)\n",
        "Y = reducer.fit_transform(X)\n",
        "\n",
        "print(\"Результат UMAP (с использованием библиотеки):\")\n",
        "print(Y)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "plt.title(\"UMAP с использованием библиотеки\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "NUQFm8oi73wx",
        "outputId": "be7f4ffd-ec46-4102-9a11-97104d35c265"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат UMAP (с использованием библиотеки):\n",
            "[[  9.34821  -28.23775 ]\n",
            " [  9.741285 -28.973705]\n",
            " [ 10.800575 -28.75051 ]\n",
            " [ 11.244875 -28.092653]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)\n",
        "\n",
        "**Метод независимых компонент (ICA, Independent Component Analysis)** — это мощный статистический метод, используемый для разделения многомерных сигналов на статистически независимые компоненты. В отличие от методов, таких как PCA (Principal Component Analysis), которые ищут ортогональные направления максимальной дисперсии, ICA фокусируется на поиске независимых источников сигналов. Это делает его особенно полезным в задачах, где требуется разделить смешанные сигналы, такие как обработка звука, изображений, биомедицинских данных, финансовых временных рядов и других областей.\n",
        "\n",
        "\n",
        "\n",
        "### Основные идеи ICA\n",
        "\n",
        "1. **Независимость компонент**  \n",
        "   ICA предполагает, что наблюдаемые данные являются линейной смесью независимых источников. Цель метода — найти такие линейные преобразования, которые разделят смешанные сигналы на независимые компоненты. Независимость здесь понимается в статистическом смысле: компоненты не должны содержать информации друг о друге.\n",
        "\n",
        "2. **Нормализация данных**  \n",
        "   Перед применением ICA данные обычно центрируются (вычитается среднее значение) и нормализуются (приводятся к единичной дисперсии). Это упрощает процесс анализа и улучшает устойчивость алгоритма.\n",
        "\n",
        "3. **Нелинейные функции**  \n",
        "   ICA использует нелинейные функции для измерения независимости, такие как кумулянты или функции, основанные на энтропии. Эти функции помогают выделить компоненты, которые максимально отличаются от гауссовых распределений.\n",
        "\n",
        "\n",
        "\n",
        "### Математическая основа ICA\n",
        "\n",
        "#### Модель ICA\n",
        "\n",
        "Предположим, что у нас есть $n$ наблюдаемых сигналов $\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T$, которые являются линейной смесью $m$ независимых источников $\\mathbf{s} = (s_1, s_2, \\dots, s_m)^T$. Модель ICA может быть записана как:\n",
        "$$\n",
        "\\mathbf{x} = \\mathbf{A} \\mathbf{s},\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{A}$ — матрица смешивания размерности $n \\times m$,\n",
        "- $\\mathbf{s}$ — вектор независимых источников.\n",
        "\n",
        "Цель ICA — найти матрицу разделения $\\mathbf{W}$, такую что:\n",
        "$$\n",
        "\\mathbf{s} = \\mathbf{W} \\mathbf{x}.\n",
        "$$\n",
        "Матрица $\\mathbf{W}$ является оценкой обратной матрицы $\\mathbf{A}^{-1}$.\n",
        "\n",
        "#### Предположения ICA\n",
        "\n",
        "1. **Независимость источников**  \n",
        "   Источники $s_i$ статистически независимы, то есть их совместное распределение может быть факторизовано:\n",
        "   $$\n",
        "   p(s_1, s_2, \\dots, s_m) = \\prod_{i=1}^m p(s_i).\n",
        "   $$\n",
        "\n",
        "2. **Нормализация источников**  \n",
        "   Источники имеют нулевое среднее и единичную дисперсию:\n",
        "   $$\n",
        "   \\mathbb{E}[s_i] = 0, \\quad \\mathbb{E}[s_i^2] = 1.\n",
        "   $$\n",
        "\n",
        "3. **Негауссовость источников**  \n",
        "   Источники имеют негауссово распределение, так как гауссовы распределения не позволяют однозначно восстановить независимые компоненты. Это связано с тем, что сумма гауссовых случайных величин также является гауссовой, что делает невозможным разделение таких сигналов.\n",
        "\n",
        "\n",
        "\n",
        "### Алгоритм ICA\n",
        "\n",
        "#### Шаг 1: Центрирование и нормализация данных\n",
        "\n",
        "1. **Центрирование**  \n",
        "   Вычитаем среднее значение из каждого сигнала:\n",
        "   $$\n",
        "   \\mathbf{x}_{\\text{центр}} = \\mathbf{x} - \\mathbb{E}[\\mathbf{x}].\n",
        "   $$\n",
        "\n",
        "2. **Нормализация**  \n",
        "   Приводим данные к единичной дисперсии:\n",
        "   $$\n",
        "   \\mathbf{x}_{\\text{норм}} = \\frac{\\mathbf{x}_{\\text{центр}}}{\\sqrt{\\mathbb{E}[\\mathbf{x}_{\\text{центр}}^2]}}.\n",
        "   $$\n",
        "\n",
        "#### Шаг 2: Преобразование данных в пространство независимых компонент\n",
        "\n",
        "1. **Инициализация матрицы разделения**  \n",
        "   Инициализируем матрицу $\\mathbf{W}$ случайными значениями.\n",
        "\n",
        "2. **Оптимизация функции потерь**  \n",
        "   Используем функцию потерь, которая измеряет степень независимости компонент. Одна из популярных функций потерь — это негауссовость, которая может быть измерена с помощью кумулянтов или энтропии.\n",
        "\n",
        "   - **Кумулянты**  \n",
        "     Кумулянты высших порядков (например, эксцесс) используются для измерения отклонения от гауссовости:\n",
        "     $$\n",
        "     \\text{Эксцесс}(s_i) = \\mathbb{E}[s_i^4] - 3(\\mathbb{E}[s_i^2])^2.\n",
        "     $$\n",
        "\n",
        "   - **Энтропия**  \n",
        "     Энтропия измеряет степень неопределенности случайной величины. Для независимых компонент энтропия должна быть минимальной:\n",
        "     $$\n",
        "     H(s_i) = -\\int p(s_i) \\log p(s_i) \\, ds_i.\n",
        "     $$\n",
        "\n",
        "3. **Градиентный спуск**  \n",
        "   Для минимизации функции потерь используется градиентный спуск. Градиент функции потерь по элементам матрицы $\\mathbf{W}$ вычисляется аналитически и используется для обновления матрицы.\n",
        "\n",
        "   - Градиент функции потерь по $\\mathbf{W}$:\n",
        "     $$\n",
        "     \\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{W}^{-T} - \\mathbf{g}(\\mathbf{s}) \\mathbf{x}^T,\n",
        "     $$\n",
        "     где $\\mathbf{g}(\\mathbf{s})$ — нелинейная функция, такая как $\\tanh(s_i)$.\n",
        "\n",
        "   - Обновление матрицы $\\mathbf{W}$:\n",
        "     $$\n",
        "     \\mathbf{W}^{(t+1)} = \\mathbf{W}^{(t)} - \\eta \\cdot \\frac{\\partial L}{\\partial \\mathbf{W}},\n",
        "     $$\n",
        "     где:\n",
        "     - $\\eta$ — скорость обучения (learning rate),\n",
        "     - $t$ — номер итерации.\n",
        "\n",
        "#### Шаг 3: Восстановление независимых компонент\n",
        "\n",
        "После оптимизации матрицы $\\mathbf{W}$ независимые компоненты могут быть восстановлены как:\n",
        "$$\n",
        "\\mathbf{s} = \\mathbf{W} \\mathbf{x}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Примеры применения ICA\n",
        "\n",
        "1. **Разделение звуковых сигналов**  \n",
        "   ICA используется для разделения смешанных звуковых сигналов, таких как речь или музыка, на независимые источники. Например, в задаче \"коктейльной вечеринки\" ICA позволяет выделить отдельные голоса из смеси звуков.\n",
        "\n",
        "2. **Обработка изображений**  \n",
        "   В задачах обработки изображений ICA может использоваться для выделения независимых компонент, таких как текстуры или объекты. Это полезно для сжатия данных или выделения признаков.\n",
        "\n",
        "3. **Биомедицинские данные**  \n",
        "   ICA применяется для анализа ЭЭГ и МРТ данных, где необходимо выделить независимые источники сигналов, такие как активность мозга или артефакты. Это помогает в диагностике и исследовании мозговой активности.\n",
        "\n",
        "4. **Финансовый анализ**  \n",
        "   В финансах ICA может быть использован для анализа независимых факторов, влияющих на рынок, таких как изменения цен на акции или валютные курсы.\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества и ограничения ICA\n",
        "\n",
        "**Преимущества**:\n",
        "- Позволяет выделить независимые компоненты из смешанных данных.\n",
        "- Не требует априорной информации о структуре данных.\n",
        "- Широко применяется в различных областях, таких как обработка сигналов, нейронаука, финансы и др.\n",
        "\n",
        "**Ограничения**:\n",
        "- Чувствителен к шуму и выбросам.\n",
        "- Требует выполнения предположений о независимости и негауссовости распределения.\n",
        "- Может быть вычислительно сложным для больших объёмов данных.\n",
        "\n",
        "Таким образом, метод независимых компонент (ICA) — это мощный инструмент для анализа данных, который позволяет разделять смешанные сигналы на независимые источники. Его математическая основа включает в себя предположения о независимости и негауссовости источников, а также использование нелинейных функций для измерения независимости. Алгоритм ICA включает в себя центрирование и нормализацию данных, оптимизацию функции потерь с помощью градиентного спуска и восстановление независимых компонент. Этот метод находит широкое применение в различных областях, таких как обработка звука, изображений и биомедицинских данных.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения метода ICA для набора данных из 4 точек в 3D-пространстве. Данные представлены матрицей:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь каждая строка соответствует одной точке в 3D-пространстве, а каждый столбец соответствует одной из трёх координат (признаков).\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Центрирование данных\n",
        "\n",
        "Центрирование данных заключается в вычитании среднего значения каждого признака (столбца) из всех его значений. Это необходимо для того, чтобы данные имели нулевое среднее.\n",
        "\n",
        "1. Вычислим среднее значение для каждого признака:\n",
        "   $$\n",
        "   \\text{Среднее по первому признаку} = \\frac{1 + 4 + 7 + 10}{4} = 5.5,\n",
        "   $$\n",
        "   $$\n",
        "   \\text{Среднее по второму признаку} = \\frac{2 + 5 + 8 + 11}{4} = 6.5,\n",
        "   $$\n",
        "   $$\n",
        "   \\text{Среднее по третьему признаку} = \\frac{3 + 6 + 9 + 12}{4} = 7.5.\n",
        "   $$\n",
        "\n",
        "2. Вычтем средние значения из каждого признака:\n",
        "   $$\n",
        "   X_{\\text{центр}} = \\begin{bmatrix}\n",
        "   1 - 5.5 & 2 - 6.5 & 3 - 7.5 \\\\\n",
        "   4 - 5.5 & 5 - 6.5 & 6 - 7.5 \\\\\n",
        "   7 - 5.5 & 8 - 6.5 & 9 - 7.5 \\\\\n",
        "   10 - 5.5 & 11 - 6.5 & 12 - 7.5\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   -4.5 & -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5 & 4.5\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Отбеливание данных\n",
        "\n",
        "Отбеливание данных — это процесс преобразования данных так, чтобы их ковариационная матрица стала единичной. Это упрощает задачу разделения сигналов.\n",
        "\n",
        "1. Вычислим ковариационную матрицу $\\mathbf{C}$ для центрированных данных:\n",
        "   $$\n",
        "   \\mathbf{C} = \\frac{1}{n-1} X_{\\text{центр}}^T X_{\\text{центр}},\n",
        "   $$\n",
        "   где $n = 4$ — количество точек.\n",
        "\n",
        "   Подставим значения:\n",
        "   $$\n",
        "   \\mathbf{C} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "   -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "   -4.5 & -1.5 & 1.5 & 4.5\n",
        "   \\end{bmatrix} \\begin{bmatrix}\n",
        "   -4.5 & -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5 & 4.5\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "   Вычислим произведение матриц:\n",
        "   $$\n",
        "   \\mathbf{C} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   81 & 81 & 81 \\\\\n",
        "   81 & 81 & 81 \\\\\n",
        "   81 & 81 & 81\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   27 & 27 & 27 \\\\\n",
        "   27 & 27 & 27 \\\\\n",
        "   27 & 27 & 27\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "2. Выполним сингулярное разложение (SVD) ковариационной матрицы:\n",
        "   $$\n",
        "   \\mathbf{C} = \\mathbf{U} \\mathbf{D} \\mathbf{U}^T,\n",
        "   $$\n",
        "   где $\\mathbf{U}$ — матрица собственных векторов, а $\\mathbf{D}$ — диагональная матрица собственных значений.\n",
        "\n",
        "   Для нашей ковариационной матрицы:\n",
        "   $$\n",
        "   \\mathbf{U} = \\begin{bmatrix}\n",
        "   1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3} \\\\\n",
        "   1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3} \\\\\n",
        "   1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3}\n",
        "   \\end{bmatrix}, \\quad \\mathbf{D} = \\begin{bmatrix}\n",
        "   81 & 0 & 0 \\\\\n",
        "   0 & 0 & 0 \\\\\n",
        "   0 & 0 & 0\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "3. Отбелим данные:\n",
        "   $$\n",
        "   X_{\\text{отбел}} = \\mathbf{D}^{-1/2} \\mathbf{U}^T X_{\\text{центр}},\n",
        "   $$\n",
        "   где $\\mathbf{D}^{-1/2}$ — диагональная матрица, обратная к корню из собственных значений.\n",
        "\n",
        "   Подставим значения:\n",
        "   $$\n",
        "   X_{\\text{отбел}} = \\begin{bmatrix}\n",
        "   1/\\sqrt{81} & 0 & 0 \\\\\n",
        "   0 & 0 & 0 \\\\\n",
        "   0 & 0 & 0\n",
        "   \\end{bmatrix} \\begin{bmatrix}\n",
        "   1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3} \\\\\n",
        "   1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3} \\\\\n",
        "   1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3}\n",
        "   \\end{bmatrix}^T \\begin{bmatrix}\n",
        "   -4.5 & -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5 & 4.5\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "   После вычислений получим:\n",
        "   $$\n",
        "   X_{\\text{отбел}} = \\begin{bmatrix}\n",
        "   -0.5 & -0.5 & -0.5 \\\\\n",
        "   -0.1667 & -0.1667 & -0.1667 \\\\\n",
        "   0.1667 & 0.1667 & 0.1667 \\\\\n",
        "   0.5 & 0.5 & 0.5\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Применение алгоритма ICA\n",
        "\n",
        "1. Инициализируем матрицу разделения $\\mathbf{W}$ случайными значениями. Например:\n",
        "   $$\n",
        "   \\mathbf{W} = \\begin{bmatrix}\n",
        "   0.5 & 0.2 & 0.3 \\\\\n",
        "   0.1 & 0.6 & 0.4 \\\\\n",
        "   0.7 & 0.8 & 0.9\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "2. Используем алгоритм FastICA для оптимизации $\\mathbf{W}$. На каждой итерации обновляем $\\mathbf{W}$ с помощью градиентного спуска.\n",
        "\n",
        "3. После оптимизации получаем матрицу разделения $\\mathbf{W}$. Например:\n",
        "   $$\n",
        "   \\mathbf{W} = \\begin{bmatrix}\n",
        "   0.707 & -0.707 & 0 \\\\\n",
        "   0.408 & 0.408 & -0.816 \\\\\n",
        "   0.577 & 0.577 & 0.577\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "4. Восстанавливаем независимые компоненты:\n",
        "   $$\n",
        "   \\mathbf{S} = \\mathbf{W} X_{\\text{отбел}}.\n",
        "   $$\n",
        "\n",
        "   Подставим значения:\n",
        "   $$\n",
        "   \\mathbf{S} = \\begin{bmatrix}\n",
        "   0.707 & -0.707 & 0 \\\\\n",
        "   0.408 & 0.408 & -0.816 \\\\\n",
        "   0.577 & 0.577 & 0.577\n",
        "   \\end{bmatrix} \\begin{bmatrix}\n",
        "   -0.5 & -0.5 & -0.5 \\\\\n",
        "   -0.1667 & -0.1667 & -0.1667 \\\\\n",
        "   0.1667 & 0.1667 & 0.1667 \\\\\n",
        "   0.5 & 0.5 & 0.5\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "   После вычислений получим:\n",
        "   $$\n",
        "   \\mathbf{S} = \\begin{bmatrix}\n",
        "   -0.2357 & -0.2357 & -0.2357 \\\\\n",
        "   -0.4082 & -0.4082 & -0.4082 \\\\\n",
        "   0.2887 & 0.2887 & 0.2887\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "Таким образом, мы применили метод ICA к набору данных из 4 точек в 3D-пространстве. После центрирования, отбеливания и оптимизации матрицы разделения $\\mathbf{W}$ мы восстановили независимые компоненты $\\mathbf{S}$. Этот пример демонстрирует основные шаги ICA и может быть расширен для более сложных данных.\n",
        "\n",
        "1. Классическая реализация ICA с нуля\n",
        "\n"
      ],
      "metadata": {
        "id": "Ge-eJWaD8prR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FastICA:\n",
        "    def __init__(self, n_components=None, max_iter=1000, tol=1e-4, random_state=None):\n",
        "        \"\"\"\n",
        "        Инициализация FastICA.\n",
        "\n",
        "        Параметры:\n",
        "        - n_components: количество независимых компонент (по умолчанию равно рангу матрицы).\n",
        "        - max_iter: максимальное количество итераций.\n",
        "        - tol: допустимая погрешность для сходимости.\n",
        "        - random_state: seed для генератора случайных чисел.\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.W = None  # Матрица разделения\n",
        "        self.mean = None  # Среднее значение для центрирования\n",
        "        self.whitening_matrix = None  # Матрица отбеливания\n",
        "\n",
        "    def _center(self, X):\n",
        "        \"\"\"Центрирование данных.\"\"\"\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        return X - self.mean\n",
        "\n",
        "    def _whiten(self, X):\n",
        "        \"\"\"Отбеливание данных и уменьшение размерности.\"\"\"\n",
        "        cov_matrix = np.cov(X, rowvar=False)  # Ковариационная матрица\n",
        "        U, D, _ = np.linalg.svd(cov_matrix)  # Сингулярное разложение\n",
        "\n",
        "        # Определение количества компонент\n",
        "        if self.n_components is None:\n",
        "            self.n_components = np.sum(D > 1e-10)  # Ранг матрицы\n",
        "\n",
        "        # Уменьшение размерности\n",
        "        U = U[:, :self.n_components]\n",
        "        D = D[:self.n_components]\n",
        "\n",
        "        # Добавляем небольшое значение к нулевым собственным значениям\n",
        "        D = np.where(D < 1e-10, 1e-10, D)\n",
        "\n",
        "        D_inv_sqrt = np.diag(1.0 / np.sqrt(D))  # Обратный квадратный корень\n",
        "        self.whitening_matrix = np.dot(D_inv_sqrt, U.T)  # Матрица отбеливания\n",
        "        return np.dot(X, self.whitening_matrix.T)  # Отбеленные данные\n",
        "\n",
        "    def _fast_ica(self, X):\n",
        "        \"\"\"Реализация алгоритма FastICA.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        np.random.seed(self.random_state)\n",
        "        W = np.random.rand(self.n_components, n_features)  # Инициализация W\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # Оценка независимых компонент\n",
        "            S = np.dot(W, X.T)\n",
        "\n",
        "            # Нелинейная функция (гиперболический тангенс)\n",
        "            g = np.tanh(S)\n",
        "            g_prime = 1 - g**2\n",
        "\n",
        "            # Обновление матрицы W\n",
        "            W_new = np.dot(g, X) / n_samples - np.dot(np.diag(g_prime.mean(axis=1)), W)\n",
        "\n",
        "            # Ортогонализация (грам-шмидт)\n",
        "            W_new, _ = np.linalg.qr(W_new.T)\n",
        "            W_new = W_new.T\n",
        "\n",
        "            # Проверка сходимости\n",
        "            if np.max(np.abs(np.abs(np.diag(np.dot(W_new, W.T))) - 1)) < self.tol:\n",
        "                break\n",
        "\n",
        "            W = W_new\n",
        "\n",
        "        return W\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Обучение модели на данных X.\"\"\"\n",
        "        X_centered = self._center(X)\n",
        "        X_whitened = self._whiten(X_centered)\n",
        "        self.W = self._fast_ica(X_whitened)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Преобразование данных X в независимые компоненты.\"\"\"\n",
        "        X_centered = X - self.mean\n",
        "        X_whitened = np.dot(X_centered, self.whitening_matrix.T)\n",
        "        return np.dot(self.W, X_whitened.T).T\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"Обучение и преобразование данных X.\"\"\"\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Исходные данные\n",
        "    X = np.array([\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9],\n",
        "        [10, 11, 12]\n",
        "    ])\n",
        "\n",
        "    # Добавляем небольшой шум к данным\n",
        "    np.random.seed(42)\n",
        "    noise = np.random.normal(0, 0.01, X.shape)\n",
        "    X_noisy = X + noise\n",
        "\n",
        "    # Создание и обучение модели FastICA\n",
        "    ica = FastICA(n_components=2, random_state=42)  # Уменьшаем размерность до 2\n",
        "    S = ica.fit_transform(X_noisy)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"Матрица разделения W:\")\n",
        "    print(ica.W)\n",
        "    print(\"\\nНезависимые компоненты S:\")\n",
        "    print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB0ffBDM-2TO",
        "outputId": "e978d378-30c6-47cd-f402-f44074ce7428"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица разделения W:\n",
            "[[-0.73629172  0.67666424]\n",
            " [ 0.67666424  0.73629172]]\n",
            "\n",
            "Независимые компоненты S:\n",
            "[[-1.4142959   0.17865921]\n",
            " [ 0.22519724  0.81740623]\n",
            " [ 0.94245413  0.45166585]\n",
            " [ 0.24664452 -1.44773129]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2. Реализация с использованием готового решения (sklearn)\n",
        "\n",
        "Для использования готового решения воспользуемся библиотекой sklearn, которая предоставляет реализацию ICA через класс FastICA.\n"
      ],
      "metadata": {
        "id": "6irAnd96-2fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Добавляем небольшой шум к данным\n",
        "np.random.seed(42)\n",
        "noise = np.random.normal(0, 0.01, X.shape)\n",
        "X_noisy = X + noise\n",
        "\n",
        "# Создание модели ICA с уменьшенным количеством компонент\n",
        "ica = FastICA(n_components=2, max_iter=10000, tol=1e-6, random_state=42)\n",
        "\n",
        "# Применение ICA к данным\n",
        "S_sklearn = ica.fit_transform(X_noisy)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Матрица разделения W (sklearn):\")\n",
        "print(ica.components_)\n",
        "print(\"\\nНезависимые компоненты S (sklearn):\")\n",
        "print(S_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO_RuNNZ-3QC",
        "outputId": "b877d138-d752-40c5-951e-6a3303618fd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица разделения W (sklearn):\n",
            "[[   1.84649463    0.73036057   -2.28159963]\n",
            " [ 108.21522855   39.07916855 -147.48249961]]\n",
            "\n",
            "Независимые компоненты S (sklearn):\n",
            "[[-1.35723461 -0.93137008]\n",
            " [-0.43309763  0.87801791]\n",
            " [ 0.46639117  1.11300272]\n",
            " [ 1.32394106 -1.05965055]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.6. Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "**Неотрицательная матричная факторизация (NMF)** — это метод анализа данных, который позволяет представить исходную матрицу данных в виде произведения двух неотрицательных матриц. Этот метод широко используется в задачах снижения размерности, обработки изображений, анализа текстов, рекомендательных систем и других областях, где данные имеют естественную неотрицательную природу.\n",
        "\n",
        "Основная идея NMF заключается в том, чтобы разложить исходную матрицу $\\mathbf{X}$ на две матрицы $\\mathbf{W}$ и $\\mathbf{H}$, такие что:\n",
        "$$\n",
        "\\mathbf{X} \\approx \\mathbf{W} \\mathbf{H},\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ — исходная матрица данных,\n",
        "- $\\mathbf{W} \\in \\mathbb{R}^{m \\times k}$ — матрица базисных векторов,\n",
        "- $\\mathbf{H} \\in \\mathbb{R}^{k \\times n}$ — матрица коэффициентов,\n",
        "- $k$ — количество компонент (задаётся пользователем).\n",
        "\n",
        "\n",
        "\n",
        "### Основные идеи NMF\n",
        "\n",
        "1. **Неотрицательность**:\n",
        "   - Все элементы матриц $\\mathbf{W}$ и $\\mathbf{H}$ должны быть неотрицательными. Это ограничение делает NMF особенно полезным для данных, которые по своей природе неотрицательны (например, изображения, тексты, спектры).\n",
        "\n",
        "2. **Интерпретируемость**:\n",
        "   - NMF часто приводит к более интерпретируемым результатам по сравнению с другими методами, такими как PCA (Principal Component Analysis). Это связано с тем, что базисные векторы и коэффициенты имеют физический смысл (например, части изображений или темы в текстах).\n",
        "\n",
        "3. **Снижение размерности**:\n",
        "   - NMF позволяет снизить размерность данных, выделяя наиболее важные компоненты. Это полезно для визуализации данных, сжатия информации и уменьшения вычислительной сложности.\n",
        "\n",
        "\n",
        "\n",
        "### Математическая основа NMF\n",
        "\n",
        "#### Постановка задачи\n",
        "\n",
        "Дана неотрицательная матрица $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$. Требуется найти неотрицательные матрицы $\\mathbf{W} \\in \\mathbb{R}^{m \\times k}$ и $\\mathbf{H} \\in \\mathbb{R}^{k \\times n}$, такие что:\n",
        "$$\n",
        "\\mathbf{X} \\approx \\mathbf{W} \\mathbf{H}.\n",
        "$$\n",
        "\n",
        "#### Целевая функция\n",
        "\n",
        "Для нахождения $\\mathbf{W}$ и $\\mathbf{H}$ минимизируется функция потерь, которая измеряет разницу между исходной матрицей $\\mathbf{X}$ и её приближением $\\mathbf{W} \\mathbf{H}$. Наиболее часто используется квадратичная функция потерь:\n",
        "$$\n",
        "L(\\mathbf{W}, \\mathbf{H}) = \\frac{1}{2} \\|\\mathbf{X} - \\mathbf{W} \\mathbf{H}\\|_F^2,\n",
        "$$\n",
        "где $\\|\\cdot\\|_F$ — норма Фробениуса, которая определяется как:\n",
        "$$\n",
        "\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}.\n",
        "$$\n",
        "\n",
        "#### Ограничения\n",
        "\n",
        "- $\\mathbf{W} \\geq 0$ (все элементы $\\mathbf{W}$ неотрицательны),\n",
        "- $\\mathbf{H} \\geq 0$ (все элементы $\\mathbf{H}$ неотрицательны).\n",
        "\n",
        "\n",
        "\n",
        "### Алгоритмы решения NMF\n",
        "\n",
        "#### 1. Градиентный спуск\n",
        "\n",
        "Один из простейших подходов — это использование градиентного спуска для минимизации функции потерь. Градиенты функции потерь по $\\mathbf{W}$ и $\\mathbf{H}$ вычисляются как:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{W}} = (\\mathbf{W} \\mathbf{H} - \\mathbf{X}) \\mathbf{H}^T,\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{H}} = \\mathbf{W}^T (\\mathbf{W} \\mathbf{H} - \\mathbf{X}).\n",
        "$$\n",
        "\n",
        "Обновление матриц $\\mathbf{W}$ и $\\mathbf{H}$ выполняется по формулам:\n",
        "$$\n",
        "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}},\n",
        "$$\n",
        "$$\n",
        "\\mathbf{H} \\leftarrow \\mathbf{H} - \\eta \\frac{\\partial L}{\\partial \\mathbf{H}},\n",
        "$$\n",
        "где $\\eta$ — скорость обучения.\n",
        "\n",
        "После каждого обновления необходимо проецировать элементы матриц $\\mathbf{W}$ и $\\mathbf{H}$ на неотрицательную область:\n",
        "$$\n",
        "\\mathbf{W} \\leftarrow \\max(\\mathbf{W}, 0),\n",
        "$$\n",
        "$$\n",
        "\\mathbf{H} \\leftarrow \\max(\\mathbf{H}, 0).\n",
        "$$\n",
        "\n",
        "#### 2. Мультипликативные правила обновления\n",
        "\n",
        "Более популярным подходом являются мультипликативные правила обновления, предложенные Ли и Сенгом (Lee & Seung, 2001). Эти правила гарантируют неотрицательность $\\mathbf{W}$ и $\\mathbf{H}$ на каждом шаге.\n",
        "\n",
        "Обновление матриц $\\mathbf{W}$ и $\\mathbf{H}$ выполняется по формулам:\n",
        "$$\n",
        "\\mathbf{W} \\leftarrow \\mathbf{W} \\odot \\frac{(\\mathbf{X} \\mathbf{H}^T)}{(\\mathbf{W} \\mathbf{H} \\mathbf{H}^T)},\n",
        "$$\n",
        "$$\n",
        "\\mathbf{H} \\leftarrow \\mathbf{H} \\odot \\frac{(\\mathbf{W}^T \\mathbf{X})}{(\\mathbf{W}^T \\mathbf{W} \\mathbf{H})},\n",
        "$$\n",
        "где $\\odot$ — поэлементное умножение, а деление также выполняется поэлементно.\n",
        "\n",
        "Эти правила обновления гарантируют, что функция потерь монотонно уменьшается на каждой итерации.\n",
        "\n",
        "\n",
        "\n",
        "### Применение NMF\n",
        "\n",
        "1. **Обработка изображений**:\n",
        "   - NMF используется для выделения базисных изображений (например, частей лица) и их комбинаций. Это полезно для задач сжатия изображений и распознавания образов.\n",
        "\n",
        "2. **Анализ текстов**:\n",
        "   - В задачах тематического моделирования NMF позволяет выделить темы в документах и представить каждый документ как смесь тем.\n",
        "\n",
        "3. **Рекомендательные системы**:\n",
        "   - NMF используется для построения рекомендаций, где $\\mathbf{X}$ — матрица пользователь-товар, $\\mathbf{W}$ — матрица пользователь-факторы, а $\\mathbf{H}$ — матрица факторы-товар.\n",
        "\n",
        "4. **Анализ спектров**:\n",
        "   - В химии и биологии NMF применяется для анализа спектроскопических данных, где $\\mathbf{X}$ — матрица спектров, $\\mathbf{W}$ — базисные спектры, а $\\mathbf{H}$ — коэффициенты смешивания.\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества и ограничения NMF\n",
        "\n",
        "#### Преимущества:\n",
        "- **Интерпретируемость**: Результаты NMF легко интерпретировать, так как базисные векторы и коэффициенты имеют физический смысл.\n",
        "- **Неотрицательность**: Ограничение на неотрицательность делает NMF подходящим для данных, которые по своей природе неотрицательны.\n",
        "- **Снижение размерности**: NMF позволяет эффективно снижать размерность данных.\n",
        "\n",
        "#### Ограничения:\n",
        "- **Локальные минимумы**: NMF может сходиться к локальным минимумам, что делает результаты зависимыми от начальной инициализации.\n",
        "- **Чувствительность к шуму**: NMF чувствителен к шуму в данных, что может ухудшить качество разложения.\n",
        "- **Выбор количества компонент**: Количество компонент $k$ необходимо задавать вручную, что может быть нетривиальной задачей.\n",
        "\n",
        "\n",
        "Таким образом, неотрицательная матричная факторизация (NMF) — это мощный метод анализа данных, который позволяет выделять скрытые структуры в данных и представлять их в виде неотрицательных базисных векторов и коэффициентов. Благодаря своей интерпретируемости и простоте, NMF находит широкое применение в различных областях, таких как обработка изображений, анализ текстов и рекомендательные системы. Однако при использовании NMF необходимо учитывать его ограничения, такие как чувствительность к начальной инициализации и выбору количества компонент.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения **неотрицательной матричной факторизации (NMF)** для набора данных из 4 точек в 3D-пространстве. Данные представлены матрицей:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $X \\in \\mathbb{R}^{4 \\times 3}$ — исходная матрица данных,\n",
        "- Каждая строка соответствует точке в 3D-пространстве,\n",
        "- Каждый столбец соответствует одной из координат (признаков).\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Постановка задачи\n",
        "\n",
        "Цель NMF — разложить матрицу $X$ на две неотрицательные матрицы $W$ и $H$, такие что:\n",
        "$$\n",
        "X \\approx W \\cdot H,\n",
        "$$\n",
        "где:\n",
        "- $W \\in \\mathbb{R}^{4 \\times k}$ — матрица базисных векторов,\n",
        "- $H \\in \\mathbb{R}^{k \\times 3}$ — матрица коэффициентов,\n",
        "- $k$ — количество компонент (задаётся пользователем).\n",
        "\n",
        "Для нашего примера выберем $k = 2$, то есть будем искать две компоненты.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Инициализация матриц $W$ и $H$\n",
        "\n",
        "Начнём с случайной инициализации матриц $W$ и $H$. Например:\n",
        "\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "0.5 & 0.2 \\\\\n",
        "0.8 & 0.4 \\\\\n",
        "1.1 & 0.6 \\\\\n",
        "1.4 & 0.8\n",
        "\\end{bmatrix}, \\quad\n",
        "H = \\begin{bmatrix}\n",
        "0.3 & 0.6 & 0.9 \\\\\n",
        "0.2 & 0.4 & 0.6\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $W \\in \\mathbb{R}^{4 \\times 2}$,\n",
        "- $H \\in \\mathbb{R}^{2 \\times 3}$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление приближения $\\hat{X}$\n",
        "\n",
        "Вычислим приближение $\\hat{X} = W \\cdot H$:\n",
        "\n",
        "$$\n",
        "\\hat{X} = W \\cdot H = \\begin{bmatrix}\n",
        "0.5 & 0.2 \\\\\n",
        "0.8 & 0.4 \\\\\n",
        "1.1 & 0.6 \\\\\n",
        "1.4 & 0.8\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "0.3 & 0.6 & 0.9 \\\\\n",
        "0.2 & 0.4 & 0.6\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Выполним умножение матриц:\n",
        "\n",
        "$$\n",
        "\\hat{X} = \\begin{bmatrix}\n",
        "0.5 \\cdot 0.3 + 0.2 \\cdot 0.2 & 0.5 \\cdot 0.6 + 0.2 \\cdot 0.4 & 0.5 \\cdot 0.9 + 0.2 \\cdot 0.6 \\\\\n",
        "0.8 \\cdot 0.3 + 0.4 \\cdot 0.2 & 0.8 \\cdot 0.6 + 0.4 \\cdot 0.4 & 0.8 \\cdot 0.9 + 0.4 \\cdot 0.6 \\\\\n",
        "1.1 \\cdot 0.3 + 0.6 \\cdot 0.2 & 1.1 \\cdot 0.6 + 0.6 \\cdot 0.4 & 1.1 \\cdot 0.9 + 0.6 \\cdot 0.6 \\\\\n",
        "1.4 \\cdot 0.3 + 0.8 \\cdot 0.2 & 1.4 \\cdot 0.6 + 0.8 \\cdot 0.4 & 1.4 \\cdot 0.9 + 0.8 \\cdot 0.6\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Результат:\n",
        "\n",
        "$$\n",
        "\\hat{X} = \\begin{bmatrix}\n",
        "0.19 & 0.38 & 0.57 \\\\\n",
        "0.32 & 0.64 & 0.96 \\\\\n",
        "0.45 & 0.90 & 1.35 \\\\\n",
        "0.58 & 1.16 & 1.74\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Вычисление функции потерь\n",
        "\n",
        "Функция потерь измеряет разницу между исходной матрицей $X$ и её приближением $\\hat{X}$. Используем квадратичную функцию потерь:\n",
        "\n",
        "$$\n",
        "L(W, H) = \\frac{1}{2} \\|X - \\hat{X}\\|_F^2,\n",
        "$$\n",
        "где $\\|\\cdot\\|_F$ — норма Фробениуса.\n",
        "\n",
        "Вычислим разность $X - \\hat{X}$:\n",
        "\n",
        "$$\n",
        "X - \\hat{X} = \\begin{bmatrix}\n",
        "1 - 0.19 & 2 - 0.38 & 3 - 0.57 \\\\\n",
        "4 - 0.32 & 5 - 0.64 & 6 - 0.96 \\\\\n",
        "7 - 0.45 & 8 - 0.90 & 9 - 1.35 \\\\\n",
        "10 - 0.58 & 11 - 1.16 & 12 - 1.74\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.81 & 1.62 & 2.43 \\\\\n",
        "3.68 & 4.36 & 5.04 \\\\\n",
        "6.55 & 7.10 & 7.65 \\\\\n",
        "9.42 & 9.84 & 10.26\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Теперь вычислим квадрат нормы Фробениуса:\n",
        "\n",
        "$$\n",
        "\\|X - \\hat{X}\\|_F^2 = \\sum_{i,j} (X_{ij} - \\hat{X}_{ij})^2.\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "\n",
        "$$\n",
        "\\|X - \\hat{X}\\|_F^2 = (0.81)^2 + (1.62)^2 + (2.43)^2 + (3.68)^2 + (4.36)^2 + (5.04)^2 + (6.55)^2 + (7.10)^2 + (7.65)^2 + (9.42)^2 + (9.84)^2 + (10.26)^2.\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "\\|X - \\hat{X}\\|_F^2 \\approx 0.6561 + 2.6244 + 5.9049 + 13.5424 + 19.0096 + 25.4016 + 42.9025 + 50.41 + 58.5225 + 88.7364 + 96.8256 + 105.2676.\n",
        "$$\n",
        "\n",
        "Сумма:\n",
        "\n",
        "$$\n",
        "\\|X - \\hat{X}\\|_F^2 \\approx 510.8036.\n",
        "$$\n",
        "\n",
        "Тогда функция потерь:\n",
        "\n",
        "$$\n",
        "L(W, H) = \\frac{1}{2} \\cdot 510.8036 \\approx 255.4018.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Обновление матриц $W$ и $H$\n",
        "\n",
        "Используем мультипликативные правила обновления для минимизации функции потерь. Формулы обновления:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W \\odot \\frac{(X \\cdot H^T)}{(W \\cdot H \\cdot H^T)},\n",
        "$$\n",
        "$$\n",
        "H \\leftarrow H \\odot \\frac{(W^T \\cdot X)}{(W^T \\cdot W \\cdot H)},\n",
        "$$\n",
        "где $\\odot$ — поэлементное умножение, а деление — поэлементное.\n",
        "\n",
        "#### Обновление $W$:\n",
        "\n",
        "1. Вычислим $X \\cdot H^T$:\n",
        "\n",
        "$$\n",
        "X \\cdot H^T = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "0.3 & 0.2 \\\\\n",
        "0.6 & 0.4 \\\\\n",
        "0.9 & 0.6\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Результат:\n",
        "\n",
        "$$\n",
        "X \\cdot H^T = \\begin{bmatrix}\n",
        "4.2 & 2.8 \\\\\n",
        "10.2 & 6.8 \\\\\n",
        "16.2 & 10.8 \\\\\n",
        "22.2 & 14.8\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "2. Вычислим $W \\cdot H \\cdot H^T$:\n",
        "\n",
        "$$\n",
        "W \\cdot H \\cdot H^T = \\begin{bmatrix}\n",
        "0.19 & 0.38 & 0.57 \\\\\n",
        "0.32 & 0.64 & 0.96 \\\\\n",
        "0.45 & 0.90 & 1.35 \\\\\n",
        "0.58 & 1.16 & 1.74\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "0.3 & 0.2 \\\\\n",
        "0.6 & 0.4 \\\\\n",
        "0.9 & 0.6\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Результат:\n",
        "\n",
        "$$\n",
        "W \\cdot H \\cdot H^T = \\begin{bmatrix}\n",
        "0.81 & 0.54 \\\\\n",
        "1.44 & 0.96 \\\\\n",
        "2.07 & 1.38 \\\\\n",
        "2.70 & 1.80\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "3. Обновим $W$:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W \\odot \\frac{X \\cdot H^T}{W \\cdot H \\cdot H^T} = \\begin{bmatrix}\n",
        "0.5 & 0.2 \\\\\n",
        "0.8 & 0.4 \\\\\n",
        "1.1 & 0.6 \\\\\n",
        "1.4 & 0.8\n",
        "\\end{bmatrix} \\odot \\begin{bmatrix}\n",
        "4.2/0.81 & 2.8/0.54 \\\\\n",
        "10.2/1.44 & 6.8/0.96 \\\\\n",
        "16.2/2.07 & 10.8/1.38 \\\\\n",
        "22.2/2.70 & 14.8/1.80\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Результат:\n",
        "\n",
        "$$\n",
        "W \\leftarrow \\begin{bmatrix}\n",
        "0.5 \\cdot 5.185 & 0.2 \\cdot 5.185 \\\\\n",
        "0.8 \\cdot 7.083 & 0.4 \\cdot 7.083 \\\\\n",
        "1.1 \\cdot 7.826 & 0.6 \\cdot 7.826 \\\\\n",
        "1.4 \\cdot 8.222 & 0.8 \\cdot 8.222\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "2.5925 & 1.037 \\\\\n",
        "5.6664 & 2.8332 \\\\\n",
        "8.6086 & 4.6956 \\\\\n",
        "11.5108 & 6.5776\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Обновление $H$:\n",
        "\n",
        "Аналогично обновим $H$, используя формулу:\n",
        "\n",
        "$$\n",
        "H \\leftarrow H \\odot \\frac{W^T \\cdot X}{W^T \\cdot W \\cdot H}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 6: Повторение итераций\n",
        "\n",
        "Процесс обновления $W$ и $H$ повторяется до тех пор, пока функция потерь не стабилизируется или не будет достигнуто максимальное число итераций.\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "\n",
        "После нескольких итераций матрицы $W$ и $H$ сойдутся к значениям, которые минимизируют функцию потерь. Например:\n",
        "\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "1.0 & 0.5 \\\\\n",
        "2.0 & 1.0 \\\\\n",
        "3.0 & 1.5 \\\\\n",
        "4.0 & 2.0\n",
        "\\end{bmatrix}, \\quad\n",
        "H = \\begin{bmatrix}\n",
        "1.0 & 2.0 & 3.0 \\\\\n",
        "0.0 & 0.0 & 0.0\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Приближение $\\hat{X} = W \\cdot H$ будет близко к исходной матрице $X$.\n",
        "\n",
        "\n",
        "\n",
        "Этот пример демонстрирует, как шаг за шагом выполняется NMF для набора данных.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Классическая реализация NMF с нуля\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r5l7Q9wfBYvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NMF:\n",
        "    def __init__(self, n_components, max_iter=1000, tol=1e-4, random_state=None, noise_level=0.01):\n",
        "        \"\"\"\n",
        "        Инициализация NMF.\n",
        "\n",
        "        Параметры:\n",
        "        - n_components: количество компонент.\n",
        "        - max_iter: максимальное количество итераций.\n",
        "        - tol: допустимая погрешность для сходимости.\n",
        "        - random_state: seed для генератора случайных чисел.\n",
        "        - noise_level: уровень шума для устранения линейной зависимости.\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.noise_level = noise_level\n",
        "        self.W = None  # Матрица базисных векторов\n",
        "        self.H = None  # Матрица коэффициентов\n",
        "\n",
        "    def _add_noise(self, X):\n",
        "        \"\"\"Добавление небольшого шума к данным.\"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        noise = np.random.normal(0, self.noise_level, X.shape)\n",
        "        return X + noise\n",
        "\n",
        "    def _initialize_matrices(self, n_samples, n_features):\n",
        "        \"\"\"Инициализация матриц W и H случайными неотрицательными значениями.\"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        self.W = np.abs(np.random.rand(n_samples, self.n_components))\n",
        "        self.H = np.abs(np.random.rand(self.n_components, n_features))\n",
        "\n",
        "    def _update_matrices(self, X):\n",
        "        \"\"\"Обновление матриц W и H с использованием мультипликативных правил.\"\"\"\n",
        "        # Обновление H\n",
        "        numerator_H = np.dot(self.W.T, X)\n",
        "        denominator_H = np.dot(np.dot(self.W.T, self.W), self.H)\n",
        "        self.H *= numerator_H / (denominator_H + 1e-10)  # Добавляем небольшое значение для стабильности\n",
        "\n",
        "        # Обновление W\n",
        "        numerator_W = np.dot(X, self.H.T)\n",
        "        denominator_W = np.dot(self.W, np.dot(self.H, self.H.T))\n",
        "        self.W *= numerator_W / (denominator_W + 1e-10)  # Добавляем небольшое значение для стабильности\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Обучение модели на данных X.\n",
        "        \"\"\"\n",
        "        # Добавляем шум к данным, если они имеют линейную зависимость\n",
        "        X_noisy = self._add_noise(X)\n",
        "\n",
        "        # Инициализация матриц W и H\n",
        "        n_samples, n_features = X_noisy.shape\n",
        "        self._initialize_matrices(n_samples, n_features)\n",
        "\n",
        "        # Итеративное обновление W и H\n",
        "        previous_loss = np.inf\n",
        "        for iteration in range(self.max_iter):\n",
        "            # Обновление матриц\n",
        "            self._update_matrices(X_noisy)\n",
        "\n",
        "            # Вычисление функции потерь\n",
        "            reconstruction = np.dot(self.W, self.H)\n",
        "            current_loss = np.linalg.norm(X_noisy - reconstruction, 'fro')\n",
        "\n",
        "            # Проверка сходимости\n",
        "            if abs(previous_loss - current_loss) < self.tol:\n",
        "                print(f\"Сходимость достигнута на итерации {iteration + 1}.\")\n",
        "                break\n",
        "\n",
        "            previous_loss = current_loss\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Преобразование данных X в пространство компонент.\n",
        "        \"\"\"\n",
        "        return np.dot(X, np.linalg.pinv(self.H))\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Обучение и преобразование данных X.\n",
        "        \"\"\"\n",
        "        self.fit(X)\n",
        "        return np.dot(self.W, self.H)\n",
        "\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Исходные данные\n",
        "    X = np.array([\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9],\n",
        "        [10, 11, 12]\n",
        "    ])\n",
        "\n",
        "    # Создание и обучение модели NMF\n",
        "    nmf = NMF(n_components=2, max_iter=1000, random_state=42, noise_level=0.01)\n",
        "    nmf.fit(X)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"Матрица базисных векторов W:\")\n",
        "    print(nmf.W)\n",
        "    print(\"\\nМатрица коэффициентов H:\")\n",
        "    print(nmf.H)\n",
        "    print(\"\\nПриближение X ≈ W * H:\")\n",
        "    print(nmf.fit_transform(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qO4eFxMDCIv",
        "outputId": "e84681ef-c556-4160-ee4c-e4b89edf66c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сходимость достигнута на итерации 288.\n",
            "Матрица базисных векторов W:\n",
            "[[0.81989238 0.08579118]\n",
            " [0.60057525 0.58658957]\n",
            " [0.3934054  1.08460779]\n",
            " [0.18142218 1.58248695]]\n",
            "\n",
            "Матрица коэффициентов H:\n",
            "[[0.57222156 1.73153203 2.90825793]\n",
            " [6.25832531 6.75095627 7.24433247]]\n",
            "\n",
            "Приближение X ≈ W * H:\n",
            "Сходимость достигнута на итерации 288.\n",
            "[[ 1.00606921  1.99884243  3.00595835]\n",
            " [ 4.01473045  4.99995582  5.99607761]\n",
            " [ 7.01294344  8.00333383  9.00138383]\n",
            " [10.00753183 10.99743853 11.99168411]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения (sklearn)\n",
        "\n",
        "Для использования готового решения воспользуемся классом NMF из библиотеки sklearn."
      ],
      "metadata": {
        "id": "nmXX0-dWDFT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Добавляем небольшой шум к данным\n",
        "np.random.seed(42)\n",
        "noise = np.random.normal(0, 0.01, X.shape)\n",
        "X_noisy = X + noise\n",
        "\n",
        "# Создание и обучение модели NMF\n",
        "nmf = NMF(n_components=2, max_iter=1000, random_state=42)  # Увеличиваем количество компонент\n",
        "W = nmf.fit_transform(X_noisy)\n",
        "H = nmf.components_\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Матрица базисных векторов W (sklearn):\")\n",
        "print(W)\n",
        "print(\"\\nМатрица коэффициентов H (sklearn):\")\n",
        "print(H)\n",
        "print(\"\\nПриближение X ≈ W * H (sklearn):\")\n",
        "print(np.dot(W, H))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfaaGT8TDLmt",
        "outputId": "0dadfb3e-f4db-4d98-896c-7e62e1a2bbee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица базисных векторов W (sklearn):\n",
            "[[1.72943716 0.        ]\n",
            " [1.38303741 0.45994798]\n",
            " [1.02718521 0.92194863]\n",
            " [0.68562232 1.38031711]]\n",
            "\n",
            "Матрица коэффициентов H (sklearn):\n",
            "[[0.58378787 1.15601428 1.7366738 ]\n",
            " [6.95997077 7.3931985  7.82530882]]\n",
            "\n",
            "Приближение X ≈ W * H (sklearn):\n",
            "[[ 1.00962444  1.99925407  3.00346821]\n",
            " [ 4.00862498  4.99929773  6.00111983]\n",
            " [ 7.0163938   8.00359003  8.99841842]\n",
            " [10.00722475 10.99754761 11.99211   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.7. Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "**Автоэнкодеры** — это класс нейронных сетей, используемых для задач снижения размерности, обучения представлений и генерации данных. Они состоят из двух основных компонентов: **энкодера** и **декодера**. Энкодер преобразует входные данные в низкоразмерное представление (скрытое пространство), а декодер восстанавливает данные из этого представления. Основная цель автоэнкодеров — научиться эффективно кодировать данные в скрытом пространстве, сохраняя при этом важные особенности исходных данных.\n",
        "\n",
        "Автоэнкодеры находят применение в задачах:\n",
        "- Снижения размерности (аналогично PCA, но с нелинейными преобразованиями).\n",
        "- Обучения представлений (feature learning).\n",
        "- Шумоподавления (denoising).\n",
        "- Генерации данных (например, в вариационных автоэнкодерах, VAE).\n",
        "\n",
        "\n",
        "\n",
        "### Основные идеи автоэнкодеров\n",
        "\n",
        "#### 1. Архитектура автоэнкодера\n",
        "\n",
        "Автоэнкодер состоит из двух частей:\n",
        "1. **Энкодер (Encoder)**:\n",
        "   - Энкодер преобразует входные данные $\\mathbf{x} \\in \\mathbb{R}^d$ в скрытое представление $\\mathbf{z} \\in \\mathbb{R}^k$, где $k < d$. Это можно записать как:\n",
        "     $$\n",
        "     \\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}),\n",
        "     $$\n",
        "     где $f_{\\text{enc}}$ — функция, реализуемая нейронной сетью. Обычно энкодер представляет собой последовательность слоёв с нелинейными функциями активации.\n",
        "\n",
        "2. **Декодер (Decoder)**:\n",
        "   - Декодер преобразует скрытое представление $\\mathbf{z}$ обратно в пространство исходных данных:\n",
        "     $$\n",
        "     \\hat{\\mathbf{x}} = f_{\\text{dec}}(\\mathbf{z}),\n",
        "     $$\n",
        "     где $f_{\\text{dec}}$ — функция, реализуемая нейронной сетью. Декодер также состоит из последовательности слоёв с нелинейными функциями активации.\n",
        "\n",
        "#### 2. Целевая функция\n",
        "\n",
        "Автоэнкодер обучается минимизировать разницу между входными данными $\\mathbf{x}$ и их восстановленной версией $\\hat{\\mathbf{x}}$. Чаще всего используется квадратичная функция потерь:\n",
        "$$\n",
        "L(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\frac{1}{2} \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2,\n",
        "$$\n",
        "где $\\|\\cdot\\|$ — евклидова норма.\n",
        "\n",
        "\n",
        "\n",
        "### Математическая основа автоэнкодеров\n",
        "\n",
        "#### 1. Энкодер\n",
        "\n",
        "Энкодер преобразует входные данные $\\mathbf{x}$ в скрытое представление $\\mathbf{z}$. Это можно выразить как:\n",
        "$$\n",
        "\\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}) = \\sigma(\\mathbf{W}_{\\text{enc}} \\mathbf{x} + \\mathbf{b}_{\\text{enc}}),\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{W}_{\\text{enc}} \\in \\mathbb{R}^{k \\times d}$ — матрица весов энкодера,\n",
        "- $\\mathbf{b}_{\\text{enc}} \\in \\mathbb{R}^k$ — вектор смещений энкодера,\n",
        "- $\\sigma$ — функция активации (например, ReLU, сигмоида или гиперболический тангенс).\n",
        "\n",
        "Если энкодер состоит из нескольких слоёв, то скрытое представление вычисляется как:\n",
        "$$\n",
        "\\mathbf{z} = \\sigma(\\mathbf{W}_{\\text{enc}}^{(L)} \\sigma(\\mathbf{W}_{\\text{enc}}^{(L-1)} \\dots \\sigma(\\mathbf{W}_{\\text{enc}}^{(1)} \\mathbf{x} + \\mathbf{b}_{\\text{enc}}^{(1)}) \\dots + \\mathbf{b}_{\\text{enc}}^{(L-1)}) + \\mathbf{b}_{\\text{enc}}^{(L)}),\n",
        "$$\n",
        "где $L$ — количество слоёв в энкодере.\n",
        "\n",
        "#### 2. Декодер\n",
        "\n",
        "Декодер преобразует скрытое представление $\\mathbf{z}$ обратно в пространство исходных данных:\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = f_{\\text{dec}}(\\mathbf{z}) = \\sigma(\\mathbf{W}_{\\text{dec}} \\mathbf{z} + \\mathbf{b}_{\\text{dec}}),\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{W}_{\\text{dec}} \\in \\mathbb{R}^{d \\times k}$ — матрица весов декодера,\n",
        "- $\\mathbf{b}_{\\text{dec}} \\in \\mathbb{R}^d$ — вектор смещений декодера.\n",
        "\n",
        "Если декодер состоит из нескольких слоёв, то восстановленные данные вычисляются как:\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\sigma(\\mathbf{W}_{\\text{dec}}^{(L)} \\sigma(\\mathbf{W}_{\\text{dec}}^{(L-1)} \\dots \\sigma(\\mathbf{W}_{\\text{dec}}^{(1)} \\mathbf{z} + \\mathbf{b}_{\\text{dec}}^{(1)}) \\dots + \\mathbf{b}_{\\text{dec}}^{(L-1)}) + \\mathbf{b}_{\\text{dec}}^{(L)}).\n",
        "$$\n",
        "\n",
        "#### 3. Обучение автоэнкодера\n",
        "\n",
        "Автоэнкодер обучается путём минимизации функции потерь $L(\\mathbf{x}, \\hat{\\mathbf{x}})$. Для этого используется метод обратного распространения ошибки (backpropagation) и градиентный спуск (gradient descent). Градиенты функции потерь по параметрам $\\mathbf{W}_{\\text{enc}}$, $\\mathbf{b}_{\\text{enc}}$, $\\mathbf{W}_{\\text{dec}}$ и $\\mathbf{b}_{\\text{dec}}$ вычисляются с помощью цепного правила (chain rule).\n",
        "\n",
        "\n",
        "\n",
        "### Типы автоэнкодеров\n",
        "\n",
        "#### 1. Обычный автоэнкодер (Vanilla Autoencoder)\n",
        "\n",
        "Обычный автоэнкодер состоит из одного скрытого слоя в энкодере и одном скрытом слое в декодере. Его функция потерь:\n",
        "$$\n",
        "L(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\frac{1}{2} \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2.\n",
        "$$\n",
        "\n",
        "#### 2. Разреженный автоэнкодер (Sparse Autoencoder)\n",
        "\n",
        "Разреженный автоэнкодер вводит дополнительное ограничение на скрытое представление $\\mathbf{z}$, чтобы большинство его элементов были нулевыми. Это достигается добавлением штрафа за разреженность в функцию потерь:\n",
        "$$\n",
        "L_{\\text{sparse}} = L(\\mathbf{x}, \\hat{\\mathbf{x}}) + \\lambda \\|\\mathbf{z}\\|_1,\n",
        "$$\n",
        "где:\n",
        "- $\\lambda$ — коэффициент регуляризации,\n",
        "- $\\|\\mathbf{z}\\|_1$ — $L_1$-норма вектора $\\mathbf{z}$.\n",
        "\n",
        "#### 3. Шумоподавляющий автоэнкодер (Denoising Autoencoder)\n",
        "\n",
        "Шумоподавляющий автоэнкодер обучается на зашумлённых данных $\\tilde{\\mathbf{x}}$, а целевыми данными являются исходные данные $\\mathbf{x}$. Его функция потерь:\n",
        "$$\n",
        "L_{\\text{denoising}} = \\frac{1}{2} \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2,\n",
        "$$\n",
        "где $\\hat{\\mathbf{x}}$ — восстановленные данные из зашумлённых данных $\\tilde{\\mathbf{x}}$.\n",
        "\n",
        "#### 4. Вариационный автоэнкодер (Variational Autoencoder, VAE)\n",
        "\n",
        "Вариационный автоэнкодер моделирует скрытое представление $\\mathbf{z}$ как случайную величину с заданным распределением (обычно гауссовским). Его функция потерь включает два компонента:\n",
        "$$\n",
        "L_{\\text{VAE}} = L(\\mathbf{x}, \\hat{\\mathbf{x}}) + \\text{KL}(q(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z})),\n",
        "$$\n",
        "где:\n",
        "- $q(\\mathbf{z}|\\mathbf{x})$ — апостериорное распределение скрытого представления,\n",
        "- $p(\\mathbf{z})$ — априорное распределение скрытого представления (обычно $\\mathcal{N}(0, \\mathbf{I})$),\n",
        "- $\\text{KL}$ — дивергенция Кульбака-Лейблера, которая измеряет разницу между двумя распределениями.\n",
        "\n",
        "\n",
        "\n",
        "### Применение автоэнкодеров\n",
        "\n",
        "1. **Снижение размерности**:\n",
        "   - Автоэнкодеры могут использоваться для снижения размерности данных, аналогично PCA, но с учётом нелинейных зависимостей.\n",
        "\n",
        "2. **Обучение представлений**:\n",
        "   - Скрытое представление $\\mathbf{z}$ может использоваться как набор признаков для других задач, таких как классификация или кластеризация.\n",
        "\n",
        "3. **Шумоподавление**:\n",
        "   - Автоэнкодеры могут быть обучены восстанавливать исходные данные из зашумлённых версий.\n",
        "\n",
        "4. **Генерация данных**:\n",
        "   - Вариационные автоэнкодеры (VAE) позволяют генерировать новые данные, похожие на обучающую выборку.\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества и ограничения автоэнкодеров\n",
        "\n",
        "#### Преимущества:\n",
        "- **Нелинейность**: Автоэнкодеры способны учитывать нелинейные зависимости в данных, в отличие от линейных методов, таких как PCA.\n",
        "- **Гибкость**: Архитектура автоэнкодера может быть адаптирована для различных задач (например, шумоподавление, генерация данных).\n",
        "- **Обучение представлений**: Скрытое представление $\\mathbf{z}$ может быть использовано для других задач машинного обучения.\n",
        "\n",
        "#### Ограничения:\n",
        "- **Требуют больших вычислительных ресурсов**: Обучение автоэнкодеров может быть computationally expensive, особенно для больших объёмов данных.\n",
        "- **Риск переобучения**: Автоэнкодеры могут переобучиться, особенно если скрытое пространство слишком большое или данных недостаточно.\n",
        "- **Интерпретируемость**: Скрытое представление $\\mathbf{z}$ может быть трудно интерпретировать, в отличие от методов, таких как PCA.\n",
        "\n",
        "Таким образом, автоэнкодеры — это мощный инструмент для снижения размерности, обучения представлений и генерации данных. Они сочетают в себе гибкость нейронных сетей и способность учитывать нелинейные зависимости в данных. Однако их использование требует тщательного выбора архитектуры и параметров, чтобы избежать переобучения и добиться наилучших результатов.\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример работы автоэнкодера для задачи снижения размерности. Мы будем использовать простой автоэнкодер с одной скрытой нейронной сетью в энкодере и декодере. Входные данные будут представлены в виде вектора $\\mathbf{x} \\in \\mathbb{R}^3$, а скрытое представление $\\mathbf{z} \\in \\mathbb{R}^2$.\n",
        "\n",
        "\n",
        "\n",
        "### Исходные данные\n",
        "\n",
        "Пусть у нас есть один вектор входных данных:\n",
        "$$\n",
        "\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Архитектура автоэнкодера\n",
        "\n",
        "1. **Энкодер**:\n",
        "   - Преобразует $\\mathbf{x} \\in \\mathbb{R}^3$ в $\\mathbf{z} \\in \\mathbb{R}^2$.\n",
        "   - Используем одну скрытую нейронную сеть с функцией активации ReLU:\n",
        "     $$\n",
        "     \\mathbf{z} = \\text{ReLU}(\\mathbf{W}_{\\text{enc}} \\mathbf{x} + \\mathbf{b}_{\\text{enc}}),\n",
        "     $$\n",
        "     где:\n",
        "     - $\\mathbf{W}_{\\text{enc}} \\in \\mathbb{R}^{2 \\times 3}$ — матрица весов энкодера,\n",
        "     - $\\mathbf{b}_{\\text{enc}} \\in \\mathbb{R}^2$ — вектор смещений энкодера.\n",
        "\n",
        "2. **Декодер**:\n",
        "   - Преобразует $\\mathbf{z} \\in \\mathbb{R}^2$ обратно в $\\hat{\\mathbf{x}} \\in \\mathbb{R}^3$.\n",
        "   - Используем одну скрытую нейронную сеть с функцией активации ReLU:\n",
        "     $$\n",
        "     \\hat{\\mathbf{x}} = \\text{ReLU}(\\mathbf{W}_{\\text{dec}} \\mathbf{z} + \\mathbf{b}_{\\text{dec}}),\n",
        "     $$\n",
        "     где:\n",
        "     - $\\mathbf{W}_{\\text{dec}} \\in \\mathbb{R}^{3 \\times 2}$ — матрица весов декодера,\n",
        "     - $\\mathbf{b}_{\\text{dec}} \\in \\mathbb{R}^3$ — вектор смещений декодера.\n",
        "\n",
        "\n",
        "\n",
        "### Инициализация параметров\n",
        "\n",
        "Зададим начальные значения для матриц весов и векторов смещений:\n",
        "\n",
        "1. **Энкодер**:\n",
        "   $$\n",
        "   \\mathbf{W}_{\\text{enc}} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 \\\\ 0.4 & 0.5 & 0.6 \\end{bmatrix}, \\quad\n",
        "   \\mathbf{b}_{\\text{enc}} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "2. **Декодер**:\n",
        "   $$\n",
        "   \\mathbf{W}_{\\text{dec}} = \\begin{bmatrix} 0.7 & 0.8 \\\\ 0.9 & 1.0 \\\\ 1.1 & 1.2 \\end{bmatrix}, \\quad\n",
        "   \\mathbf{b}_{\\text{dec}} = \\begin{bmatrix} 0.3 \\\\ 0.4 \\\\ 0.5 \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Применение энкодера\n",
        "\n",
        "Вычислим скрытое представление $\\mathbf{z}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\text{ReLU}(\\mathbf{W}_{\\text{enc}} \\mathbf{x} + \\mathbf{b}_{\\text{enc}}).\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}_{\\text{enc}} \\mathbf{x} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 \\\\ 0.4 & 0.5 & 0.6 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 0.1 \\cdot 1 + 0.2 \\cdot 2 + 0.3 \\cdot 3 \\\\ 0.4 \\cdot 1 + 0.5 \\cdot 2 + 0.6 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 1.4 \\\\ 3.2 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Добавим смещение:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}_{\\text{enc}} \\mathbf{x} + \\mathbf{b}_{\\text{enc}} = \\begin{bmatrix} 1.4 \\\\ 3.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 3.4 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Применим функцию активации ReLU (ReLU(x) = max(0, x)):\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\text{ReLU}\\left(\\begin{bmatrix} 1.5 \\\\ 3.4 \\end{bmatrix}\\right) = \\begin{bmatrix} 1.5 \\\\ 3.4 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Применение декодера\n",
        "\n",
        "Вычислим восстановленные данные $\\hat{\\mathbf{x}}$:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\text{ReLU}(\\mathbf{W}_{\\text{dec}} \\mathbf{z} + \\mathbf{b}_{\\text{dec}}).\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}_{\\text{dec}} \\mathbf{z} = \\begin{bmatrix} 0.7 & 0.8 \\\\ 0.9 & 1.0 \\\\ 1.1 & 1.2 \\end{bmatrix} \\begin{bmatrix} 1.5 \\\\ 3.4 \\end{bmatrix} = \\begin{bmatrix} 0.7 \\cdot 1.5 + 0.8 \\cdot 3.4 \\\\ 0.9 \\cdot 1.5 + 1.0 \\cdot 3.4 \\\\ 1.1 \\cdot 1.5 + 1.2 \\cdot 3.4 \\end{bmatrix} = \\begin{bmatrix} 3.77 \\\\ 4.85 \\\\ 5.93 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Добавим смещение:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}_{\\text{dec}} \\mathbf{z} + \\mathbf{b}_{\\text{dec}} = \\begin{bmatrix} 3.77 \\\\ 4.85 \\\\ 5.93 \\end{bmatrix} + \\begin{bmatrix} 0.3 \\\\ 0.4 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 4.07 \\\\ 5.25 \\\\ 6.43 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Применим функцию активации ReLU:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\text{ReLU}\\left(\\begin{bmatrix} 4.07 \\\\ 5.25 \\\\ 6.43 \\end{bmatrix}\\right) = \\begin{bmatrix} 4.07 \\\\ 5.25 \\\\ 6.43 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление функции потерь\n",
        "\n",
        "Функция потерь измеряет разницу между исходными данными $\\mathbf{x}$ и восстановленными данными $\\hat{\\mathbf{x}}$. Используем квадратичную функцию потерь:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\frac{1}{2} \\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2.\n",
        "$$\n",
        "\n",
        "Вычислим разность:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} - \\hat{\\mathbf{x}} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 4.07 \\\\ 5.25 \\\\ 6.43 \\end{bmatrix} = \\begin{bmatrix} -3.07 \\\\ -3.25 \\\\ -3.43 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Вычислим квадрат нормы:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2 = (-3.07)^2 + (-3.25)^2 + (-3.43)^2 = 9.4249 + 10.5625 + 11.7649 = 31.7523.\n",
        "$$\n",
        "\n",
        "Тогда функция потерь:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\frac{1}{2} \\cdot 31.7523 \\approx 15.8761.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Обновление параметров\n",
        "\n",
        "Для обновления параметров $\\mathbf{W}_{\\text{enc}}$, $\\mathbf{b}_{\\text{enc}}$, $\\mathbf{W}_{\\text{dec}}$ и $\\mathbf{b}_{\\text{dec}}$ используем метод градиентного спуска. Градиенты функции потерь по параметрам вычисляются с помощью метода обратного распространения ошибки (backpropagation).\n",
        "\n",
        "\n",
        "После нескольких итераций градиентного спуска параметры автоэнкодера будут обновлены, и функция потерь уменьшится. В результате автоэнкодер научится более точно восстанавливать исходные данные из скрытого представления.\n",
        "\n",
        "\n",
        "решению питон\n",
        "Реализуем пример автоэнкодера на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки PyTorch.\n",
        "\n",
        "1. Классическая реализация автоэнкодера с нуля\n"
      ],
      "metadata": {
        "id": "n7QPMveREKzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Autoencoder:\n",
        "    def __init__(self, input_dim, hidden_dim, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Инициализация автоэнкодера.\n",
        "\n",
        "        Параметры:\n",
        "        - input_dim: размерность входных данных.\n",
        "        - hidden_dim: размерность скрытого представления.\n",
        "        - learning_rate: скорость обучения.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Инициализация весов и смещений\n",
        "        self.W_enc = np.random.randn(hidden_dim, input_dim) * 0.01  # Веса энкодера\n",
        "        self.b_enc = np.zeros((hidden_dim, 1))  # Смещения энкодера\n",
        "        self.W_dec = np.random.randn(input_dim, hidden_dim) * 0.01  # Веса декодера\n",
        "        self.b_dec = np.zeros((input_dim, 1))  # Смещения декодера\n",
        "\n",
        "    def relu(self, x):\n",
        "        \"\"\"Функция активации ReLU.\"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        \"\"\"Производная функции ReLU.\"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Применение энкодера.\"\"\"\n",
        "        return self.relu(np.dot(self.W_enc, x) + self.b_enc)\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Применение декодера.\"\"\"\n",
        "        return self.relu(np.dot(self.W_dec, z) + self.b_dec)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Прямой проход через автоэнкодер.\"\"\"\n",
        "        self.z = self.encode(x)\n",
        "        self.x_hat = self.decode(self.z)\n",
        "        return self.x_hat\n",
        "\n",
        "    def compute_loss(self, x, x_hat):\n",
        "        \"\"\"Вычисление функции потерь.\"\"\"\n",
        "        return 0.5 * np.sum((x - x_hat) ** 2)\n",
        "\n",
        "    def backward(self, x):\n",
        "        \"\"\"Обратное распространение ошибки.\"\"\"\n",
        "        # Градиент по выходу декодера\n",
        "        d_loss = -(x - self.x_hat)\n",
        "\n",
        "        # Градиент по весам и смещениям декодера\n",
        "        dW_dec = np.dot(d_loss, self.z.T)\n",
        "        db_dec = np.sum(d_loss, axis=1, keepdims=True)\n",
        "\n",
        "        # Градиент по скрытому представлению\n",
        "        d_z = np.dot(self.W_dec.T, d_loss) * self.relu_derivative(self.z)\n",
        "\n",
        "        # Градиент по весам и смещениям энкодера\n",
        "        dW_enc = np.dot(d_z, x.T)\n",
        "        db_enc = np.sum(d_z, axis=1, keepdims=True)\n",
        "\n",
        "        # Обновление параметров\n",
        "        self.W_enc -= self.learning_rate * dW_enc\n",
        "        self.b_enc -= self.learning_rate * db_enc\n",
        "        self.W_dec -= self.learning_rate * dW_dec\n",
        "        self.b_dec -= self.learning_rate * db_dec\n",
        "\n",
        "    def train(self, x, epochs=1000):\n",
        "        \"\"\"Обучение автоэнкодера.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Прямой проход\n",
        "            x_hat = self.forward(x)\n",
        "\n",
        "            # Обратное распространение\n",
        "            self.backward(x)\n",
        "\n",
        "            # Вывод функции потерь\n",
        "            if epoch % 100 == 0:\n",
        "                loss = self.compute_loss(x, x_hat)\n",
        "                print(f\"Эпоха {epoch}, Функция потерь: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Исходные данные\n",
        "    x = np.array([[1], [2], [3]])\n",
        "\n",
        "    # Создание и обучение автоэнкодера\n",
        "    autoencoder = Autoencoder(input_dim=3, hidden_dim=2, learning_rate=0.01)\n",
        "    autoencoder.train(x, epochs=1000)\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"\\nСкрытое представление z:\")\n",
        "    print(autoencoder.z)\n",
        "    print(\"\\nВосстановленные данные x_hat:\")\n",
        "    print(autoencoder.x_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qEvKtaZF95e",
        "outputId": "36fe1c27-31ba-4841-b4a9-9010d2a73438"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0, Функция потерь: 7.0000\n",
            "Эпоха 100, Функция потерь: 0.9379\n",
            "Эпоха 200, Функция потерь: 0.1257\n",
            "Эпоха 300, Функция потерь: 0.0168\n",
            "Эпоха 400, Функция потерь: 0.0023\n",
            "Эпоха 500, Функция потерь: 0.0003\n",
            "Эпоха 600, Функция потерь: 0.0000\n",
            "Эпоха 700, Функция потерь: 0.0000\n",
            "Эпоха 800, Функция потерь: 0.0000\n",
            "Эпоха 900, Функция потерь: 0.0000\n",
            "\n",
            "Скрытое представление z:\n",
            "[[0.]\n",
            " [0.]]\n",
            "\n",
            "Восстановленные данные x_hat:\n",
            "[[0.99995639]\n",
            " [1.99991279]\n",
            " [2.99986918]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения (PyTorch)\n",
        "\n",
        "Для использования готового решения воспользуемся библиотекой PyTorch."
      ],
      "metadata": {
        "id": "Tpfc7BqdGDCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Определение архитектуры автоэнкодера\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Исходные данные\n",
        "    x = torch.tensor([[1.0, 2.0, 3.0]])\n",
        "\n",
        "    # Создание и обучение автоэнкодера\n",
        "    autoencoder = Autoencoder(input_dim=3, hidden_dim=2)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(1000):\n",
        "        # Прямой проход\n",
        "        x_hat = autoencoder(x)\n",
        "        loss = criterion(x_hat, x)\n",
        "\n",
        "        # Обратное распространение и обновление параметров\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Вывод функции потерь\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Эпоха {epoch}, Функция потерь: {loss.item():.4f}\")\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"\\nСкрытое представление z:\")\n",
        "    print(autoencoder.encoder(x))\n",
        "    print(\"\\nВосстановленные данные x_hat:\")\n",
        "    print(autoencoder(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYrWFC_wGKK_",
        "outputId": "4ba8f386-dcdc-472c-c73a-e23c71ff0013"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0, Функция потерь: 4.5029\n",
            "Эпоха 100, Функция потерь: 4.3333\n",
            "Эпоха 200, Функция потерь: 4.3333\n",
            "Эпоха 300, Функция потерь: 4.3333\n",
            "Эпоха 400, Функция потерь: 4.3333\n",
            "Эпоха 500, Функция потерь: 4.3333\n",
            "Эпоха 600, Функция потерь: 4.3333\n",
            "Эпоха 700, Функция потерь: 4.3333\n",
            "Эпоха 800, Функция потерь: 4.3333\n",
            "Эпоха 900, Функция потерь: 4.3333\n",
            "\n",
            "Скрытое представление z:\n",
            "tensor([[0.0000, 2.3156]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Восстановленные данные x_hat:\n",
            "tensor([[1., 0., 0.]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.8. Изометрическое отображение (Isomap, Isometric Mapping)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "Изометрическое отображение (**Isomap**, **Isometric Mapping**) — это метод нелинейного снижения размерности, разработанный для анализа данных, которые лежат на многообразии сложной формы. В отличие от линейных методов, таких как метод главных компонент (**PCA**), Isomap учитывает нелинейную структуру данных, что позволяет более точно сохранять **геодезические расстояния** между точками. Этот метод находит применение в задачах визуализации данных, классификации и кластеризации, где важно учитывать глобальную структуру данных.\n",
        "\n",
        "\n",
        "\n",
        "#### Теоретические основы\n",
        "Isomap основывается на предположении, что данные лежат на гладком многообразии $\\mathcal{M}$, вложенном в высокоразмерное пространство $\\mathbb{R}^D$. Цель метода — найти низкоразмерное представление данных $Y = \\{y_1, y_2, \\dots, y_n\\}$, где $y_i \\in \\mathbb{R}^d$ и $d \\ll D$, такое, что геодезические расстояния между точками на многообразии $\\mathcal{M}$ сохраняются в низкоразмерном пространстве.\n",
        "\n",
        "**Геодезическое расстояние** $d_{\\mathcal{M}}(x_i, x_j)$ между двумя точками $x_i$ и $x_j$ на многообразии $\\mathcal{M}$ определяется как длина кратчайшего пути, соединяющего эти точки вдоль поверхности многообразия. В отличие от **евклидова расстояния** $d_E(x_i, x_j)$, которое измеряется напрямую в исходном пространстве, геодезическое расстояние учитывает кривизну многообразия.\n",
        "\n",
        "\n",
        "\n",
        "#### Геодезическое и евклидово расстояние\n",
        "##### **Евклидово расстояние**\n",
        "Евклидово расстояние — это стандартная мера расстояния между двумя точками в евклидовом пространстве. Для двух точек $x_i = (x_{i1}, x_{i2}, \\dots, x_{iD})$ и $x_j = (x_{j1}, x_{j2}, \\dots, x_{jD})$ в $D$-мерном пространстве евклидово расстояние $d_E(x_i, x_j)$ вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "d_E(x_i, x_j) = \\sqrt{\\sum_{k=1}^{D} (x_{ik} - x_{jk})^2}.\n",
        "$$\n",
        "\n",
        "Эта формула является обобщением теоремы Пифагора для многомерного пространства. Евклидово расстояние измеряет \"прямолинейное\" расстояние между двумя точками, игнорируя любую кривизну или структуру пространства.\n",
        "\n",
        "##### **Геодезическое расстояние**\n",
        "Геодезическое расстояние — это мера расстояния между двумя точками на многообразии $\\mathcal{M}$, учитывающая кривизну поверхности. Оно определяется как длина кратчайшего пути между точками $x_i$ и $x_j$, лежащего на поверхности многообразия $\\mathcal{M}$. Формально, геодезическое расстояние $d_{\\mathcal{M}}(x_i, x_j)$ определяется как:\n",
        "\n",
        "$$\n",
        "d_{\\mathcal{M}}(x_i, x_j) = \\inf_{\\gamma} \\int_{\\gamma} ds,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\gamma$ — гладкая кривая, соединяющая точки $x_i$ и $x_j$ на многообразии $\\mathcal{M}$,\n",
        "- $ds$ — элемент длины вдоль кривой $\\gamma$.\n",
        "\n",
        "На практике геодезическое расстояние аппроксимируется с помощью графа ближайших соседей. Для этого:\n",
        "1. Строится граф $G$, где вершины соответствуют точкам данных, а рёбра соединяют ближайшие соседи с весами, равными евклидовым расстояниям.\n",
        "2. Геодезическое расстояние $d_G(x_i, x_j)$ между точками $x_i$ и $x_j$ вычисляется как длина кратчайшего пути в графе $G$. Это можно выразить как:\n",
        "\n",
        "$$\n",
        "d_G(x_i, x_j) = \\min_{\\text{пути } P} \\sum_{(u, v) \\in P} d_E(u, v),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P$ — путь между $x_i$ и $x_j$ в графе $G$,\n",
        "- $d_E(u, v)$ — евклидово расстояние между соседними вершинами $u$ и $v$ на пути $P$.\n",
        "\n",
        "\n",
        "\n",
        "#### Алгоритм Isomap\n",
        "Алгоритм Isomap состоит из трёх основных этапов:\n",
        "\n",
        "##### **1. Построение графа ближайших соседей**\n",
        "- Для каждой точки $x_i$ в наборе данных $X = \\{x_1, x_2, \\dots, x_n\\}$ определяются $k$ ближайших соседей на основе евклидова расстояния $d_E(x_i, x_j)$.\n",
        "- Строится граф $G$, вершины которого соответствуют точкам данных, а рёбра соединяют каждую точку с её $k$ ближайшими соседями. Вес ребра между точками $x_i$ и $x_j$ равен евклидову расстоянию $d_E(x_i, x_j)$.\n",
        "\n",
        "##### **2. Вычисление геодезических расстояний**\n",
        "- Для каждой пары точек $(x_i, x_j)$ вычисляется кратчайший путь в графе $G$. Это расстояние называется **геодезическим расстоянием** $d_G(x_i, x_j)$ и служит аппроксимацией истинного геодезического расстояния $d_{\\mathcal{M}}(x_i, x_j)$ на многообразии $\\mathcal{M}$.\n",
        "- Для нахождения кратчайших путей используются алгоритмы, такие как **алгоритм Дейкстры** или **Флойда-Уоршелла**. Матрица геодезических расстояний $D_G = [d_G(x_i, x_j)]$ формируется на основе этих вычислений.\n",
        "\n",
        "##### **3. Многомерное шкалирование (MDS)**\n",
        "- На основе матрицы геодезических расстояний $D_G$ применяется метод **многомерного шкалирования (MDS)** для нахождения низкоразмерного представления данных $Y = \\{y_1, y_2, \\dots, y_n\\}$.\n",
        "- MDS минимизирует целевую функцию, называемую **\"стрессом\" (stress)**:\n",
        "  $$\n",
        "  \\text{Stress}(Y) = \\sum_{i < j} \\left( d_G(x_i, x_j) - d_E(y_i, y_j) \\right)^2,\n",
        "  $$\n",
        "  где $d_E(y_i, y_j)$ — евклидово расстояние между точками $y_i$ и $y_j$ в низкоразмерном пространстве.\n",
        "- Решение задачи MDS сводится к нахождению собственных значений и собственных векторов матрицы $\\tau(D_G)$, где $\\tau$ — оператор двойного центрирования:\n",
        "  $$\n",
        "  \\tau(D_G) = -\\frac{1}{2} H D_G^2 H,\n",
        "  $$\n",
        "  где $H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T$ — матрица центрирования, $I$ — единичная матрица, а $\\mathbf{1}$ — вектор из единиц.\n",
        "\n",
        "\n",
        "\n",
        "#### Математическое обоснование\n",
        "Isomap основывается на предположении, что многообразие $\\mathcal{M}$ является изометрически вложенным в высокоразмерное пространство. Это означает, что геодезические расстояния на многообразии $\\mathcal{M}$ могут быть аппроксимированы кратчайшими путями в графе ближайших соседей. Таким образом, Isomap стремится сохранить глобальную структуру данных, что делает его более подходящим для анализа нелинейных данных по сравнению с методами, которые учитывают только локальные расстояния (например, t-SNE).\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и ограничения\n",
        "##### **Преимущества**:\n",
        "- **Учёт нелинейной структуры данных**: Isomap эффективен для анализа данных, лежащих на многообразии сложной формы, где линейные методы, такие как PCA, оказываются неэффективными.\n",
        "- **Сохранение глобальной структуры**: Isomap сохраняет как локальные, так и глобальные отношения между точками данных, что важно для задач визуализации и кластеризации.\n",
        "- **Универсальность**: Isomap применяется в различных областях, включая обработку изображений, видео, текстов и обнаружение аномалий.\n",
        "\n",
        "##### **Ограничения**:\n",
        "- **Вычислительная сложность**: Вычисление геодезических расстояний требует значительных вычислительных ресурсов, особенно для больших наборов данных.\n",
        "- **Чувствительность к параметрам**: Результаты работы Isomap зависят от выбора параметра $k$ (числа ближайших соседей). Неправильный выбор $k$ может привести к нарушению структуры данных.\n",
        "- **Чувствительность к шуму и выбросам**: Наличие шума или выбросов в данных может исказить геодезические расстояния и привести к некорректным результатам.\n",
        "\n",
        "\n",
        "\n",
        "#### Дополнение к разделу \"Многомерное шкалирование (MDS)\"\n",
        "Метод многомерного шкалирования (MDS) минимизирует целевую функцию, называемую **\"стрессом\" (stress)**, которая измеряет разницу между геодезическими расстояниями в исходном пространстве и евклидовыми расстояниями в низкоразмерном пространстве. Формально, целевая функция определяется следующим образом:\n",
        "\n",
        "$$\n",
        "\\text{Stress}(Y) = \\sum_{i < j} \\left( d_G(x_i, x_j) - d_E(y_i, y_j) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $d_G(x_i, x_j)$ — геодезическое расстояние между точками $x_i$ и $x_j$ в исходном пространстве,\n",
        "- $d_E(y_i, y_j)$ — евклидово расстояние между точками $y_i$ и $y_j$ в низкоразмерном пространстве,\n",
        "- $Y = \\{y_1, y_2, \\dots, y_n\\}$ — низкоразмерное представление данных.\n",
        "\n",
        "Для минимизации функции \\(\\text{Stress}(Y)\\) используется градиентный спуск или другие методы оптимизации. Градиент функции \\(\\text{Stress}(Y)\\) по координатам точек $y_i$ в низкоразмерном пространстве вычисляется следующим образом:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{Stress}(Y)}{\\partial y_i} = -2 \\sum_{j \\neq i} \\left( d_G(x_i, x_j) - d_E(y_i, y_j) \\right) \\cdot \\frac{y_i - y_j}{d_E(y_i, y_j)}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $\\frac{y_i - y_j}{d_E(y_i, y_j)}$ — единичный вектор, направленный от точки $y_j$ к точке $y_i$,\n",
        "- $d_E(y_i, y_j) = \\| y_i - y_j \\|$ — евклидово расстояние между точками $y_i$ и $y_j$.\n",
        "\n",
        "\n",
        "\n",
        "#### Пояснение к градиенту\n",
        "Градиент функции \\(\\text{Stress}(Y)\\) показывает, как изменяется целевая функция при малых изменениях координат точек $y_i$. Каждая компонента градиента \\(\\frac{\\partial \\text{Stress}(Y)}{\\partial y_i}\\) указывает направление, в котором нужно переместить точку $y_i$, чтобы уменьшить значение функции \\(\\text{Stress}(Y)\\).\n",
        "\n",
        "1. **Первая часть градиента**: $-2 \\left( d_G(x_i, x_j) - d_E(y_i, y_j) \\right)$ определяет величину ошибки между геодезическим расстоянием $d_G(x_i, x_j)$ и евклидовым расстоянием $d_E(y_i, y_j)$. Если $d_E(y_i, y_j)$ меньше, чем $d_G(x_i, x_j)$, то точка $y_i$ должна быть отодвинута от точки $y_j$, и наоборот.\n",
        "\n",
        "2. **Вторая часть градиента**: $\\frac{y_i - y_j}{d_E(y_i, y_j)}$ задаёт направление, в котором нужно переместить точку $y_i$. Это единичный вектор, указывающий от $y_j$ к $y_i$.\n",
        "\n",
        "\n",
        "\n",
        "#### Процесс оптимизации\n",
        "Для минимизации функции \\(\\text{Stress}(Y)\\) используется итеративный процесс градиентного спуска:\n",
        "1. Инициализируются начальные координаты точек $y_i$ в низкоразмерном пространстве (например, случайным образом или с помощью PCA).\n",
        "2. На каждом шаге вычисляется градиент \\(\\frac{\\partial \\text{Stress}(Y)}{\\partial y_i}\\) для всех точек $y_i$.\n",
        "3. Координаты точек обновляются по формуле:\n",
        "$$\n",
        "   y_i^{(t+1)} = y_i^{(t)} - \\eta \\cdot \\frac{\\partial \\text{Stress}(Y)}{\\partial y_i},\n",
        "$$\n",
        "   где $\\eta$ — скорость обучения (learning rate), а $t$ — номер итерации.\n",
        "4. Процесс повторяется до тех пор, пока значение функции \\(\\text{Stress}(Y)}\\) не стабилизируется или не достигнет заданного порога.\n",
        "\n",
        "\n",
        "\n",
        "#### Альтернативные подходы\n",
        "Вместо градиентного спуска для минимизации функции \\(\\text{Stress}(Y)\\) могут использоваться другие методы оптимизации, такие как:\n",
        "- **Метод сопряжённых градиентов** (Conjugate Gradient), который ускоряет сходимость по сравнению с градиентным спуском.\n",
        "- **Алгоритм Левенберга-Марквардта** (Levenberg-Marquardt), который сочетает в себе преимущества градиентного спуска и метода Ньютона.\n",
        "\n",
        "Таким образом, Isomap представляет собой мощный инструмент для нелинейного снижения размерности, который позволяет сохранять геодезические расстояния между точками данных. Этот метод особенно полезен для анализа данных, лежащих на многообразии сложной формы, где линейные методы, такие как PCA, оказываются неэффективными. Однако применение Isomap требует внимательного выбора параметров и учёта его вычислительной сложности. В целом, Isomap является важным методом в арсенале современных методов анализа данных и машинного обучения.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример работы алгоритма Isomap на наборе данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь каждая строка матрицы $X$ представляет координаты точки в трёхмерном пространстве. Наша цель — снизить размерность данных до 2D, сохранив геодезические расстояния между точками.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Построение графа ближайших соседей\n",
        "\n",
        "1. **Вычисление евклидовых расстояний**:\n",
        "   Для каждой пары точек $x_i$ и $x_j$ вычислим евклидово расстояние $d_E(x_i, x_j)$:\n",
        "\n",
        "$$\n",
        "   d_E(x_i, x_j) = \\sqrt{\\sum_{k=1}^{3} (x_{ik} - x_{jk})^2}.\n",
        "$$\n",
        "\n",
        "   Например, расстояние между точками $x_1 = (1, 2, 3)$ и $x_2 = (4, 5, 6)$:\n",
        "\n",
        "$$\n",
        "   d_E(x_1, x_2) = \\sqrt{(4-1)^2 + (5-2)^2 + (6-3)^2} = \\sqrt{9 + 9 + 9} = \\sqrt{27} \\approx 5.196.\n",
        "$$\n",
        "\n",
        "   Аналогично вычисляем расстояния для всех пар точек. Матрица евклидовых расстояний $D_E$ будет выглядеть так:\n",
        "\n",
        "$$\n",
        "   D_E = \\begin{bmatrix}\n",
        "   0 & 5.196 & 10.392 & 15.588 \\\\\n",
        "   5.196 & 0 & 5.196 & 10.392 \\\\\n",
        "   10.392 & 5.196 & 0 & 5.196 \\\\\n",
        "   15.588 & 10.392 & 5.196 & 0\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "2. **Построение графа ближайших соседей**:\n",
        "   Выберем параметр $k = 2$ (каждая точка соединяется с двумя ближайшими соседями). Граф $G$ будет иметь следующие рёбра:\n",
        "   - $x_1$ соединяется с $x_2$ и $x_3$,\n",
        "   - $x_2$ соединяется с $x_1$ и $x_3$,\n",
        "   - $x_3$ соединяется с $x_2$ и $x_4$,\n",
        "   - $x_4$ соединяется с $x_3$ и $x_2$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление геодезических расстояний\n",
        "\n",
        "1. **Аппроксимация геодезических расстояний**:\n",
        "   Используем алгоритм Флойда-Уоршелла для нахождения кратчайших путей в графе $G$. Изначально матрица геодезических расстояний $D_G$ совпадает с $D_E$, но для несвязанных точек (например, $x_1$ и $x_4$) расстояние будет бесконечным.\n",
        "\n",
        "   После применения алгоритма Флойда-Уоршелла получаем:\n",
        "\n",
        "$$\n",
        "   D_G = \\begin{bmatrix}\n",
        "   0 & 5.196 & 10.392 & 15.588 \\\\\n",
        "   5.196 & 0 & 5.196 & 10.392 \\\\\n",
        "   10.392 & 5.196 & 0 & 5.196 \\\\\n",
        "   15.588 & 10.392 & 5.196 & 0\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "   В данном случае граф $G$ полностью связный, поэтому геодезические расстояния совпадают с евклидовыми.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Многомерное шкалирование (MDS)\n",
        "\n",
        "1. **Центрирование матрицы геодезических расстояний**:\n",
        "   Применим оператор двойного центрирования к матрице $D_G$:\n",
        "\n",
        "$$\n",
        "   \\tau(D_G) = -\\frac{1}{2} H D_G^2 H,\n",
        "$$\n",
        "\n",
        "   где $H = I - \\frac{1}{4} \\mathbf{1} \\mathbf{1}^T$ — матрица центрирования, $I$ — единичная матрица, а $\\mathbf{1}$ — вектор из единиц.\n",
        "\n",
        "   Вычислим $D_G^2$:\n",
        "\n",
        "$$\n",
        "   D_G^2 = \\begin{bmatrix}\n",
        "   0 & 27 & 108 & 243 \\\\\n",
        "   27 & 0 & 27 & 108 \\\\\n",
        "   108 & 27 & 0 & 27 \\\\\n",
        "   243 & 108 & 27 & 0\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "   Затем вычислим $\\tau(D_G)$:\n",
        "\n",
        "$$\n",
        "   \\tau(D_G) = -\\frac{1}{2} H D_G^2 H.\n",
        "$$\n",
        "\n",
        "2. **Нахождение собственных значений и собственных векторов**:\n",
        "   Найдём собственные значения и собственные векторы матрицы $\\tau(D_G)$. Предположим, что два наибольших собственных значения равны $\\lambda_1 = 100$ и $\\lambda_2 = 50$, а соответствующие собственные векторы:\n",
        "\n",
        "$$\n",
        "   v_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}, \\quad\n",
        "   v_2 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "3. **Формирование низкоразмерного представления**:\n",
        "   Низкоразмерное представление $Y$ получается умножением собственных векторов на корни из соответствующих собственных значений:\n",
        "\n",
        "$$\n",
        "   Y = \\begin{bmatrix}\n",
        "   \\sqrt{\\lambda_1} v_1 & \\sqrt{\\lambda_2} v_2\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "   Подставляем значения:\n",
        "\n",
        "$$\n",
        "   Y = \\begin{bmatrix}\n",
        "   0.5 \\cdot \\sqrt{100} & 0.5 \\cdot \\sqrt{50} \\\\\n",
        "   0.5 \\cdot \\sqrt{100} & -0.5 \\cdot \\sqrt{50} \\\\\n",
        "   -0.5 \\cdot \\sqrt{100} & 0.5 \\cdot \\sqrt{50} \\\\\n",
        "   -0.5 \\cdot \\sqrt{100} & -0.5 \\cdot \\sqrt{50}\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   5 & 3.535 \\\\\n",
        "   5 & -3.535 \\\\\n",
        "   -5 & 3.535 \\\\\n",
        "   -5 & -3.535\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итоговое низкоразмерное представление\n",
        "Низкоразмерное представление данных в 2D:\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "5 & 3.535 \\\\\n",
        "5 & -3.535 \\\\\n",
        "-5 & 3.535 \\\\\n",
        "-5 & -3.535\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Этот результат показывает, как исходные точки в 3D-пространстве были спроецированы на 2D-плоскость с сохранением геодезических расстояний.\n",
        "\n",
        "\n",
        "Таким образом, мы рассмотрели пошаговый пример работы алгоритма Isomap на небольшом наборе данных. На каждом этапе были применены соответствующие формулы и методы, что позволило получить низкоразмерное представление данных с сохранением их структуры. Этот пример демонстрирует, как Isomap может быть использован для анализа данных, лежащих на многообразии сложной формы.\n",
        "\n",
        "\n",
        "\n",
        "####Реализация Isomap с нуля на Python\n",
        "Для начала создадим класс Isomap, который будет реализовывать алгоритм Isomap с нуля. Затем мы сравним его с готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Создание класса Isomap\n"
      ],
      "metadata": {
        "id": "_EozPhGYF_zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "class Isomap:\n",
        "    def __init__(self, n_components=2, n_neighbors=2):\n",
        "        self.n_components = n_components  # Целевая размерность\n",
        "        self.n_neighbors = n_neighbors    # Количество ближайших соседей\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        # Шаг 1: Построение графа ближайших соседей\n",
        "        n_samples = X.shape[0]\n",
        "        nbrs = NearestNeighbors(n_neighbors=self.n_neighbors + 1).fit(X)  # +1, чтобы исключить саму точку\n",
        "        distances, indices = nbrs.kneighbors(X)\n",
        "\n",
        "        # Создание матрицы смежности\n",
        "        graph = np.zeros((n_samples, n_samples))\n",
        "        for i in range(n_samples):\n",
        "            for j, dist in zip(indices[i][1:], distances[i][1:]):  # Исключаем саму точку\n",
        "                graph[i, j] = dist\n",
        "                graph[j, i] = dist\n",
        "\n",
        "        # Шаг 2: Вычисление геодезических расстояний\n",
        "        geodesic_distances = shortest_path(graph, directed=False)\n",
        "\n",
        "        # Шаг 3: Многомерное шкалирование (MDS)\n",
        "        mds = MDS(n_components=self.n_components, dissimilarity='precomputed', random_state=42)\n",
        "        embedding = mds.fit_transform(geodesic_distances)\n",
        "\n",
        "        return embedding\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Создание и применение улучшенного Isomap\n",
        "isomap = Isomap(n_components=2, n_neighbors=2)\n",
        "Y = isomap.fit_transform(X)\n",
        "\n",
        "print(\"Низкоразмерное представление (улучшенный Isomap):\")\n",
        "print(Y)\n",
        "\n",
        "# Применение Isomap из scikit-learn\n",
        "sklearn_isomap = SklearnIsomap(n_components=2, n_neighbors=2)\n",
        "Y_sklearn = sklearn_isomap.fit_transform(X)\n",
        "\n",
        "print(\"\\nНизкоразмерное представление (scikit-learn Isomap):\")\n",
        "print(Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-USBIlQWJxax",
        "outputId": "6449c6bc-69e4-422b-cccd-61e28e6b6e21"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Низкоразмерное представление (улучшенный Isomap):\n",
            "[[ 2.67367969  7.27936677]\n",
            " [-0.07950459  2.70129347]\n",
            " [ 0.12921585 -2.72245889]\n",
            " [-2.72339095 -7.25820136]]\n",
            "\n",
            "Низкоразмерное представление (scikit-learn Isomap):\n",
            "[[ 7.79422863 -0.        ]\n",
            " [ 2.59807621 -0.        ]\n",
            " [-2.59807621  0.        ]\n",
            " [-7.79422863 -0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn\n",
        "\n",
        "Теперь сравним наш класс с готовым решением из библиотеки scikit-learn."
      ],
      "metadata": {
        "id": "Pf4xTVMuJ5k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import Isomap as SklearnIsomap\n",
        "\n",
        "# Применение Isomap из scikit-learn\n",
        "sklearn_isomap = SklearnIsomap(n_components=2, n_neighbors=2)\n",
        "Y_sklearn = sklearn_isomap.fit_transform(X)\n",
        "\n",
        "print(\"Низкоразмерное представление (scikit-learn Isomap):\")\n",
        "print(Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nzLrUDdKAW8",
        "outputId": "7a147708-7d12-48db-8cd7-6b0d3c589f29"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Низкоразмерное представление (scikit-learn Isomap):\n",
            "[[ 7.79422863 -0.        ]\n",
            " [ 2.59807621 -0.        ]\n",
            " [-2.59807621  0.        ]\n",
            " [-7.79422863 -0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "Локально линейное вложение (LLE, Locally Linear Embedding) — это метод нелинейного снижения размерности, который сохраняет локальную структуру данных при переходе в пространство меньшей размерности. Основная идея LLE заключается в том, что каждая точка данных может быть аппроксимирована как линейная комбинация своих ближайших соседей. Этот подход особенно полезен для анализа данных, лежащих на искривлённых многообразиях (manifolds), где традиционные линейные методы, такие как PCA (Principal Component Analysis), оказываются недостаточно эффективными.\n",
        "\n",
        "\n",
        "\n",
        "### Основные этапы LLE\n",
        "LLE включает три основных шага:\n",
        "1. **Определение ближайших соседей**.\n",
        "2. **Вычисление весов реконструкции**.\n",
        "3. **Формирование низкоразмерного представления**.\n",
        "\n",
        "Каждый из этих этапов имеет строгую математическую основу, которую мы рассмотрим подробно.\n",
        "\n",
        "\n",
        "\n",
        "#### 1. Определение ближайших соседей\n",
        "Для каждой точки данных $x_i$ в исходном пространстве высокой размерности $\\mathbb{R}^D$ выбирается $k$ ближайших соседей. Близость между точками определяется с использованием евклидова расстояния или другой подходящей метрики. Множество ближайших соседей точки $x_i$ обозначается как $N(i)$.\n",
        "\n",
        "Формально, для каждой точки $x_i$ находятся $k$ точек $x_j$, таких что:\n",
        "$$\n",
        "d(x_i, x_j) \\leq d(x_i, x_l) \\quad \\forall j \\in N(i), \\, l \\notin N(i),\n",
        "$$\n",
        "где $d(x_i, x_j)$ — расстояние между точками $x_i$ и $x_j$.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Вычисление весов реконструкции\n",
        "На этом этапе для каждой точки $x_i$ вычисляются веса $w_{ij}$, которые минимизируют ошибку реконструкции точки на основе её соседей. Ошибка реконструкции определяется как:\n",
        "$$\n",
        "\\varepsilon_i = \\left\\| x_i - \\sum_{j \\in N(i)} w_{ij} x_j \\right\\|^2.\n",
        "$$\n",
        "\n",
        "Цель — найти такие веса $w_{ij}$, чтобы минимизировать суммарную ошибку реконструкции для всех точек:\n",
        "$$\n",
        "\\min_{w_{ij}} \\sum_{i} \\left\\| x_i - \\sum_{j \\in N(i)} w_{ij} x_j \\right\\|^2.\n",
        "$$\n",
        "\n",
        "При этом на веса накладываются два ограничения:\n",
        "1. **Условие нормировки**: Сумма весов для каждой точки должна быть равна 1:\n",
        "   $$\n",
        "   \\sum_{j \\in N(i)} w_{ij} = 1.\n",
        "   $$\n",
        "2. **Локальность**: Веса $w_{ij}$ равны нулю для всех точек, не являющихся соседями $x_i$:\n",
        "   $$\n",
        "   w_{ij} = 0 \\quad \\text{если} \\quad j \\notin N(i).\n",
        "   $$\n",
        "\n",
        "##### Минимизация ошибки реконструкции\n",
        "Для минимизации суммарной ошибки реконструкции $\\Phi(W)$ используется метод наименьших квадратов с учётом ограничений. Рассмотрим шаги решения.\n",
        "\n",
        "###### Шаг 1: Локальная ковариационная матрица\n",
        "Для каждой точки $x_i$ строится локальная ковариационная матрица $C_i$, которая отражает взаимодействие между соседями. Элементы матрицы $C_i$ вычисляются как:\n",
        "$$\n",
        "C_{jk} = (x_i - x_j)^T (x_i - x_k),\n",
        "$$\n",
        "где $j, k \\in N(i)$.\n",
        "\n",
        "###### Шаг 2: Решение системы уравнений\n",
        "Для нахождения весов $w_{ij}$ решается система линейных уравнений. Вектор весов $w_i$ для точки $x_i$ находится как решение следующей задачи:\n",
        "$$\n",
        "w_i = \\argmin_{w_i} \\left\\| x_i - \\sum_{j \\in N(i)} w_{ij} x_j \\right\\|^2,\n",
        "$$\n",
        "при условии $\\sum_{j \\in N(i)} w_{ij} = 1$.\n",
        "\n",
        "Эта задача эквивалентна решению системы уравнений:\n",
        "$$\n",
        "C_i w_i = \\mathbf{1},\n",
        "$$\n",
        "где $\\mathbf{1}$ — вектор, состоящий из единиц.\n",
        "\n",
        "###### Шаг 3: Нормировка весов\n",
        "После нахождения весов $w_{ij}$ они нормируются, чтобы удовлетворить условие $\\sum_{j \\in N(i)} w_{ij} = 1$. Это делается путём деления каждого веса на сумму всех весов для данной точки:\n",
        "$$\n",
        "w_{ij} = \\frac{w_{ij}}{\\sum_{k \\in N(i)} w_{ik}}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Формирование низкоразмерного представления\n",
        "После вычисления весов $w_{ij}$ строится низкоразмерное представление данных $y_i$ в пространстве $\\mathbb{R}^d$, где $d \\ll D$. Это делается путём минимизации функции ошибки, которая сохраняет локальные отношения между точками:\n",
        "$$\n",
        "\\Phi(Y) = \\sum_{i} \\left\\| y_i - \\sum_{j \\in N(i)} w_{ij} y_j \\right\\|^2.\n",
        "$$\n",
        "\n",
        "Здесь $Y$ — матрица низкоразмерных представлений точек $y_i$. Минимизация этой функции ошибки позволяет сохранить локальную структуру данных в низкоразмерном пространстве.\n",
        "\n",
        "Для решения задачи минимизации используется метод собственных значений (спектральное разложение). Сначала строится матрица $M$:\n",
        "$$\n",
        "M = (I - W)^T (I - W),\n",
        "$$\n",
        "где:\n",
        "- $W$ — матрица весов $w_{ij}$,\n",
        "- $I$ — единичная матрица.\n",
        "\n",
        "Затем находятся собственные векторы матрицы $M$, соответствующие наименьшим собственным значениям. Эти собственные векторы и образуют низкоразмерное представление данных.\n",
        "\n",
        "\n",
        "\n",
        "### Вычисление градиента для задачи минимизации\n",
        "\n",
        "Для минимизации функции ошибки $\\Phi(Y)$ в LLE используется метод градиентного спуска. Рассмотрим, как вычисляется градиент этой функции.\n",
        "\n",
        "#### Формулировка задачи\n",
        "Функция ошибки для низкоразмерного представления $Y$ задаётся как:\n",
        "$$\n",
        "\\Phi(Y) = \\sum_{i} \\left\\| y_i - \\sum_{j \\in N(i)} w_{ij} y_j \\right\\|^2.\n",
        "$$\n",
        "\n",
        "Цель — минимизировать $\\Phi(Y)$ по $Y$, сохраняя локальные отношения между точками.\n",
        "\n",
        "#### Градиент функции ошибки\n",
        "Градиент функции $\\Phi(Y)$ по $Y$ вычисляется как:\n",
        "$$\n",
        "\\frac{\\partial \\Phi(Y)}{\\partial y_i} = 2 \\left( y_i - \\sum_{j \\in N(i)} w_{ij} y_j \\right) - 2 \\sum_{j \\in N(i)} w_{ji} \\left( y_j - \\sum_{k \\in N(j)} w_{jk} y_k \\right).\n",
        "$$\n",
        "\n",
        "Это выражение можно упростить, если представить его в матричной форме. Обозначим $M = (I - W)^T (I - W)$, тогда градиент функции ошибки можно записать как:\n",
        "$$\n",
        "\\frac{\\partial \\Phi(Y)}{\\partial Y} = 2 M Y.\n",
        "$$\n",
        "\n",
        "#### Алгоритм градиентного спуска\n",
        "1. Инициализируем низкоразмерное представление $Y$ случайными значениями.\n",
        "2. Вычисляем градиент $\\frac{\\partial \\Phi(Y)}{\\partial Y}$.\n",
        "3. Обновляем $Y$ по формуле:\n",
        "   $$\n",
        "   Y_{\\text{new}} = Y_{\\text{old}} - \\eta \\frac{\\partial \\Phi(Y)}{\\partial Y},\n",
        "   $$\n",
        "   где $\\eta$ — скорость обучения (learning rate).\n",
        "4. Повторяем шаги 2–3 до сходимости.\n",
        "\n",
        "\n",
        "\n",
        "### Ключевые допущения LLE\n",
        "1. **Данные лежат на гладком многообразии**: LLE предполагает, что данные можно аппроксимировать локально линейными участками. Если данные зашумлены или не соответствуют этому допущению, LLE может работать плохо.\n",
        "2. **Число соседей $k$ критично**: Слишком мало соседей — и локальная структура не будет захвачена. Слишком много — и метод начнёт вести себя как глобальный, теряя способность захватывать нелинейность.\n",
        "3. **Проблемы с граничными точками и выбросами**: LLE может искажать данные на границах или при наличии выбросов.\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества и ограничения LLE\n",
        "\n",
        "#### Преимущества:\n",
        "1. **Сохраняет локальную геометрию данных**: LLE фокусируется на локальных окрестностях, сохраняя их структуру.\n",
        "2. **Эффективен для нелинейных данных**: В отличие от PCA, LLE хорошо работает с данными, лежащими на искривлённых многообразиях.\n",
        "3. **Непараметрический метод**: LLE не требует предварительных предположений о форме многообразия.\n",
        "\n",
        "#### Ограничения:\n",
        "1. **Вычислительная сложность**: LLE требует значительных вычислительных ресурсов для больших наборов данных.\n",
        "2. **Чувствительность к шуму и выбору числа соседей**: Неправильный выбор $k$ или наличие шума могут ухудшить результаты.\n",
        "3. **Проблемы с граничными точками и выбросами**: LLE может искажать данные на границах или при наличии выбросов.\n",
        "\n",
        "\n",
        "\n",
        "### Сравнение LLE с другими методами снижения размерности\n",
        "\n",
        "Для компактности и наглядности сравнение LLE с другими методами представлено в виде таблицы:\n",
        "\n",
        "| **Метод** | **Основная идея** | **Преимущества** | **Недостатки** | **Применение** |\n",
        "|--|--||-|-|\n",
        "| **LLE**   | Сохраняет локальную структуру данных, аппроксимируя каждую точку как линейную комбинацию соседей. | Эффективен для нелинейных данных, сохраняет локальную геометрию. | Чувствителен к шуму, вычислительно сложен для больших данных. | Визуализация, извлечение признаков, обработка изображений и звука. |\n",
        "| **PCA**   | Проецирует данные на оси, максимизирующие дисперсию. | Прост в реализации, эффективен для линейных данных. | Не сохраняет нелинейные структуры. | Снижение размерности, шумоподавление. |\n",
        "| **Isomap**| Сохраняет глобальные геодезические расстояния между точками. | Хорошо работает с данными на искривлённых многообразиях. | Вычислительно сложен, требует точного выбора параметров. | Визуализация, анализ данных на многообразиях. |\n",
        "| **t-SNE** | Минимизирует расхождение между распределениями в высоко- и низкоразмерном пространствах. | Отлично подходит для визуализации кластеров. | Не сохраняет глобальную структуру, вычислительно затратен. | Визуализация высокомерных данных. |\n",
        "\n",
        "\n",
        "\n",
        "### Применение LLE\n",
        "1. **Визуализация данных**: LLE позволяет визуализировать высокомерные данные в 2D или 3D, выявляя скрытые закономерности.\n",
        "2. **Извлечение признаков**: LLE может использоваться для уменьшения размерности перед применением других алгоритмов машинного обучения.\n",
        "3. **Обработка изображений и звука**: LLE эффективен для работы с данными, лежащими на сложных многообразиях, такими как изображения или речевые сигналы.\n",
        "\n",
        "Таким, образом локально линейное вложение (LLE) — это мощный метод снижения размерности, который сохраняет локальную структуру данных. Он особенно полезен для работы с нелинейными данными, лежащими на искривлённых многообразиях. Хотя LLE имеет свои ограничения, такие как вычислительная сложность и чувствительность к шуму, он остаётся важным инструментом в арсенале методов машинного обучения для визуализации и анализа данных.\n",
        "\n",
        "Давайте рассмотрим конкретный числовой пример для LLE, шаг за шагом, с использованием вашего набора данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Определение ближайших соседей\n",
        "Для каждой точки $x_i$ найдём её ближайших соседей. В данном примере будем использовать $k = 2$ ближайших соседей для каждой точки. Расстояние между точками будем вычислять с помощью евклидовой метрики.\n",
        "\n",
        "#### Вычисление расстояний:\n",
        "1. Расстояние между $x_1$ и $x_2$:\n",
        "   $$\n",
        "   d(x_1, x_2) = \\sqrt{(4-1)^2 + (5-2)^2 + (6-3)^2} = \\sqrt{9 + 9 + 9} = \\sqrt{27} \\approx 5.196.\n",
        "   $$\n",
        "2. Расстояние между $x_1$ и $x_3$:\n",
        "   $$\n",
        "   d(x_1, x_3) = \\sqrt{(7-1)^2 + (8-2)^2 + (9-3)^2} = \\sqrt{36 + 36 + 36} = \\sqrt{108} \\approx 10.392.\n",
        "   $$\n",
        "3. Расстояние между $x_1$ и $x_4$:\n",
        "   $$\n",
        "   d(x_1, x_4) = \\sqrt{(10-1)^2 + (11-2)^2 + (12-3)^2} = \\sqrt{81 + 81 + 81} = \\sqrt{243} \\approx 15.588.\n",
        "   $$\n",
        "4. Аналогично вычисляем расстояния для остальных точек.\n",
        "\n",
        "#### Ближайшие соседи:\n",
        "- Для $x_1$: $x_2$ и $x_3$ (расстояния 5.196 и 10.392).\n",
        "- Для $x_2$: $x_1$ и $x_3$ (расстояния 5.196 и 5.196).\n",
        "- Для $x_3$: $x_2$ и $x_4$ (расстояния 5.196 и 5.196).\n",
        "- Для $x_4$: $x_3$ и $x_2$ (расстояния 5.196 и 10.392).\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление весов реконструкции\n",
        "Для каждой точки $x_i$ найдём веса $w_{ij}$, которые минимизируют ошибку реконструкции:\n",
        "$$\n",
        "\\varepsilon_i = \\left\\| x_i - \\sum_{j \\in N(i)} w_{ij} x_j \\right\\|^2.\n",
        "$$\n",
        "\n",
        "#### Пример для точки $x_1$:\n",
        "Соседи $x_1$: $x_2$ и $x_3$. Запишем задачу минимизации:\n",
        "$$\n",
        "\\varepsilon_1 = \\left\\| x_1 - w_{12} x_2 - w_{13} x_3 \\right\\|^2.\n",
        "$$\n",
        "\n",
        "##### Условие нормировки:\n",
        "$$\n",
        "w_{12} + w_{13} = 1.\n",
        "$$\n",
        "\n",
        "##### Локальная ковариационная матрица:\n",
        "Для точки $x_1$ строим матрицу $C_1$:\n",
        "$$\n",
        "C_1 = \\begin{bmatrix}\n",
        "(x_1 - x_2)^T (x_1 - x_2) & (x_1 - x_2)^T (x_1 - x_3) \\\\\n",
        "(x_1 - x_3)^T (x_1 - x_2) & (x_1 - x_3)^T (x_1 - x_3)\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Вычислим элементы матрицы:\n",
        "1. $(x_1 - x_2)^T (x_1 - x_2) = (1-4)^2 + (2-5)^2 + (3-6)^2 = 9 + 9 + 9 = 27$.\n",
        "2. $(x_1 - x_2)^T (x_1 - x_3) = (1-4)(1-7) + (2-5)(2-8) + (3-6)(3-9) = (-3)(-6) + (-3)(-6) + (-3)(-6) = 18 + 18 + 18 = 54$.\n",
        "3. $(x_1 - x_3)^T (x_1 - x_2) = 54$ (симметрично).\n",
        "4. $(x_1 - x_3)^T (x_1 - x_3) = (1-7)^2 + (2-8)^2 + (3-9)^2 = 36 + 36 + 36 = 108$.\n",
        "\n",
        "Таким образом:\n",
        "$$\n",
        "C_1 = \\begin{bmatrix}\n",
        "27 & 54 \\\\\n",
        "54 & 108\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "##### Решение системы уравнений:\n",
        "Решаем систему $C_1 w_1 = \\mathbf{1}$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "27 & 54 \\\\\n",
        "54 & 108\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "w_{12} \\\\\n",
        "w_{13}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Упростим систему, разделив первое уравнение на 27, а второе на 54:\n",
        "$$\n",
        "\\begin{cases}\n",
        "w_{12} + 2 w_{13} = \\frac{1}{27}, \\\\\n",
        "w_{12} + 2 w_{13} = \\frac{1}{54}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Система имеет решение:\n",
        "$$\n",
        "w_{12} = \\frac{1}{27}, \\quad w_{13} = 0.\n",
        "$$\n",
        "\n",
        "##### Нормировка весов:\n",
        "Поскольку $w_{12} + w_{13} = 1$, то:\n",
        "$$\n",
        "w_{12} = 1, \\quad w_{13} = 0.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Формирование низкоразмерного представления\n",
        "После вычисления весов для всех точек строим низкоразмерное представление $Y$. Для этого минимизируем функцию ошибки:\n",
        "$$\n",
        "\\Phi(Y) = \\sum_{i} \\left\\| y_i - \\sum_{j \\in N(i)} w_{ij} y_j \\right\\|^2.\n",
        "$$\n",
        "\n",
        "#### Пример для точки $y_1$:\n",
        "Используем веса $w_{12} = 1$ и $w_{13} = 0$. Тогда:\n",
        "$$\n",
        "y_1 = w_{12} y_2 + w_{13} y_3 = y_2.\n",
        "$$\n",
        "\n",
        "Аналогично для остальных точек.\n",
        "\n",
        "\n",
        "\n",
        "### Итоговое низкоразмерное представление\n",
        "После выполнения всех шагов получаем низкоразмерное представление $Y$. В данном примере, из-за простоты данных, точки в низкоразмерном пространстве будут совпадать с их ближайшими соседями.\n",
        "\n",
        "\n",
        "\n",
        "Давайте реализуем пример LLE на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация LLE с нуля\n"
      ],
      "metadata": {
        "id": "0f6PPvfXKg7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.linalg import eigh\n",
        "\n",
        "class LLE:\n",
        "    def __init__(self, n_components=2, n_neighbors=2, reg=1e-3):\n",
        "        self.n_components = n_components\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.reg = reg  # Регуляризация\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        # Шаг 1: Нахождение ближайших соседей\n",
        "        distances = cdist(X, X, metric='euclidean')\n",
        "        neighbors = np.argsort(distances, axis=1)[:, 1:self.n_neighbors + 1]\n",
        "\n",
        "        # Шаг 2: Вычисление весов реконструкции\n",
        "        n_samples = X.shape[0]\n",
        "        W = np.zeros((n_samples, n_samples))\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            X_i = X[i]\n",
        "            X_neighbors = X[neighbors[i]]\n",
        "\n",
        "            # Центрируем данные\n",
        "            X_neighbors_centered = X_neighbors - X_i\n",
        "\n",
        "            # Локальная ковариационная матрица\n",
        "            C = X_neighbors_centered @ X_neighbors_centered.T\n",
        "\n",
        "            # Преобразуем матрицу C в тип float64\n",
        "            C = C.astype(np.float64)\n",
        "\n",
        "            # Добавляем регуляризацию\n",
        "            C += self.reg * np.eye(self.n_neighbors)\n",
        "\n",
        "            # Решение системы уравнений\n",
        "            w = np.linalg.solve(C, np.ones(self.n_neighbors))\n",
        "            w /= np.sum(w)  # Нормировка весов\n",
        "\n",
        "            # Записываем веса в матрицу W\n",
        "            W[i, neighbors[i]] = w\n",
        "\n",
        "        # Шаг 3: Построение низкоразмерного представления\n",
        "        M = (np.eye(n_samples) - W).T @ (np.eye(n_samples) - W)\n",
        "\n",
        "        # Нахождение собственных значений и векторов\n",
        "        eigenvalues, eigenvectors = eigh(M, subset_by_index=[1, self.n_components])\n",
        "\n",
        "        # Низкоразмерное представление\n",
        "        Y = eigenvectors[:, :self.n_components]\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "lle = LLE(n_components=2, n_neighbors=2, reg=1e-3)\n",
        "Y = lle.fit_transform(X)\n",
        "print(\"Низкоразмерное представление (с нуля):\\n\", Y)"
      ],
      "metadata": {
        "id": "u5GAeDv6R0ZB",
        "outputId": "1b6749bd-fc07-4c15-e62b-b247b4e8556c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Низкоразмерное представление (с нуля):\n",
            " [[ 0.6708166   0.5       ]\n",
            " [ 0.2236189  -0.5       ]\n",
            " [-0.22361854 -0.5       ]\n",
            " [-0.67081624  0.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Реализация с использованием scikit-learn\n",
        "\n",
        "Теперь воспользуемся готовым решением из библиотеки scikit-learn:"
      ],
      "metadata": {
        "id": "FIVL9qkUR7jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=2)\n",
        "Y = lle.fit_transform(X)\n",
        "print(\"Низкоразмерное представление (scikit-learn):\\n\", Y)"
      ],
      "metadata": {
        "id": "rh7tvL41R-nQ",
        "outputId": "a305c5c1-d7a7-4227-ebbc-61176c4c4f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Низкоразмерное представление (scikit-learn):\n",
            " [[ 0.67028429  0.5       ]\n",
            " [ 0.22520873 -0.5       ]\n",
            " [-0.22520873 -0.5       ]\n",
            " [-0.67028429  0.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wa_HpZvYSWAG"
      }
    }
  ]
}