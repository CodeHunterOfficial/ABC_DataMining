{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm3WS8g4o8nr6ocLcKIgWD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_Feature_Engineering_%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8_(Dimension_Reduction).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "_ZiknElU3C2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "\n",
        "Снижение размерности (Dimension Reduction) — это процесс уменьшения количества признаков (фичей) в наборе данных, при этом сохраняя как можно больше полезной информации. Это важный этап в feature engineering, который помогает улучшить производительность моделей машинного обучения, сократить время обучения, уменьшить риск переобучения и упростить интерпретацию данных.\n",
        "\n",
        "#### Зачем нужно снижение размерности?\n",
        "\n",
        "1. **Устранение избыточности данных**: Многие признаки могут быть коррелированы между собой, что приводит к избыточности информации. Снижение размерности помогает устранить эту проблему.\n",
        "2. **Ускорение обучения моделей**: Меньшее количество признаков означает меньше вычислений, что ускоряет процесс обучения.\n",
        "3. **Уменьшение переобучения**: Модели, обученные на данных с меньшим количеством признаков, менее склонны к переобучению, особенно если исходный набор данных был очень большим.\n",
        "4. **Визуализация данных**: Снижение размерности до 2 или 3 измерений позволяет визуализировать данные, что помогает лучше понять их структуру.\n"
      ],
      "metadata": {
        "id": "0UN4OfqS7-Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Неконтролируемые методы (Unsupervised Methods)\n",
        "\n",
        "Неконтролируемые методы снижения размерности — это подходы, которые не используют информацию о целевой переменной (если она есть). Они работают исключительно на основе структуры данных, пытаясь сохранить важные закономерности или отношения между объектами. Эти методы особенно полезны, когда у нас нет размеченных данных или когда мы хотим исследовать структуру данных без явного целевого признака.\n"
      ],
      "metadata": {
        "id": "vTyRDLl57-n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод главных компонент (PCA, Principal Components Analysis)\n",
        "\n",
        "Метод главных компонент (PCA) — это метод снижения размерности данных, который позволяет выделить наиболее важные направления (компоненты) в данных и спроецировать данные на эти направления. PCA широко используется для визуализации данных, сжатия информации и удаления шума.\n",
        "\n",
        "#### Основные шаги PCA:\n",
        "\n",
        "1. **Стандартизация данных**  \n",
        "   Если признаки имеют разные масштабы, их необходимо стандартизировать, чтобы каждый признак вносил равный вклад в анализ.  \n",
        "   Формула стандартизации:  \n",
        "$$\n",
        "   z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$  \n",
        "   где:  \n",
        "   - $x_{ij}$ — значение $i$-го объекта по $j$-му признаку,  \n",
        "   - $\\mu_j$ — среднее значение $j$-го признака,  \n",
        "   - $\\sigma_j$ — стандартное отклонение $j$-го признака.\n",
        "\n",
        "2. **Вычисление ковариационной матрицы**  \n",
        "   Ковариационная матрица показывает, как признаки коррелируют друг с другом.  \n",
        "   Формула ковариационной матрицы $\\Sigma$:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$  \n",
        "   где:  \n",
        "   - $Z$ — матрица стандартизированных данных,  \n",
        "   - $n$ — количество объектов.\n",
        "\n",
        "3. **Вычисление собственных значений и собственных векторов**  \n",
        "   Собственные значения ($\\lambda$) и собственные векторы ($v$) ковариационной матрицы находятся из уравнения:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$  \n",
        "   Собственные векторы определяют направления главных компонент, а собственные значения — их важность (дисперсию).\n",
        "\n",
        "4. **Сортировка собственных значений и векторов**  \n",
        "   Собственные значения сортируются в порядке убывания. Соответствующие собственные векторы также переупорядочиваются. Первые $k$ собственных векторов соответствуют $k$ главным компонентам.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на выбранные главные компоненты для снижения размерности.  \n",
        "   Формула проекции:  \n",
        "$$\n",
        "   Y = Z W\n",
        "$$  \n",
        "   где:  \n",
        "   - $Y$ — матрица данных в новом пространстве,  \n",
        "   - $W$ — матрица, содержащая $k$ собственных векторов.\n",
        "\n",
        "#### Пример алгоритма PCA:\n",
        "\n",
        "1. Стандартизируем данные:  \n",
        "$$\n",
        "   Z = \\begin{bmatrix}\n",
        "   z_{11} & z_{12} & \\dots & z_{1p} \\\\\n",
        "   z_{21} & z_{22} & \\dots & z_{2p} \\\\\n",
        "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   z_{n1} & z_{n2} & \\dots & z_{np}\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычисляем ковариационную матрицу:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "3. Находим собственные значения и векторы:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$\n",
        "\n",
        "4. Сортируем собственные значения и векторы:  \n",
        "$$\n",
        "   \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\n",
        "$$\n",
        "\n",
        "5. Выбираем $k$ главных компонент и проецируем данные:  \n",
        "$$\n",
        "   Y = Z W_k\n",
        "$$  \n",
        "   где $W_k$ — матрица из $k$ собственных векторов.\n",
        "\n",
        "#### Интерпретация результатов:\n",
        "\n",
        "- **Главные компоненты**: Новые оси, вдоль которых данные имеют наибольшую дисперсию.  \n",
        "- **Доля объясненной дисперсии**: Доля дисперсии, объясняемая каждой компонентой:  \n",
        "$$\n",
        "   \\text{Доля} = \\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n",
        "$$  \n",
        "- **Снижение размерности**: Выбор $k$ компонент, которые объясняют большую часть дисперсии.\n",
        "\n",
        "#### Преимущества PCA:\n",
        "- Уменьшает размерность данных.  \n",
        "- Удаляет корреляции между признаками.  \n",
        "- Упрощает визуализацию данных.\n",
        "\n",
        "#### Недостатки PCA:\n",
        "- Линейность: PCA предполагает линейные зависимости между признаками.  \n",
        "- Интерпретируемость: Главные компоненты могут быть трудны для интерпретации.  \n",
        "- Потеря информации: При сильном снижении размерности часть информации может быть потеряна.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения метода главных компонент (PCA) для снижения размерности данных.\n",
        "\n",
        "### Исходные данные\n",
        "Пусть у нас есть следующие данные с двумя признаками $X_1$ и $X_2$:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь 4 объекта и 2 признака.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "Сначала стандартизируем данные, чтобы каждый признак имел среднее значение 0 и стандартное отклонение 1.\n",
        "\n",
        "Средние значения признаков:\n",
        "$$\n",
        "\\mu_1 = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\mu_2 = \\frac{2 + 3 + 4 + 5}{4} = 3.5\n",
        "$$\n",
        "\n",
        "Стандартные отклонения признаков:\n",
        "$$\n",
        "\\sigma_1 = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4}} = 1.118, \\quad \\sigma_2 = 1.118\n",
        "$$\n",
        "\n",
        "Стандартизированные данные:\n",
        "$$\n",
        "Z = \\begin{bmatrix}\n",
        "\\frac{1-2.5}{1.118} & \\frac{2-3.5}{1.118} \\\\\n",
        "\\frac{2-2.5}{1.118} & \\frac{3-3.5}{1.118} \\\\\n",
        "\\frac{3-2.5}{1.118} & \\frac{4-3.5}{1.118} \\\\\n",
        "\\frac{4-2.5}{1.118} & \\frac{5-3.5}{1.118}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "Ковариационная матрица $\\Sigma$ вычисляется по формуле:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "Подставляем данные:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "-1.342 & -0.447 & 0.447 & 1.342 \\\\\n",
        "-1.342 & -0.447 & 0.447 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "4 & 4 \\\\\n",
        "4 & 4\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение $\\Sigma v = \\lambda v$.\n",
        "\n",
        "Характеристическое уравнение:\n",
        "$$\n",
        "\\text{det}(\\Sigma - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Подставляем:\n",
        "$$\n",
        "\\text{det}\\left(\\begin{bmatrix}\n",
        "1.333 - \\lambda & 1.333 \\\\\n",
        "1.333 & 1.333 - \\lambda\n",
        "\\end{bmatrix}\\right) = 0\n",
        "$$\n",
        "\n",
        "Раскрываем определитель:\n",
        "$$\n",
        "(1.333 - \\lambda)^2 - (1.333)^2 = 0\n",
        "$$\n",
        "\n",
        "Решаем:\n",
        "$$\n",
        "\\lambda^2 - 2.666\\lambda = 0 \\implies \\lambda(\\lambda - 2.666) = 0\n",
        "$$\n",
        "\n",
        "Собственные значения:\n",
        "$$\n",
        "\\lambda_1 = 2.666, \\quad \\lambda_2 = 0\n",
        "$$\n",
        "\n",
        "Собственные векторы:\n",
        "Для $\\lambda_1 = 2.666$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 - 2.666 & 1.333 \\\\\n",
        "1.333 & 1.333 - 2.666\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1.333 & 1.333 \\\\\n",
        "1.333 & -1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "-1.333 v_{11} + 1.333 v_{12} = 0 \\implies v_{11} = v_{12}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_1 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Для $\\lambda_2 = 0$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{21} \\\\\n",
        "v_{22}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "1.333 v_{21} + 1.333 v_{22} = 0 \\implies v_{21} = -v_{22}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_2 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "-\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "-0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Сортировка собственных значений и векторов\n",
        "Собственные значения уже отсортированы: $\\lambda_1 = 2.666$, $\\lambda_2 = 0$.  \n",
        "Соответствующие собственные векторы: $v_1 = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}$, $v_2 = \\begin{bmatrix} 0.707 \\\\ -0.707 \\end{bmatrix}$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё.\n",
        "\n",
        "Матрица проекции:\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Проекция данных:\n",
        "$$\n",
        "Y = Z W = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 \\cdot 0.707 + (-1.342) \\cdot 0.707 \\\\\n",
        "-0.447 \\cdot 0.707 + (-0.447) \\cdot 0.707 \\\\\n",
        "0.447 \\cdot 0.707 + 0.447 \\cdot 0.707 \\\\\n",
        "1.342 \\cdot 0.707 + 1.342 \\cdot 0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как PCA может быть использован для снижения размерности данных.\n",
        "\n",
        "\n",
        "Реализуем метод главных компонент (PCA) на Python для приведенного числового примера. Мы будем использовать библиотеку `numpy` для выполнения всех вычислений.\n",
        "\n",
        "### Код Python:\n"
      ],
      "metadata": {
        "id": "3EvC8uww9erl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Шаг 1: Стандартизация данных\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "Z = (X - mean) / std\n",
        "\n",
        "print(\"Стандартизированные данные:\\n\", Z)\n",
        "\n",
        "# Шаг 2: Вычисление ковариационной матрицы\n",
        "cov_matrix = np.cov(Z, rowvar=False)\n",
        "\n",
        "print(\"Ковариационная матрица:\\n\", cov_matrix)\n",
        "\n",
        "# Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Собственные значения:\\n\", eigenvalues)\n",
        "print(\"Собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 4: Сортировка собственных значений и векторов\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "print(\"Отсортированные собственные значения:\\n\", eigenvalues)\n",
        "print(\"Отсортированные собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 5: Проецирование данных на главные компоненты\n",
        "# Выберем первую главную компоненту (с наибольшим собственным значением)\n",
        "k = 1\n",
        "W = eigenvectors[:, :k]\n",
        "\n",
        "Y = Z.dot(W)\n",
        "\n",
        "print(\"Проекция данных на главные компоненты:\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H1Nb2mf9gj8",
        "outputId": "d783edf8-93ce-417c-b52d-48f6f67970a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стандартизированные данные:\n",
            " [[-1.34164079 -1.34164079]\n",
            " [-0.4472136  -0.4472136 ]\n",
            " [ 0.4472136   0.4472136 ]\n",
            " [ 1.34164079  1.34164079]]\n",
            "Ковариационная матрица:\n",
            " [[1.33333333 1.33333333]\n",
            " [1.33333333 1.33333333]]\n",
            "Собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Отсортированные собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Отсортированные собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Проекция данных на главные компоненты:\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Проверка с использованием `sklearn`:\n",
        "Для проверки воспользуемся библиотекой `sklearn`, которая предоставляет встроенную реализацию PCA.\n",
        "\n"
      ],
      "metadata": {
        "id": "LP2xWNCJ9g9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Применение PCA\n",
        "pca = PCA(n_components=1)\n",
        "Y_sklearn = pca.fit_transform(Z)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2GPg0N-9iVz",
        "outputId": "12807f79-8ac1-4e87-f46a-fbcd6a22ac02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Результаты совпадают, что подтверждает правильность нашей реализации."
      ],
      "metadata": {
        "id": "pUmCqFIm7tx1"
      }
    }
  ]
}