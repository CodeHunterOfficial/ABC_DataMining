{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKyAINWypknjOERRmeIipL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_Feature_Engineering_%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8_(Dimension_Reduction).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "_ZiknElU3C2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "\n",
        "Снижение размерности (Dimension Reduction) — это процесс уменьшения количества признаков (фичей) в наборе данных, при этом сохраняя как можно больше полезной информации. Это важный этап в feature engineering, который помогает улучшить производительность моделей машинного обучения, сократить время обучения, уменьшить риск переобучения и упростить интерпретацию данных.\n",
        "\n",
        "#### Зачем нужно снижение размерности?\n",
        "\n",
        "1. **Устранение избыточности данных**: Многие признаки могут быть коррелированы между собой, что приводит к избыточности информации. Снижение размерности помогает устранить эту проблему.\n",
        "2. **Ускорение обучения моделей**: Меньшее количество признаков означает меньше вычислений, что ускоряет процесс обучения.\n",
        "3. **Уменьшение переобучения**: Модели, обученные на данных с меньшим количеством признаков, менее склонны к переобучению, особенно если исходный набор данных был очень большим.\n",
        "4. **Визуализация данных**: Снижение размерности до 2 или 3 измерений позволяет визуализировать данные, что помогает лучше понять их структуру.\n"
      ],
      "metadata": {
        "id": "0UN4OfqS7-Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Неконтролируемые методы (Unsupervised Methods)\n",
        "\n",
        "Неконтролируемые методы снижения размерности — это подходы, которые не используют информацию о целевой переменной (если она есть). Они работают исключительно на основе структуры данных, пытаясь сохранить важные закономерности или отношения между объектами. Эти методы особенно полезны, когда у нас нет размеченных данных или когда мы хотим исследовать структуру данных без явного целевого признака.\n"
      ],
      "metadata": {
        "id": "vTyRDLl57-n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод главных компонент (PCA, Principal Components Analysis)\n",
        "\n",
        "Метод главных компонент (PCA) — это метод снижения размерности данных, который позволяет выделить наиболее важные направления (компоненты) в данных и спроецировать данные на эти направления. PCA широко используется для визуализации данных, сжатия информации и удаления шума.\n",
        "\n",
        "#### Основные шаги PCA:\n",
        "\n",
        "1. **Стандартизация данных**  \n",
        "   Если признаки имеют разные масштабы, их необходимо стандартизировать, чтобы каждый признак вносил равный вклад в анализ.  \n",
        "   Формула стандартизации:  \n",
        "$$\n",
        "   z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$  \n",
        "   где:  \n",
        "   - $x_{ij}$ — значение $i$-го объекта по $j$-му признаку,  \n",
        "   - $\\mu_j$ — среднее значение $j$-го признака,  \n",
        "   - $\\sigma_j$ — стандартное отклонение $j$-го признака.\n",
        "\n",
        "2. **Вычисление ковариационной матрицы**  \n",
        "   Ковариационная матрица показывает, как признаки коррелируют друг с другом.  \n",
        "   Формула ковариационной матрицы $\\Sigma$:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$  \n",
        "   где:  \n",
        "   - $Z$ — матрица стандартизированных данных,  \n",
        "   - $n$ — количество объектов.\n",
        "\n",
        "3. **Вычисление собственных значений и собственных векторов**  \n",
        "   Собственные значения ($\\lambda$) и собственные векторы ($v$) ковариационной матрицы находятся из уравнения:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$  \n",
        "   Собственные векторы определяют направления главных компонент, а собственные значения — их важность (дисперсию).\n",
        "\n",
        "4. **Сортировка собственных значений и векторов**  \n",
        "   Собственные значения сортируются в порядке убывания. Соответствующие собственные векторы также переупорядочиваются. Первые $k$ собственных векторов соответствуют $k$ главным компонентам.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на выбранные главные компоненты для снижения размерности.  \n",
        "   Формула проекции:  \n",
        "$$\n",
        "   Y = Z W\n",
        "$$  \n",
        "   где:  \n",
        "   - $Y$ — матрица данных в новом пространстве,  \n",
        "   - $W$ — матрица, содержащая $k$ собственных векторов.\n",
        "\n",
        "#### Пример алгоритма PCA:\n",
        "\n",
        "1. Стандартизируем данные:  \n",
        "$$\n",
        "   Z = \\begin{bmatrix}\n",
        "   z_{11} & z_{12} & \\dots & z_{1p} \\\\\n",
        "   z_{21} & z_{22} & \\dots & z_{2p} \\\\\n",
        "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   z_{n1} & z_{n2} & \\dots & z_{np}\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычисляем ковариационную матрицу:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "3. Находим собственные значения и векторы:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$\n",
        "\n",
        "4. Сортируем собственные значения и векторы:  \n",
        "$$\n",
        "   \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\n",
        "$$\n",
        "\n",
        "5. Выбираем $k$ главных компонент и проецируем данные:  \n",
        "$$\n",
        "   Y = Z W_k\n",
        "$$  \n",
        "   где $W_k$ — матрица из $k$ собственных векторов.\n",
        "\n",
        "#### Интерпретация результатов:\n",
        "\n",
        "- **Главные компоненты**: Новые оси, вдоль которых данные имеют наибольшую дисперсию.  \n",
        "- **Доля объясненной дисперсии**: Доля дисперсии, объясняемая каждой компонентой:  \n",
        "$$\n",
        "   \\text{Доля} = \\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n",
        "$$  \n",
        "- **Снижение размерности**: Выбор $k$ компонент, которые объясняют большую часть дисперсии.\n",
        "\n",
        "#### Преимущества PCA:\n",
        "- Уменьшает размерность данных.  \n",
        "- Удаляет корреляции между признаками.  \n",
        "- Упрощает визуализацию данных.\n",
        "\n",
        "#### Недостатки PCA:\n",
        "- Линейность: PCA предполагает линейные зависимости между признаками.  \n",
        "- Интерпретируемость: Главные компоненты могут быть трудны для интерпретации.  \n",
        "- Потеря информации: При сильном снижении размерности часть информации может быть потеряна.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения метода главных компонент (PCA) для снижения размерности данных.\n",
        "\n",
        "### Исходные данные\n",
        "Пусть у нас есть следующие данные с двумя признаками $X_1$ и $X_2$:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь 4 объекта и 2 признака.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "Сначала стандартизируем данные, чтобы каждый признак имел среднее значение 0 и стандартное отклонение 1.\n",
        "\n",
        "Средние значения признаков:\n",
        "$$\n",
        "\\mu_1 = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\mu_2 = \\frac{2 + 3 + 4 + 5}{4} = 3.5\n",
        "$$\n",
        "\n",
        "Стандартные отклонения признаков:\n",
        "$$\n",
        "\\sigma_1 = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4}} = 1.118, \\quad \\sigma_2 = 1.118\n",
        "$$\n",
        "\n",
        "Стандартизированные данные:\n",
        "$$\n",
        "Z = \\begin{bmatrix}\n",
        "\\frac{1-2.5}{1.118} & \\frac{2-3.5}{1.118} \\\\\n",
        "\\frac{2-2.5}{1.118} & \\frac{3-3.5}{1.118} \\\\\n",
        "\\frac{3-2.5}{1.118} & \\frac{4-3.5}{1.118} \\\\\n",
        "\\frac{4-2.5}{1.118} & \\frac{5-3.5}{1.118}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "Ковариационная матрица $\\Sigma$ вычисляется по формуле:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "Подставляем данные:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "-1.342 & -0.447 & 0.447 & 1.342 \\\\\n",
        "-1.342 & -0.447 & 0.447 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "4 & 4 \\\\\n",
        "4 & 4\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение $\\Sigma v = \\lambda v$.\n",
        "\n",
        "Характеристическое уравнение:\n",
        "$$\n",
        "\\text{det}(\\Sigma - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Подставляем:\n",
        "$$\n",
        "\\text{det}\\left(\\begin{bmatrix}\n",
        "1.333 - \\lambda & 1.333 \\\\\n",
        "1.333 & 1.333 - \\lambda\n",
        "\\end{bmatrix}\\right) = 0\n",
        "$$\n",
        "\n",
        "Раскрываем определитель:\n",
        "$$\n",
        "(1.333 - \\lambda)^2 - (1.333)^2 = 0\n",
        "$$\n",
        "\n",
        "Решаем:\n",
        "$$\n",
        "\\lambda^2 - 2.666\\lambda = 0 \\implies \\lambda(\\lambda - 2.666) = 0\n",
        "$$\n",
        "\n",
        "Собственные значения:\n",
        "$$\n",
        "\\lambda_1 = 2.666, \\quad \\lambda_2 = 0\n",
        "$$\n",
        "\n",
        "Собственные векторы:\n",
        "Для $\\lambda_1 = 2.666$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 - 2.666 & 1.333 \\\\\n",
        "1.333 & 1.333 - 2.666\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1.333 & 1.333 \\\\\n",
        "1.333 & -1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "-1.333 v_{11} + 1.333 v_{12} = 0 \\implies v_{11} = v_{12}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_1 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Для $\\lambda_2 = 0$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{21} \\\\\n",
        "v_{22}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "1.333 v_{21} + 1.333 v_{22} = 0 \\implies v_{21} = -v_{22}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_2 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "-\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "-0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Сортировка собственных значений и векторов\n",
        "Собственные значения уже отсортированы: $\\lambda_1 = 2.666$, $\\lambda_2 = 0$.  \n",
        "Соответствующие собственные векторы: $v_1 = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}$, $v_2 = \\begin{bmatrix} 0.707 \\\\ -0.707 \\end{bmatrix}$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё.\n",
        "\n",
        "Матрица проекции:\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Проекция данных:\n",
        "$$\n",
        "Y = Z W = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 \\cdot 0.707 + (-1.342) \\cdot 0.707 \\\\\n",
        "-0.447 \\cdot 0.707 + (-0.447) \\cdot 0.707 \\\\\n",
        "0.447 \\cdot 0.707 + 0.447 \\cdot 0.707 \\\\\n",
        "1.342 \\cdot 0.707 + 1.342 \\cdot 0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как PCA может быть использован для снижения размерности данных.\n",
        "\n",
        "\n",
        "Реализуем метод главных компонент (PCA) на Python для приведенного числового примера. Мы будем использовать библиотеку `numpy` для выполнения всех вычислений.\n",
        "\n",
        "### Код Python:\n"
      ],
      "metadata": {
        "id": "3EvC8uww9erl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Шаг 1: Стандартизация данных\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "Z = (X - mean) / std\n",
        "\n",
        "print(\"Стандартизированные данные:\\n\", Z)\n",
        "\n",
        "# Шаг 2: Вычисление ковариационной матрицы\n",
        "cov_matrix = np.cov(Z, rowvar=False)\n",
        "\n",
        "print(\"Ковариационная матрица:\\n\", cov_matrix)\n",
        "\n",
        "# Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Собственные значения:\\n\", eigenvalues)\n",
        "print(\"Собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 4: Сортировка собственных значений и векторов\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "print(\"Отсортированные собственные значения:\\n\", eigenvalues)\n",
        "print(\"Отсортированные собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 5: Проецирование данных на главные компоненты\n",
        "# Выберем первую главную компоненту (с наибольшим собственным значением)\n",
        "k = 1\n",
        "W = eigenvectors[:, :k]\n",
        "\n",
        "Y = Z.dot(W)\n",
        "\n",
        "print(\"Проекция данных на главные компоненты:\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H1Nb2mf9gj8",
        "outputId": "d783edf8-93ce-417c-b52d-48f6f67970a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стандартизированные данные:\n",
            " [[-1.34164079 -1.34164079]\n",
            " [-0.4472136  -0.4472136 ]\n",
            " [ 0.4472136   0.4472136 ]\n",
            " [ 1.34164079  1.34164079]]\n",
            "Ковариационная матрица:\n",
            " [[1.33333333 1.33333333]\n",
            " [1.33333333 1.33333333]]\n",
            "Собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Отсортированные собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Отсортированные собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Проекция данных на главные компоненты:\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Проверка с использованием `sklearn`:\n",
        "Для проверки воспользуемся библиотекой `sklearn`, которая предоставляет встроенную реализацию PCA.\n",
        "\n"
      ],
      "metadata": {
        "id": "LP2xWNCJ9g9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Применение PCA\n",
        "pca = PCA(n_components=1)\n",
        "Y_sklearn = pca.fit_transform(Z)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2GPg0N-9iVz",
        "outputId": "12807f79-8ac1-4e87-f46a-fbcd6a22ac02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Результаты совпадают, что подтверждает правильность нашей реализации."
      ],
      "metadata": {
        "id": "pUmCqFIm7tx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "\n",
        "Ядерный метод главных компонент (Kernel PCA) — это расширение классического PCA, которое позволяет работать с нелинейными данными. В отличие от PCA, который находит линейные комбинации признаков, Kernel PCA использует ядерные функции для отображения данных в пространство более высокой размерности, где они могут быть линейно разделимы.\n",
        "\n",
        "#### Основные шаги Kernel PCA:\n",
        "\n",
        "1. **Выбор ядерной функции**  \n",
        "   Ядерная функция $K(x_i, x_j)$ вычисляет скалярное произведение в пространстве более высокой размерности без явного вычисления этого пространства.  \n",
        "   Примеры ядерных функций:  \n",
        "   - Линейное ядро: $K(x_i, x_j) = x_i^T x_j$  \n",
        "   - Полиномиальное ядро: $K(x_i, x_j) = (x_i^T x_j + c)^d$  \n",
        "   - Гауссово (RBF) ядро: $K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)$  \n",
        "\n",
        "2. **Вычисление ядерной матрицы**  \n",
        "   Ядерная матрица $K$ вычисляется для всех пар объектов:  \n",
        "$$\n",
        "   K_{ij} = K(x_i, x_j)\n",
        "$$  \n",
        "   Матрица $K$ является симметричной и положительно полуопределенной.\n",
        "\n",
        "3. **Центрирование ядерной матрицы**  \n",
        "   Для того чтобы ядерная матрица соответствовала центрированным данным, выполняется центрирование:  \n",
        "$$\n",
        "   K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$  \n",
        "   где $1_N$ — матрица, состоящая из $\\frac{1}{N}$, и $N$ — количество объектов.\n",
        "\n",
        "4. **Вычисление собственных значений и собственных векторов**  \n",
        "   Находятся собственные значения $\\lambda$ и собственные векторы $\\alpha$ центрированной ядерной матрицы:  \n",
        "$$\n",
        "   K_{\\text{центр}} \\alpha = \\lambda \\alpha\n",
        "$$  \n",
        "   Собственные векторы нормируются так, чтобы $\\alpha^T \\alpha = \\frac{1}{\\lambda}$.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на главные компоненты с использованием собственных векторов:  \n",
        "$$\n",
        "   Y_i = \\sum_{j=1}^N \\alpha_j K(x_j, x_i)\n",
        "$$  \n",
        "   где $Y_i$ — проекция $i$-го объекта на главные компоненты.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим пошаговое решение задачи с использованием ядерного метода главных компонент (Kernel PCA) на примере данных:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Выбор ядерной функции\n",
        "Мы используем **гауссово (RBF) ядро** с параметром $\\sigma = 1$. Формула гауссова ядра:\n",
        "$$\n",
        "K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2}\n",
        "\\right)\n",
        "$$\n",
        "Здесь:\n",
        "- $x_i$ и $x_j$ — два объекта (строки матрицы $X$),\n",
        "- $\\|x_i - x_j\\|^2$ — квадрат евклидова расстояния между объектами.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление ядерной матрицы\n",
        "Ядерная матрица $K$ вычисляется для всех пар объектов. Её размерность $4 \\times 4$, так как у нас 4 объекта.\n",
        "\n",
        "Формула для вычисления элементов матрицы:\n",
        "$$\n",
        "K_{ij} = K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "Вычислим каждый элемент матрицы $K$:\n",
        "\n",
        "1. $K(x_1, x_1) = \\exp\\left(-\\frac{\\|x_1 - x_1\\|^2}{2}\\right) = \\exp(0) = 1$\n",
        "2. $K(x_1, x_2) = \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-2)^2 + (2-3)^2}{2}\\right) = \\exp(-1) \\approx 0.3679$\n",
        "3. $K(x_1, x_3) = \\exp\\left(-\\frac{\\|x_1 - x_3\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-3)^2 + (2-4)^2}{2}\\right) = \\exp(-4) \\approx 0.0183$\n",
        "4. $K(x_1, x_4) = \\exp\\left(-\\frac{\\|x_1 - x_4\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-4)^2 + (2-5)^2}{2}\\right) = \\exp(-9) \\approx 0.0001$\n",
        "\n",
        "Аналогично вычисляем остальные элементы матрицы. В итоге получаем:\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "1 & 0.3679 & 0.0183 & 0.0001 \\\\\n",
        "0.3679 & 1 & 0.3679 & 0.0183 \\\\\n",
        "0.0183 & 0.3679 & 1 & 0.3679 \\\\\n",
        "0.0001 & 0.0183 & 0.3679 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Центрирование ядерной матрицы\n",
        "Центрирование ядерной матрицы необходимо для того, чтобы данные в новом пространстве имели нулевое среднее. Формула центрирования:\n",
        "$$\n",
        "K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$\n",
        "где $1_N$ — матрица $4 \\times 4$, состоящая из $\\frac{1}{4}$.\n",
        "\n",
        "1. Вычислим $1_N K$:\n",
        "$$\n",
        "1_N K = \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix} \\cdot K\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "1_N K = \\begin{bmatrix}\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычислим $K 1_N$:\n",
        "$$\n",
        "K 1_N = K \\cdot \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "K 1_N = \\begin{bmatrix}\n",
        "0.3466 & 0.3466 & 0.3466 & 0.3466 \\\\\n",
        "0.4385 & 0.4385 & 0.4385 & 0.4385 \\\\\n",
        "0.4385 & 0.4385 & 0.4385 & 0.4385 \\\\\n",
        "0.3466 & 0.3466 & 0.3466 & 0.3466\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. Вычислим $1_N K 1_N$:\n",
        "$$\n",
        "1_N K 1_N = \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix} \\cdot K \\cdot \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "1_N K 1_N = \\begin{bmatrix}\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Подставляем в формулу центрирования:\n",
        "$$\n",
        "K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "K_{\\text{центр}} = \\begin{bmatrix}\n",
        "0.2698 & -0.1164 & -0.2698 & -0.1164 \\\\\n",
        "-0.1164 & 0.2698 & -0.1164 & -0.2698 \\\\\n",
        "-0.2698 & -0.1164 & 0.2698 & -0.1164 \\\\\n",
        "-0.1164 & -0.2698 & -0.1164 & 0.2698\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 4: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение:\n",
        "$$\n",
        "K_{\\text{центр}} \\alpha = \\lambda \\alpha\n",
        "$$\n",
        "\n",
        "Находим собственные значения и векторы:\n",
        "$$\n",
        "\\lambda_1 = 0.5397, \\quad \\lambda_2 = 0.5397, \\quad \\lambda_3 = 0, \\quad \\lambda_4 = 0\n",
        "$$\n",
        "Соответствующие собственные векторы:\n",
        "$$\n",
        "\\alpha_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}, \\quad \\alpha_2 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}, \\quad \\alpha_3 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}, \\quad \\alpha_4 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Нормируем собственные векторы:\n",
        "$$\n",
        "\\alpha_1 = \\frac{\\alpha_1}{\\sqrt{\\lambda_1}}, \\quad \\alpha_2 = \\frac{\\alpha_2}{\\sqrt{\\lambda_2}}, \\quad \\alpha_3 = \\frac{\\alpha_3}{\\sqrt{\\lambda_3}}, \\quad \\alpha_4 = \\frac{\\alpha_4}{\\sqrt{\\lambda_4}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё:\n",
        "$$\n",
        "Y_i = \\sum_{j=1}^N \\alpha_j K(x_j, x_i)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.2698 \\\\\n",
        "-0.1164 \\\\\n",
        "-0.2698 \\\\\n",
        "-0.1164\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.2698 \\\\\n",
        "-0.1164 \\\\\n",
        "-0.2698 \\\\\n",
        "-0.1164\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как Kernel PCA может быть использован для снижения размерности нелинейных данных.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uo3PHZAndQea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Реализация Kernel PCA с нуля\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "RaNR3s5edgmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class KernelPCA:\n",
        "    def __init__(self, kernel='rbf', gamma=1):\n",
        "        self.kernel = kernel\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def _rbf_kernel(self, X1, X2):\n",
        "        # Гауссово (RBF) ядро\n",
        "        return np.exp(-self.gamma * np.linalg.norm(X1[:, np.newaxis] - X2[np.newaxis, :], axis=2)**2)\n",
        "\n",
        "    def _center_kernel(self, K):\n",
        "        # Центрирование ядерной матрицы\n",
        "        N = K.shape[0]\n",
        "        one_N = np.ones((N, N)) / N\n",
        "        K_centered = K - one_N.dot(K) - K.dot(one_N) + one_N.dot(K).dot(one_N)\n",
        "        return K_centered\n",
        "\n",
        "    def fit_transform(self, X, n_components=1):\n",
        "        # Вычисление ядерной матрицы\n",
        "        K = self._rbf_kernel(X, X)\n",
        "\n",
        "        # Центрирование ядерной матрицы\n",
        "        K_centered = self._center_kernel(K)\n",
        "\n",
        "        # Вычисление собственных значений и векторов\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(K_centered)\n",
        "\n",
        "        # Сортировка собственных значений и векторов\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[sorted_indices]\n",
        "        eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Нормировка собственных векторов\n",
        "        eigenvectors = eigenvectors / np.sqrt(eigenvalues)\n",
        "\n",
        "        # Выбор первых n_components компонент\n",
        "        alpha = eigenvectors[:, :n_components]\n",
        "\n",
        "        # Проецирование данных\n",
        "        Y = K_centered.dot(alpha)\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Создаем объект KernelPCA\n",
        "kpca = KernelPCA(kernel='rbf', gamma=1)\n",
        "\n",
        "# Применяем Kernel PCA\n",
        "Y = kpca.fit_transform(X, n_components=1)\n",
        "\n",
        "print(\"Проекция данных (реализация с нуля):\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcwx4WlGdi19",
        "outputId": "cb21e789-0dc9-4289-959a-72ac4500e398"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (реализация с нуля):\n",
            " [[ 0.62625661]\n",
            " [ 0.38661812]\n",
            " [-0.38661812]\n",
            " [-0.62625661]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из sklearn"
      ],
      "metadata": {
        "id": "cl_tX18IdjCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Создаем объект KernelPCA\n",
        "kpca = KernelPCA(n_components=1, kernel='rbf', gamma=1)\n",
        "\n",
        "# Применяем Kernel PCA\n",
        "Y_sklearn = kpca.fit_transform(X)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhWIBrS6djha",
        "outputId": "027769d6-eaa6-47c5-8792-93484610f0f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[ 0.62625661]\n",
            " [ 0.38661812]\n",
            " [-0.38661812]\n",
            " [-0.62625661]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "**t-SNE (t-Distributed Stochastic Neighbor Embedding)** — это метод снижения размерности, который используется для визуализации данных в пространстве меньшей размерности (обычно 2D или 3D). Он минимизирует расхождение между распределениями вероятностей, которые описывают попарные сходства объектов в исходном и целевом пространствах.\n",
        "\n",
        "\n",
        "\n",
        "### Основные идеи t-SNE\n",
        "\n",
        "1. **Попарные сходства**:\n",
        "   - В исходном пространстве (высокой размерности) t-SNE вычисляет вероятности, которые отражают сходство между объектами.\n",
        "   - В целевом пространстве (низкой размерности) t-SNE строит аналогичные вероятности и минимизирует разницу между двумя распределениями.\n",
        "\n",
        "2. **t-распределение**:\n",
        "   - В целевом пространстве t-SNE использует t-распределение (распределение Стьюдента) для вычисления сходств. Это позволяет избежать проблемы \"скученности\" (crowding problem), когда точки в низкой размерности располагаются слишком близко друг к другу.\n",
        "\n",
        "3. **Стохастичность**:\n",
        "   - t-SNE использует случайные начальные значения для точек в целевом пространстве, поэтому результаты могут немного отличаться при разных запусках.\n",
        "\n",
        "\n",
        "\n",
        "### Алгоритм t-SNE\n",
        "\n",
        "#### Шаг 1: Вычисление попарных сходств в исходном пространстве\n",
        "Для каждой пары объектов $x_i$ и $x_j$ в исходном пространстве вычисляется вероятность $p_{ij}$, которая отражает сходство между ними. Формула для условной вероятности $p_{j|i}$:\n",
        "\n",
        "$$\n",
        "p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\|x_i - x_j\\|^2$ — квадрат евклидова расстояния между объектами $x_i$ и $x_j$.\n",
        "- $\\sigma_i$ — параметр, который выбирается для каждого объекта $x_i$ так, чтобы распределение вероятностей имело заданную \"перплексию\" (perplexity).\n",
        "\n",
        "Затем симметризуется вероятность:\n",
        "\n",
        "$$\n",
        "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
        "$$\n",
        "\n",
        "где $N$ — количество объектов.\n",
        "\n",
        "#### Шаг 2: Вычисление попарных сходств в целевом пространстве\n",
        "Для каждой пары объектов $y_i$ и $y_j$ в целевом пространстве вычисляется вероятность $q_{ij}$ с использованием t-распределения:\n",
        "\n",
        "$$\n",
        "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\|y_i - y_j\\|^2$ — квадрат евклидова расстояния между объектами $y_i$ и $y_j$ в целевом пространстве.\n",
        "\n",
        "#### Шаг 3: Минимизация расхождения между распределениями\n",
        "t-SNE минимизирует расхождение между распределениями $P$ и $Q$ с использованием функции потерь, которая измеряется как **KL-дивергенция** (Kullback-Leibler divergence):\n",
        "\n",
        "$$\n",
        "KL(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
        "$$\n",
        "\n",
        "Минимизация выполняется с помощью градиентного спуска.\n",
        "\n",
        "#### Шаг 4: Оптимизация\n",
        "Градиент функции потерь по $y_i$ вычисляется как:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial KL}{\\partial y_i} = 4 \\sum_{j \\neq i} (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\n",
        "$$\n",
        "\n",
        "На каждом шаге градиентного спуска координаты $y_i$ обновляются:\n",
        "\n",
        "$$\n",
        "y_i^{(t+1)} = y_i^{(t)} - \\eta \\frac{\\partial KL}{\\partial y_i}\n",
        "$$\n",
        "\n",
        "где $\\eta$ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "\n",
        "### Параметры t-SNE\n",
        "\n",
        "1. **Перплексия (perplexity)**:\n",
        "   - Параметр, который определяет, сколько ближайших соседей учитывается для каждого объекта. Обычно выбирается в диапазоне от 5 до 50.\n",
        "\n",
        "2. **Количество итераций**:\n",
        "   - Количество шагов градиентного спуска. Обычно используется 1000–2000 итераций.\n",
        "\n",
        "3. **Скорость обучения (learning rate)**:\n",
        "   - Параметр, который определяет размер шага градиентного спуска. Обычно выбирается в диапазоне от 10 до 1000.\n",
        "\n",
        "4. **Метрика расстояния**:\n",
        "   - Метрика, используемая для вычисления расстояний между объектами. По умолчанию используется евклидово расстояние.\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества и ограничения t-SNE\n",
        "\n",
        "**Преимущества**:\n",
        "- Эффективен для визуализации сложных нелинейных структур данных.\n",
        "- Хорошо сохраняет локальные кластеры и группы точек.\n",
        "- Позволяет выявлять скрытые закономерности в данных.\n",
        "\n",
        "**Ограничения**:\n",
        "- Результаты могут зависеть от выбора параметров, таких как перплексия и скорость обучения.\n",
        "- t-SNE не сохраняет глобальную структуру данных, что может привести к искажениям на больших масштабах.\n",
        "- Вычислительная сложность алгоритма высока, что делает его менее пригодным для очень больших наборов данных.\n",
        "\n",
        "\n",
        "\n",
        "### Применение t-SNE\n",
        "\n",
        "t-SNE широко используется в различных областях, таких как:\n",
        "- Визуализация данных в машинном обучении и анализе данных.\n",
        "- Исследование структуры данных, включая кластеризацию и классификацию.\n",
        "- Анализ изображений, текстов и других сложных типов данных.\n",
        "\n",
        "Таким образом, t-SNE является мощным инструментом для визуализации и исследования данных, особенно когда необходимо выявить локальные структуры и кластеры. Однако его использование требует понимания параметров и ограничений для получения интерпретируемых результатов.\n",
        "\n",
        "\n",
        "Рассмотрим подробное пошаговое решение задачи с использованием **t-SNE** для набора данных из 4 точек в 3D-пространстве. Мы снизим размерность до 2D. Для простоты будем использовать фиксированные значения параметров и упрощенные вычисления.\n",
        "\n",
        "\n",
        "\n",
        "### Исходные данные\n",
        "Даны 4 точки в 3D-пространстве:\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Цель: снизить размерность до 2D с использованием t-SNE.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Вычисление попарных сходств в исходном пространстве\n",
        "\n",
        "1. **Вычисляем евклидовы расстояния между точками**:\n",
        "$$\n",
        "   \\|x_i - x_j\\|^2 = \\sum_{k=1}^3 (x_{ik} - x_{jk})^2\n",
        "$$\n",
        "   Например:\n",
        "$$\n",
        "   \\|x_1 - x_2\\|^2 = (1-4)^2 + (2-5)^2 + (3-6)^2 = 9 + 9 + 9 = 27\n",
        "$$\n",
        "   Аналогично вычисляем все попарные расстояния:\n",
        "$$\n",
        "   \\begin{bmatrix}\n",
        "   0 & 27 & 108 & 243 \\\\\n",
        "   27 & 0 & 27 & 108 \\\\\n",
        "   108 & 27 & 0 & 27 \\\\\n",
        "   243 & 108 & 27 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Вычисляем условные вероятности $p_{j|i}$**:\n",
        "   Используем формулу:\n",
        "$$\n",
        "   p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
        "$$\n",
        "   Для простоты предположим, что $\\sigma_i = 1$ для всех точек.\n",
        "\n",
        "   Для $x_1$:\n",
        "$$\n",
        "   p_{2|1} = \\frac{\\exp(-27/2)}{\\exp(-27/2) + \\exp(-108/2) + \\exp(-243/2)} \\approx \\frac{e^{-13.5}}{e^{-13.5} + e^{-54} + e^{-121.5}} \\approx 1\n",
        "$$\n",
        "   (Остальные значения $p_{j|1}$ близки к 0 из-за экспоненциального убывания.)\n",
        "\n",
        "   Аналогично вычисляем для всех точек. В результате получаем:\n",
        "$$\n",
        "   P = \\begin{bmatrix}\n",
        "   0 & 1 & 0 & 0 \\\\\n",
        "   1 & 0 & 1 & 0 \\\\\n",
        "   0 & 1 & 0 & 1 \\\\\n",
        "   0 & 0 & 1 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Симметризуем вероятности**:\n",
        "$$\n",
        "   p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
        "$$\n",
        "   где $N = 4$.\n",
        "\n",
        "   Например:\n",
        "$$\n",
        "   p_{12} = \\frac{p_{2|1} + p_{1|2}}{2 \\cdot 4} = \\frac{1 + 1}{8} = 0.25\n",
        "$$\n",
        "   Аналогично вычисляем все $p_{ij}$:\n",
        "$$\n",
        "   P = \\begin{bmatrix}\n",
        "   0 & 0.25 & 0 & 0 \\\\\n",
        "   0.25 & 0 & 0.25 & 0 \\\\\n",
        "   0 & 0.25 & 0 & 0.25 \\\\\n",
        "   0 & 0 & 0.25 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Инициализация точек в целевом пространстве\n",
        "Инициализируем точки в 2D случайным образом:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.3 & 0.4 \\\\\n",
        "0.5 & 0.6 \\\\\n",
        "0.7 & 0.8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление попарных сходств в целевом пространстве\n",
        "Используем t-распределение для вычисления $q_{ij}$:\n",
        "\n",
        "$$\n",
        "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
        "$$\n",
        "\n",
        "1. **Вычисляем расстояния в целевом пространстве**:\n",
        "$$\n",
        "   \\|y_i - y_j\\|^2 = (y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2\n",
        "$$\n",
        "   Например:\n",
        "$$\n",
        "   \\|y_1 - y_2\\|^2 = (0.1 - 0.3)^2 + (0.2 - 0.4)^2 = 0.04 + 0.04 = 0.08\n",
        "$$\n",
        "   Аналогично вычисляем все попарные расстояния:\n",
        "$$\n",
        "   \\begin{bmatrix}\n",
        "   0 & 0.08 & 0.32 & 0.72 \\\\\n",
        "   0.08 & 0 & 0.08 & 0.32 \\\\\n",
        "   0.32 & 0.08 & 0 & 0.08 \\\\\n",
        "   0.72 & 0.32 & 0.08 & 0\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Вычисляем $q_{ij}$**:\n",
        "   Например:\n",
        "$$\n",
        "   q_{12} = \\frac{(1 + 0.08)^{-1}}{(1 + 0.08)^{-1} + (1 + 0.32)^{-1} + (1 + 0.72)^{-1} + \\dots} \\approx \\frac{0.9259}{0.9259 + 0.7576 + 0.5814 + \\dots}\n",
        "$$\n",
        "   (Продолжаем вычисления для всех $q_{ij}$.)\n",
        "\n",
        "\n",
        "### Шаг 4: Минимизация KL-дивергенции\n",
        "\n",
        "На этом шаге мы минимизируем расхождение между распределениями $P$ (исходное пространство) и $Q$ (целевое пространство) с использованием **KL-дивергенции** (Kullback-Leibler divergence). Формула KL-дивергенции:\n",
        "\n",
        "$$\n",
        "KL(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
        "$$\n",
        "\n",
        "Для минимизации KL-дивергенции используется **градиентный спуск**. Градиент функции потерь по координатам $y_i$ вычисляется следующим образом:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial KL}{\\partial y_i} = 4 \\sum_{j \\neq i} (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Пошаговое вычисление градиента и обновление координат\n",
        "\n",
        "1. **Вычисляем градиент для каждой точки**:\n",
        "   Для каждой точки $y_i$ вычисляем градиент по формуле выше. Например, для $y_1$:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\sum_{j \\neq 1} (p_{1j} - q_{1j})(y_1 - y_j)(1 + \\|y_1 - y_j\\|^2)^{-1}\n",
        "$$\n",
        "   Подставляем значения $p_{1j}$, $q_{1j}$ и расстояния $\\|y_1 - y_j\\|^2$, которые мы вычислили ранее.\n",
        "\n",
        "2. **Обновляем координаты**:\n",
        "   Координаты $y_i$ обновляются по формуле градиентного спуска:\n",
        "$$\n",
        "   y_i^{(t+1)} = y_i^{(t)} - \\eta \\frac{\\partial KL}{\\partial y_i}\n",
        "$$\n",
        "   где $\\eta$ — скорость обучения (learning rate). Обычно $\\eta$ выбирается в диапазоне от 10 до 1000.\n",
        "\n",
        "3. **Повторяем процесс**:\n",
        "   Шаги 1 и 2 повторяются для каждой точки и на каждой итерации градиентного спуска. Процесс продолжается до достижения заданного количества итераций или сходимости.\n",
        "\n",
        "\n",
        "\n",
        "#### Пример вычисления градиента для $y_1$\n",
        "\n",
        "1. **Вычисляем градиент**:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\left[ (p_{12} - q_{12})(y_1 - y_2)(1 + \\|y_1 - y_2\\|^2)^{-1} + (p_{13} - q_{13})(y_1 - y_3)(1 + \\|y_1 - y_3\\|^2)^{-1} + (p_{14} - q_{14})(y_1 - y_4)(1 + \\|y_1 - y_4\\|^2)^{-1} \\right]\n",
        "$$\n",
        "   Подставляем значения:\n",
        "$$\n",
        "   p_{12} = 0.25, \\quad q_{12} \\approx 0.334, \\quad \\|y_1 - y_2\\|^2 = 0.08\n",
        "$$\n",
        "$$\n",
        "   p_{13} = 0, \\quad q_{13} \\approx 0.324, \\quad \\|y_1 - y_3\\|^2 = 0.32\n",
        "$$\n",
        "$$\n",
        "   p_{14} = 0, \\quad q_{14} \\approx 0.334, \\quad \\|y_1 - y_4\\|^2 = 0.72\n",
        "$$\n",
        "   Тогда:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\left[ (0.25 - 0.334)(y_1 - y_2)(1 + 0.08)^{-1} + (0 - 0.324)(y_1 - y_3)(1 + 0.32)^{-1} + (0 - 0.334)(y_1 - y_4)(1 + 0.72)^{-1} \\right]\n",
        "$$\n",
        "   Упрощаем:\n",
        "$$\n",
        "   \\frac{\\partial KL}{\\partial y_1} = 4 \\left[ (-0.084)(y_1 - y_2)(0.9259) + (-0.324)(y_1 - y_3)(0.7576) + (-0.334)(y_1 - y_4)(0.5814) \\right]\n",
        "$$\n",
        "\n",
        "2. **Обновляем координаты**:\n",
        "   Предположим, скорость обучения $\\eta = 100$. Тогда:\n",
        "$$\n",
        "   y_1^{(t+1)} = y_1^{(t)} - 100 \\cdot \\frac{\\partial KL}{\\partial y_1}\n",
        "$$\n",
        "\n",
        "Таким образом, после выполнения всех итераций градиентного спуска координаты точек в целевом пространстве $Y$ будут обновлены так, чтобы минимизировать KL-дивергенцию между распределениями $P$ и $Q$. Это позволяет сохранить локальные сходства между точками в низкоразмерном пространстве.\n",
        "\n",
        "\n",
        "1. Реализация t-SNE с нуля\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w9O82CQvduFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class tSNE:\n",
        "    def __init__(self, n_components=2, perplexity=30, learning_rate=200, n_iter=1000):\n",
        "        self.n_components = n_components\n",
        "        self.perplexity = perplexity\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def _compute_pairwise_affinities(self, X):\n",
        "        distances = squareform(pdist(X, 'euclidean'))\n",
        "        affinities = np.zeros((X.shape[0], X.shape[0]))\n",
        "        for i in range(X.shape[0]):\n",
        "            beta = self._binary_search_beta(distances[i], self.perplexity)\n",
        "            affinities[i] = np.exp(-beta * distances[i])\n",
        "            affinities[i, i] = 0  # Исключаем диагональные элементы\n",
        "        affinities /= np.sum(affinities, axis=1, keepdims=True)\n",
        "        return (affinities + affinities.T) / (2 * X.shape[0])\n",
        "\n",
        "    def _binary_search_beta(self, distances, perplexity):\n",
        "        beta = 1.0\n",
        "        beta_min, beta_max = -np.inf, np.inf\n",
        "        tolerance = 1e-5\n",
        "        max_iter = 50\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            affinities = np.exp(-beta * distances)\n",
        "            sum_affinities = np.sum(affinities)\n",
        "            entropy = np.log(sum_affinities) + beta * np.sum(distances * affinities) / sum_affinities\n",
        "            entropy_diff = entropy - np.log(perplexity)\n",
        "\n",
        "            if np.abs(entropy_diff) < tolerance:\n",
        "                break\n",
        "\n",
        "            if entropy_diff > 0:\n",
        "                beta_min = beta\n",
        "                beta = (beta_max + beta) / 2 if beta_max != np.inf else beta * 2\n",
        "            else:\n",
        "                beta_max = beta\n",
        "                beta = (beta_min + beta) / 2 if beta_min != -np.inf else beta / 2\n",
        "\n",
        "        return beta\n",
        "\n",
        "    def _compute_low_dimensional_affinities(self, Y):\n",
        "        distances = squareform(pdist(Y, 'euclidean'))\n",
        "        affinities = 1 / (1 + distances**2)\n",
        "        np.fill_diagonal(affinities, 0)\n",
        "        affinities /= np.sum(affinities)\n",
        "        return affinities\n",
        "\n",
        "    def _compute_gradient(self, P, Q, Y):\n",
        "        n = Y.shape[0]\n",
        "        gradient = np.zeros_like(Y)\n",
        "        for i in range(n):\n",
        "            diff = Y[i] - Y\n",
        "            grad = 4 * np.sum((P[i] - Q[i]) * diff.T * (1 + np.sum(diff**2, axis=1))**(-1), axis=1)\n",
        "            gradient[i] = grad\n",
        "        return gradient\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        Y = np.random.randn(n_samples, self.n_components) * 1e-4\n",
        "        P = self._compute_pairwise_affinities(X)\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            Q = self._compute_low_dimensional_affinities(Y)\n",
        "            gradient = self._compute_gradient(P, Q, Y)\n",
        "            Y -= self.learning_rate * gradient\n",
        "\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "tsne = tSNE(n_components=2, perplexity=2, learning_rate=100, n_iter=1000)\n",
        "Y = tsne.fit_transform(X)\n",
        "\n",
        "print(\"Результат t-SNE (с нуля):\")\n",
        "print(Y)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "plt.title(\"t-SNE (с нуля)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bkqZN28MgLwp",
        "outputId": "4c14eb66-fc2f-46f8-e542-1067b8038301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат t-SNE (с нуля):\n",
            "[[ 10.65466838 -19.32408985]\n",
            " [  3.91914037  -7.10808937]\n",
            " [ -3.91924071   7.10814647]\n",
            " [-10.65474846  19.32415812]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn"
      ],
      "metadata": {
        "id": "hXppAc9igNqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=2, learning_rate=100, n_iter=1000)\n",
        "Y = tsne.fit_transform(X)\n",
        "\n",
        "print(\"Результат t-SNE (scikit-learn):\")\n",
        "print(Y)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "plt.title(\"t-SNE (scikit-learn)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0o0eUVumgRps",
        "outputId": "c14eddeb-eac3-4eca-9420-a7835c9616ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат t-SNE (scikit-learn):\n",
            "[[ 1.6463555e+03  2.9633252e-06]\n",
            " [ 7.4033746e+02 -9.2934698e-07]\n",
            " [-1.2766664e+03 -8.9721962e-06]\n",
            " [-4.8086075e+02  6.8781947e-06]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGzCAYAAAASZnxRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwIklEQVR4nO3deXgUVaL+8bcTyMLSnQAJCRJCgrLEsAYJiAtC2B4EtwvqBQXkIgIuI24wOgTGGcOiOPN4HWS8YxivioMLCCqZiQgugIBsiuwCwkDCTicsCYSc3x/+6EubHemcdPL9PE89M33qVJ1Tx276TdWpaocxxggAAMCCANsdAAAANRdBBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQRAqWbMmKHWrVursLDwiu537969cjgcmjt3rqdsxIgRqlevXpnb9ujRQz169PhV7V+JfVSGe+65R0OGDLHdDcBnCCLAFbRy5UpNmTJFJ0+eLPc2p06dUmpqqhITE1W3bl01bNhQHTp00GOPPaaDBw966k2ZMkUOh0ONGzfWmTNniuynefPmuvXWW73KHA5HictDDz1UZt9ycnI0ffp0PfPMMwoIqLr/XBw8eFBTpkzRxo0bbXflinvmmWf0wQcfaNOmTba7AvhELdsdAKqTlStXaurUqRoxYoTCwsLKrH/+/HnddNNN2rZtm4YPH65HHnlEp06d0g8//KB33nlHd9xxh5o0aeK1zeHDhzV79mw98cQT5epT7969df/99xcpb9myZZnbvvHGGyooKNC9995brrYqIjY2VmfPnlXt2rUrvO2//vUvr9cHDx7U1KlT1bx5c3Xo0OEK9bBq6Nixozp37qyXXnpJb775pu3uAFccQQSwaOHChdqwYYPefvtt/ed//qfXury8PJ07d67INh06dNDMmTM1btw4hYaGltlGy5YtNWzYsMvqX3p6ugYNGqSQkJDL2r40DofjsvcbFBR0hXtTOYwxysvLK9d/t0sNGTJEqamp+stf/lKuS1eAP6m651oBPzNlyhQ99dRTkqS4uDjPJZC9e/eWuM2PP/4oSerevXuRdSEhIXI6nUXKJ0+erEOHDmn27NlXpuMl2LNnj7777julpKQUWffuu+8qKSlJ9evXl9PpVNu2bfXnP//Zq87Jkyf1+OOPq3nz5goODlbTpk11//336+jRo5KKnyNSnI0bNyoiIkI9evTQqVOnJHnP71i+fLmuu+46SdLIkSM9417WfouTn5+v1NRUXX311QoODlZMTIyefvpp5efne9VLT09Xz549FRkZqeDgYCUkJBT73+Pi5bJ//vOf6ty5s0JDQzVnzhwtX75cDodD8+fP1x//+Ec1bdpUISEh6tWrl3bt2lVkP71799bp06eVmZlZ4WMCqjrOiABXyJ133qkdO3Zo3rx5evnll9WoUSNJUkRERInbxMbGSpLefPNNPffcc3I4HGW2c+ONN6pnz56aMWOGxo4dW+Zf13l5eZ4v/0s5nc5SzyysXLlSktSpUyev8szMTN17773q1auXpk+fLknaunWrVqxYoccee0zSz/NebrzxRm3dulUPPPCAOnXqpKNHj2rRokX697//7Rmbsqxdu1Z9+/ZV586d9dFHHxV7rG3atNHvf/97TZ48WQ8++KBuvPFGSdL1119frjYuKiws1KBBg/T111/rwQcfVJs2bfT999/r5Zdf1o4dO7Rw4UJP3dmzZ+vaa6/VoEGDVKtWLS1evFjjxo1TYWGhxo8f77Xf7du3695779WYMWM0evRotWrVyrNu2rRpCggI0JNPPim3260ZM2Zo6NChWr16tdc+EhISFBoaqhUrVuiOO+6o0HEBVZ4BcMXMnDnTSDJ79uwpV/0zZ86YVq1aGUkmNjbWjBgxwvztb38zhw4dKlI3NTXVSDJHjhwxX3zxhZFkZs2a5VkfGxtrBgwY4LWNpBKXefPmldq35557zkgyubm5XuWPPfaYcTqdpqCgoMRtJ0+ebCSZDz/8sMi6wsJCY4wxe/bsMZJMenq6Z93w4cNN3bp1jTHGfP3118bpdJoBAwaYvLw8r33cfPPN5uabb/a8Xrt2bZF9leWX+/jf//1fExAQYL766iuveq+99pqRZFasWOEpO3PmTJH99e3b18THx3uVxcbGGkkmIyPDq3zZsmVGkmnTpo3Jz8/3lP/5z382ksz3339fZP8tW7Y0/fv3L/fxAf6CSzOARaGhoVq9erXnks7cuXM1atQoRUdH65FHHilySeCim266SbfccotmzJihs2fPltrGbbfdpszMzCLLLbfcUup2x44dU61atYrMSQgLCyvzMsEHH3yg9u3bF/vXe3nO+ixbtkx9+/ZVr1699OGHHyo4OLjMbX6t9957T23atFHr1q119OhRz9KzZ09Pny669MyM2+3W0aNHdfPNN2v37t1yu91e+42Li1Pfvn2LbXPkyJFeZ6Uuns3ZvXt3kbrh4eHFntkC/J3fBJEvv/xSAwcOVJMmTeRwOLxOk/rKgQMHNGzYMDVs2FChoaFq27atvv32W5+3i+rn+PHjys7O9iyXflm5XC7NmDFDe/fu1d69e/W3v/1NrVq10n//93/r+eefL3GfU6ZMUXZ2tl577bVS227atKlSUlKKLI0bN76sYxk3bpxatmyp/v37q2nTpnrggQeUkZHhVefHH39UYmLiZe0/Ly9PAwYMUMeOHTV//vxfNTH11KlTXuN+5MiREuvu3LlTP/zwgyIiIryWi3cXHT582FN3xYoVSklJUd26dRUWFqaIiAj99re/laRig0hJmjVr5vU6PDxcknTixIkidY0x5QpxgL/xmyBy+vRptW/fXq+++mqltHfixAl1795dtWvX1pIlS7Rlyxa99NJLnn8ogIq48847FR0d7VkuzqX4pdjYWD3wwANasWKFwsLC9Pbbb5e4z5tuukk9evQo11mRy9GwYUMVFBQoNzfXqzwyMlIbN27UokWLNGjQIC1btkz9+/fX8OHDr0i7wcHBGjBggFavXl0k4FTUiy++6DXuFye1FqewsFBt27Yt9uxRZmamxo0bJ+nnkNWrVy8dPXpUs2bN0ieffKLMzEw9/vjjnv1cqrQ5PIGBgcWWG2OKlJ04caLcc2sAf+I3k1X79++v/v37l7g+Pz9fzz77rObNm6eTJ08qMTFR06dPv+wnJ06fPl0xMTFKT0/3lJX2lw0glXzZ4aWXXvL6K/eXzwb5pfDwcLVo0UKbN28utd6UKVPUo0cPzZkzp+KdLUPr1q0l/Xz3TLt27bzWBQUFaeDAgRo4cKAKCws1btw4zZkzR7/73e909dVXl6vvJXE4HHr77bd12223afDgwVqyZEmZn+OSxv3+++/XDTfc4HldWiho0aKFNm3apF69epV65mHx4sXKz8/XokWLvM5oXHrp5korKCjQ/v37NWjQIJ+1AdjiN2dEyvLwww9r1apVevfdd/Xdd99p8ODB6tevn3bu3HlZ+1u0aJE6d+6swYMHKzIyUh07dtTrr79+hXuN6qZu3bqSVOTJqklJSV6XRRISEiRJmzZtKva6/08//aQtW7Z43WFRnJtvvlk9evTQ9OnTlZeXd2UO4v/r1q2bJBW5HHns2DGv1wEBAZ6gcnFOy1133aVNmzZpwYIFRfZb3F/7vxQUFKQPP/xQ1113nQYOHKg1a9aUWr+kcY+Pj/ca9+Juk75oyJAhOnDgQLGf87Nnz+r06dOS/u8sxqXH4Xa7vf5oudK2bNmivLy8Ct8JBPgDvzkjUpp9+/YpPT1d+/bt8/yl+eSTTyojI0Pp6el64YUXKrzP3bt3a/bs2ZowYYJ++9vfau3atXr00UcVFBR0xU5Bo/pJSkqSJD377LO65557VLt2bQ0cONDzRflLmZmZSk1N1aBBg9S1a1fVq1dPu3fv1htvvKH8/HxNmTKlzDZTU1NLnXi6Y8cOvfXWW0XKGzdurN69e5e4XXx8vBITE/XZZ5/pgQce8JT/13/9l44fP66ePXuqadOm+umnn/TKK6+oQ4cOatOmjSTpqaee0vvvv6/BgwfrgQceUFJSko4fP65FixbptddeU/v27cs8rtDQUH388cfq2bOn+vfvry+++KLEeSctWrRQWFiYXnvtNdWvX19169ZVcnJyhc5i3nfffZo/f74eeughLVu2TN27d9eFCxe0bds2zZ8/3/MskD59+njOCI0ZM0anTp3S66+/rsjISGVlZZW7vYrIzMxUnTp1Sv3vBfgtuzftXB5JZsGCBZ7XH3/8sZFk6tat67XUqlXLDBkyxBhjzNatW0u9lVGSeeaZZzz7rF27tunWrZtXu4888ojp2rVrpRwj/Nfzzz9vrrrqKhMQEFDmrby7d+82kydPNl27djWRkZGmVq1aJiIiwgwYMMB8/vnnXnUvvX33l26++WYjqUK3715662pJZs2aZerVq+d1u+r7779v+vTpYyIjI01QUJBp1qyZGTNmjMnKyvLa9tixY+bhhx82V111lQkKCjJNmzY1w4cPN0ePHjXGlH377kVHjx41CQkJJioqyuzcudNzvL/s/0cffWQSEhJMrVq1ynUrb3H7OHfunJk+fbq59tprTXBwsAkPDzdJSUlm6tSpxu12e+otWrTItGvXzoSEhJjmzZub6dOnmzfeeKPIf+/ibqk25v9u333vvfe8yosbE2OMSU5ONsOGDSv1eAB/5TCmHOdJqxiHw6EFCxbo9ttvlyT94x//0NChQ/XDDz8UmfxVr149RUVF6dy5c8XeEnephg0beh4+FRsbq969e+t//ud/POtnz56tP/zhDzpw4MCVPSCginK73YqPj9eMGTM0atQo292pkTZu3KhOnTpp/fr11e53dACpmlya6dixoy5cuKDDhw977sP/paCgIM/ku/Lo3r27tm/f7lW2Y8cOz5MwgZrA5XLp6aef1syZMzVy5Mgq/Qu81dW0adP0H//xH4QQVFt+c0bk1KlTnt9g6Nixo2bNmqVbbrlFDRo0ULNmzTRs2DCtWLFCL730kjp27KgjR45o6dKlateunQYMGFDh9tauXavrr79eU6dO1ZAhQ7RmzRqNHj1af/3rXzV06NArfXgAANRIfhNEli9fXuyEvOHDh2vu3Lk6f/68/vCHP+jNN9/UgQMH1KhRI3Xt2lVTp05V27ZtL6vNjz/+WJMmTdLOnTsVFxenCRMmaPTo0b/2UAAAwP/nN0EEAABUP1zwBQAA1hBEAACANVX6rpnCwkIdPHhQ9evX58eeAADwE8YY5ebmqkmTJmXebVelg8jBgwcVExNjuxsAAOAy7N+/X02bNi21TpUOIvXr15f084E4nU7LvQEAAOWRk5OjmJgYz/d4aap0ELl4OcbpdBJEAADwM+WZVsFkVQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1VfqBZsCvdaHQaM2e4zqcm6fI+iHqEtdAgQH8bhEAVBUEEVRbGZuzNHXxFmW58zxl0a4QpQ5MUL/EaIs9AwBcxKUZVEsZm7M09q31XiFEkrLdeRr71nplbM6y1DMAwKUIIqh2LhQaTV28RaaYdRfLpi7eoguFxdUAAFQmggiqnTV7jhc5E3IpIynLnac1e45XXqcAAMUiiKDaOZxbcgi5nHoAAN8hiKDaiawfckXrAQB8hyCCaqdLXANFu0JU0k26Dv1890yXuAaV2S0AQDEIIqh2AgMcSh2YIElFwsjF16kDE3ieCABUAQQRVEv9EqM1e1gnRbm8L79EuUI0e1gnniMCAFUEDzRDtdUvMVq9E6J4sioAVGEEEVRrgQEOdWvR0HY3AAAl4NIMAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqfB5EDBw5o2LBhatiwoUJDQ9W2bVt9++23vm4WAAD4AZ8+WfXEiRPq3r27brnlFi1ZskQRERHauXOnwsPDfdksAADwEz4NItOnT1dMTIzS09M9ZXFxcb5sEgAA+BGfXppZtGiROnfurMGDBysyMlIdO3bU66+/XmL9/Px85eTkeC0AAKD68mkQ2b17t2bPnq1rrrlG//znPzV27Fg9+uij+vvf/15s/bS0NLlcLs8SExPjy+4BAADLHMYY46udBwUFqXPnzlq5cqWn7NFHH9XatWu1atWqIvXz8/OVn5/veZ2Tk6OYmBi53W45nU5fdRMAAFxBOTk5crlc5fr+9ukZkejoaCUkJHiVtWnTRvv27Su2fnBwsJxOp9cCAACqL58Gke7du2v79u1eZTt27FBsbKwvmwUAAH7Cp0Hk8ccf1zfffKMXXnhBu3bt0jvvvKO//vWvGj9+vC+bBQAAfsKnQeS6667TggULNG/ePCUmJur555/Xn/70Jw0dOtSXzQIAAD/h08mqv1ZFJrsAAICqocpMVgUAACgNQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANZUWRKZNmyaHw6Hf/OY3ldUkAACo4ioliKxdu1Zz5sxRu3btKqM5AADgJ3weRE6dOqWhQ4fq9ddfV3h4eKl18/PzlZOT47UAAIDqy+dBZPz48RowYIBSUlLKrJuWliaXy+VZYmJifN09AABgkU+DyLvvvqv169crLS2tXPUnTZokt9vtWfbv3+/L7gEAAMtq+WrH+/fv12OPPabMzEyFhISUa5vg4GAFBwf7qksAAKCKcRhjjC92vHDhQt1xxx0KDAz0lF24cEEOh0MBAQHKz8/3WlecnJwcuVwuud1uOZ1OX3QTAABcYRX5/vbZGZFevXrp+++/9yobOXKkWrdurWeeeabMEAIAAKo/nwWR+vXrKzEx0ausbt26atiwYZFyAABQM/FkVQAAYI3PzogUZ/ny5ZXZHAAAqOI4IwIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsqdQnqwIAgKrhQqHRmj3HdTg3T5H1Q9QlroECAxyV3g+CCAAANUzG5ixNXbxFWe48T1m0K0SpAxPULzG6UvvCpRkAAGqQjM1ZGvvWeq8QIknZ7jyNfWu9MjZnVWp/CCIAANQQFwqNpi7eIlPMuotlUxdv0YXC4mr4BkEEAIAaYs2e40XOhFzKSMpy52nNnuOV1ieCCAAANcTh3JJDyOXUuxIIIgAA1BCR9UOuaL0rgSACAEAN0SWugaJdISrpJl2Hfr57pktcg0rrE0EEAIAaIjDAodSBCZJUJIxcfJ06MKFSnydCEAEAoAbplxit2cM6KcrlffklyhWi2cM6VfpzRHigGQAANUy/xGj1TojiyaoAAMCOwACHurVoaLsbXJoBAAD2EEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANT4NImlpabruuutUv359RUZG6vbbb9f27dt92SQAAPAjPg0iX3zxhcaPH69vvvlGmZmZOn/+vPr06aPTp0/7slkAAOAnHMYYU1mNHTlyRJGRkfriiy900003lVk/JydHLpdLbrdbTqezEnoIAAB+rYp8f1fqr++63W5JUoMGDYpdn5+fr/z8fM/rnJycSukXAACwo9ImqxYWFuo3v/mNunfvrsTExGLrpKWlyeVyeZaYmJjK6h4AALCg0i7NjB07VkuWLNHXX3+tpk2bFlunuDMiMTExXJoBAMCPVLlLMw8//LA+/vhjffnllyWGEEkKDg5WcHBwZXQJAABUAT4NIsYYPfLII1qwYIGWL1+uuLg4XzYHAAD8jE+DyPjx4/XOO+/oo48+Uv369ZWdnS1JcrlcCg0N9WXTAADAD/h0jojD4Si2PD09XSNGjChze27fBQDA/1SZOSKV+IgSAADgh/itGQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWVEoQefXVV9W8eXOFhIQoOTlZa9asqYxmAQBAFefzIPKPf/xDEyZMUGpqqtavX6/27durb9++Onz4sK+bBgAAVZzPg8isWbM0evRojRw5UgkJCXrttddUp04dvfHGG75uGgAAVHE+DSLnzp3TunXrlJKS8n8NBgQoJSVFq1atKlI/Pz9fOTk5XgsAAKi+fBpEjh49qgsXLqhx48Ze5Y0bN1Z2dnaR+mlpaXK5XJ4lJibGl90DAACWVam7ZiZNmiS32+1Z9u/fb7tLAADAh2r5cueNGjVSYGCgDh065FV+6NAhRUVFFakfHBys4OBgX3YJAABUIT49IxIUFKSkpCQtXbrUU1ZYWKilS5eqW7duvmwaAAD4AZ+eEZGkCRMmaPjw4ercubO6dOmiP/3pTzp9+rRGjhzp66YBAEAV5/Mgcvfdd+vIkSOaPHmysrOz1aFDB2VkZBSZwAoAAGoehzHG2O5ESXJycuRyueR2u+V0Om13BwAAlENFvr+r1F0zAACgZiGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALDG509WBQBUjguFRmv2HNfh3DxF1g9Rl7gGCgxw2O4WUCqCCABUAxmbszR18RZlufM8ZdGuEKUOTFC/xGiLPQNKx6UZAPBzGZuzNPat9V4hRJKy3Xka+9Z6ZWzOstQzoGwEEQDwYxcKjaYu3qLifjTsYtnUxVt0obDK/qwYajiCCAD4sTV7jhc5E3IpIynLnac1e45XXqeACiCIAIAfO5xbcgi5nHpAZSOIAIAfi6wfckXrAZWNIAIAfqxLXANFu0JU0k26Dv1890yXuAaV2S2g3AgiAODHAgMcSh2YIElFwsjF16kDE3ieCKosgggA+Ll+idGaPayTolzel1+iXCGaPawTzxFBlcYDzQCgGuiXGK3eCVE8WRV+hyACANVEYIBD3Vo0tN0NoEK4NAMAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALDGZ0Fk7969GjVqlOLi4hQaGqoWLVooNTVV586d81WTAADAz/js13e3bdumwsJCzZkzR1dffbU2b96s0aNH6/Tp03rxxRd91SwAAPAjDmOMqazGZs6cqdmzZ2v37t3lqp+TkyOXyyW32y2n0+nj3gEAgCuhIt/fPjsjUhy3260GDRqUuD4/P1/5+fme1zk5OZXRLQAAYEmlTVbdtWuXXnnlFY0ZM6bEOmlpaXK5XJ4lJiamsroHAAAsqHAQmThxohwOR6nLtm3bvLY5cOCA+vXrp8GDB2v06NEl7nvSpElyu92eZf/+/RU/IgAA4DcqPEfkyJEjOnbsWKl14uPjFRQUJEk6ePCgevTooa5du2ru3LkKCCh/9mGOCAAA/senc0QiIiIUERFRrroHDhzQLbfcoqSkJKWnp1cohAAAgOrPZ5NVDxw4oB49eig2NlYvvviijhw54lkXFRXlq2YBAIAf8VkQyczM1K5du7Rr1y41bdrUa10l3jEMAACqMJ9dKxkxYoSMMcUuAAAAEr81AwAALCKIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMCaSgki+fn56tChgxwOhzZu3FgZTQIAAD9QKUHk6aefVpMmTSqjKQAA4Ed8HkSWLFmif/3rX3rxxRd93RQAAPAztXy580OHDmn06NFauHCh6tSpU2b9/Px85efne17n5OT4snsAAMAyn50RMcZoxIgReuihh9S5c+dybZOWliaXy+VZYmJifNU9AABQBVQ4iEycOFEOh6PUZdu2bXrllVeUm5urSZMmlXvfkyZNktvt9iz79++vaPcAAIAfcRhjTEU2OHLkiI4dO1Zqnfj4eA0ZMkSLFy+Ww+HwlF+4cEGBgYEaOnSo/v73v5fZVk5Ojlwul9xut5xOZ0W6CQAALKnI93eFg0h57du3z2uOx8GDB9W3b1+9//77Sk5OVtOmTcvcB0EEAAD/U5Hvb59NVm3WrJnX63r16kmSWrRoUa4QAgAAqj+erAoAAKzx6e27l2revLl8dBUIAAD4Kc6IAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKwhiAAAAGsIIgAAwBqCCAAAsIYgAgAArCGIAAAAawgiAADAGoIIAACwhiACAACsIYgAAABrCCIAAMAagggAALCGIAIAAKzxaRD55JNPlJycrNDQUIWHh+v222/3ZXMAAMDP1PLVjj/44AONHj1aL7zwgnr27KmCggJt3rzZV80BAAA/5JMgUlBQoMcee0wzZ87UqFGjPOUJCQmlbpefn6/8/HzP65ycHF90DwAAVBE+uTSzfv16HThwQAEBAerYsaOio6PVv3//Ms+IpKWlyeVyeZaYmBhfdA8AAFQRPgkiu3fvliRNmTJFzz33nD7++GOFh4erR48eOn78eInbTZo0SW6327Ps37/fF90DAABVRIWCyMSJE+VwOEpdtm3bpsLCQknSs88+q7vuuktJSUlKT0+Xw+HQe++9V+L+g4OD5XQ6vRYAAFB9VWiOyBNPPKERI0aUWic+Pl5ZWVmSvOeEBAcHKz4+Xvv27at4LwEAQLVUoSASERGhiIiIMuslJSUpODhY27dv1w033CBJOn/+vPbu3avY2NjL6ykAAKh2fHLXjNPp1EMPPaTU1FTFxMQoNjZWM2fOlCQNHjzYF00CAAA/5LPniMycOVO1atXSfffdp7Nnzyo5OVmff/65wsPDfdUkAADwMw5jjLHdiZLk5OTI5XLJ7XYzcRUAAD9Rke9vfmsGAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1BBEAAGANQQQAAFhDEAEAANYQRAAAgDU+CyI7duzQbbfdpkaNGsnpdOqGG27QsmXLfNUcAADwQz4LIrfeeqsKCgr0+eefa926dWrfvr1uvfVWZWdn+6pJAADgZ3wSRI4ePaqdO3dq4sSJateuna655hpNmzZNZ86c0ebNm33RJAAA8EM+CSINGzZUq1at9Oabb+r06dMqKCjQnDlzFBkZqaSkpBK3y8/PV05OjtcCAACqr1q+2KnD4dBnn32m22+/XfXr11dAQIAiIyOVkZGh8PDwErdLS0vT1KlTfdElAABQBVXojMjEiRPlcDhKXbZt2yZjjMaPH6/IyEh99dVXWrNmjW6//XYNHDhQWVlZJe5/0qRJcrvdnmX//v2/+gABAEDV5TDGmPJWPnLkiI4dO1Zqnfj4eH311Vfq06ePTpw4IafT6Vl3zTXXaNSoUZo4cWK52svJyZHL5ZLb7fbaDwAAqLoq8v1doUszERERioiIKLPemTNnJEkBAd4nXAICAlRYWFiRJgEAQDXmk8mq3bp1U3h4uIYPH65NmzZpx44deuqpp7Rnzx4NGDDAF00CAAA/5JMg0qhRI2VkZOjUqVPq2bOnOnfurK+//lofffSR2rdv74smAQCAH6rQHJHKxhwRAAD8T0W+v/mtGQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1FfqtmeriQqHRmj3HdTg3T5H1Q9QlroECAxy2uwUAQI1T44JIxuYsTV28RVnuPE9ZtCtEqQMT1C8x2mLPAACoeWrUpZmMzVka+9Z6rxAiSdnuPI19a70yNmdZ6hkAADVTjQkiFwqNpi7eouJ+WOdi2dTFW3ShsMr+9A4AANVOjQkia/YcL3Im5FJGUpY7T2v2HK+8TgEAUMPVmCByOLfkEHI59QAAwK9XY4JIZP2QK1oPAAD8ejUmiHSJa6BoV4hKuknXoZ/vnukS16AyuwUAQI1WY4JIYIBDqQMTJKlIGLn4OnVgAs8TAQCgEtWYICJJ/RKjNXtYJ0W5vC+/RLlCNHtYJ54jAgBAJatxDzTrlxit3glRPFkVAIAqoMYFEennyzTdWjS03Q0AAGq8GnVpBgAAVC0EEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADWEEQAAIA1VfrJqsYYSVJOTo7lngAAgPK6+L198Xu8NFU6iOTm5kqSYmJiLPcEAABUVG5urlwuV6l1HKY8ccWSwsJCHTx4UPXr15fDUTN/lC4nJ0cxMTHav3+/nE6n7e74Dcat4hizimPMKo4xqzh/HDNjjHJzc9WkSRMFBJQ+C6RKnxEJCAhQ06ZNbXejSnA6nX7zBqxKGLeKY8wqjjGrOMas4vxtzMo6E3IRk1UBAIA1BBEAAGANQaSKCw4OVmpqqoKDg213xa8wbhXHmFUcY1ZxjFnFVfcxq9KTVQEAQPXGGREAAGANQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BxKI//vGPuv7661WnTh2FhYUVW2ffvn0aMGCA6tSpo8jISD311FMqKCjwqrN8+XJ16tRJwcHBuvrqqzV37twi+3n11VfVvHlzhYSEKDk5WWvWrPHBEVW+5s2by+FweC3Tpk3zqvPdd9/pxhtvVEhIiGJiYjRjxowi+3nvvffUunVrhYSEqG3btvr0008r6xCqhOr6/rgcU6ZMKfKeat26tWd9Xl6exo8fr4YNG6pevXq66667dOjQIa99lOdz68++/PJLDRw4UE2aNJHD4dDChQu91htjNHnyZEVHRys0NFQpKSnauXOnV53jx49r6NChcjqdCgsL06hRo3Tq1CmvOuX57PqLssZsxIgRRd53/fr186pTbcfMwJrJkyebWbNmmQkTJhiXy1VkfUFBgUlMTDQpKSlmw4YN5tNPPzWNGjUykyZN8tTZvXu3qVOnjpkwYYLZsmWLeeWVV0xgYKDJyMjw1Hn33XdNUFCQeeONN8wPP/xgRo8ebcLCwsyhQ4cq4zB9KjY21vz+9783WVlZnuXUqVOe9W632zRu3NgMHTrUbN682cybN8+EhoaaOXPmeOqsWLHCBAYGmhkzZpgtW7aY5557ztSuXdt8//33Ng6p0lXn98flSE1NNddee63Xe+rIkSOe9Q899JCJiYkxS5cuNd9++63p2rWruf766z3ry/O59XeffvqpefbZZ82HH35oJJkFCxZ4rZ82bZpxuVxm4cKFZtOmTWbQoEEmLi7OnD171lOnX79+pn379uabb74xX331lbn66qvNvffe61lfns+uPylrzIYPH2769evn9b47fvy4V53qOmYEkSogPT292CDy6aefmoCAAJOdne0pmz17tnE6nSY/P98YY8zTTz9trr32Wq/t7r77btO3b1/P6y5dupjx48d7Xl+4cME0adLEpKWlXeEjqXyxsbHm5ZdfLnH9X/7yFxMeHu4ZL2OMeeaZZ0yrVq08r4cMGWIGDBjgtV1ycrIZM2bMFe9vVVSd3x+XIzU11bRv377YdSdPnjS1a9c27733nqds69atRpJZtWqVMaZ8n9vq5JdfqoWFhSYqKsrMnDnTU3by5EkTHBxs5s2bZ4wxZsuWLUaSWbt2rafOkiVLjMPhMAcOHDDGlO+z669KCiK33XZbidtU5zHj0kwVtmrVKrVt21aNGzf2lPXt21c5OTn64YcfPHVSUlK8tuvbt69WrVolSTp37pzWrVvnVScgIEApKSmeOv5u2rRpatiwoTp27KiZM2d6nQJftWqVbrrpJgUFBXnK+vbtq+3bt+vEiROeOqWNYXVWE94fl2Pnzp1q0qSJ4uPjNXToUO3bt0+StG7dOp0/f95rvFq3bq1mzZp5xqs8n9vqbM+ePcrOzvYaI5fLpeTkZK8xCgsLU+fOnT11UlJSFBAQoNWrV3vqlPXZrW6WL1+uyMhItWrVSmPHjtWxY8c866rzmFXpX9+t6bKzs73+MZPkeZ2dnV1qnZycHJ09e1YnTpzQhQsXiq2zbds2H/a+cjz66KPq1KmTGjRooJUrV2rSpEnKysrSrFmzJP08PnFxcV7bXDqG4eHhJY7hxTGuzo4ePVqt3x+XIzk5WXPnzlWrVq2UlZWlqVOn6sYbb9TmzZuVnZ2toKCgInO6Ln2/lOdzW51dPMbSPlPZ2dmKjIz0Wl+rVi01aNDAq05Zn93qpF+/frrzzjsVFxenH3/8Ub/97W/Vv39/rVq1SoGBgdV6zAgiV9jEiRM1ffr0Uuts3brVa/IbvFVkDCdMmOApa9eunYKCgjRmzBilpaVV299lgG/179/f8//btWun5ORkxcbGav78+QoNDbXYM1Rn99xzj+f/t23bVu3atVOLFi20fPly9erVy2LPfI8gcoU98cQTGjFiRKl14uPjy7WvqKioIncvXJydHxUV5fnfX87YP3TokJxOp0JDQxUYGKjAwMBi61zcR1Xza8YwOTlZBQUF2rt3r1q1alXi+Ehlj2FVHZ8rqVGjRn73/qhsYWFhatmypXbt2qXevXvr3LlzOnnypNdZkUvHqzyf2+rs4jEeOnRI0dHRnvJDhw6pQ4cOnjqHDx/22q6goEDHjx8v83N5aRvVWXx8vBo1aqRdu3apV69e1XrMmCNyhUVERKh169alLpdevytNt27d9P3333u9+TIzM+V0OpWQkOCps3TpUq/tMjMz1a1bN0lSUFCQkpKSvOoUFhZq6dKlnjpVza8Zw40bNyogIMBzCrNbt2768ssvdf78eU+dzMxMtWrVynOasqwxrM788f1R2U6dOqUff/xR0dHRSkpKUu3atb3Ga/v27dq3b59nvMrzua3O4uLiFBUV5TVGOTk5Wr16tdcYnTx5UuvWrfPU+fzzz1VYWKjk5GRPnbI+u9XZv//9bx07dswT5qr1mNmeLVuT/fTTT2bDhg1m6tSppl69embDhg1mw4YNJjc31xjzf7cB9unTx2zcuNFkZGSYiIiIYm/ffeqpp8zWrVvNq6++Wuztu8HBwWbu3Llmy5Yt5sEHHzRhYWFes/r90cqVK83LL79sNm7caH788Ufz1ltvmYiICHP//fd76pw8edI0btzY3HfffWbz5s3m3XffNXXq1Cly+26tWrXMiy++aLZu3WpSU1Nr3O271fH9cbmeeOIJs3z5crNnzx6zYsUKk5KSYho1amQOHz5sjPn59t1mzZqZzz//3Hz77bemW7duplu3bp7ty/O59Xe5ubmef68kmVmzZpkNGzaYn376yRjz8+27YWFh5qOPPjLfffedue2224q9fbdjx45m9erV5uuvvzbXXHON162o5fns+pPSxiw3N9c8+eSTZtWqVWbPnj3ms88+M506dTLXXHONycvL8+yjuo4ZQcSi4cOHG0lFlmXLlnnq7N271/Tv39+EhoaaRo0amSeeeMKcP3/eaz/Lli0zHTp0MEFBQSY+Pt6kp6cXaeuVV14xzZo1M0FBQaZLly7mm2++8fHR+d66detMcnKycblcJiQkxLRp08a88MILXh9cY4zZtGmTueGGG0xwcLC56qqrzLRp04rsa/78+aZly5YmKCjIXHvtteaTTz6prMOoEqrj++Ny3X333SY6OtoEBQWZq666ytx9991m165dnvVnz54148aNM+Hh4aZOnTrmjjvuMFlZWV77KM/n1p8tW7as2H+7hg8fboz5+Rbe3/3ud6Zx48YmODjY9OrVy2zfvt1rH8eOHTP33nuvqVevnnE6nWbkyJGeP8IuKs9n11+UNmZnzpwxffr0MREREaZ27domNjbWjB49usgfA9V1zBzGGFPpp2EAAADEHBEAAGARQQQAAFhDEAEAANYQRAAAgDUEEQAAYA1BBAAAWEMQAQAA1hBEAACANQQRAABgDUEEAABYQxABAADW/D+euEVKGqBi8gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}