{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Pcaa9BRRpHwKzeyL6SaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_Feature_Engineering_%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8_(Dimension_Reduction).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "_ZiknElU3C2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "\n",
        "Снижение размерности (Dimension Reduction) — это процесс уменьшения количества признаков (фичей) в наборе данных, при этом сохраняя как можно больше полезной информации. Это важный этап в feature engineering, который помогает улучшить производительность моделей машинного обучения, сократить время обучения, уменьшить риск переобучения и упростить интерпретацию данных.\n",
        "\n",
        "#### Зачем нужно снижение размерности?\n",
        "\n",
        "1. **Устранение избыточности данных**: Многие признаки могут быть коррелированы между собой, что приводит к избыточности информации. Снижение размерности помогает устранить эту проблему.\n",
        "2. **Ускорение обучения моделей**: Меньшее количество признаков означает меньше вычислений, что ускоряет процесс обучения.\n",
        "3. **Уменьшение переобучения**: Модели, обученные на данных с меньшим количеством признаков, менее склонны к переобучению, особенно если исходный набор данных был очень большим.\n",
        "4. **Визуализация данных**: Снижение размерности до 2 или 3 измерений позволяет визуализировать данные, что помогает лучше понять их структуру.\n"
      ],
      "metadata": {
        "id": "0UN4OfqS7-Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Неконтролируемые методы (Unsupervised Methods)\n",
        "\n",
        "Неконтролируемые методы снижения размерности — это подходы, которые не используют информацию о целевой переменной (если она есть). Они работают исключительно на основе структуры данных, пытаясь сохранить важные закономерности или отношения между объектами. Эти методы особенно полезны, когда у нас нет размеченных данных или когда мы хотим исследовать структуру данных без явного целевого признака.\n"
      ],
      "metadata": {
        "id": "vTyRDLl57-n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод главных компонент (PCA, Principal Components Analysis)\n",
        "\n",
        "Метод главных компонент (PCA) — это метод снижения размерности данных, который позволяет выделить наиболее важные направления (компоненты) в данных и спроецировать данные на эти направления. PCA широко используется для визуализации данных, сжатия информации и удаления шума.\n",
        "\n",
        "#### Основные шаги PCA:\n",
        "\n",
        "1. **Стандартизация данных**  \n",
        "   Если признаки имеют разные масштабы, их необходимо стандартизировать, чтобы каждый признак вносил равный вклад в анализ.  \n",
        "   Формула стандартизации:  \n",
        "$$\n",
        "   z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$  \n",
        "   где:  \n",
        "   - $x_{ij}$ — значение $i$-го объекта по $j$-му признаку,  \n",
        "   - $\\mu_j$ — среднее значение $j$-го признака,  \n",
        "   - $\\sigma_j$ — стандартное отклонение $j$-го признака.\n",
        "\n",
        "2. **Вычисление ковариационной матрицы**  \n",
        "   Ковариационная матрица показывает, как признаки коррелируют друг с другом.  \n",
        "   Формула ковариационной матрицы $\\Sigma$:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$  \n",
        "   где:  \n",
        "   - $Z$ — матрица стандартизированных данных,  \n",
        "   - $n$ — количество объектов.\n",
        "\n",
        "3. **Вычисление собственных значений и собственных векторов**  \n",
        "   Собственные значения ($\\lambda$) и собственные векторы ($v$) ковариационной матрицы находятся из уравнения:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$  \n",
        "   Собственные векторы определяют направления главных компонент, а собственные значения — их важность (дисперсию).\n",
        "\n",
        "4. **Сортировка собственных значений и векторов**  \n",
        "   Собственные значения сортируются в порядке убывания. Соответствующие собственные векторы также переупорядочиваются. Первые $k$ собственных векторов соответствуют $k$ главным компонентам.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на выбранные главные компоненты для снижения размерности.  \n",
        "   Формула проекции:  \n",
        "$$\n",
        "   Y = Z W\n",
        "$$  \n",
        "   где:  \n",
        "   - $Y$ — матрица данных в новом пространстве,  \n",
        "   - $W$ — матрица, содержащая $k$ собственных векторов.\n",
        "\n",
        "#### Пример алгоритма PCA:\n",
        "\n",
        "1. Стандартизируем данные:  \n",
        "$$\n",
        "   Z = \\begin{bmatrix}\n",
        "   z_{11} & z_{12} & \\dots & z_{1p} \\\\\n",
        "   z_{21} & z_{22} & \\dots & z_{2p} \\\\\n",
        "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   z_{n1} & z_{n2} & \\dots & z_{np}\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычисляем ковариационную матрицу:  \n",
        "$$\n",
        "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "3. Находим собственные значения и векторы:  \n",
        "$$\n",
        "   \\Sigma v = \\lambda v\n",
        "$$\n",
        "\n",
        "4. Сортируем собственные значения и векторы:  \n",
        "$$\n",
        "   \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\n",
        "$$\n",
        "\n",
        "5. Выбираем $k$ главных компонент и проецируем данные:  \n",
        "$$\n",
        "   Y = Z W_k\n",
        "$$  \n",
        "   где $W_k$ — матрица из $k$ собственных векторов.\n",
        "\n",
        "#### Интерпретация результатов:\n",
        "\n",
        "- **Главные компоненты**: Новые оси, вдоль которых данные имеют наибольшую дисперсию.  \n",
        "- **Доля объясненной дисперсии**: Доля дисперсии, объясняемая каждой компонентой:  \n",
        "$$\n",
        "   \\text{Доля} = \\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n",
        "$$  \n",
        "- **Снижение размерности**: Выбор $k$ компонент, которые объясняют большую часть дисперсии.\n",
        "\n",
        "#### Преимущества PCA:\n",
        "- Уменьшает размерность данных.  \n",
        "- Удаляет корреляции между признаками.  \n",
        "- Упрощает визуализацию данных.\n",
        "\n",
        "#### Недостатки PCA:\n",
        "- Линейность: PCA предполагает линейные зависимости между признаками.  \n",
        "- Интерпретируемость: Главные компоненты могут быть трудны для интерпретации.  \n",
        "- Потеря информации: При сильном снижении размерности часть информации может быть потеряна.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения метода главных компонент (PCA) для снижения размерности данных.\n",
        "\n",
        "### Исходные данные\n",
        "Пусть у нас есть следующие данные с двумя признаками $X_1$ и $X_2$:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь 4 объекта и 2 признака.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "Сначала стандартизируем данные, чтобы каждый признак имел среднее значение 0 и стандартное отклонение 1.\n",
        "\n",
        "Средние значения признаков:\n",
        "$$\n",
        "\\mu_1 = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\mu_2 = \\frac{2 + 3 + 4 + 5}{4} = 3.5\n",
        "$$\n",
        "\n",
        "Стандартные отклонения признаков:\n",
        "$$\n",
        "\\sigma_1 = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4}} = 1.118, \\quad \\sigma_2 = 1.118\n",
        "$$\n",
        "\n",
        "Стандартизированные данные:\n",
        "$$\n",
        "Z = \\begin{bmatrix}\n",
        "\\frac{1-2.5}{1.118} & \\frac{2-3.5}{1.118} \\\\\n",
        "\\frac{2-2.5}{1.118} & \\frac{3-3.5}{1.118} \\\\\n",
        "\\frac{3-2.5}{1.118} & \\frac{4-3.5}{1.118} \\\\\n",
        "\\frac{4-2.5}{1.118} & \\frac{5-3.5}{1.118}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "Ковариационная матрица $\\Sigma$ вычисляется по формуле:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "Подставляем данные:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "-1.342 & -0.447 & 0.447 & 1.342 \\\\\n",
        "-1.342 & -0.447 & 0.447 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляем:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "4 & 4 \\\\\n",
        "4 & 4\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение $\\Sigma v = \\lambda v$.\n",
        "\n",
        "Характеристическое уравнение:\n",
        "$$\n",
        "\\text{det}(\\Sigma - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Подставляем:\n",
        "$$\n",
        "\\text{det}\\left(\\begin{bmatrix}\n",
        "1.333 - \\lambda & 1.333 \\\\\n",
        "1.333 & 1.333 - \\lambda\n",
        "\\end{bmatrix}\\right) = 0\n",
        "$$\n",
        "\n",
        "Раскрываем определитель:\n",
        "$$\n",
        "(1.333 - \\lambda)^2 - (1.333)^2 = 0\n",
        "$$\n",
        "\n",
        "Решаем:\n",
        "$$\n",
        "\\lambda^2 - 2.666\\lambda = 0 \\implies \\lambda(\\lambda - 2.666) = 0\n",
        "$$\n",
        "\n",
        "Собственные значения:\n",
        "$$\n",
        "\\lambda_1 = 2.666, \\quad \\lambda_2 = 0\n",
        "$$\n",
        "\n",
        "Собственные векторы:\n",
        "Для $\\lambda_1 = 2.666$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 - 2.666 & 1.333 \\\\\n",
        "1.333 & 1.333 - 2.666\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1.333 & 1.333 \\\\\n",
        "1.333 & -1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{11} \\\\\n",
        "v_{12}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "-1.333 v_{11} + 1.333 v_{12} = 0 \\implies v_{11} = v_{12}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_1 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Для $\\lambda_2 = 0$:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.333 & 1.333 \\\\\n",
        "1.333 & 1.333\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "v_{21} \\\\\n",
        "v_{22}\n",
        "\\end{bmatrix} = 0\n",
        "$$\n",
        "Решаем систему:\n",
        "$$\n",
        "1.333 v_{21} + 1.333 v_{22} = 0 \\implies v_{21} = -v_{22}\n",
        "$$\n",
        "Нормируем:\n",
        "$$\n",
        "v_2 = \\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}} \\\\\n",
        "-\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "-0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Сортировка собственных значений и векторов\n",
        "Собственные значения уже отсортированы: $\\lambda_1 = 2.666$, $\\lambda_2 = 0$.  \n",
        "Соответствующие собственные векторы: $v_1 = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}$, $v_2 = \\begin{bmatrix} 0.707 \\\\ -0.707 \\end{bmatrix}$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё.\n",
        "\n",
        "Матрица проекции:\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Проекция данных:\n",
        "$$\n",
        "Y = Z W = \\begin{bmatrix}\n",
        "-1.342 & -1.342 \\\\\n",
        "-0.447 & -0.447 \\\\\n",
        "0.447 & 0.447 \\\\\n",
        "1.342 & 1.342\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "0.707 \\\\\n",
        "0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.342 \\cdot 0.707 + (-1.342) \\cdot 0.707 \\\\\n",
        "-0.447 \\cdot 0.707 + (-0.447) \\cdot 0.707 \\\\\n",
        "0.447 \\cdot 0.707 + 0.447 \\cdot 0.707 \\\\\n",
        "1.342 \\cdot 0.707 + 1.342 \\cdot 0.707\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "-1.897 \\\\\n",
        "-0.632 \\\\\n",
        "0.632 \\\\\n",
        "1.897\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как PCA может быть использован для снижения размерности данных.\n",
        "\n",
        "\n",
        "Реализуем метод главных компонент (PCA) на Python для приведенного числового примера. Мы будем использовать библиотеку `numpy` для выполнения всех вычислений.\n",
        "\n",
        "### Код Python:\n"
      ],
      "metadata": {
        "id": "3EvC8uww9erl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Исходные данные\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Шаг 1: Стандартизация данных\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "Z = (X - mean) / std\n",
        "\n",
        "print(\"Стандартизированные данные:\\n\", Z)\n",
        "\n",
        "# Шаг 2: Вычисление ковариационной матрицы\n",
        "cov_matrix = np.cov(Z, rowvar=False)\n",
        "\n",
        "print(\"Ковариационная матрица:\\n\", cov_matrix)\n",
        "\n",
        "# Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"Собственные значения:\\n\", eigenvalues)\n",
        "print(\"Собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 4: Сортировка собственных значений и векторов\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "print(\"Отсортированные собственные значения:\\n\", eigenvalues)\n",
        "print(\"Отсортированные собственные векторы:\\n\", eigenvectors)\n",
        "\n",
        "# Шаг 5: Проецирование данных на главные компоненты\n",
        "# Выберем первую главную компоненту (с наибольшим собственным значением)\n",
        "k = 1\n",
        "W = eigenvectors[:, :k]\n",
        "\n",
        "Y = Z.dot(W)\n",
        "\n",
        "print(\"Проекция данных на главные компоненты:\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H1Nb2mf9gj8",
        "outputId": "d783edf8-93ce-417c-b52d-48f6f67970a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стандартизированные данные:\n",
            " [[-1.34164079 -1.34164079]\n",
            " [-0.4472136  -0.4472136 ]\n",
            " [ 0.4472136   0.4472136 ]\n",
            " [ 1.34164079  1.34164079]]\n",
            "Ковариационная матрица:\n",
            " [[1.33333333 1.33333333]\n",
            " [1.33333333 1.33333333]]\n",
            "Собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Отсортированные собственные значения:\n",
            " [2.66666667 0.        ]\n",
            "Отсортированные собственные векторы:\n",
            " [[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "Проекция данных на главные компоненты:\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Проверка с использованием `sklearn`:\n",
        "Для проверки воспользуемся библиотекой `sklearn`, которая предоставляет встроенную реализацию PCA.\n",
        "\n"
      ],
      "metadata": {
        "id": "LP2xWNCJ9g9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Применение PCA\n",
        "pca = PCA(n_components=1)\n",
        "Y_sklearn = pca.fit_transform(Z)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2GPg0N-9iVz",
        "outputId": "12807f79-8ac1-4e87-f46a-fbcd6a22ac02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[-1.8973666 ]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.8973666 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Результаты совпадают, что подтверждает правильность нашей реализации."
      ],
      "metadata": {
        "id": "pUmCqFIm7tx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "\n",
        "Ядерный метод главных компонент (Kernel PCA) — это расширение классического PCA, которое позволяет работать с нелинейными данными. В отличие от PCA, который находит линейные комбинации признаков, Kernel PCA использует ядерные функции для отображения данных в пространство более высокой размерности, где они могут быть линейно разделимы.\n",
        "\n",
        "#### Основные шаги Kernel PCA:\n",
        "\n",
        "1. **Выбор ядерной функции**  \n",
        "   Ядерная функция $K(x_i, x_j)$ вычисляет скалярное произведение в пространстве более высокой размерности без явного вычисления этого пространства.  \n",
        "   Примеры ядерных функций:  \n",
        "   - Линейное ядро: $K(x_i, x_j) = x_i^T x_j$  \n",
        "   - Полиномиальное ядро: $K(x_i, x_j) = (x_i^T x_j + c)^d$  \n",
        "   - Гауссово (RBF) ядро: $K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)$  \n",
        "\n",
        "2. **Вычисление ядерной матрицы**  \n",
        "   Ядерная матрица $K$ вычисляется для всех пар объектов:  \n",
        "$$\n",
        "   K_{ij} = K(x_i, x_j)\n",
        "$$  \n",
        "   Матрица $K$ является симметричной и положительно полуопределенной.\n",
        "\n",
        "3. **Центрирование ядерной матрицы**  \n",
        "   Для того чтобы ядерная матрица соответствовала центрированным данным, выполняется центрирование:  \n",
        "$$\n",
        "   K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$  \n",
        "   где $1_N$ — матрица, состоящая из $\\frac{1}{N}$, и $N$ — количество объектов.\n",
        "\n",
        "4. **Вычисление собственных значений и собственных векторов**  \n",
        "   Находятся собственные значения $\\lambda$ и собственные векторы $\\alpha$ центрированной ядерной матрицы:  \n",
        "$$\n",
        "   K_{\\text{центр}} \\alpha = \\lambda \\alpha\n",
        "$$  \n",
        "   Собственные векторы нормируются так, чтобы $\\alpha^T \\alpha = \\frac{1}{\\lambda}$.\n",
        "\n",
        "5. **Проецирование данных на главные компоненты**  \n",
        "   Данные проецируются на главные компоненты с использованием собственных векторов:  \n",
        "$$\n",
        "   Y_i = \\sum_{j=1}^N \\alpha_j K(x_j, x_i)\n",
        "$$  \n",
        "   где $Y_i$ — проекция $i$-го объекта на главные компоненты.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим пошаговое решение задачи с использованием ядерного метода главных компонент (Kernel PCA) на примере данных:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 3 \\\\\n",
        "3 & 4 \\\\\n",
        "4 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Выбор ядерной функции\n",
        "Мы используем **гауссово (RBF) ядро** с параметром $\\sigma = 1$. Формула гауссова ядра:\n",
        "$$\n",
        "K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2}\n",
        "\\right)\n",
        "$$\n",
        "Здесь:\n",
        "- $x_i$ и $x_j$ — два объекта (строки матрицы $X$),\n",
        "- $\\|x_i - x_j\\|^2$ — квадрат евклидова расстояния между объектами.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление ядерной матрицы\n",
        "Ядерная матрица $K$ вычисляется для всех пар объектов. Её размерность $4 \\times 4$, так как у нас 4 объекта.\n",
        "\n",
        "Формула для вычисления элементов матрицы:\n",
        "$$\n",
        "K_{ij} = K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "Вычислим каждый элемент матрицы $K$:\n",
        "\n",
        "1. $K(x_1, x_1) = \\exp\\left(-\\frac{\\|x_1 - x_1\\|^2}{2}\\right) = \\exp(0) = 1$\n",
        "2. $K(x_1, x_2) = \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-2)^2 + (2-3)^2}{2}\\right) = \\exp(-1) \\approx 0.3679$\n",
        "3. $K(x_1, x_3) = \\exp\\left(-\\frac{\\|x_1 - x_3\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-3)^2 + (2-4)^2}{2}\\right) = \\exp(-4) \\approx 0.0183$\n",
        "4. $K(x_1, x_4) = \\exp\\left(-\\frac{\\|x_1 - x_4\\|^2}{2}\\right) = \\exp\\left(-\\frac{(1-4)^2 + (2-5)^2}{2}\\right) = \\exp(-9) \\approx 0.0001$\n",
        "\n",
        "Аналогично вычисляем остальные элементы матрицы. В итоге получаем:\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "1 & 0.3679 & 0.0183 & 0.0001 \\\\\n",
        "0.3679 & 1 & 0.3679 & 0.0183 \\\\\n",
        "0.0183 & 0.3679 & 1 & 0.3679 \\\\\n",
        "0.0001 & 0.0183 & 0.3679 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Центрирование ядерной матрицы\n",
        "Центрирование ядерной матрицы необходимо для того, чтобы данные в новом пространстве имели нулевое среднее. Формула центрирования:\n",
        "$$\n",
        "K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$\n",
        "где $1_N$ — матрица $4 \\times 4$, состоящая из $\\frac{1}{4}$.\n",
        "\n",
        "1. Вычислим $1_N K$:\n",
        "$$\n",
        "1_N K = \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix} \\cdot K\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "1_N K = \\begin{bmatrix}\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466 \\\\\n",
        "0.3466 & 0.4385 & 0.4385 & 0.3466\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Вычислим $K 1_N$:\n",
        "$$\n",
        "K 1_N = K \\cdot \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "K 1_N = \\begin{bmatrix}\n",
        "0.3466 & 0.3466 & 0.3466 & 0.3466 \\\\\n",
        "0.4385 & 0.4385 & 0.4385 & 0.4385 \\\\\n",
        "0.4385 & 0.4385 & 0.4385 & 0.4385 \\\\\n",
        "0.3466 & 0.3466 & 0.3466 & 0.3466\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. Вычислим $1_N K 1_N$:\n",
        "$$\n",
        "1_N K 1_N = \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix} \\cdot K \\cdot \\begin{bmatrix}\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25 \\\\\n",
        "0.25 & 0.25 & 0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "1_N K 1_N = \\begin{bmatrix}\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926 \\\\\n",
        "0.3926 & 0.3926 & 0.3926 & 0.3926\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Подставляем в формулу центрирования:\n",
        "$$\n",
        "K_{\\text{центр}} = K - 1_N K - K 1_N + 1_N K 1_N\n",
        "$$\n",
        "Результат:\n",
        "$$\n",
        "K_{\\text{центр}} = \\begin{bmatrix}\n",
        "0.2698 & -0.1164 & -0.2698 & -0.1164 \\\\\n",
        "-0.1164 & 0.2698 & -0.1164 & -0.2698 \\\\\n",
        "-0.2698 & -0.1164 & 0.2698 & -0.1164 \\\\\n",
        "-0.1164 & -0.2698 & -0.1164 & 0.2698\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 4: Вычисление собственных значений и собственных векторов\n",
        "Решаем уравнение:\n",
        "$$\n",
        "K_{\\text{центр}} \\alpha = \\lambda \\alpha\n",
        "$$\n",
        "\n",
        "Находим собственные значения и векторы:\n",
        "$$\n",
        "\\lambda_1 = 0.5397, \\quad \\lambda_2 = 0.5397, \\quad \\lambda_3 = 0, \\quad \\lambda_4 = 0\n",
        "$$\n",
        "Соответствующие собственные векторы:\n",
        "$$\n",
        "\\alpha_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}, \\quad \\alpha_2 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}, \\quad \\alpha_3 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}, \\quad \\alpha_4 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Нормируем собственные векторы:\n",
        "$$\n",
        "\\alpha_1 = \\frac{\\alpha_1}{\\sqrt{\\lambda_1}}, \\quad \\alpha_2 = \\frac{\\alpha_2}{\\sqrt{\\lambda_2}}, \\quad \\alpha_3 = \\frac{\\alpha_3}{\\sqrt{\\lambda_3}}, \\quad \\alpha_4 = \\frac{\\alpha_4}{\\sqrt{\\lambda_4}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 5: Проецирование данных на главные компоненты\n",
        "Выберем первую главную компоненту (с наибольшим собственным значением) и спроецируем данные на неё:\n",
        "$$\n",
        "Y_i = \\sum_{j=1}^N \\alpha_j K(x_j, x_i)\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.2698 \\\\\n",
        "-0.1164 \\\\\n",
        "-0.2698 \\\\\n",
        "-0.1164\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Данные были спроецированы на первую главную компоненту, и их размерность снижена с 2 до 1.  \n",
        "Результат проекции:\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "0.2698 \\\\\n",
        "-0.1164 \\\\\n",
        "-0.2698 \\\\\n",
        "-0.1164\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Этот пример иллюстрирует, как Kernel PCA может быть использован для снижения размерности нелинейных данных.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uo3PHZAndQea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Реализация Kernel PCA с нуля\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "RaNR3s5edgmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class KernelPCA:\n",
        "    def __init__(self, kernel='rbf', gamma=1):\n",
        "        self.kernel = kernel\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def _rbf_kernel(self, X1, X2):\n",
        "        # Гауссово (RBF) ядро\n",
        "        return np.exp(-self.gamma * np.linalg.norm(X1[:, np.newaxis] - X2[np.newaxis, :], axis=2)**2)\n",
        "\n",
        "    def _center_kernel(self, K):\n",
        "        # Центрирование ядерной матрицы\n",
        "        N = K.shape[0]\n",
        "        one_N = np.ones((N, N)) / N\n",
        "        K_centered = K - one_N.dot(K) - K.dot(one_N) + one_N.dot(K).dot(one_N)\n",
        "        return K_centered\n",
        "\n",
        "    def fit_transform(self, X, n_components=1):\n",
        "        # Вычисление ядерной матрицы\n",
        "        K = self._rbf_kernel(X, X)\n",
        "\n",
        "        # Центрирование ядерной матрицы\n",
        "        K_centered = self._center_kernel(K)\n",
        "\n",
        "        # Вычисление собственных значений и векторов\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(K_centered)\n",
        "\n",
        "        # Сортировка собственных значений и векторов\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[sorted_indices]\n",
        "        eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Нормировка собственных векторов\n",
        "        eigenvectors = eigenvectors / np.sqrt(eigenvalues)\n",
        "\n",
        "        # Выбор первых n_components компонент\n",
        "        alpha = eigenvectors[:, :n_components]\n",
        "\n",
        "        # Проецирование данных\n",
        "        Y = K_centered.dot(alpha)\n",
        "        return Y\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Создаем объект KernelPCA\n",
        "kpca = KernelPCA(kernel='rbf', gamma=1)\n",
        "\n",
        "# Применяем Kernel PCA\n",
        "Y = kpca.fit_transform(X, n_components=1)\n",
        "\n",
        "print(\"Проекция данных (реализация с нуля):\\n\", Y)"
      ],
      "metadata": {
        "id": "wcwx4WlGdi19",
        "outputId": "cb21e789-0dc9-4289-959a-72ac4500e398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (реализация с нуля):\n",
            " [[ 0.62625661]\n",
            " [ 0.38661812]\n",
            " [-0.38661812]\n",
            " [-0.62625661]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из sklearn"
      ],
      "metadata": {
        "id": "cl_tX18IdjCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([[1, 2],\n",
        "              [2, 3],\n",
        "              [3, 4],\n",
        "              [4, 5]])\n",
        "\n",
        "# Создаем объект KernelPCA\n",
        "kpca = KernelPCA(n_components=1, kernel='rbf', gamma=1)\n",
        "\n",
        "# Применяем Kernel PCA\n",
        "Y_sklearn = kpca.fit_transform(X)\n",
        "\n",
        "print(\"Проекция данных (sklearn):\\n\", Y_sklearn)"
      ],
      "metadata": {
        "id": "YhWIBrS6djha",
        "outputId": "027769d6-eaa6-47c5-8792-93484610f0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (sklearn):\n",
            " [[ 0.62625661]\n",
            " [ 0.38661812]\n",
            " [-0.38661812]\n",
            " [-0.62625661]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w9O82CQvduFA"
      }
    }
  ]
}