{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTc/IgVIfrSuWncA7wUesV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_2_Feature_Engineering_Dimension_Reduction_%D0%9A%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D1%8B%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_(Supervised_Methods).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "-ZZUrvb9XMXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 **Контролируемые методы (Supervised Methods)**\n",
        "\n",
        "Контролируемые методы (Supervised Methods) — это один из основных подходов в машинном обучении, где модель обучается на размеченных данных, то есть на данных, для которых известны правильные ответы (метки классов или целевые значения). Цель контролируемого обучения — построить модель, которая сможет предсказывать метки для новых, ранее не встречавшихся данных. Эти методы широко применяются в задачах классификации (когда нужно отнести объект к одному из классов) и регрессии (когда нужно предсказать числовое значение).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5xEiuB2XdD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis) — это один из ключевых методов машинного обучения, который используется для снижения размерности данных и классификации. Основная цель LDA — найти такие линейные комбинации признаков, которые наилучшим образом разделяют классы. В отличие от метода главных компонент (PCA), который максимизирует общую дисперсию данных, LDA фокусируется на максимизации разделимости классов.\n",
        "\n",
        "LDA широко применяется в задачах распознавания образов, анализа данных, биоинформатики и других областях, где важно учитывать информацию о классах. В этой лекции мы подробно разберём математические основы LDA, его алгоритм, а также обсудим его преимущества и ограничения.\n",
        "\n",
        "\n",
        "\n",
        "#### Основные понятия и постановка задачи\n",
        "\n",
        "1. **Данные и классы**:\n",
        "   Предположим, у нас есть набор данных $X = \\{x_1, x_2, \\dots, x_n\\}$, где каждый объект $x_i$ — это вектор признаков размерности $D$. Каждый объект принадлежит одному из $K$ классов. Обозначим классы как $C_1, C_2, \\dots, C_K$.\n",
        "\n",
        "2. **Цель LDA**:\n",
        "   LDA стремится найти такое направление (или набор направлений) в пространстве признаков, при котором проекции объектов разных классов максимально разделены, а проекции объектов одного класса максимально сгруппированы. Это достигается за счёт максимизации отношения межклассовой дисперсии к внутриклассовой дисперсии.\n",
        "\n",
        "3. **Математическая формулировка**:\n",
        "   Формально задача LDA заключается в нахождении вектора $w$, который максимизирует критерий Фишера:\n",
        "\n",
        "$$\n",
        "   J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $S_B$ — межклассовая матрица рассеяния (between-class scatter matrix).\n",
        "   - $S_W$ — внутриклассовая матрица рассеяния (within-class scatter matrix).\n",
        "\n",
        "\n",
        "\n",
        "#### Матрицы рассеяния\n",
        "\n",
        "1. **Внутриклассовая матрица рассеяния $S_W$**:\n",
        "   Эта матрица описывает разброс объектов внутри каждого класса. Она вычисляется как сумма ковариационных матриц каждого класса:\n",
        "\n",
        "$$\n",
        "   S_W = \\sum_{k=1}^K \\sum_{x_i \\in C_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $C_k$ — множество объектов, принадлежащих классу $k$.\n",
        "   - $\\mu_k$ — среднее значение объектов класса $k$:\n",
        "  $$\n",
        "     \\mu_k = \\frac{1}{N_k} \\sum_{x_i \\in C_k} x_i\n",
        "  $$\n",
        "     где $N_k$ — количество объектов в классе $k$.\n",
        "\n",
        "2. **Межклассовая матрица рассеяния $S_B$**:\n",
        "   Эта матрица описывает разброс между средними значениями разных классов. Она вычисляется как:\n",
        "\n",
        "$$\n",
        "   S_B = \\sum_{k=1}^K N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $N_k$ — количество объектов в классе $k$.\n",
        "   - $\\mu$ — общее среднее значение всех объектов:\n",
        "  $$\n",
        "     \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
        "  $$\n",
        "     где $N$ — общее количество объектов.\n",
        "\n",
        "\n",
        "\n",
        "#### Максимизация критерия Фишера\n",
        "\n",
        "Критерий Фишера $J(w)$ определяется как отношение межклассовой дисперсии к внутриклассовой дисперсии:\n",
        "\n",
        "$$\n",
        "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "Наша цель — найти такой вектор $w$, который максимизирует это отношение. Для этого необходимо решить задачу оптимизации:\n",
        "\n",
        "$$\n",
        "\\max_w J(w) = \\max_w \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Решение задачи оптимизации\n",
        "\n",
        "1. **Градиентный подход**:\n",
        "   Чтобы найти максимум $J(w)$, воспользуемся методом множителей Лагранжа. Введём ограничение $w^T S_W w = 1$, чтобы упростить задачу. Тогда задача оптимизации примет вид:\n",
        "\n",
        "$$\n",
        "   \\max_w w^T S_B w \\quad \\text{при условии} \\quad w^T S_W w = 1\n",
        "$$\n",
        "\n",
        "2. **Функция Лагранжа**:\n",
        "   Составим функцию Лагранжа:\n",
        "\n",
        "$$\n",
        "   \\mathcal{L}(w, \\lambda) = w^T S_B w - \\lambda (w^T S_W w - 1)\n",
        "$$\n",
        "\n",
        "   где $\\lambda$ — множитель Лагранжа.\n",
        "\n",
        "3. **Дифференцирование по $w$**:\n",
        "   Продифференцируем функцию Лагранжа по $w$ и приравняем производную к нулю:\n",
        "\n",
        "$$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial w} = 2 S_B w - 2 \\lambda S_W w = 0\n",
        "$$\n",
        "\n",
        "   Упростив, получим:\n",
        "\n",
        "$$\n",
        "   S_B w = \\lambda S_W w\n",
        "$$\n",
        "\n",
        "   Это уравнение известно как обобщённая задача на собственные значения.\n",
        "\n",
        "4. **Решение обобщённой задачи на собственные значения**:\n",
        "   Чтобы найти $w$, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "   Здесь $S_W^{-1} S_B$ — матрица, а $\\lambda$ — её собственное значение, соответствующее собственному вектору $w$.\n",
        "\n",
        "5. **Интерпретация собственных значений и векторов**:\n",
        "   - Собственные значения $\\lambda$ показывают, насколько хорошо соответствующие собственные векторы $w$ разделяют классы.\n",
        "   - Собственные векторы $w$ определяют направления в пространстве признаков, вдоль которых классы максимально разделены.\n",
        "\n",
        "\n",
        "\n",
        "#### Выбор оптимальных направлений\n",
        "\n",
        "1. **Ранг матрицы $S_B$**:\n",
        "   Матрица $S_B$ имеет ранг не более $K-1$, где $K$ — количество классов. Это означает, что существует не более $K-1$ независимых направлений, которые можно использовать для разделения классов.\n",
        "\n",
        "2. **Собственные векторы**:\n",
        "   Для нахождения оптимальных направлений выбираются собственные векторы, соответствующие наибольшим собственным значениям. Эти векторы образуют матрицу проекции $W$:\n",
        "\n",
        "$$\n",
        "   W = [w_1, w_2, \\dots, w_{K-1}]\n",
        "$$\n",
        "\n",
        "   где $w_1, w_2, \\dots, w_{K-1}$ — собственные векторы, упорядоченные по убыванию собственных значений.\n",
        "\n",
        "3. **Проекция данных**:\n",
        "   Данные проецируются на новое пространство с помощью матрицы $W$:\n",
        "\n",
        "$$\n",
        "   y_i = W^T x_i\n",
        "$$\n",
        "\n",
        "   где $y_i$ — проекция объекта $x_i$ на новое пространство размерности $K-1$.\n",
        "\n",
        "\n",
        "\n",
        "#### Геометрическая интерпретация\n",
        "\n",
        "1. **Внутриклассовая дисперсия**:\n",
        "   Внутриклассовая дисперсия $w^T S_W w$ характеризует разброс объектов внутри каждого класса после проекции на направление $w$. Чем меньше это значение, тем более компактно расположены объекты одного класса.\n",
        "\n",
        "2. **Межклассовая дисперсия**:\n",
        "   Межклассовая дисперсия $w^T S_B w$ характеризует расстояние между средними значениями разных классов после проекции на направление $w$. Чем больше это значение, тем дальше друг от друга находятся классы.\n",
        "\n",
        "3. **Максимизация отношения**:\n",
        "   Максимизация отношения $\\frac{w^T S_B w}{w^T S_W w}$ означает поиск такого направления $w$, при котором классы максимально удалены друг от друга, а объекты внутри каждого класса максимально сгруппированы.\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и ограничения LDA\n",
        "\n",
        "1. **Преимущества**:\n",
        "   - LDA эффективно снижает размерность данных, сохраняя информацию о разделимости классов.\n",
        "   - Метод устойчив к переобучению, особенно при небольшом количестве обучающих данных.\n",
        "   - LDA может быть использован как для классификации, так и для визуализации данных.\n",
        "\n",
        "2. **Ограничения**:\n",
        "   - LDA предполагает, что данные каждого класса имеют нормальное распределение.\n",
        "   - Классы должны иметь одинаковую ковариационную матрицу.\n",
        "   - LDA может быть неэффективен, если классы сильно перекрываются в исходном пространстве.\n",
        "\n",
        "\n",
        "\n",
        "Линейный дискриминантный анализ (LDA) — это мощный метод, который позволяет эффективно снижать размерность данных, сохраняя информацию о разделимости классов. Основная идея LDA заключается в максимизации отношения межклассовой дисперсии к внутриклассовой дисперсии, что достигается через решение обобщённой задачи на собственные значения.\n",
        "\n",
        "Метод имеет свои ограничения, такие как предположение о нормальности распределения данных и равенстве ковариационных матриц классов. Однако при выполнении этих условий LDA демонстрирует высокую эффективность в задачах классификации и визуализации данных.\n",
        "\n",
        "Таким образом, LDA является важным инструментом в машинном обучении, который позволяет не только снижать размерность данных, но и улучшать качество классификации за счёт учёта информации о классах.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что первые две точки принадлежат классу $C_1$, а последние две — классу $C_2$. Наша цель — применить линейный дискриминантный анализ (LDA) для нахождения направления, которое наилучшим образом разделяет эти два класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Вычисление средних значений классов\n",
        "\n",
        "1. **Среднее значение класса $C_1$**:\n",
        "$$\n",
        "   \\mu_1 = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\right) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Среднее значение класса $C_2$**:\n",
        "$$\n",
        "   \\mu_2 = \\frac{1}{2} \\left( \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Общее среднее значение $\\mu$**:\n",
        "$$\n",
        "   \\mu = \\frac{1}{4} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление внутриклассовой матрицы рассеяния $S_W$\n",
        "\n",
        "Внутриклассовая матрица рассеяния $S_W$ вычисляется как сумма ковариационных матриц каждого класса.\n",
        "\n",
        "1. **Для класса $C_1$**:\n",
        "$$\n",
        "   S_{W1} = \\sum_{x_i \\in C_1} (x_i - \\mu_1)(x_i - \\mu_1)^T\n",
        "$$\n",
        "   Вычислим для каждой точки:\n",
        "$$\n",
        "   (x_1 - \\mu_1) = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (x_1 - \\mu_1)(x_1 - \\mu_1)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "$$\n",
        "   Аналогично для $x_2$:\n",
        "$$\n",
        "   (x_2 - \\mu_1) = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (x_2 - \\mu_1)(x_2 - \\mu_1)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "$$\n",
        "   Суммируем:\n",
        "$$\n",
        "   S_{W1} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Для класса $C_2$**:\n",
        "   Аналогично вычисляем $S_{W2}$:\n",
        "$$\n",
        "   S_{W2} = \\sum_{x_i \\in C_2} (x_i - \\mu_2)(x_i - \\mu_2)^T\n",
        "$$\n",
        "   Вычисления аналогичны классу $C_1$, и в результате получим:\n",
        "$$\n",
        "   S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Итоговая внутриклассовая матрица рассеяния $S_W$**:\n",
        "$$\n",
        "   S_W = S_{W1} + S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} + \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Вычисление межклассовой матрицы рассеяния $S_B$\n",
        "\n",
        "Межклассовая матрица рассеяния $S_B$ вычисляется как:\n",
        "\n",
        "$$\n",
        "S_B = \\sum_{k=1}^K N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
        "$$\n",
        "\n",
        "1. **Для класса $C_1$**:\n",
        "$$\n",
        "   (\\mu_1 - \\mu) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix} \\begin{bmatrix} -3 & -3 & -3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "   Учитывая $N_1 = 2$:\n",
        "$$\n",
        "   N_1 (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Для класса $C_2$**:\n",
        "$$\n",
        "   (\\mu_2 - \\mu) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 3 & 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "   Учитывая $N_2 = 2$:\n",
        "$$\n",
        "   N_2 (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Итоговая межклассовая матрица рассеяния $S_B$**:\n",
        "$$\n",
        "   S_B = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} + \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} = \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 4: Решение обобщённой задачи на собственные значения\n",
        "\n",
        "Теперь мы переходим к ключевому этапу линейного дискриминантного анализа (LDA) — решению обобщённой задачи на собственные значения. Наша цель — найти вектор $w$, который максимизирует критерий Фишера:\n",
        "\n",
        "$$\n",
        "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "Это эквивалентно решению уравнения:\n",
        "\n",
        "$$\n",
        "S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $S_W$ — внутриклассовая матрица рассеяния.\n",
        "- $S_B$ — межклассовая матрица рассеяния.\n",
        "- $\\lambda$ — собственное значение, соответствующее собственному вектору $w$.\n",
        "\n",
        "\n",
        "\n",
        "#### Подробное решение\n",
        "\n",
        "1. **Проверка обратимости матрицы $S_W$**:\n",
        "   Прежде чем решать уравнение, необходимо убедиться, что матрица $S_W$ обратима. В нашем примере:\n",
        "\n",
        "$$\n",
        "   S_W = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Определитель этой матрицы равен нулю, так как все строки линейно зависимы. Это означает, что $S_W$ — вырожденная матрица, и обратной матрицы $S_W^{-1}$ не существует.\n",
        "\n",
        "2. **Регуляризация матрицы $S_W$**:\n",
        "   Чтобы обойти проблему вырожденности, можно добавить небольшое значение $\\alpha$ к диагональным элементам матрицы $S_W$. Это называется регуляризацией:\n",
        "\n",
        "$$\n",
        "   S_W' = S_W + \\alpha I\n",
        "$$\n",
        "\n",
        "   где $I$ — единичная матрица, а $\\alpha$ — параметр регуляризации (например, $\\alpha = 0.01$).\n",
        "\n",
        "   После регуляризации:\n",
        "\n",
        "$$\n",
        "   S_W' = \\begin{bmatrix} 9.01 & 9 & 9 \\\\ 9 & 9.01 & 9 \\\\ 9 & 9 & 9.01 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Теперь матрица $S_W'$ обратима.\n",
        "\n",
        "3. **Вычисление обратной матрицы $S_W'^{-1}$**:\n",
        "   Обратная матрица $S_W'^{-1}$ вычисляется с использованием стандартных методов линейной алгебры. В данном случае:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} \\approx \\begin{bmatrix} 111.11 & -111.11 & -111.11 \\\\ -111.11 & 111.11 & -111.11 \\\\ -111.11 & -111.11 & 111.11 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   (Значения округлены для упрощения.)\n",
        "\n",
        "4. **Вычисление матрицы $S_W'^{-1} S_B$**:\n",
        "   Теперь вычислим матрицу $S_W'^{-1} S_B$:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} S_B = S_W'^{-1} \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Умножение даёт:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} S_B \\approx \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Это означает, что все собственные значения $\\lambda$ равны нулю, что указывает на отсутствие полезных направлений для разделения классов.\n",
        "\n",
        "5. **Интерпретация результата**:\n",
        "   В данном примере все собственные значения равны нулю, что говорит о том, что данные не могут быть разделены с помощью LDA. Это связано с тем, что:\n",
        "   - Все точки в каждом классе идентичны (разброс внутри классов равен нулю).\n",
        "   - Средние значения классов также не различаются (разброс между классами равен нулю).\n",
        "\n",
        "\n",
        "\n",
        "#### Общий алгоритм решения обобщённой задачи на собственные значения\n",
        "\n",
        "Если матрица $S_W$ не вырождена, то решение обобщённой задачи на собственные значения выполняется следующим образом:\n",
        "\n",
        "1. **Вычисление обратной матрицы $S_W^{-1}$**:\n",
        "   Если $S_W$ обратима, находим $S_W^{-1}$.\n",
        "\n",
        "2. **Вычисление матрицы $S_W^{-1} S_B$**:\n",
        "   Умножаем $S_W^{-1}$ на $S_B$.\n",
        "\n",
        "3. **Нахождение собственных значений и векторов**:\n",
        "   Решаем уравнение:\n",
        "\n",
        "$$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "   Это стандартная задача на собственные значения, которая решается с использованием численных методов (например, метода QR-разложения).\n",
        "\n",
        "4. **Выбор собственных векторов**:\n",
        "   Собственные векторы, соответствующие наибольшим собственным значениям, дают направления, которые наилучшим образом разделяют классы.\n",
        "\n",
        "5. **Проекция данных**:\n",
        "   Данные проецируются на новое пространство с использованием выбранных собственных векторов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Реализуем пример линейного дискриминантного анализа (LDA) на Python. Сначала создадим класс LDA с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация LDA с нуля\n"
      ],
      "metadata": {
        "id": "YqdpKK8gX1UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LDA:\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.w = None  # Вектор весов (направление проекции)\n",
        "        self.alpha = alpha  # Параметр регуляризации\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели LDA.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :param y: Вектор меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        classes = np.unique(y)\n",
        "\n",
        "        # Вычисление средних значений для каждого класса\n",
        "        class_means = []\n",
        "        for c in classes:\n",
        "            class_means.append(np.mean(X[y == c], axis=0))\n",
        "        class_means = np.array(class_means)\n",
        "\n",
        "        # Общее среднее значение\n",
        "        overall_mean = np.mean(X, axis=0)\n",
        "\n",
        "        # Внутриклассовая матрица рассеяния (S_W)\n",
        "        S_W = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            class_scatter = np.zeros((n_features, n_features))\n",
        "            for sample in X[y == c]:\n",
        "                sample, mean = sample.reshape(n_features, 1), mean.reshape(n_features, 1)\n",
        "                class_scatter += (sample - mean).dot((sample - mean).T)\n",
        "            S_W += class_scatter\n",
        "\n",
        "        # Регуляризация матрицы S_W\n",
        "        S_W += self.alpha * np.eye(n_features)\n",
        "\n",
        "        # Межклассовая матрица рассеяния (S_B)\n",
        "        S_B = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            n_c = X[y == c].shape[0]\n",
        "            mean = mean.reshape(n_features, 1)\n",
        "            overall_mean = overall_mean.reshape(n_features, 1)\n",
        "            S_B += n_c * (mean - overall_mean).dot((mean - overall_mean).T)\n",
        "\n",
        "        # Решение обобщённой задачи на собственные значения\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "        # Сортировка собственных векторов по убыванию собственных значений\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "        # Выбор первого собственного вектора (направление проекции)\n",
        "        self.w = eigenvectors[:, 0]\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Проекция данных на новое пространство.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :return: Проекция данных (n_samples, 1).\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Создаём объект LDA с регуляризацией\n",
        "lda_custom = LDA(alpha=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "lda_custom.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_transformed_custom = lda_custom.transform(X)\n",
        "print(\"Проекция данных (с нуля, с регуляризацией):\\n\", X_transformed_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQlr0c2IZrCb",
        "outputId": "b9c3ee1a-8816-46b0-ccdf-b758e3be6dd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (с нуля, с регуляризацией):\n",
            " [ 3.46410162  8.66025404 13.85640646 19.05255888]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn"
      ],
      "metadata": {
        "id": "fnzS_bzFZ25k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Готовое решение LDA\n",
        "lda_sklearn = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Данные\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Обучаем модель\n",
        "lda_sklearn.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_transformed_sklearn = lda_sklearn.transform(X)\n",
        "print(\"Проекция данных (scikit-learn):\\n\", X_transformed_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7u671LZ3eD",
        "outputId": "65e4036f-1a9b-46b0-d420-0b0c4486becf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (scikit-learn):\n",
            " [[-2.12132034]\n",
            " [-0.70710678]\n",
            " [ 0.70710678]\n",
            " [ 2.12132034]]\n"
          ]
        }
      ]
    }
  ]
}