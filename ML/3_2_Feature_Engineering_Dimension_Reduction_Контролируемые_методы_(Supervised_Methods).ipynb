{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm/PTR5q67/YY4p197HBOV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_2_Feature_Engineering_Dimension_Reduction_%D0%9A%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D1%8B%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_(Supervised_Methods).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "-ZZUrvb9XMXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 **Контролируемые методы (Supervised Methods)**\n",
        "\n",
        "Контролируемые методы (Supervised Methods) — это один из основных подходов в машинном обучении, где модель обучается на размеченных данных, то есть на данных, для которых известны правильные ответы (метки классов или целевые значения). Цель контролируемого обучения — построить модель, которая сможет предсказывать метки для новых, ранее не встречавшихся данных. Эти методы широко применяются в задачах классификации (когда нужно отнести объект к одному из классов) и регрессии (когда нужно предсказать числовое значение).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5xEiuB2XdD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis) — это один из ключевых методов машинного обучения, который используется для снижения размерности данных и классификации. Основная цель LDA — найти такие линейные комбинации признаков, которые наилучшим образом разделяют классы. В отличие от метода главных компонент (PCA), который максимизирует общую дисперсию данных, LDA фокусируется на максимизации разделимости классов.\n",
        "\n",
        "LDA широко применяется в задачах распознавания образов, анализа данных, биоинформатики и других областях, где важно учитывать информацию о классах. В этой лекции мы подробно разберём математические основы LDA, его алгоритм, а также обсудим его преимущества и ограничения.\n",
        "\n",
        "\n",
        "\n",
        "#### Основные понятия и постановка задачи\n",
        "\n",
        "1. **Данные и классы**:\n",
        "   Предположим, у нас есть набор данных $X = \\{x_1, x_2, \\dots, x_n\\}$, где каждый объект $x_i$ — это вектор признаков размерности $D$. Каждый объект принадлежит одному из $K$ классов. Обозначим классы как $C_1, C_2, \\dots, C_K$.\n",
        "\n",
        "2. **Цель LDA**:\n",
        "   LDA стремится найти такое направление (или набор направлений) в пространстве признаков, при котором проекции объектов разных классов максимально разделены, а проекции объектов одного класса максимально сгруппированы. Это достигается за счёт максимизации отношения межклассовой дисперсии к внутриклассовой дисперсии.\n",
        "\n",
        "3. **Математическая формулировка**:\n",
        "   Формально задача LDA заключается в нахождении вектора $w$, который максимизирует критерий Фишера:\n",
        "\n",
        "$$\n",
        "   J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $S_B$ — межклассовая матрица рассеяния (between-class scatter matrix).\n",
        "   - $S_W$ — внутриклассовая матрица рассеяния (within-class scatter matrix).\n",
        "\n",
        "\n",
        "\n",
        "#### Матрицы рассеяния\n",
        "\n",
        "1. **Внутриклассовая матрица рассеяния $S_W$**:\n",
        "   Эта матрица описывает разброс объектов внутри каждого класса. Она вычисляется как сумма ковариационных матриц каждого класса:\n",
        "\n",
        "$$\n",
        "   S_W = \\sum_{k=1}^K \\sum_{x_i \\in C_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $C_k$ — множество объектов, принадлежащих классу $k$.\n",
        "   - $\\mu_k$ — среднее значение объектов класса $k$:\n",
        "  $$\n",
        "     \\mu_k = \\frac{1}{N_k} \\sum_{x_i \\in C_k} x_i\n",
        "  $$\n",
        "     где $N_k$ — количество объектов в классе $k$.\n",
        "\n",
        "2. **Межклассовая матрица рассеяния $S_B$**:\n",
        "   Эта матрица описывает разброс между средними значениями разных классов. Она вычисляется как:\n",
        "\n",
        "$$\n",
        "   S_B = \\sum_{k=1}^K N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $N_k$ — количество объектов в классе $k$.\n",
        "   - $\\mu$ — общее среднее значение всех объектов:\n",
        "  $$\n",
        "     \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
        "  $$\n",
        "     где $N$ — общее количество объектов.\n",
        "\n",
        "\n",
        "\n",
        "#### Максимизация критерия Фишера\n",
        "\n",
        "Критерий Фишера $J(w)$ определяется как отношение межклассовой дисперсии к внутриклассовой дисперсии:\n",
        "\n",
        "$$\n",
        "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "Наша цель — найти такой вектор $w$, который максимизирует это отношение. Для этого необходимо решить задачу оптимизации:\n",
        "\n",
        "$$\n",
        "\\max_w J(w) = \\max_w \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Решение задачи оптимизации\n",
        "\n",
        "1. **Градиентный подход**:\n",
        "   Чтобы найти максимум $J(w)$, воспользуемся методом множителей Лагранжа. Введём ограничение $w^T S_W w = 1$, чтобы упростить задачу. Тогда задача оптимизации примет вид:\n",
        "\n",
        "$$\n",
        "   \\max_w w^T S_B w \\quad \\text{при условии} \\quad w^T S_W w = 1\n",
        "$$\n",
        "\n",
        "2. **Функция Лагранжа**:\n",
        "   Составим функцию Лагранжа:\n",
        "\n",
        "$$\n",
        "   \\mathcal{L}(w, \\lambda) = w^T S_B w - \\lambda (w^T S_W w - 1)\n",
        "$$\n",
        "\n",
        "   где $\\lambda$ — множитель Лагранжа.\n",
        "\n",
        "3. **Дифференцирование по $w$**:\n",
        "   Продифференцируем функцию Лагранжа по $w$ и приравняем производную к нулю:\n",
        "\n",
        "$$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial w} = 2 S_B w - 2 \\lambda S_W w = 0\n",
        "$$\n",
        "\n",
        "   Упростив, получим:\n",
        "\n",
        "$$\n",
        "   S_B w = \\lambda S_W w\n",
        "$$\n",
        "\n",
        "   Это уравнение известно как обобщённая задача на собственные значения.\n",
        "\n",
        "4. **Решение обобщённой задачи на собственные значения**:\n",
        "   Чтобы найти $w$, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "   Здесь $S_W^{-1} S_B$ — матрица, а $\\lambda$ — её собственное значение, соответствующее собственному вектору $w$.\n",
        "\n",
        "5. **Интерпретация собственных значений и векторов**:\n",
        "   - Собственные значения $\\lambda$ показывают, насколько хорошо соответствующие собственные векторы $w$ разделяют классы.\n",
        "   - Собственные векторы $w$ определяют направления в пространстве признаков, вдоль которых классы максимально разделены.\n",
        "\n",
        "\n",
        "\n",
        "#### Выбор оптимальных направлений\n",
        "\n",
        "1. **Ранг матрицы $S_B$**:\n",
        "   Матрица $S_B$ имеет ранг не более $K-1$, где $K$ — количество классов. Это означает, что существует не более $K-1$ независимых направлений, которые можно использовать для разделения классов.\n",
        "\n",
        "2. **Собственные векторы**:\n",
        "   Для нахождения оптимальных направлений выбираются собственные векторы, соответствующие наибольшим собственным значениям. Эти векторы образуют матрицу проекции $W$:\n",
        "\n",
        "$$\n",
        "   W = [w_1, w_2, \\dots, w_{K-1}]\n",
        "$$\n",
        "\n",
        "   где $w_1, w_2, \\dots, w_{K-1}$ — собственные векторы, упорядоченные по убыванию собственных значений.\n",
        "\n",
        "3. **Проекция данных**:\n",
        "   Данные проецируются на новое пространство с помощью матрицы $W$:\n",
        "\n",
        "$$\n",
        "   y_i = W^T x_i\n",
        "$$\n",
        "\n",
        "   где $y_i$ — проекция объекта $x_i$ на новое пространство размерности $K-1$.\n",
        "\n",
        "\n",
        "\n",
        "#### Геометрическая интерпретация\n",
        "\n",
        "1. **Внутриклассовая дисперсия**:\n",
        "   Внутриклассовая дисперсия $w^T S_W w$ характеризует разброс объектов внутри каждого класса после проекции на направление $w$. Чем меньше это значение, тем более компактно расположены объекты одного класса.\n",
        "\n",
        "2. **Межклассовая дисперсия**:\n",
        "   Межклассовая дисперсия $w^T S_B w$ характеризует расстояние между средними значениями разных классов после проекции на направление $w$. Чем больше это значение, тем дальше друг от друга находятся классы.\n",
        "\n",
        "3. **Максимизация отношения**:\n",
        "   Максимизация отношения $\\frac{w^T S_B w}{w^T S_W w}$ означает поиск такого направления $w$, при котором классы максимально удалены друг от друга, а объекты внутри каждого класса максимально сгруппированы.\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и ограничения LDA\n",
        "\n",
        "1. **Преимущества**:\n",
        "   - LDA эффективно снижает размерность данных, сохраняя информацию о разделимости классов.\n",
        "   - Метод устойчив к переобучению, особенно при небольшом количестве обучающих данных.\n",
        "   - LDA может быть использован как для классификации, так и для визуализации данных.\n",
        "\n",
        "2. **Ограничения**:\n",
        "   - LDA предполагает, что данные каждого класса имеют нормальное распределение.\n",
        "   - Классы должны иметь одинаковую ковариационную матрицу.\n",
        "   - LDA может быть неэффективен, если классы сильно перекрываются в исходном пространстве.\n",
        "\n",
        "\n",
        "\n",
        "Линейный дискриминантный анализ (LDA) — это мощный метод, который позволяет эффективно снижать размерность данных, сохраняя информацию о разделимости классов. Основная идея LDA заключается в максимизации отношения межклассовой дисперсии к внутриклассовой дисперсии, что достигается через решение обобщённой задачи на собственные значения.\n",
        "\n",
        "Метод имеет свои ограничения, такие как предположение о нормальности распределения данных и равенстве ковариационных матриц классов. Однако при выполнении этих условий LDA демонстрирует высокую эффективность в задачах классификации и визуализации данных.\n",
        "\n",
        "Таким образом, LDA является важным инструментом в машинном обучении, который позволяет не только снижать размерность данных, но и улучшать качество классификации за счёт учёта информации о классах.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что первые две точки принадлежат классу $C_1$, а последние две — классу $C_2$. Наша цель — применить линейный дискриминантный анализ (LDA) для нахождения направления, которое наилучшим образом разделяет эти два класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Вычисление средних значений классов\n",
        "\n",
        "1. **Среднее значение класса $C_1$**:\n",
        "$$\n",
        "   \\mu_1 = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\right) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Среднее значение класса $C_2$**:\n",
        "$$\n",
        "   \\mu_2 = \\frac{1}{2} \\left( \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Общее среднее значение $\\mu$**:\n",
        "$$\n",
        "   \\mu = \\frac{1}{4} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление внутриклассовой матрицы рассеяния $S_W$\n",
        "\n",
        "Внутриклассовая матрица рассеяния $S_W$ вычисляется как сумма ковариационных матриц каждого класса.\n",
        "\n",
        "1. **Для класса $C_1$**:\n",
        "$$\n",
        "   S_{W1} = \\sum_{x_i \\in C_1} (x_i - \\mu_1)(x_i - \\mu_1)^T\n",
        "$$\n",
        "   Вычислим для каждой точки:\n",
        "$$\n",
        "   (x_1 - \\mu_1) = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (x_1 - \\mu_1)(x_1 - \\mu_1)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "$$\n",
        "   Аналогично для $x_2$:\n",
        "$$\n",
        "   (x_2 - \\mu_1) = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (x_2 - \\mu_1)(x_2 - \\mu_1)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "$$\n",
        "   Суммируем:\n",
        "$$\n",
        "   S_{W1} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Для класса $C_2$**:\n",
        "   Аналогично вычисляем $S_{W2}$:\n",
        "$$\n",
        "   S_{W2} = \\sum_{x_i \\in C_2} (x_i - \\mu_2)(x_i - \\mu_2)^T\n",
        "$$\n",
        "   Вычисления аналогичны классу $C_1$, и в результате получим:\n",
        "$$\n",
        "   S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Итоговая внутриклассовая матрица рассеяния $S_W$**:\n",
        "$$\n",
        "   S_W = S_{W1} + S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} + \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Вычисление межклассовой матрицы рассеяния $S_B$\n",
        "\n",
        "Межклассовая матрица рассеяния $S_B$ вычисляется как:\n",
        "\n",
        "$$\n",
        "S_B = \\sum_{k=1}^K N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
        "$$\n",
        "\n",
        "1. **Для класса $C_1$**:\n",
        "$$\n",
        "   (\\mu_1 - \\mu) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix} \\begin{bmatrix} -3 & -3 & -3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "   Учитывая $N_1 = 2$:\n",
        "$$\n",
        "   N_1 (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Для класса $C_2$**:\n",
        "$$\n",
        "   (\\mu_2 - \\mu) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 3 & 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "   Учитывая $N_2 = 2$:\n",
        "$$\n",
        "   N_2 (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Итоговая межклассовая матрица рассеяния $S_B$**:\n",
        "$$\n",
        "   S_B = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} + \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} = \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 4: Решение обобщённой задачи на собственные значения\n",
        "\n",
        "Теперь мы переходим к ключевому этапу линейного дискриминантного анализа (LDA) — решению обобщённой задачи на собственные значения. Наша цель — найти вектор $w$, который максимизирует критерий Фишера:\n",
        "\n",
        "$$\n",
        "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "Это эквивалентно решению уравнения:\n",
        "\n",
        "$$\n",
        "S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $S_W$ — внутриклассовая матрица рассеяния.\n",
        "- $S_B$ — межклассовая матрица рассеяния.\n",
        "- $\\lambda$ — собственное значение, соответствующее собственному вектору $w$.\n",
        "\n",
        "\n",
        "\n",
        "#### Подробное решение\n",
        "\n",
        "1. **Проверка обратимости матрицы $S_W$**:\n",
        "   Прежде чем решать уравнение, необходимо убедиться, что матрица $S_W$ обратима. В нашем примере:\n",
        "\n",
        "$$\n",
        "   S_W = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Определитель этой матрицы равен нулю, так как все строки линейно зависимы. Это означает, что $S_W$ — вырожденная матрица, и обратной матрицы $S_W^{-1}$ не существует.\n",
        "\n",
        "2. **Регуляризация матрицы $S_W$**:\n",
        "   Чтобы обойти проблему вырожденности, можно добавить небольшое значение $\\alpha$ к диагональным элементам матрицы $S_W$. Это называется регуляризацией:\n",
        "\n",
        "$$\n",
        "   S_W' = S_W + \\alpha I\n",
        "$$\n",
        "\n",
        "   где $I$ — единичная матрица, а $\\alpha$ — параметр регуляризации (например, $\\alpha = 0.01$).\n",
        "\n",
        "   После регуляризации:\n",
        "\n",
        "$$\n",
        "   S_W' = \\begin{bmatrix} 9.01 & 9 & 9 \\\\ 9 & 9.01 & 9 \\\\ 9 & 9 & 9.01 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Теперь матрица $S_W'$ обратима.\n",
        "\n",
        "3. **Вычисление обратной матрицы $S_W'^{-1}$**:\n",
        "   Обратная матрица $S_W'^{-1}$ вычисляется с использованием стандартных методов линейной алгебры. В данном случае:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} \\approx \\begin{bmatrix} 111.11 & -111.11 & -111.11 \\\\ -111.11 & 111.11 & -111.11 \\\\ -111.11 & -111.11 & 111.11 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   (Значения округлены для упрощения.)\n",
        "\n",
        "4. **Вычисление матрицы $S_W'^{-1} S_B$**:\n",
        "   Теперь вычислим матрицу $S_W'^{-1} S_B$:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} S_B = S_W'^{-1} \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Умножение даёт:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} S_B \\approx \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Это означает, что все собственные значения $\\lambda$ равны нулю, что указывает на отсутствие полезных направлений для разделения классов.\n",
        "\n",
        "5. **Интерпретация результата**:\n",
        "   В данном примере все собственные значения равны нулю, что говорит о том, что данные не могут быть разделены с помощью LDA. Это связано с тем, что:\n",
        "   - Все точки в каждом классе идентичны (разброс внутри классов равен нулю).\n",
        "   - Средние значения классов также не различаются (разброс между классами равен нулю).\n",
        "\n",
        "\n",
        "\n",
        "#### Общий алгоритм решения обобщённой задачи на собственные значения\n",
        "\n",
        "Если матрица $S_W$ не вырождена, то решение обобщённой задачи на собственные значения выполняется следующим образом:\n",
        "\n",
        "1. **Вычисление обратной матрицы $S_W^{-1}$**:\n",
        "   Если $S_W$ обратима, находим $S_W^{-1}$.\n",
        "\n",
        "2. **Вычисление матрицы $S_W^{-1} S_B$**:\n",
        "   Умножаем $S_W^{-1}$ на $S_B$.\n",
        "\n",
        "3. **Нахождение собственных значений и векторов**:\n",
        "   Решаем уравнение:\n",
        "\n",
        "$$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "   Это стандартная задача на собственные значения, которая решается с использованием численных методов (например, метода QR-разложения).\n",
        "\n",
        "4. **Выбор собственных векторов**:\n",
        "   Собственные векторы, соответствующие наибольшим собственным значениям, дают направления, которые наилучшим образом разделяют классы.\n",
        "\n",
        "5. **Проекция данных**:\n",
        "   Данные проецируются на новое пространство с использованием выбранных собственных векторов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Реализуем пример линейного дискриминантного анализа (LDA) на Python. Сначала создадим класс LDA с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация LDA с нуля\n"
      ],
      "metadata": {
        "id": "YqdpKK8gX1UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LDA:\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.w = None  # Вектор весов (направление проекции)\n",
        "        self.alpha = alpha  # Параметр регуляризации\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели LDA.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :param y: Вектор меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        classes = np.unique(y)\n",
        "\n",
        "        # Вычисление средних значений для каждого класса\n",
        "        class_means = []\n",
        "        for c in classes:\n",
        "            class_means.append(np.mean(X[y == c], axis=0))\n",
        "        class_means = np.array(class_means)\n",
        "\n",
        "        # Общее среднее значение\n",
        "        overall_mean = np.mean(X, axis=0)\n",
        "\n",
        "        # Внутриклассовая матрица рассеяния (S_W)\n",
        "        S_W = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            class_scatter = np.zeros((n_features, n_features))\n",
        "            for sample in X[y == c]:\n",
        "                sample, mean = sample.reshape(n_features, 1), mean.reshape(n_features, 1)\n",
        "                class_scatter += (sample - mean).dot((sample - mean).T)\n",
        "            S_W += class_scatter\n",
        "\n",
        "        # Регуляризация матрицы S_W\n",
        "        S_W += self.alpha * np.eye(n_features)\n",
        "\n",
        "        # Межклассовая матрица рассеяния (S_B)\n",
        "        S_B = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            n_c = X[y == c].shape[0]\n",
        "            mean = mean.reshape(n_features, 1)\n",
        "            overall_mean = overall_mean.reshape(n_features, 1)\n",
        "            S_B += n_c * (mean - overall_mean).dot((mean - overall_mean).T)\n",
        "\n",
        "        # Решение обобщённой задачи на собственные значения\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "        # Сортировка собственных векторов по убыванию собственных значений\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "        # Выбор первого собственного вектора (направление проекции)\n",
        "        self.w = eigenvectors[:, 0]\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Проекция данных на новое пространство.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :return: Проекция данных (n_samples, 1).\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Создаём объект LDA с регуляризацией\n",
        "lda_custom = LDA(alpha=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "lda_custom.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_transformed_custom = lda_custom.transform(X)\n",
        "print(\"Проекция данных (с нуля, с регуляризацией):\\n\", X_transformed_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQlr0c2IZrCb",
        "outputId": "b9c3ee1a-8816-46b0-ccdf-b758e3be6dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (с нуля, с регуляризацией):\n",
            " [ 3.46410162  8.66025404 13.85640646 19.05255888]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn"
      ],
      "metadata": {
        "id": "fnzS_bzFZ25k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Готовое решение LDA\n",
        "lda_sklearn = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Данные\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Обучаем модель\n",
        "lda_sklearn.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_transformed_sklearn = lda_sklearn.transform(X)\n",
        "print(\"Проекция данных (scikit-learn):\\n\", X_transformed_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7u671LZ3eD",
        "outputId": "65e4036f-1a9b-46b0-d420-0b0c4486becf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (scikit-learn):\n",
            " [[-2.12132034]\n",
            " [-0.70710678]\n",
            " [ 0.70710678]\n",
            " [ 2.12132034]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis) — это метод классификации, который используется для разделения объектов на классы на основе их признаков. QDA является обобщением линейного дискриминантного анализа (LDA), но, в отличие от LDA, QDA не предполагает, что ковариационные матрицы всех классов одинаковы. Это позволяет QDA учитывать более сложные структуры данных, где классы могут иметь различную форму и ориентацию в пространстве признаков.\n",
        "\n",
        "\n",
        "\n",
        "#### Основные понятия\n",
        "\n",
        "1. **Классификация**:\n",
        "   Задача классификации заключается в отнесении объекта к одному из нескольких классов на основе его признаков. В QDA предполагается, что данные каждого класса распределены по многомерному нормальному распределению.\n",
        "\n",
        "2. **Многомерное нормальное распределение**:\n",
        "   Для класса $k$ вероятность наблюдения вектора признаков $\\mathbf{x}$ задается плотностью многомерного нормального распределения:\n",
        "$$\n",
        "   P(\\mathbf{x} | Y = k) = \\frac{1}{(2\\pi)^{p/2} |\\mathbf{\\Sigma}_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^T \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\right),\n",
        "$$\n",
        "   где:\n",
        "   - $\\mathbf{x}$ — вектор признаков размерности $p$,\n",
        "   - $Y = k$ — принадлежность объекта к классу $k$,\n",
        "   - $\\mathbf{\\mu}_k$ — вектор средних значений для класса $k$,\n",
        "   - $\\mathbf{\\Sigma}_k$ — ковариационная матрица для класса $k$,\n",
        "   - $|\\mathbf{\\Sigma}_k|$ — определитель ковариационной матрицы.\n",
        "\n",
        "3. **Апостериорная вероятность**:\n",
        "   QDA использует теорему Байеса для вычисления апостериорной вероятности принадлежности объекта к классу $k$:\n",
        "$$\n",
        "   P(Y = k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | Y = k) P(Y = k)}{P(\\mathbf{x})},\n",
        "$$\n",
        "   где:\n",
        "   - $P(Y = k)$ — априорная вероятность класса $k$,\n",
        "   - $P(\\mathbf{x})$ — нормировочная константа, которая не зависит от класса.\n",
        "\n",
        "4. **Решающее правило**:\n",
        "   Объект $\\mathbf{x}$ относится к классу $k$, для которого апостериорная вероятность $P(Y = k | \\mathbf{x})$ максимальна. Это эквивалентно максимизации логарифма апостериорной вероятности:\n",
        "$$\n",
        "   \\delta_k(\\mathbf{x}) = \\log P(Y = k) - \\frac{1}{2} \\log |\\mathbf{\\Sigma}_k| - \\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^T \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k).\n",
        "$$\n",
        "   Здесь $\\delta_k(\\mathbf{x})$ называется дискриминантной функцией для класса $k$.\n",
        "\n",
        "\n",
        "\n",
        "#### Сравнение QDA и LDA\n",
        "\n",
        "Квадратичный дискриминантный анализ (QDA) очень похож на линейный дискриминантный анализ (LDA), но с одним ключевым отличием: в QDA мы отказываемся от предположения, что средние значения и ковариационные матрицы всех классов одинаковы. Вместо этого для каждого класса $y$ мы оцениваем отдельные средние значения $\\mu_y$ и ковариационные матрицы $\\Sigma_y$. Это позволяет QDA учитывать более сложные структуры данных, где классы могут иметь различную форму и ориентацию в пространстве признаков.\n",
        "\n",
        "\n",
        "\n",
        "#### Оценка ковариационной матрицы для каждого класса\n",
        "\n",
        "Для каждого класса $y$ ковариационная матрица $\\Sigma_y$ оценивается по формуле:\n",
        "$$\n",
        "\\Sigma_y = \\frac{1}{N_y - 1} \\sum_{y_i = y} (x_i - \\mu_y)(x_i - \\mu_y)^T,\n",
        "$$\n",
        "где:\n",
        "- $N_y$ — количество объектов в классе $y$,\n",
        "- $x_i$ — вектор признаков $i$-го объекта,\n",
        "- $\\mu_y$ — вектор средних значений для класса $y$, вычисляемый как:\n",
        "  $$\n",
        "  \\mu_y = \\frac{1}{N_y} \\sum_{y_i = y} x_i.\n",
        "  $$\n",
        "\n",
        "Эта формула аналогична оценке ковариационной матрицы в LDA, но в QDA она вычисляется отдельно для каждого класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Квадратичная дискриминантная функция\n",
        "\n",
        "Дискриминантная функция $\\delta_k(x)$ в QDA имеет квадратичную форму и выражается следующим образом:\n",
        "$$\n",
        "\\delta_k(x) = \\log \\pi_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k + x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} x^T \\Sigma_k^{-1} x - \\frac{1}{2} \\log |\\Sigma_k|.\n",
        "$$\n",
        "\n",
        "Давайте разберем каждый член этой функции:\n",
        "\n",
        "1. **$\\log \\pi_k$**:\n",
        "   Это логарифм априорной вероятности класса $k$. Априорная вероятность $\\pi_k$ оценивается как доля объектов класса $k$ в обучающей выборке:\n",
        "$$\n",
        "   \\pi_k = \\frac{N_k}{N},\n",
        "$$\n",
        "   где $N_k$ — количество объектов в классе $k$, а $N$ — общее количество объектов.\n",
        "\n",
        "2. **$-\\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k$**:\n",
        "   Этот член представляет собой квадратичную форму, связанную с центром класса $k$. Он учитывает положение центра класса в пространстве признаков.\n",
        "\n",
        "3. **$x^T \\Sigma_k^{-1} \\mu_k$**:\n",
        "   Это линейный член, который связывает объект $x$ с центром класса $k$. Он учитывает \"близость\" объекта $x$ к центру класса $k$ в метрике, заданной ковариационной матрицей $\\Sigma_k$.\n",
        "\n",
        "4. **$-\\frac{1}{2} x^T \\Sigma_k^{-1} x$**:\n",
        "   Это квадратичный член, который учитывает \"расстояние\" объекта $x$ до центра класса $k$ в метрике, заданной ковариационной матрицей $\\Sigma_k$.\n",
        "\n",
        "5. **$-\\frac{1}{2} \\log |\\Sigma_k|$**:\n",
        "   Этот член учитывает \"объём\" ковариационной матрицы класса $k$. Чем больше разброс данных в классе, тем меньше значение этого члена.\n",
        "\n",
        "\n",
        "\n",
        "#### Вывод дискриминантной функции\n",
        "\n",
        "Дискриминантная функция $\\delta_k(x)$ получается путём логарифмирования апостериорной вероятности $P(Y = k | x)$ и упрощения выражения. Напомним, что апостериорная вероятность задаётся по теореме Байеса:\n",
        "$$\n",
        "P(Y = k | x) = \\frac{P(x | Y = k) P(Y = k)}{P(x)}.\n",
        "$$\n",
        "\n",
        "Логарифмируя это выражение и опуская нормировочную константу $P(x)$, получаем:\n",
        "$$\n",
        "\\log P(Y = k | x) = \\log P(x | Y = k) + \\log P(Y = k).\n",
        "$$\n",
        "\n",
        "Подставляя выражение для плотности многомерного нормального распределения $P(x | Y = k)$, получаем:\n",
        "$$\n",
        "\\log P(Y = k | x) = -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k + \\text{const}.\n",
        "$$\n",
        "\n",
        "Раскрывая квадратичную форму $(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)$, получаем:\n",
        "$$\n",
        "(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) = x^T \\Sigma_k^{-1} x - 2 x^T \\Sigma_k^{-1} \\mu_k + \\mu_k^T \\Sigma_k^{-1} \\mu_k.\n",
        "$$\n",
        "\n",
        "Подставляя это обратно в выражение для $\\log P(Y = k | x)$, получаем окончательную формулу для дискриминантной функции:\n",
        "$$\n",
        "\\delta_k(x) = \\log \\pi_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k + x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} x^T \\Sigma_k^{-1} x - \\frac{1}{2} \\log |\\Sigma_k|.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и недостатки QDA\n",
        "\n",
        "**Преимущества**:\n",
        "1. QDA позволяет моделировать более сложные границы между классами, чем LDA, благодаря использованию различных ковариационных матриц.\n",
        "2. Хорошо работает, когда классы имеют различную форму и ориентацию в пространстве признаков.\n",
        "\n",
        "**Недостатки**:\n",
        "1. QDA требует оценки большего количества параметров, чем LDA, что может привести к переобучению при малом количестве данных.\n",
        "2. Вычислительная сложность выше, чем у LDA, из-за необходимости обращения и хранения отдельных ковариационных матриц для каждого класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Заключение\n",
        "\n",
        "Квадратичный дискриминантный анализ (QDA) — это мощный метод классификации, который обобщает линейный дискриминантный анализ, позволяя учитывать различные ковариационные структуры для каждого класса. QDA особенно полезен в задачах, где классы имеют сложные формы распределений. Однако его применение требует достаточного количества данных для точной оценки параметров модели.\n",
        "\n",
        "\n",
        "\n",
        "#### Практическое применение QDA\n",
        "\n",
        "После получения окончательной формулы для дискриминантной функции в QDA, её можно использовать для классификации новых объектов. Для этого необходимо:\n",
        "1. Оценить параметры модели (средние значения, ковариационные матрицы и априорные вероятности).\n",
        "2. Вычислить значение дискриминантной функции для каждого класса.\n",
        "3. Отнести объект к классу с максимальным значением дискриминантной функции.\n",
        "\n",
        "QDA является мощным инструментом для задач классификации, особенно когда классы имеют различную форму и ориентацию в пространстве признаков. Однако его применение требует внимательного подхода к оценке параметров и учёта вычислительной сложности.\n",
        "\n",
        "\n",
        "\n",
        "#### Дополнительные замечания\n",
        "\n",
        "1. **Регуляризация**:\n",
        "   В случаях, когда количество данных ограничено, можно использовать регуляризацию для предотвращения переобучения. Например, можно добавить небольшое значение к диагонали ковариационной матрицы, чтобы сделать её более устойчивой.\n",
        "\n",
        "2. **Вычислительная эффективность**:\n",
        "   Для ускорения вычислений можно использовать методы оптимизации, такие как разложение Холецкого или LU-разложение, для обращения ковариационных матриц.\n",
        "\n",
        "3. **Интерпретация результатов**:\n",
        "   QDA позволяет моделировать сложные границы между классами, но интерпретация этих границ может быть затруднена из-за квадратичной природы дискриминантной функции.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, квадратичный дискриминантный анализ (QDA) представляет собой гибкий и мощный метод классификации, который может быть эффективно использован в задачах с нелинейными границами между классами. Однако его применение требует тщательной подготовки данных и внимательного подхода к оценке параметров модели.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что первые две точки принадлежат классу 0, а последние две — классу 1. Таким образом, метки классов будут:\n",
        "\n",
        "$$\n",
        "y = \\begin{bmatrix}\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Разделение данных по классам\n",
        "\n",
        "Разделим данные на два класса:\n",
        "\n",
        "- **Класс 0**:\n",
        "  $$\n",
        "  X_0 = \\begin{bmatrix}\n",
        "  1 & 2 & 3 \\\\\n",
        "  4 & 5 & 6\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Класс 1**:\n",
        "  $$\n",
        "  X_1 = \\begin{bmatrix}\n",
        "  7 & 8 & 9 \\\\\n",
        "  10 & 11 & 12\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление средних значений для каждого класса\n",
        "\n",
        "Среднее значение для каждого класса вычисляется как среднее арифметическое по всем точкам класса.\n",
        "\n",
        "1. **Класс 0**:\n",
        "$$\n",
        "   \\mu_0 = \\frac{1}{2} \\begin{bmatrix}\n",
        "   1 + 4 \\\\\n",
        "   2 + 5 \\\\\n",
        "   3 + 6\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   2.5 \\\\\n",
        "   3.5 \\\\\n",
        "   4.5\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Класс 1**:\n",
        "$$\n",
        "   \\mu_1 = \\frac{1}{2} \\begin{bmatrix}\n",
        "   7 + 10 \\\\\n",
        "   8 + 11 \\\\\n",
        "   9 + 12\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   8.5 \\\\\n",
        "   9.5 \\\\\n",
        "   10.5\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Вычисление ковариационных матриц для каждого класса\n",
        "\n",
        "Ковариационная матрица для каждого класса вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "\\Sigma_k = \\frac{1}{N_k - 1} \\sum_{i=1}^{N_k} (x_i - \\mu_k)(x_i - \\mu_k)^T,\n",
        "$$\n",
        "\n",
        "где $N_k$ — количество точек в классе $k$, $x_i$ — $i$-я точка класса $k$, $\\mu_k$ — среднее значение класса $k$.\n",
        "\n",
        "1. **Класс 0**:\n",
        "   - Центрируем данные:\n",
        "  $$\n",
        "     X_0 - \\mu_0 = \\begin{bmatrix}\n",
        "     1 - 2.5 & 2 - 3.5 & 3 - 4.5 \\\\\n",
        "     4 - 2.5 & 5 - 3.5 & 6 - 4.5\n",
        "     \\end{bmatrix} = \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5 \\\\\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "   - Вычисляем ковариационную матрицу:\n",
        "  $$\n",
        "     \\Sigma_0 = \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     -1.5 \\\\\n",
        "     -1.5 \\\\\n",
        "     -1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5\n",
        "     \\end{bmatrix} + \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     1.5 \\\\\n",
        "     1.5 \\\\\n",
        "     1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "  $$\n",
        "     \\Sigma_0 = \\begin{bmatrix}\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "2. **Класс 1**:\n",
        "   - Центрируем данные:\n",
        "  $$\n",
        "     X_1 - \\mu_1 = \\begin{bmatrix}\n",
        "     7 - 8.5 & 8 - 9.5 & 9 - 10.5 \\\\\n",
        "     10 - 8.5 & 11 - 9.5 & 12 - 10.5\n",
        "     \\end{bmatrix} = \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5 \\\\\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "   - Вычисляем ковариационную матрицу:\n",
        "  $$\n",
        "     \\Sigma_1 = \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     -1.5 \\\\\n",
        "     -1.5 \\\\\n",
        "     -1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5\n",
        "     \\end{bmatrix} + \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     1.5 \\\\\n",
        "     1.5 \\\\\n",
        "     1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "  $$\n",
        "     \\Sigma_1 = \\begin{bmatrix}\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 4: Вычисление априорных вероятностей\n",
        "\n",
        "Априорная вероятность $\\pi_k$ для каждого класса вычисляется как доля точек класса $k$ в общем количестве точек:\n",
        "\n",
        "$$\n",
        "\\pi_k = \\frac{N_k}{N},\n",
        "$$\n",
        "\n",
        "где $N_k$ — количество точек в классе $k$, $N$ — общее количество точек.\n",
        "\n",
        "1. **Класс 0**:\n",
        "$$\n",
        "   \\pi_0 = \\frac{2}{4} = 0.5\n",
        "$$\n",
        "\n",
        "2. **Класс 1**:\n",
        "$$\n",
        "   \\pi_1 = \\frac{2}{4} = 0.5\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 5: Вычисление дискриминантной функции\n",
        "\n",
        "Дискриминантная функция для класса $k$ имеет вид:\n",
        "\n",
        "$$\n",
        "\\delta_k(x) = \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k).\n",
        "$$\n",
        "\n",
        "1. **Вычисление определителя ковариационной матрицы**:\n",
        "   - Для обоих классов $\\Sigma_0$ и $\\Sigma_1$ определитель равен 0, так как матрицы вырождены (все строки и столбцы линейно зависимы). Это означает, что данные лежат на прямой линии, и QDA не может быть применён напрямую. В реальных задачах для таких случаев используется регуляризация (например, добавление небольшого значения к диагонали матрицы).\n",
        "\n",
        "2. **Регуляризация**:\n",
        "   Добавим небольшое значение $\\epsilon = 0.01$ к диагонали ковариационных матриц:\n",
        "$$\n",
        "   \\Sigma_0 = \\begin{bmatrix}\n",
        "   4.5 + 0.01 & 4.5 & 4.5 \\\\\n",
        "   4.5 & 4.5 + 0.01 & 4.5 \\\\\n",
        "   4.5 & 4.5 & 4.5 + 0.01\n",
        "   \\end{bmatrix}, \\quad\n",
        "   \\Sigma_1 = \\begin{bmatrix}\n",
        "   4.5 + 0.01 & 4.5 & 4.5 \\\\\n",
        "   4.5 & 4.5 + 0.01 & 4.5 \\\\\n",
        "   4.5 & 4.5 & 4.5 + 0.01\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "3. **Вычисление обратной матрицы**:\n",
        "   Обратные матрицы $\\Sigma_0^{-1}$ и $\\Sigma_1^{-1}$ могут быть вычислены с использованием численных методов.\n",
        "\n",
        "4. **Вычисление дискриминантной функции**:\n",
        "   Для новой точки $x = \\begin{bmatrix} 5 & 6 & 7 \\end{bmatrix}$ вычислим $\\delta_0(x)$ и $\\delta_1(x)$.\n",
        "\n",
        "Таким образом, в данном примере мы рассмотрели основные шаги QDA, включая вычисление средних значений, ковариационных матриц и дискриминантной функции. Однако из-за вырожденности ковариационных матриц потребовалась регуляризация. В реальных задачах QDA применяется к данным, где классы имеют различные ковариационные структуры, и регуляризация помогает избежать проблем с вырожденными матрицами.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####Реализация квадратичного дискриминантного анализа (QDA) на Python\n",
        "\n",
        "1. Реализация QDA с нуля\n"
      ],
      "metadata": {
        "id": "epooty_dseQH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6luAZFgew0_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components  # Количество главных компонент\n",
        "        self.components = None  # Главные компоненты\n",
        "        self.mean = None  # Средние значения для центрирования данных\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Шаг 1: Центрирование данных\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        # Шаг 2: Вычисление ковариационной матрицы\n",
        "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "        # Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "        # Шаг 4: Сортировка собственных векторов по убыванию собственных значений\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        self.components = eigenvectors[:, sorted_indices[:self.n_components]]\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Центрирование данных\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        # Проецирование данных на главные компоненты\n",
        "        X_projected = np.dot(X_centered, self.components)\n",
        "        return X_projected\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Создаем объект PCA с 1 компонентой\n",
        "pca = PCA(n_components=1)\n",
        "\n",
        "# Обучаем модель на данных\n",
        "pca.fit(X)\n",
        "\n",
        "# Преобразуем данные\n",
        "X_projected = pca.transform(X)\n",
        "\n",
        "print(\"Исходные данные:\\n\", X)\n",
        "print(\"Данные после PCA (1 компонента):\\n\", X_projected)"
      ],
      "metadata": {
        "id": "tgDnhh94vFO3",
        "outputId": "58a27496-dd16-4921-94cf-daced11303b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходные данные:\n",
            " [[ 1  2  3]\n",
            " [ 4  5  6]\n",
            " [ 7  8  9]\n",
            " [10 11 12]]\n",
            "Данные после PCA (1 компонента):\n",
            " [[-7.79422863]\n",
            " [-2.59807621]\n",
            " [ 2.59807621]\n",
            " [ 7.79422863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения (scikit-learn)"
      ],
      "metadata": {
        "id": "SJO8xjoGvIXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class QDA:\n",
        "    def __init__(self, reg_param=0.01):\n",
        "        self.classes = None\n",
        "        self.priors = None\n",
        "        self.means = None\n",
        "        self.covariances = None\n",
        "        self.reg_param = reg_param  # Параметр регуляризации\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели QDA.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :param y: Вектор меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Инициализация\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        self.means = np.zeros((n_classes, n_features))\n",
        "        self.covariances = np.zeros((n_classes, n_features, n_features))\n",
        "\n",
        "        # Вычисление априорных вероятностей, средних и ковариационных матриц\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            self.priors[i] = X_c.shape[0] / n_samples\n",
        "            self.means[i] = np.mean(X_c, axis=0)\n",
        "            self.covariances[i] = np.cov(X_c, rowvar=False)\n",
        "\n",
        "            # Регуляризация: добавление небольшого значения к диагонали\n",
        "            self.covariances[i] += self.reg_param * np.eye(n_features)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказание классов для новых данных.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :return: Вектор предсказанных меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        posteriors = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        # Вычисление апостериорных вероятностей для каждого класса\n",
        "        for i in range(n_classes):\n",
        "            likelihood = multivariate_normal.pdf(X, mean=self.means[i], cov=self.covariances[i])\n",
        "            posteriors[:, i] = self.priors[i] * likelihood\n",
        "\n",
        "        # Выбор класса с максимальной апостериорной вероятностью\n",
        "        return self.classes[np.argmax(posteriors, axis=1)]\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "# Создаем объект QDA с регуляризацией\n",
        "qda = QDA(reg_param=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "qda.fit(X, y)\n",
        "\n",
        "# Предсказываем классы\n",
        "X_test = np.array([\n",
        "    [5, 6, 7],\n",
        "    [8, 9, 10]\n",
        "])\n",
        "predictions = qda.predict(X_test)\n",
        "\n",
        "print(\"Предсказанные классы:\", predictions)"
      ],
      "metadata": {
        "id": "1fOE5y1yvLiU",
        "outputId": "ebc8cf49-4164-42b1-da6d-482be9a79034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные классы: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения (scikit-learn)"
      ],
      "metadata": {
        "id": "EYy60jE6w4m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "# Создаем объект QDA с увеличенным параметром регуляризации\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param=0.1)  # Увеличиваем reg_param\n",
        "\n",
        "# Обучаем модель\n",
        "qda.fit(X, y)\n",
        "\n",
        "# Предсказываем классы\n",
        "X_test = np.array([\n",
        "    [5, 6, 7],\n",
        "    [8, 9, 10]\n",
        "])\n",
        "predictions = qda.predict(X_test)\n",
        "\n",
        "print(\"Предсказанные классы:\", predictions)"
      ],
      "metadata": {
        "id": "rWqSWCDIw75N",
        "outputId": "c38aa19d-2ab5-479b-ebad-e73851fdf0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные классы: [0 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат `Предсказанные классы: [0 1]` означает, что модель квадратичного дискриминантного анализа (QDA) предсказала классы для двух тестовых точек:\n",
        "\n",
        "1. Для точки `[5, 6, 7]` модель предсказала класс **0**.\n",
        "2. Для точки `[8, 9, 10]` модель предсказала класс **1**.\n",
        "\n",
        "Этот результат говорит о том, что модель успешно разделила данные на два класса в соответствии с обучением. Давайте разберём, что это означает и как интерпретировать результат.\n",
        "\n",
        "### Интерпретация результата\n",
        "\n",
        "1. **Обучение модели**:\n",
        "   - Модель QDA была обучена на данных, где:\n",
        "     - Точки `[1, 2, 3]` и `[4, 5, 6]` принадлежат классу **0**.\n",
        "     - Точки `[7, 8, 9]` и `[10, 11, 12]` принадлежат классу **1**.\n",
        "\n",
        "2. **Предсказание для новых данных**:\n",
        "   - Для точки `[5, 6, 7]` модель решила, что она ближе к классу **0**.\n",
        "   - Для точки `[8, 9, 10]` модель решила, что она ближе к классу **1**.\n",
        "\n",
        "3. **Граница между классами**:\n",
        "   - QDA строит квадратичную границу между классами, учитывая различные ковариационные матрицы для каждого класса.\n",
        "   - В данном случае граница проходит между точками `[5, 6, 7]` и `[8, 9, 10]`.\n",
        "\n",
        "\n",
        "\n",
        "### Почему именно такие предсказания?\n",
        "\n",
        "1. **Класс 0**:\n",
        "   - Точка `[5, 6, 7]` ближе к данным класса **0** (`[1, 2, 3]` и `[4, 5, 6]`), поэтому модель отнесла её к классу **0**.\n",
        "\n",
        "2. **Класс 1**:\n",
        "   - Точка `[8, 9, 10]` ближе к данным класса **1** (`[7, 8, 9]` и `[10, 11, 12]`), поэтому модель отнесла её к классу **1**.\n",
        "\n",
        "\n",
        "\n",
        "### Что можно улучшить?\n",
        "\n",
        "1. **Регуляризация**:\n",
        "   - Если данные имеют вырожденные ковариационные матрицы (как в вашем примере), увеличьте параметр `reg_param`, чтобы избежать предупреждений и улучшить устойчивость модели.\n",
        "\n",
        "2. **Уменьшение размерности**:\n",
        "   - Если данные лежат на прямой линии (как в вашем примере), можно использовать PCA для уменьшения размерности. Это упростит задачу и устранит вырожденность.\n",
        "\n",
        "3. **Визуализация**:\n",
        "   - Если данные имеют 2 или 3 признака, можно визуализировать границу между классами, чтобы лучше понять, как модель принимает решения.\n",
        "\n",
        "\n",
        "\n",
        "### Пример визуализации (для 2D-данных)\n",
        "\n",
        "Если бы данные были двумерными, мы могли бы построить график с границей между классами. Например:\n"
      ],
      "metadata": {
        "id": "aJpdlTyox7iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "# Пример 2D-данных\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 3],\n",
        "    [6, 7],\n",
        "    [7, 8],\n",
        "    [8, 8]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "# Обучаем модель\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
        "qda.fit(X, y)\n",
        "\n",
        "# Визуализация\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "Z = qda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
        "plt.title(\"Граница между классами (QDA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "STVVYB6Px4Q0",
        "outputId": "b7f0c40f-3ada-4d47-ec63-1b71fabca496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IzI7KweLyaiJ"
      }
    }
  ]
}