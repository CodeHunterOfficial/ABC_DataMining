{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr72JvwG0yv5UTiBkHMvIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/3_2_Feature_Engineering_Dimension_Reduction_%D0%9A%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D1%8B%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_(Supervised_Methods).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering. Снижение размерности (Dimension Reduction)\n",
        "### Оглавление\n",
        "\n",
        "### 1. **Снижение размерности (Dimension Reduction)**  \n",
        "#### 1.1 **Неконтролируемые методы (Unsupervised Methods)**  \n",
        "   - 1.1.1 Метод главных компонент (PCA, Principal Components Analysis)  \n",
        "   - 1.1.2 Ядерный метод главных компонент (Kernel PCA, Kernel Principal Components Analysis)  \n",
        "   - 1.1.3 t-SNE (t-Distributed Stochastic Neighbor Embedding)  \n",
        "   - 1.1.4 UMAP (Uniform Manifold Approximation and Projection)  \n",
        "   - 1.1.5 Метод независимых компонент (ICA, Independent Component Analysis)  \n",
        "   - 1.1.6 Неотрицательная матричная факторизация (NMF, Non-Negative Matrix Factorization)  \n",
        "   - 1.1.7 Автоэнкодеры (Autoencoders, Neural Network-Based Dimensionality Reduction)  \n",
        "   - 1.1.8 Изометрическое отображение (Isomap, Isometric Mapping)  \n",
        "   - 1.1.9 Локально линейное вложение (LLE, Locally Linear Embedding)  \n",
        "   - 1.1.10 Собственные отображения Лапласа (Laplacian Eigenmaps)  \n",
        "\n",
        "#### 1.2 **Контролируемые методы (Supervised Methods)**  \n",
        "   - 1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)  \n",
        "   - 1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)  \n",
        "   - 1.2.3 Метод частичных наименьших квадратов (PLS, Partial Least Squares)  \n",
        "   - 1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)  \n",
        "   - 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)  \n",
        "   - 1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)  \n",
        "   - 1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)  \n"
      ],
      "metadata": {
        "id": "-ZZUrvb9XMXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 **Контролируемые методы (Supervised Methods)**\n",
        "\n",
        "Контролируемые методы (Supervised Methods) — это один из основных подходов в машинном обучении, где модель обучается на размеченных данных, то есть на данных, для которых известны правильные ответы (метки классов или целевые значения). Цель контролируемого обучения — построить модель, которая сможет предсказывать метки для новых, ранее не встречавшихся данных. Эти методы широко применяются в задачах классификации (когда нужно отнести объект к одному из классов) и регрессии (когда нужно предсказать числовое значение).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5xEiuB2XdD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.1 Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Линейный дискриминантный анализ (LDA, Linear Discriminant Analysis) — это один из ключевых методов машинного обучения, который используется для снижения размерности данных и классификации. Основная цель LDA — найти такие линейные комбинации признаков, которые наилучшим образом разделяют классы. В отличие от метода главных компонент (PCA), который максимизирует общую дисперсию данных, LDA фокусируется на максимизации разделимости классов.\n",
        "\n",
        "LDA широко применяется в задачах распознавания образов, анализа данных, биоинформатики и других областях, где важно учитывать информацию о классах. В этой лекции мы подробно разберём математические основы LDA, его алгоритм, а также обсудим его преимущества и ограничения.\n",
        "\n",
        "\n",
        "\n",
        "#### Основные понятия и постановка задачи\n",
        "\n",
        "1. **Данные и классы**:\n",
        "   Предположим, у нас есть набор данных $X = \\{x_1, x_2, \\dots, x_n\\}$, где каждый объект $x_i$ — это вектор признаков размерности $D$. Каждый объект принадлежит одному из $K$ классов. Обозначим классы как $C_1, C_2, \\dots, C_K$.\n",
        "\n",
        "2. **Цель LDA**:\n",
        "   LDA стремится найти такое направление (или набор направлений) в пространстве признаков, при котором проекции объектов разных классов максимально разделены, а проекции объектов одного класса максимально сгруппированы. Это достигается за счёт максимизации отношения межклассовой дисперсии к внутриклассовой дисперсии.\n",
        "\n",
        "3. **Математическая формулировка**:\n",
        "   Формально задача LDA заключается в нахождении вектора $w$, который максимизирует критерий Фишера:\n",
        "\n",
        "$$\n",
        "   J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $S_B$ — межклассовая матрица рассеяния (between-class scatter matrix).\n",
        "   - $S_W$ — внутриклассовая матрица рассеяния (within-class scatter matrix).\n",
        "\n",
        "\n",
        "\n",
        "#### Матрицы рассеяния\n",
        "\n",
        "1. **Внутриклассовая матрица рассеяния $S_W$**:\n",
        "   Эта матрица описывает разброс объектов внутри каждого класса. Она вычисляется как сумма ковариационных матриц каждого класса:\n",
        "\n",
        "$$\n",
        "   S_W = \\sum_{k=1}^K \\sum_{x_i \\in C_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $C_k$ — множество объектов, принадлежащих классу $k$.\n",
        "   - $\\mu_k$ — среднее значение объектов класса $k$:\n",
        "  $$\n",
        "     \\mu_k = \\frac{1}{N_k} \\sum_{x_i \\in C_k} x_i\n",
        "  $$\n",
        "     где $N_k$ — количество объектов в классе $k$.\n",
        "\n",
        "2. **Межклассовая матрица рассеяния $S_B$**:\n",
        "   Эта матрица описывает разброс между средними значениями разных классов. Она вычисляется как:\n",
        "\n",
        "$$\n",
        "   S_B = \\sum_{k=1}^K N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
        "$$\n",
        "\n",
        "   где:\n",
        "   - $N_k$ — количество объектов в классе $k$.\n",
        "   - $\\mu$ — общее среднее значение всех объектов:\n",
        "  $$\n",
        "     \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
        "  $$\n",
        "     где $N$ — общее количество объектов.\n",
        "\n",
        "\n",
        "\n",
        "#### Максимизация критерия Фишера\n",
        "\n",
        "Критерий Фишера $J(w)$ определяется как отношение межклассовой дисперсии к внутриклассовой дисперсии:\n",
        "\n",
        "$$\n",
        "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "Наша цель — найти такой вектор $w$, который максимизирует это отношение. Для этого необходимо решить задачу оптимизации:\n",
        "\n",
        "$$\n",
        "\\max_w J(w) = \\max_w \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Решение задачи оптимизации\n",
        "\n",
        "1. **Градиентный подход**:\n",
        "   Чтобы найти максимум $J(w)$, воспользуемся методом множителей Лагранжа. Введём ограничение $w^T S_W w = 1$, чтобы упростить задачу. Тогда задача оптимизации примет вид:\n",
        "\n",
        "$$\n",
        "   \\max_w w^T S_B w \\quad \\text{при условии} \\quad w^T S_W w = 1\n",
        "$$\n",
        "\n",
        "2. **Функция Лагранжа**:\n",
        "   Составим функцию Лагранжа:\n",
        "\n",
        "$$\n",
        "   \\mathcal{L}(w, \\lambda) = w^T S_B w - \\lambda (w^T S_W w - 1)\n",
        "$$\n",
        "\n",
        "   где $\\lambda$ — множитель Лагранжа.\n",
        "\n",
        "3. **Дифференцирование по $w$**:\n",
        "   Продифференцируем функцию Лагранжа по $w$ и приравняем производную к нулю:\n",
        "\n",
        "$$\n",
        "   \\frac{\\partial \\mathcal{L}}{\\partial w} = 2 S_B w - 2 \\lambda S_W w = 0\n",
        "$$\n",
        "\n",
        "   Упростив, получим:\n",
        "\n",
        "$$\n",
        "   S_B w = \\lambda S_W w\n",
        "$$\n",
        "\n",
        "   Это уравнение известно как обобщённая задача на собственные значения.\n",
        "\n",
        "4. **Решение обобщённой задачи на собственные значения**:\n",
        "   Чтобы найти $w$, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "   Здесь $S_W^{-1} S_B$ — матрица, а $\\lambda$ — её собственное значение, соответствующее собственному вектору $w$.\n",
        "\n",
        "5. **Интерпретация собственных значений и векторов**:\n",
        "   - Собственные значения $\\lambda$ показывают, насколько хорошо соответствующие собственные векторы $w$ разделяют классы.\n",
        "   - Собственные векторы $w$ определяют направления в пространстве признаков, вдоль которых классы максимально разделены.\n",
        "\n",
        "\n",
        "\n",
        "#### Выбор оптимальных направлений\n",
        "\n",
        "1. **Ранг матрицы $S_B$**:\n",
        "   Матрица $S_B$ имеет ранг не более $K-1$, где $K$ — количество классов. Это означает, что существует не более $K-1$ независимых направлений, которые можно использовать для разделения классов.\n",
        "\n",
        "2. **Собственные векторы**:\n",
        "   Для нахождения оптимальных направлений выбираются собственные векторы, соответствующие наибольшим собственным значениям. Эти векторы образуют матрицу проекции $W$:\n",
        "\n",
        "$$\n",
        "   W = [w_1, w_2, \\dots, w_{K-1}]\n",
        "$$\n",
        "\n",
        "   где $w_1, w_2, \\dots, w_{K-1}$ — собственные векторы, упорядоченные по убыванию собственных значений.\n",
        "\n",
        "3. **Проекция данных**:\n",
        "   Данные проецируются на новое пространство с помощью матрицы $W$:\n",
        "\n",
        "$$\n",
        "   y_i = W^T x_i\n",
        "$$\n",
        "\n",
        "   где $y_i$ — проекция объекта $x_i$ на новое пространство размерности $K-1$.\n",
        "\n",
        "\n",
        "\n",
        "#### Геометрическая интерпретация\n",
        "\n",
        "1. **Внутриклассовая дисперсия**:\n",
        "   Внутриклассовая дисперсия $w^T S_W w$ характеризует разброс объектов внутри каждого класса после проекции на направление $w$. Чем меньше это значение, тем более компактно расположены объекты одного класса.\n",
        "\n",
        "2. **Межклассовая дисперсия**:\n",
        "   Межклассовая дисперсия $w^T S_B w$ характеризует расстояние между средними значениями разных классов после проекции на направление $w$. Чем больше это значение, тем дальше друг от друга находятся классы.\n",
        "\n",
        "3. **Максимизация отношения**:\n",
        "   Максимизация отношения $\\frac{w^T S_B w}{w^T S_W w}$ означает поиск такого направления $w$, при котором классы максимально удалены друг от друга, а объекты внутри каждого класса максимально сгруппированы.\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и ограничения LDA\n",
        "\n",
        "1. **Преимущества**:\n",
        "   - LDA эффективно снижает размерность данных, сохраняя информацию о разделимости классов.\n",
        "   - Метод устойчив к переобучению, особенно при небольшом количестве обучающих данных.\n",
        "   - LDA может быть использован как для классификации, так и для визуализации данных.\n",
        "\n",
        "2. **Ограничения**:\n",
        "   - LDA предполагает, что данные каждого класса имеют нормальное распределение.\n",
        "   - Классы должны иметь одинаковую ковариационную матрицу.\n",
        "   - LDA может быть неэффективен, если классы сильно перекрываются в исходном пространстве.\n",
        "\n",
        "\n",
        "\n",
        "Линейный дискриминантный анализ (LDA) — это мощный метод, который позволяет эффективно снижать размерность данных, сохраняя информацию о разделимости классов. Основная идея LDA заключается в максимизации отношения межклассовой дисперсии к внутриклассовой дисперсии, что достигается через решение обобщённой задачи на собственные значения.\n",
        "\n",
        "Метод имеет свои ограничения, такие как предположение о нормальности распределения данных и равенстве ковариационных матриц классов. Однако при выполнении этих условий LDA демонстрирует высокую эффективность в задачах классификации и визуализации данных.\n",
        "\n",
        "Таким образом, LDA является важным инструментом в машинном обучении, который позволяет не только снижать размерность данных, но и улучшать качество классификации за счёт учёта информации о классах.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что первые две точки принадлежат классу $C_1$, а последние две — классу $C_2$. Наша цель — применить линейный дискриминантный анализ (LDA) для нахождения направления, которое наилучшим образом разделяет эти два класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Вычисление средних значений классов\n",
        "\n",
        "1. **Среднее значение класса $C_1$**:\n",
        "$$\n",
        "   \\mu_1 = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\right) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Среднее значение класса $C_2$**:\n",
        "$$\n",
        "   \\mu_2 = \\frac{1}{2} \\left( \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Общее среднее значение $\\mu$**:\n",
        "$$\n",
        "   \\mu = \\frac{1}{4} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление внутриклассовой матрицы рассеяния $S_W$\n",
        "\n",
        "Внутриклассовая матрица рассеяния $S_W$ вычисляется как сумма ковариационных матриц каждого класса.\n",
        "\n",
        "1. **Для класса $C_1$**:\n",
        "$$\n",
        "   S_{W1} = \\sum_{x_i \\in C_1} (x_i - \\mu_1)(x_i - \\mu_1)^T\n",
        "$$\n",
        "   Вычислим для каждой точки:\n",
        "$$\n",
        "   (x_1 - \\mu_1) = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (x_1 - \\mu_1)(x_1 - \\mu_1)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "$$\n",
        "   Аналогично для $x_2$:\n",
        "$$\n",
        "   (x_2 - \\mu_1) = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (x_2 - \\mu_1)(x_2 - \\mu_1)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "$$\n",
        "   Суммируем:\n",
        "$$\n",
        "   S_{W1} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Для класса $C_2$**:\n",
        "   Аналогично вычисляем $S_{W2}$:\n",
        "$$\n",
        "   S_{W2} = \\sum_{x_i \\in C_2} (x_i - \\mu_2)(x_i - \\mu_2)^T\n",
        "$$\n",
        "   Вычисления аналогичны классу $C_1$, и в результате получим:\n",
        "$$\n",
        "   S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Итоговая внутриклассовая матрица рассеяния $S_W$**:\n",
        "$$\n",
        "   S_W = S_{W1} + S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} + \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Вычисление межклассовой матрицы рассеяния $S_B$\n",
        "\n",
        "Межклассовая матрица рассеяния $S_B$ вычисляется как:\n",
        "\n",
        "$$\n",
        "S_B = \\sum_{k=1}^K N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
        "$$\n",
        "\n",
        "1. **Для класса $C_1$**:\n",
        "$$\n",
        "   (\\mu_1 - \\mu) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix} \\begin{bmatrix} -3 & -3 & -3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "   Учитывая $N_1 = 2$:\n",
        "$$\n",
        "   N_1 (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Для класса $C_2$**:\n",
        "$$\n",
        "   (\\mu_2 - \\mu) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix}\n",
        "$$\n",
        "$$\n",
        "   (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 3 & 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "   Учитывая $N_2 = 2$:\n",
        "$$\n",
        "   N_2 (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Итоговая межклассовая матрица рассеяния $S_B$**:\n",
        "$$\n",
        "   S_B = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} + \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} = \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 4: Решение обобщённой задачи на собственные значения\n",
        "\n",
        "Теперь мы переходим к ключевому этапу линейного дискриминантного анализа (LDA) — решению обобщённой задачи на собственные значения. Наша цель — найти вектор $w$, который максимизирует критерий Фишера:\n",
        "\n",
        "$$\n",
        "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "$$\n",
        "\n",
        "Это эквивалентно решению уравнения:\n",
        "\n",
        "$$\n",
        "S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $S_W$ — внутриклассовая матрица рассеяния.\n",
        "- $S_B$ — межклассовая матрица рассеяния.\n",
        "- $\\lambda$ — собственное значение, соответствующее собственному вектору $w$.\n",
        "\n",
        "\n",
        "\n",
        "#### Подробное решение\n",
        "\n",
        "1. **Проверка обратимости матрицы $S_W$**:\n",
        "   Прежде чем решать уравнение, необходимо убедиться, что матрица $S_W$ обратима. В нашем примере:\n",
        "\n",
        "$$\n",
        "   S_W = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Определитель этой матрицы равен нулю, так как все строки линейно зависимы. Это означает, что $S_W$ — вырожденная матрица, и обратной матрицы $S_W^{-1}$ не существует.\n",
        "\n",
        "2. **Регуляризация матрицы $S_W$**:\n",
        "   Чтобы обойти проблему вырожденности, можно добавить небольшое значение $\\alpha$ к диагональным элементам матрицы $S_W$. Это называется регуляризацией:\n",
        "\n",
        "$$\n",
        "   S_W' = S_W + \\alpha I\n",
        "$$\n",
        "\n",
        "   где $I$ — единичная матрица, а $\\alpha$ — параметр регуляризации (например, $\\alpha = 0.01$).\n",
        "\n",
        "   После регуляризации:\n",
        "\n",
        "$$\n",
        "   S_W' = \\begin{bmatrix} 9.01 & 9 & 9 \\\\ 9 & 9.01 & 9 \\\\ 9 & 9 & 9.01 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Теперь матрица $S_W'$ обратима.\n",
        "\n",
        "3. **Вычисление обратной матрицы $S_W'^{-1}$**:\n",
        "   Обратная матрица $S_W'^{-1}$ вычисляется с использованием стандартных методов линейной алгебры. В данном случае:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} \\approx \\begin{bmatrix} 111.11 & -111.11 & -111.11 \\\\ -111.11 & 111.11 & -111.11 \\\\ -111.11 & -111.11 & 111.11 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   (Значения округлены для упрощения.)\n",
        "\n",
        "4. **Вычисление матрицы $S_W'^{-1} S_B$**:\n",
        "   Теперь вычислим матрицу $S_W'^{-1} S_B$:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} S_B = S_W'^{-1} \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Умножение даёт:\n",
        "\n",
        "$$\n",
        "   S_W'^{-1} S_B \\approx \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Это означает, что все собственные значения $\\lambda$ равны нулю, что указывает на отсутствие полезных направлений для разделения классов.\n",
        "\n",
        "5. **Интерпретация результата**:\n",
        "   В данном примере все собственные значения равны нулю, что говорит о том, что данные не могут быть разделены с помощью LDA. Это связано с тем, что:\n",
        "   - Все точки в каждом классе идентичны (разброс внутри классов равен нулю).\n",
        "   - Средние значения классов также не различаются (разброс между классами равен нулю).\n",
        "\n",
        "\n",
        "\n",
        "#### Общий алгоритм решения обобщённой задачи на собственные значения\n",
        "\n",
        "Если матрица $S_W$ не вырождена, то решение обобщённой задачи на собственные значения выполняется следующим образом:\n",
        "\n",
        "1. **Вычисление обратной матрицы $S_W^{-1}$**:\n",
        "   Если $S_W$ обратима, находим $S_W^{-1}$.\n",
        "\n",
        "2. **Вычисление матрицы $S_W^{-1} S_B$**:\n",
        "   Умножаем $S_W^{-1}$ на $S_B$.\n",
        "\n",
        "3. **Нахождение собственных значений и векторов**:\n",
        "   Решаем уравнение:\n",
        "\n",
        "$$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "   Это стандартная задача на собственные значения, которая решается с использованием численных методов (например, метода QR-разложения).\n",
        "\n",
        "4. **Выбор собственных векторов**:\n",
        "   Собственные векторы, соответствующие наибольшим собственным значениям, дают направления, которые наилучшим образом разделяют классы.\n",
        "\n",
        "5. **Проекция данных**:\n",
        "   Данные проецируются на новое пространство с использованием выбранных собственных векторов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Реализуем пример линейного дискриминантного анализа (LDA) на Python. Сначала создадим класс LDA с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация LDA с нуля\n"
      ],
      "metadata": {
        "id": "YqdpKK8gX1UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LDA:\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.w = None  # Вектор весов (направление проекции)\n",
        "        self.alpha = alpha  # Параметр регуляризации\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели LDA.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :param y: Вектор меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        classes = np.unique(y)\n",
        "\n",
        "        # Вычисление средних значений для каждого класса\n",
        "        class_means = []\n",
        "        for c in classes:\n",
        "            class_means.append(np.mean(X[y == c], axis=0))\n",
        "        class_means = np.array(class_means)\n",
        "\n",
        "        # Общее среднее значение\n",
        "        overall_mean = np.mean(X, axis=0)\n",
        "\n",
        "        # Внутриклассовая матрица рассеяния (S_W)\n",
        "        S_W = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            class_scatter = np.zeros((n_features, n_features))\n",
        "            for sample in X[y == c]:\n",
        "                sample, mean = sample.reshape(n_features, 1), mean.reshape(n_features, 1)\n",
        "                class_scatter += (sample - mean).dot((sample - mean).T)\n",
        "            S_W += class_scatter\n",
        "\n",
        "        # Регуляризация матрицы S_W\n",
        "        S_W += self.alpha * np.eye(n_features)\n",
        "\n",
        "        # Межклассовая матрица рассеяния (S_B)\n",
        "        S_B = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            n_c = X[y == c].shape[0]\n",
        "            mean = mean.reshape(n_features, 1)\n",
        "            overall_mean = overall_mean.reshape(n_features, 1)\n",
        "            S_B += n_c * (mean - overall_mean).dot((mean - overall_mean).T)\n",
        "\n",
        "        # Решение обобщённой задачи на собственные значения\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "        # Сортировка собственных векторов по убыванию собственных значений\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "        # Выбор первого собственного вектора (направление проекции)\n",
        "        self.w = eigenvectors[:, 0]\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Проекция данных на новое пространство.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :return: Проекция данных (n_samples, 1).\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Создаём объект LDA с регуляризацией\n",
        "lda_custom = LDA(alpha=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "lda_custom.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_transformed_custom = lda_custom.transform(X)\n",
        "print(\"Проекция данных (с нуля, с регуляризацией):\\n\", X_transformed_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQlr0c2IZrCb",
        "outputId": "b9c3ee1a-8816-46b0-ccdf-b758e3be6dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (с нуля, с регуляризацией):\n",
            " [ 3.46410162  8.66025404 13.85640646 19.05255888]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn"
      ],
      "metadata": {
        "id": "fnzS_bzFZ25k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Готовое решение LDA\n",
        "lda_sklearn = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Данные\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Обучаем модель\n",
        "lda_sklearn.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_transformed_sklearn = lda_sklearn.transform(X)\n",
        "print(\"Проекция данных (scikit-learn):\\n\", X_transformed_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7u671LZ3eD",
        "outputId": "65e4036f-1a9b-46b0-d420-0b0c4486becf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (scikit-learn):\n",
            " [[-2.12132034]\n",
            " [-0.70710678]\n",
            " [ 0.70710678]\n",
            " [ 2.12132034]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.2 Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Квадратичный дискриминантный анализ (QDA, Quadratic Discriminant Analysis) — это метод классификации, который используется для разделения объектов на классы на основе их признаков. QDA является обобщением линейного дискриминантного анализа (LDA), но, в отличие от LDA, QDA не предполагает, что ковариационные матрицы всех классов одинаковы. Это позволяет QDA учитывать более сложные структуры данных, где классы могут иметь различную форму и ориентацию в пространстве признаков.\n",
        "\n",
        "\n",
        "\n",
        "#### Основные понятия\n",
        "\n",
        "1. **Классификация**:\n",
        "   Задача классификации заключается в отнесении объекта к одному из нескольких классов на основе его признаков. В QDA предполагается, что данные каждого класса распределены по многомерному нормальному распределению.\n",
        "\n",
        "2. **Многомерное нормальное распределение**:\n",
        "   Для класса $k$ вероятность наблюдения вектора признаков $\\mathbf{x}$ задается плотностью многомерного нормального распределения:\n",
        "$$\n",
        "   P(\\mathbf{x} | Y = k) = \\frac{1}{(2\\pi)^{p/2} |\\mathbf{\\Sigma}_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^T \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\right),\n",
        "$$\n",
        "   где:\n",
        "   - $\\mathbf{x}$ — вектор признаков размерности $p$,\n",
        "   - $Y = k$ — принадлежность объекта к классу $k$,\n",
        "   - $\\mathbf{\\mu}_k$ — вектор средних значений для класса $k$,\n",
        "   - $\\mathbf{\\Sigma}_k$ — ковариационная матрица для класса $k$,\n",
        "   - $|\\mathbf{\\Sigma}_k|$ — определитель ковариационной матрицы.\n",
        "\n",
        "3. **Апостериорная вероятность**:\n",
        "   QDA использует теорему Байеса для вычисления апостериорной вероятности принадлежности объекта к классу $k$:\n",
        "$$\n",
        "   P(Y = k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | Y = k) P(Y = k)}{P(\\mathbf{x})},\n",
        "$$\n",
        "   где:\n",
        "   - $P(Y = k)$ — априорная вероятность класса $k$,\n",
        "   - $P(\\mathbf{x})$ — нормировочная константа, которая не зависит от класса.\n",
        "\n",
        "4. **Решающее правило**:\n",
        "   Объект $\\mathbf{x}$ относится к классу $k$, для которого апостериорная вероятность $P(Y = k | \\mathbf{x})$ максимальна. Это эквивалентно максимизации логарифма апостериорной вероятности:\n",
        "$$\n",
        "   \\delta_k(\\mathbf{x}) = \\log P(Y = k) - \\frac{1}{2} \\log |\\mathbf{\\Sigma}_k| - \\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^T \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k).\n",
        "$$\n",
        "   Здесь $\\delta_k(\\mathbf{x})$ называется дискриминантной функцией для класса $k$.\n",
        "\n",
        "\n",
        "\n",
        "#### Сравнение QDA и LDA\n",
        "\n",
        "Квадратичный дискриминантный анализ (QDA) очень похож на линейный дискриминантный анализ (LDA), но с одним ключевым отличием: в QDA мы отказываемся от предположения, что средние значения и ковариационные матрицы всех классов одинаковы. Вместо этого для каждого класса $y$ мы оцениваем отдельные средние значения $\\mu_y$ и ковариационные матрицы $\\Sigma_y$. Это позволяет QDA учитывать более сложные структуры данных, где классы могут иметь различную форму и ориентацию в пространстве признаков.\n",
        "\n",
        "\n",
        "\n",
        "#### Оценка ковариационной матрицы для каждого класса\n",
        "\n",
        "Для каждого класса $y$ ковариационная матрица $\\Sigma_y$ оценивается по формуле:\n",
        "$$\n",
        "\\Sigma_y = \\frac{1}{N_y - 1} \\sum_{y_i = y} (x_i - \\mu_y)(x_i - \\mu_y)^T,\n",
        "$$\n",
        "где:\n",
        "- $N_y$ — количество объектов в классе $y$,\n",
        "- $x_i$ — вектор признаков $i$-го объекта,\n",
        "- $\\mu_y$ — вектор средних значений для класса $y$, вычисляемый как:\n",
        "  $$\n",
        "  \\mu_y = \\frac{1}{N_y} \\sum_{y_i = y} x_i.\n",
        "  $$\n",
        "\n",
        "Эта формула аналогична оценке ковариационной матрицы в LDA, но в QDA она вычисляется отдельно для каждого класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Квадратичная дискриминантная функция\n",
        "\n",
        "Дискриминантная функция $\\delta_k(x)$ в QDA имеет квадратичную форму и выражается следующим образом:\n",
        "$$\n",
        "\\delta_k(x) = \\log \\pi_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k + x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} x^T \\Sigma_k^{-1} x - \\frac{1}{2} \\log |\\Sigma_k|.\n",
        "$$\n",
        "\n",
        "Давайте разберем каждый член этой функции:\n",
        "\n",
        "1. **$\\log \\pi_k$**:\n",
        "   Это логарифм априорной вероятности класса $k$. Априорная вероятность $\\pi_k$ оценивается как доля объектов класса $k$ в обучающей выборке:\n",
        "$$\n",
        "   \\pi_k = \\frac{N_k}{N},\n",
        "$$\n",
        "   где $N_k$ — количество объектов в классе $k$, а $N$ — общее количество объектов.\n",
        "\n",
        "2. **$-\\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k$**:\n",
        "   Этот член представляет собой квадратичную форму, связанную с центром класса $k$. Он учитывает положение центра класса в пространстве признаков.\n",
        "\n",
        "3. **$x^T \\Sigma_k^{-1} \\mu_k$**:\n",
        "   Это линейный член, который связывает объект $x$ с центром класса $k$. Он учитывает \"близость\" объекта $x$ к центру класса $k$ в метрике, заданной ковариационной матрицей $\\Sigma_k$.\n",
        "\n",
        "4. **$-\\frac{1}{2} x^T \\Sigma_k^{-1} x$**:\n",
        "   Это квадратичный член, который учитывает \"расстояние\" объекта $x$ до центра класса $k$ в метрике, заданной ковариационной матрицей $\\Sigma_k$.\n",
        "\n",
        "5. **$-\\frac{1}{2} \\log |\\Sigma_k|$**:\n",
        "   Этот член учитывает \"объём\" ковариационной матрицы класса $k$. Чем больше разброс данных в классе, тем меньше значение этого члена.\n",
        "\n",
        "\n",
        "\n",
        "#### Вывод дискриминантной функции\n",
        "\n",
        "Дискриминантная функция $\\delta_k(x)$ получается путём логарифмирования апостериорной вероятности $P(Y = k | x)$ и упрощения выражения. Напомним, что апостериорная вероятность задаётся по теореме Байеса:\n",
        "$$\n",
        "P(Y = k | x) = \\frac{P(x | Y = k) P(Y = k)}{P(x)}.\n",
        "$$\n",
        "\n",
        "Логарифмируя это выражение и опуская нормировочную константу $P(x)$, получаем:\n",
        "$$\n",
        "\\log P(Y = k | x) = \\log P(x | Y = k) + \\log P(Y = k).\n",
        "$$\n",
        "\n",
        "Подставляя выражение для плотности многомерного нормального распределения $P(x | Y = k)$, получаем:\n",
        "$$\n",
        "\\log P(Y = k | x) = -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k + \\text{const}.\n",
        "$$\n",
        "\n",
        "Раскрывая квадратичную форму $(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)$, получаем:\n",
        "$$\n",
        "(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) = x^T \\Sigma_k^{-1} x - 2 x^T \\Sigma_k^{-1} \\mu_k + \\mu_k^T \\Sigma_k^{-1} \\mu_k.\n",
        "$$\n",
        "\n",
        "Подставляя это обратно в выражение для $\\log P(Y = k | x)$, получаем окончательную формулу для дискриминантной функции:\n",
        "$$\n",
        "\\delta_k(x) = \\log \\pi_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k + x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} x^T \\Sigma_k^{-1} x - \\frac{1}{2} \\log |\\Sigma_k|.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и недостатки QDA\n",
        "\n",
        "**Преимущества**:\n",
        "1. QDA позволяет моделировать более сложные границы между классами, чем LDA, благодаря использованию различных ковариационных матриц.\n",
        "2. Хорошо работает, когда классы имеют различную форму и ориентацию в пространстве признаков.\n",
        "\n",
        "**Недостатки**:\n",
        "1. QDA требует оценки большего количества параметров, чем LDA, что может привести к переобучению при малом количестве данных.\n",
        "2. Вычислительная сложность выше, чем у LDA, из-за необходимости обращения и хранения отдельных ковариационных матриц для каждого класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Заключение\n",
        "\n",
        "Квадратичный дискриминантный анализ (QDA) — это мощный метод классификации, который обобщает линейный дискриминантный анализ, позволяя учитывать различные ковариационные структуры для каждого класса. QDA особенно полезен в задачах, где классы имеют сложные формы распределений. Однако его применение требует достаточного количества данных для точной оценки параметров модели.\n",
        "\n",
        "\n",
        "\n",
        "#### Практическое применение QDA\n",
        "\n",
        "После получения окончательной формулы для дискриминантной функции в QDA, её можно использовать для классификации новых объектов. Для этого необходимо:\n",
        "1. Оценить параметры модели (средние значения, ковариационные матрицы и априорные вероятности).\n",
        "2. Вычислить значение дискриминантной функции для каждого класса.\n",
        "3. Отнести объект к классу с максимальным значением дискриминантной функции.\n",
        "\n",
        "QDA является мощным инструментом для задач классификации, особенно когда классы имеют различную форму и ориентацию в пространстве признаков. Однако его применение требует внимательного подхода к оценке параметров и учёта вычислительной сложности.\n",
        "\n",
        "\n",
        "\n",
        "#### Дополнительные замечания\n",
        "\n",
        "1. **Регуляризация**:\n",
        "   В случаях, когда количество данных ограничено, можно использовать регуляризацию для предотвращения переобучения. Например, можно добавить небольшое значение к диагонали ковариационной матрицы, чтобы сделать её более устойчивой.\n",
        "\n",
        "2. **Вычислительная эффективность**:\n",
        "   Для ускорения вычислений можно использовать методы оптимизации, такие как разложение Холецкого или LU-разложение, для обращения ковариационных матриц.\n",
        "\n",
        "3. **Интерпретация результатов**:\n",
        "   QDA позволяет моделировать сложные границы между классами, но интерпретация этих границ может быть затруднена из-за квадратичной природы дискриминантной функции.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, квадратичный дискриминантный анализ (QDA) представляет собой гибкий и мощный метод классификации, который может быть эффективно использован в задачах с нелинейными границами между классами. Однако его применение требует тщательной подготовки данных и внимательного подхода к оценке параметров модели.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что первые две точки принадлежат классу 0, а последние две — классу 1. Таким образом, метки классов будут:\n",
        "\n",
        "$$\n",
        "y = \\begin{bmatrix}\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Разделение данных по классам\n",
        "\n",
        "Разделим данные на два класса:\n",
        "\n",
        "- **Класс 0**:\n",
        "  $$\n",
        "  X_0 = \\begin{bmatrix}\n",
        "  1 & 2 & 3 \\\\\n",
        "  4 & 5 & 6\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Класс 1**:\n",
        "  $$\n",
        "  X_1 = \\begin{bmatrix}\n",
        "  7 & 8 & 9 \\\\\n",
        "  10 & 11 & 12\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление средних значений для каждого класса\n",
        "\n",
        "Среднее значение для каждого класса вычисляется как среднее арифметическое по всем точкам класса.\n",
        "\n",
        "1. **Класс 0**:\n",
        "$$\n",
        "   \\mu_0 = \\frac{1}{2} \\begin{bmatrix}\n",
        "   1 + 4 \\\\\n",
        "   2 + 5 \\\\\n",
        "   3 + 6\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   2.5 \\\\\n",
        "   3.5 \\\\\n",
        "   4.5\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Класс 1**:\n",
        "$$\n",
        "   \\mu_1 = \\frac{1}{2} \\begin{bmatrix}\n",
        "   7 + 10 \\\\\n",
        "   8 + 11 \\\\\n",
        "   9 + 12\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   8.5 \\\\\n",
        "   9.5 \\\\\n",
        "   10.5\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Вычисление ковариационных матриц для каждого класса\n",
        "\n",
        "Ковариационная матрица для каждого класса вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "\\Sigma_k = \\frac{1}{N_k - 1} \\sum_{i=1}^{N_k} (x_i - \\mu_k)(x_i - \\mu_k)^T,\n",
        "$$\n",
        "\n",
        "где $N_k$ — количество точек в классе $k$, $x_i$ — $i$-я точка класса $k$, $\\mu_k$ — среднее значение класса $k$.\n",
        "\n",
        "1. **Класс 0**:\n",
        "   - Центрируем данные:\n",
        "  $$\n",
        "     X_0 - \\mu_0 = \\begin{bmatrix}\n",
        "     1 - 2.5 & 2 - 3.5 & 3 - 4.5 \\\\\n",
        "     4 - 2.5 & 5 - 3.5 & 6 - 4.5\n",
        "     \\end{bmatrix} = \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5 \\\\\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "   - Вычисляем ковариационную матрицу:\n",
        "  $$\n",
        "     \\Sigma_0 = \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     -1.5 \\\\\n",
        "     -1.5 \\\\\n",
        "     -1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5\n",
        "     \\end{bmatrix} + \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     1.5 \\\\\n",
        "     1.5 \\\\\n",
        "     1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "  $$\n",
        "     \\Sigma_0 = \\begin{bmatrix}\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "2. **Класс 1**:\n",
        "   - Центрируем данные:\n",
        "  $$\n",
        "     X_1 - \\mu_1 = \\begin{bmatrix}\n",
        "     7 - 8.5 & 8 - 9.5 & 9 - 10.5 \\\\\n",
        "     10 - 8.5 & 11 - 9.5 & 12 - 10.5\n",
        "     \\end{bmatrix} = \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5 \\\\\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "   - Вычисляем ковариационную матрицу:\n",
        "  $$\n",
        "     \\Sigma_1 = \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     -1.5 \\\\\n",
        "     -1.5 \\\\\n",
        "     -1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     -1.5 & -1.5 & -1.5\n",
        "     \\end{bmatrix} + \\frac{1}{2 - 1} \\begin{bmatrix}\n",
        "     1.5 \\\\\n",
        "     1.5 \\\\\n",
        "     1.5\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "     1.5 & 1.5 & 1.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "  $$\n",
        "     \\Sigma_1 = \\begin{bmatrix}\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5 \\\\\n",
        "     4.5 & 4.5 & 4.5\n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 4: Вычисление априорных вероятностей\n",
        "\n",
        "Априорная вероятность $\\pi_k$ для каждого класса вычисляется как доля точек класса $k$ в общем количестве точек:\n",
        "\n",
        "$$\n",
        "\\pi_k = \\frac{N_k}{N},\n",
        "$$\n",
        "\n",
        "где $N_k$ — количество точек в классе $k$, $N$ — общее количество точек.\n",
        "\n",
        "1. **Класс 0**:\n",
        "$$\n",
        "   \\pi_0 = \\frac{2}{4} = 0.5\n",
        "$$\n",
        "\n",
        "2. **Класс 1**:\n",
        "$$\n",
        "   \\pi_1 = \\frac{2}{4} = 0.5\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 5: Вычисление дискриминантной функции\n",
        "\n",
        "Дискриминантная функция для класса $k$ имеет вид:\n",
        "\n",
        "$$\n",
        "\\delta_k(x) = \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k).\n",
        "$$\n",
        "\n",
        "1. **Вычисление определителя ковариационной матрицы**:\n",
        "   - Для обоих классов $\\Sigma_0$ и $\\Sigma_1$ определитель равен 0, так как матрицы вырождены (все строки и столбцы линейно зависимы). Это означает, что данные лежат на прямой линии, и QDA не может быть применён напрямую. В реальных задачах для таких случаев используется регуляризация (например, добавление небольшого значения к диагонали матрицы).\n",
        "\n",
        "2. **Регуляризация**:\n",
        "   Добавим небольшое значение $\\epsilon = 0.01$ к диагонали ковариационных матриц:\n",
        "$$\n",
        "   \\Sigma_0 = \\begin{bmatrix}\n",
        "   4.5 + 0.01 & 4.5 & 4.5 \\\\\n",
        "   4.5 & 4.5 + 0.01 & 4.5 \\\\\n",
        "   4.5 & 4.5 & 4.5 + 0.01\n",
        "   \\end{bmatrix}, \\quad\n",
        "   \\Sigma_1 = \\begin{bmatrix}\n",
        "   4.5 + 0.01 & 4.5 & 4.5 \\\\\n",
        "   4.5 & 4.5 + 0.01 & 4.5 \\\\\n",
        "   4.5 & 4.5 & 4.5 + 0.01\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "3. **Вычисление обратной матрицы**:\n",
        "   Обратные матрицы $\\Sigma_0^{-1}$ и $\\Sigma_1^{-1}$ могут быть вычислены с использованием численных методов.\n",
        "\n",
        "4. **Вычисление дискриминантной функции**:\n",
        "   Для новой точки $x = \\begin{bmatrix} 5 & 6 & 7 \\end{bmatrix}$ вычислим $\\delta_0(x)$ и $\\delta_1(x)$.\n",
        "\n",
        "Таким образом, в данном примере мы рассмотрели основные шаги QDA, включая вычисление средних значений, ковариационных матриц и дискриминантной функции. Однако из-за вырожденности ковариационных матриц потребовалась регуляризация. В реальных задачах QDA применяется к данным, где классы имеют различные ковариационные структуры, и регуляризация помогает избежать проблем с вырожденными матрицами.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####Реализация квадратичного дискриминантного анализа (QDA) на Python\n",
        "\n",
        "1. Реализация QDA с нуля\n"
      ],
      "metadata": {
        "id": "epooty_dseQH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6luAZFgew0_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components  # Количество главных компонент\n",
        "        self.components = None  # Главные компоненты\n",
        "        self.mean = None  # Средние значения для центрирования данных\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Шаг 1: Центрирование данных\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        # Шаг 2: Вычисление ковариационной матрицы\n",
        "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "        # Шаг 3: Вычисление собственных значений и собственных векторов\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "        # Шаг 4: Сортировка собственных векторов по убыванию собственных значений\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        self.components = eigenvectors[:, sorted_indices[:self.n_components]]\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Центрирование данных\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        # Проецирование данных на главные компоненты\n",
        "        X_projected = np.dot(X_centered, self.components)\n",
        "        return X_projected\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Создаем объект PCA с 1 компонентой\n",
        "pca = PCA(n_components=1)\n",
        "\n",
        "# Обучаем модель на данных\n",
        "pca.fit(X)\n",
        "\n",
        "# Преобразуем данные\n",
        "X_projected = pca.transform(X)\n",
        "\n",
        "print(\"Исходные данные:\\n\", X)\n",
        "print(\"Данные после PCA (1 компонента):\\n\", X_projected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgDnhh94vFO3",
        "outputId": "58a27496-dd16-4921-94cf-daced11303b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходные данные:\n",
            " [[ 1  2  3]\n",
            " [ 4  5  6]\n",
            " [ 7  8  9]\n",
            " [10 11 12]]\n",
            "Данные после PCA (1 компонента):\n",
            " [[-7.79422863]\n",
            " [-2.59807621]\n",
            " [ 2.59807621]\n",
            " [ 7.79422863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения (scikit-learn)"
      ],
      "metadata": {
        "id": "SJO8xjoGvIXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class QDA:\n",
        "    def __init__(self, reg_param=0.01):\n",
        "        self.classes = None\n",
        "        self.priors = None\n",
        "        self.means = None\n",
        "        self.covariances = None\n",
        "        self.reg_param = reg_param  # Параметр регуляризации\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели QDA.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :param y: Вектор меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Инициализация\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        self.means = np.zeros((n_classes, n_features))\n",
        "        self.covariances = np.zeros((n_classes, n_features, n_features))\n",
        "\n",
        "        # Вычисление априорных вероятностей, средних и ковариационных матриц\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            self.priors[i] = X_c.shape[0] / n_samples\n",
        "            self.means[i] = np.mean(X_c, axis=0)\n",
        "            self.covariances[i] = np.cov(X_c, rowvar=False)\n",
        "\n",
        "            # Регуляризация: добавление небольшого значения к диагонали\n",
        "            self.covariances[i] += self.reg_param * np.eye(n_features)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказание классов для новых данных.\n",
        "        :param X: Матрица признаков (n_samples, n_features).\n",
        "        :return: Вектор предсказанных меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        posteriors = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        # Вычисление апостериорных вероятностей для каждого класса\n",
        "        for i in range(n_classes):\n",
        "            likelihood = multivariate_normal.pdf(X, mean=self.means[i], cov=self.covariances[i])\n",
        "            posteriors[:, i] = self.priors[i] * likelihood\n",
        "\n",
        "        # Выбор класса с максимальной апостериорной вероятностью\n",
        "        return self.classes[np.argmax(posteriors, axis=1)]\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "# Создаем объект QDA с регуляризацией\n",
        "qda = QDA(reg_param=0.01)\n",
        "\n",
        "# Обучаем модель\n",
        "qda.fit(X, y)\n",
        "\n",
        "# Предсказываем классы\n",
        "X_test = np.array([\n",
        "    [5, 6, 7],\n",
        "    [8, 9, 10]\n",
        "])\n",
        "predictions = qda.predict(X_test)\n",
        "\n",
        "print(\"Предсказанные классы:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOE5y1yvLiU",
        "outputId": "ebc8cf49-4164-42b1-da6d-482be9a79034"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные классы: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения (scikit-learn)"
      ],
      "metadata": {
        "id": "EYy60jE6w4m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "# Создаем объект QDA с увеличенным параметром регуляризации\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param=0.1)  # Увеличиваем reg_param\n",
        "\n",
        "# Обучаем модель\n",
        "qda.fit(X, y)\n",
        "\n",
        "# Предсказываем классы\n",
        "X_test = np.array([\n",
        "    [5, 6, 7],\n",
        "    [8, 9, 10]\n",
        "])\n",
        "predictions = qda.predict(X_test)\n",
        "\n",
        "print(\"Предсказанные классы:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWqSWCDIw75N",
        "outputId": "c38aa19d-2ab5-479b-ebad-e73851fdf0ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные классы: [0 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат `Предсказанные классы: [0 1]` означает, что модель квадратичного дискриминантного анализа (QDA) предсказала классы для двух тестовых точек:\n",
        "\n",
        "1. Для точки `[5, 6, 7]` модель предсказала класс **0**.\n",
        "2. Для точки `[8, 9, 10]` модель предсказала класс **1**.\n",
        "\n",
        "Этот результат говорит о том, что модель успешно разделила данные на два класса в соответствии с обучением. Давайте разберём, что это означает и как интерпретировать результат.\n",
        "\n",
        "### Интерпретация результата\n",
        "\n",
        "1. **Обучение модели**:\n",
        "   - Модель QDA была обучена на данных, где:\n",
        "     - Точки `[1, 2, 3]` и `[4, 5, 6]` принадлежат классу **0**.\n",
        "     - Точки `[7, 8, 9]` и `[10, 11, 12]` принадлежат классу **1**.\n",
        "\n",
        "2. **Предсказание для новых данных**:\n",
        "   - Для точки `[5, 6, 7]` модель решила, что она ближе к классу **0**.\n",
        "   - Для точки `[8, 9, 10]` модель решила, что она ближе к классу **1**.\n",
        "\n",
        "3. **Граница между классами**:\n",
        "   - QDA строит квадратичную границу между классами, учитывая различные ковариационные матрицы для каждого класса.\n",
        "   - В данном случае граница проходит между точками `[5, 6, 7]` и `[8, 9, 10]`.\n",
        "\n",
        "\n",
        "\n",
        "### Почему именно такие предсказания?\n",
        "\n",
        "1. **Класс 0**:\n",
        "   - Точка `[5, 6, 7]` ближе к данным класса **0** (`[1, 2, 3]` и `[4, 5, 6]`), поэтому модель отнесла её к классу **0**.\n",
        "\n",
        "2. **Класс 1**:\n",
        "   - Точка `[8, 9, 10]` ближе к данным класса **1** (`[7, 8, 9]` и `[10, 11, 12]`), поэтому модель отнесла её к классу **1**.\n",
        "\n",
        "\n",
        "\n",
        "### Что можно улучшить?\n",
        "\n",
        "1. **Регуляризация**:\n",
        "   - Если данные имеют вырожденные ковариационные матрицы (как в вашем примере), увеличьте параметр `reg_param`, чтобы избежать предупреждений и улучшить устойчивость модели.\n",
        "\n",
        "2. **Уменьшение размерности**:\n",
        "   - Если данные лежат на прямой линии (как в вашем примере), можно использовать PCA для уменьшения размерности. Это упростит задачу и устранит вырожденность.\n",
        "\n",
        "3. **Визуализация**:\n",
        "   - Если данные имеют 2 или 3 признака, можно визуализировать границу между классами, чтобы лучше понять, как модель принимает решения.\n",
        "\n",
        "\n",
        "\n",
        "### Пример визуализации (для 2D-данных)\n",
        "\n",
        "Если бы данные были двумерными, мы могли бы построить график с границей между классами. Например:\n"
      ],
      "metadata": {
        "id": "aJpdlTyox7iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "# Пример 2D-данных\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 3],\n",
        "    [6, 7],\n",
        "    [7, 8],\n",
        "    [8, 8]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "# Обучаем модель\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
        "qda.fit(X, y)\n",
        "\n",
        "# Визуализация\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "Z = qda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
        "plt.title(\"Граница между классами (QDA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "STVVYB6Px4Q0",
        "outputId": "b7f0c40f-3ada-4d47-ec63-1b71fabca496"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGzCAYAAABzfl4TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3GklEQVR4nO3deXhU5cH+8XsSzCSEJAQMS4QEiCyyFkEpm4CAmAIFrCAYIYCvWgUB/UkBrSJYCNSlUqoIikAFRJFVrCIgi1rRsIPsyBIQZc1OAmTO7w/fzMuQBDLkSU4Svp/rmqudM8/Muc9kcO45q8OyLEsAAAAG+NgdAAAAlB4UCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAKOESEhLk7++vb7/91u4o19S3b1/16dPH7hgoZBQLXNfs2bPlcDjyvB0/ftzuiMBNbfz48WrRooVat26d47EVK1bo/vvvV8WKFeXv7686depo5MiROnfuXI6xAwcO9Pi3Xa5cOdWqVUsPPvigFi1aJJfLlWeGxMRE+fv7y+FwaM+ePbmOGTVqlBYtWqTt27ff+MKi2CtjdwCUHOPHj1fNmjVzTK9QoYINaQBI0unTpzVnzhzNmTMnx2PPPfecXn/9dTVp0kSjRo1ShQoVtGXLFk2dOlUfffSR1qxZo9q1a3s8x+l06r333pMkXbhwQUePHtWnn36qBx98UO3bt9eyZcsUHBycY14LFy6Uw+FQlSpVNG/ePP3tb3/LMaZp06Zq3ry5Xn/9df373/829A6g2LGA65g1a5YlyYqPj7c7CoCrvPHGG1ZAQICVkpLiMX3+/PmWJOuhhx6yLl++7PHY999/b5UtW9Zq0qSJdenSJff02NhYKzAwMNf5xMXFWZKsPn365Pr4PffcYz3wwAPWM888Y9WsWTPPvK+99poVGBiYIy9KDzaFwJjsTSYbNmzQE088oYoVKyo4OFgDBgzQ+fPnPcYuW7ZMXbt2VXh4uJxOp6KiovTKK68oKyvLY1z79u3VsGHDHPN67bXX5HA4dOTIEfe0GjVqaODAgR7jsn9F1ahRwz3tyJEjcjgceu2113K8bsOGDdW+fXuPaV9//bV69+6tiIgIOZ1OVa9eXc8884wuXLiQ7/fEz89Pp0+f9njsu+++c69y3rRpk8dj33//ve6//36FhISobNmyateuXY7t52PGjMmxXX3dunVyOBxat26de9q3334rf39/jRkzJke+9u3b57p56+WXX3aPadeunZo0aZLr8tWtW1ddunS55nuQ29/l8ccfl7+/v0fOK/Nffbvy73fx4kW99NJLatasmUJCQhQYGKi2bdtq7dq1Oebtcrk0ZcoUNWrUSP7+/goLC9P999+f4/2eO3eu7r77bpUtW1ahoaG655579OWXX7of9+bz6nA41LNnzxxZnnjiCTkcDo/Pc/Zncfbs2R5jhwwZIofDkeN9y83SpUvVokULlStXzmP6uHHjFBoaqhkzZsjX19fjsbvvvlujRo3S9u3btXjx4uvOQ5JGjx6t++67TwsXLtT+/fs9Hjt27Ji+/vpr9e3bV3379tXhw4f13//+N9fX6dy5s9LS0rRq1ap8zRclD8UCxg0dOlR79uzRyy+/rAEDBmjevHnq2bOnLMtyj5k9e7bKlSunZ599VlOmTFGzZs300ksvafTo0cZyXL58WS+88EKBX2fhwoVKT0/Xk08+qalTp6pLly6aOnWqBgwYkO/X8PX11dy5cz2mzZo1S/7+/jnGfvXVV7rnnnuUnJyssWPHauLEiUpMTNS9996rH374wT1u4sSJ6tatm3r16qXDhw/nOt/Dhw+rZ8+e6tatmyZOnJjrmGrVqumDDz7QBx98oGnTpuV4vH///tqxY4d27drlMT0+Pl779+/XI488ct3lv9LYsWM1c+ZMzZ07N0eJy/b888+7M7Vt29bjseTkZL333ntq3769Jk+erJdfflmnT59Wly5dtG3bNo+xjz76qEaMGKHq1atr8uTJGj16tPz9/bVx40b3mHHjxql///665ZZbNH78eI0bN07Vq1fXV1995R7jzefV399fn332mU6dOuWeduHCBX300Ue5/r2vdvDgQb377rvXHSdJly5dUnx8vO68806P6QcOHNC+ffvUo0ePXDdbSHJ/fj/99NN8zUv67bNgWVaOUvDhhx8qMDBQ3bp10913362oqCjNmzcv19eoX7++AgICiv2OpigAu1eZoPjL76aQ7HHNmjWzLl686J7+97//3ZJkLVu2zD0tPT09x/OfeOIJq2zZslZGRoZ7Wrt27awGDRrkGPvqq69akqzDhw+7p0VGRlqxsbHu+2+//bbldDqtDh06WJGRke7phw8ftiRZr776ao7XbdCggdWuXTuPablljYuLsxwOh3X06NEcj10p+z3p16+f1ahRI/f0tLQ0Kzg42Hr44Yc93luXy2XVrl3b6tKli+VyuTwy1KxZ0+rcubPH66elpVnNmze3GjRoYCUlJVlr1661JFlr1661EhMTrfr161t33XVXrstgWZbVqlUrq2HDhu77p0+ftiRZY8eOdU9LTEy0/P39rVGjRnk8d9iwYVZgYKCVmpp6zffgyr/L9OnTLUnW1KlTcx27atUqS5K1fv1697TY2FiPv9/ly5etzMxMj+edP3/eqly5sjV48GD3tK+++sqSZA0bNizHfLLf2wMHDlg+Pj5Wr169rKysrFzHWJb3n9fGjRtbr732mnv6Bx98YFWrVs1q27atx+c5+7M4a9Ys97Q+ffpYDRs2tKpXr+7xec7NwYMHc30/ly5dakmy/vGPf1zz+cHBwdadd97pvn+tTSGWZVlbt261JFnPPPOMx/RGjRpZMTEx7vvPP/+8deutt3psZrlSnTp1rOjo6GtmQ8nFGgsY9/jjj+uWW25x33/yySdVpkwZ/ec//3FPCwgIcP//lJQUnTlzRm3btlV6err27t3r8XpZWVk6c+aMxy09Pf2aGdLT0zV+/HgNHTpUERERBVqeK7OmpaXpzJkzatWqlSzL0tatW/P1Gv3799fevXvdq+AXLVqkkJAQdezY0WPctm3bdODAAT388MM6e/ase3nT0tLUsWNHbdiwwWPP/LJly+rTTz/VuXPn1KdPH/eq+aysLD300EM6f/68li9f7rEMV8rIyLjur+iQkBD16NFDH374oXutU1ZWlj766CP17NlTgYGB+XoPli1bpqeeekojR47U0KFDcx1z8eJFSb/tQJgXX19f+fn5SfptU8e5c+d0+fJlNW/eXFu2bHGPW7RokRwOh8aOHZvjNRwOh6TfNiO4XC699NJL8vHxyXWM5N3nVZIGDRqkWbNmue/PmjVLsbGxOeZxtc2bN2vhwoWKi4u77lhJOnv2rCQpNDTUY3pKSookKSgo6JrPDwoKco/Nj+zNLVc+Z8eOHdq5c6f69evnntavXz+dOXNGK1euzPV1QkNDdebMmXzPFyULxQLGXb2Xebly5VS1alWP/SF+/PFH9erVSyEhIQoODlZYWJh7lXpSUpLH8/fu3auwsDCPW25fFld64403lJGRoeeff77Ay3Ps2DENHDhQFSpUULly5RQWFqZ27drlmjUvYWFh6tq1q95//31J0vvvv5/rF82BAwckSbGxsTmW+b333lNmZmaOeWZkZCgxMVErV65070cxZswYrVy5UklJScrMzMwz15kzZxQSEnLd/AMGDHBvR5ek1atX69dff1X//v3ztfzbtm1Tv379lJWVlethjtkSExMlKcf+AlebM2eOGjduLH9/f1WsWFFhYWH67LPPPN6bQ4cOKTw8/JpHLR06dEg+Pj6qX7/+NefnzedVkmJiYrR//3798MMPOnLkiNatW5ev/SVGjx6ttm3bqlu3btcdeyXris2M0v8ViuuVhpSUFFWqVCnf80lNTfV4fem3/VMCAwNVq1YtHTx4UAcPHpS/v79q1KiR5+YQy7I8ihtKFw43RZFLTExUu3btFBwcrPHjxysqKkr+/v7asmWLRo0aleNY+Ro1auTY5rxw4ULNmDEj19c/c+aMXn31VY0ZM6bAh8JmZWWpc+fOOnfunEaNGqV69eopMDBQJ06c0MCBA695XP/VBg8erAEDBujpp5/Whg0b9N5777m/qLNlv96rr76q3/3ud7m+ztVfusOHD1fVqlU1YcIExcTESPrtl++CBQs0ZswYDR8+XEuXLs3xOhcvXtTJkyfVuXPn62bv0qWLKleurLlz5+qee+7R3LlzVaVKFXXq1CkfSy5t375d0dHR6tixo0aOHKlHHnkk1/0rfvnlF0lSlSpV8nytuXPnauDAgerZs6dGjhypSpUqydfXV3FxcTp06FC+8njD28+r9FuR7N69u2bNmqXKlSurdevWuv322685ny+//FKrV6/Wd999l+9sFStWlKQcO0dnF6UdO3bk+dyjR48qOTlZtWrVyvf8svezyV4Wy7L04YcfKi0tLddydurUKaWmpub4zJ4/fz7HDxCUHhQLGHfgwAF16NDBfT81NVUnT57UH/7wB0m/7fl/9uxZLV68WPfcc497XF47IAYGBub4Art6J70r/e1vf1NQUJCGDx9egKX4zc6dO7V//37NmTPHY2fNG9mjPTo6Wv7+/urbt6/atGmjqKioHMUiKipKkhQcHJyvL+0VK1Zo+fLlWrFihbp27aqffvpJL7zwgl555RU99NBDCgwMVPfu3fXZZ5+pa9euHs/dvn27Ll26pObNm193Pr6+vnr44Yc1e/ZsTZ48WUuXLtVjjz2W42iDvDRq1EgLFy5UQECAFi5cqMcff1w7duzIsRlm9+7dCgsLc39h5uaTTz5RrVq1tHjxYo9fvVevxYqKitLKlSt17ty5PAtmVFSUXC6Xdu/enWeR8/bzmm3w4MGKiYlRSEiIx1E2ubEsS6NHj1avXr30+9///ppjrxQREaGAgIAcWWrXrq26detq6dKlmjJlSq6bRLLPI9G7d+98z++DDz6Qw+Fwl9H169fr+PHjGj9+vO644w6PsefPn9fjjz+upUuXeuzge/nyZSUkJOiPf/xjvueLkoVNITBuxowZunTpkvv+tGnTdPnyZUVHR0uS+8voytW3Fy9e1Ntvv13geR85ckTTpk3Tyy+/nOd+Bd7ILatlWZoyZYrXr1WmTBkNGDBAO3bs0ODBg3Md06xZM0VFRem1115zr3a+0pWHrF64cEFPP/20evTo4S4NrVq18vjfbt266Y9//KOefvrpHIfHLly4UL6+vvle7d6/f3+dP39eTzzxhFJTU706GuTOO+9UYGCgfHx89N577+nIkSMaP368x5iUlBT95z//0b333nvN18rtb/L999/n+KX/pz/9SZZlady4cTleI/u5PXv2lI+Pj8aPH59jzUP2mBv9vN5///0KDAx07/9yLQsWLNCOHTsUFxd3zXFXu+WWW9S8efMch89KvxWt8+fP689//nOOw2I3b96syZMnq2nTpu5/l9czadIkffnll3rooYfcaxuyN4OMHDlSDz74oMftscceU+3atXNsDtm9e7cyMjLcn1GUPqyxgHEXL15Ux44d1adPH+3bt09vv/222rRp4/6F0qpVK4WGhio2NlbDhg2Tw+HQBx98kGM78Y1Yv3697rjjDg0aNOi6Y/ft26cvvvjCY1pqaqp8fHz0xRdf6P7771e9evUUFRWl5557TidOnFBwcLAWLVqUY9Vzfr3yyisaOXJkjp3tsmV/8UZHR6tBgwYaNGiQbrvtNp04cUJr165VcHCw+/DAiRMn6tSpU9ctOf/85z9Vv359xcXFafz48UpLS9Nbb72lf/7zn6pTp47HuSSyy8yOHTv03XffqWXLlu7HmjZtqoYNG2rhwoW64447chzimF8NGzbUqFGjNGnSJPXt21eNGzfWxx9/rHHjxun8+fPXPeS4W7duWrx4sXr16qWuXbvq8OHDeuedd1S/fn2PMtahQwf1799f//znP3XgwAHdf//9crlc+vrrr9WhQwcNHTpUt99+u3sNT9u2bfXAAw/I6XQqPj5e4eHhiouLu+HPq6+vr/bs2SPLsq67g+uXX36pxx57THXr1s3/G/m/evTooRdeeEHJyckeh5b269dPmzZt0htvvKHdu3crJiZGoaGh2rJli95//32FhYXpk08+UZkynl8Dly9fdh8anZGRoaNHj2r58uXasWOHOnTo4N4EmZmZqUWLFqlz58557gD8xz/+UVOmTNGpU6fc+3KsWrVKZcuWzdcmOJRQRX4cCkocbw83Xb9+vfX4449boaGhVrly5ayYmBjr7NmzHmO//fZb6/e//70VEBBghYeHW3/5y1+slStXug+VzObt4aaSrCVLlniMvfpwxexD/K53y7Z7926rU6dOVrly5axbb73Veuyxx6zt27fnOEzwRt67vB7funWr9cADD1gVK1a0nE6nFRkZafXp08das2aNZVmWtX//fsvpdFoTJ070eN6Vh5teacKECZbT6bT279+f7+XP7VDH7EOHr57vtVx9GLBlWVZGRoZVr14966677rIuX75s9erVy4qOjra+//77HM+/+u/ncrmsiRMnWpGRkZbT6bSaNm1qrVixIsc4y/rt0NRXX33VqlevnuXn52eFhYVZ0dHR1ubNmz3Gvf/++1bTpk0tp9NphYaGWu3atbNWrVrlfrygn9e8Hs/+WwQEBFgnTpy47vuWm19//dUqU6aM9cEHH+T6+PLly61OnTpZ5cuXd/9tsw9PvlpsbKzHZ6Bs2bJWjRo1rD/96U/WJ5984nFI7qJFiyxJ1syZM/PMtm7dOkuSNWXKFPe0Fi1aWI888sh1lwslF8UCxpSWU39nfzmXVtlfZleWsquNHTs21y+1N998M1/n70DRGjx4sNWmTZt8jX300UctSda7775byKly2rp1q+VwOKytW7cW+bxRdNjHAkC+WJalmTNnql27dgU+NwjMGjt2rOLj4/N1Nsvp06erW7duevLJJz3OLVMUJk2apAcffDDPHWVROrCPBXCVChUqXPf6FyVZuXLlFBMTc81zRTRu3Fjh4eGSfjsp2PLly7V27Vrt3LlTy5YtK6qoyKeIiAhlZGTka6yvr69Xp/E2acGCBbbMF0XLYVkG9pgD9Nv1FAYNGqT4+Ph8HcKIkuHIkSOqWbOmypcvr6eeekoTJkywOxKAYoxiAQAAjGEfCwAAYAzFAgAAGFPkO2+6XC79/PPPCgoK4iI0AACUEJZlKSUlReHh4de8+m6RF4uff/5Z1atXL+rZAgAAAxISElStWrU8Hy/yYpF9MZwde6coKKjg13IAAACFLyXlghrXG57rRe2uVOTFInvzR1BQgIKCyxb17AEAQAFcbzcGdt4EAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAICtXC6X3REKVWlfvqt5VSyysrL04osvqmbNmgoICFBUVJReeeUVWZZVWPkAAKXUl19s1X3tX1SlkAGqW+MJjXtxgdLTM+2OZURWlktT31yh39V/WpVCBqhls/+nef9ef1N8X5bxZvDkyZM1bdo0zZkzRw0aNNCmTZs0aNAghYSEaNiwYYWVEQBQyvzn000a8PCbateqrP41KUyHDl/SO9P/o107DuvjpaPlcDjsjlggz4/8t2bNXKOBfYN01+8qafWGNA0f8q7OnUvR0yO62R2vUHlVLP773/+qR48e6tq1qySpRo0a+vDDD/XDDz8USjgAQOljWZYmvvKRunQI1Iq5VeXj81uJaNcqQD1jf9R/v92r1m3usDnljTtx/KxmzVyjyS9W1LN/DpUkPd4/RMP/ekr/eHWpBj/WSYGB/janLDxebQpp1aqV1qxZo/3790uStm/frm+++UbR0dF5PiczM1PJyckeNwDAzSvxfJr27jmpAX2C3KVCkrp1DlTFCmX03bd7bUxXcD98f0Aul6XYPsEe02P7BCs5OUN7fjxuU7Ki4dUai9GjRys5OVn16tWTr6+vsrKyNGHCBMXExOT5nLi4OI0bN67AQQEApYN/gJ9uucVHP/9y2WN6YpJLKakuBQeXtSmZGcHBAZKkE79cVsUKvu7px09e9ni8tPJqjcXHH3+sefPmaf78+dqyZYvmzJmj1157TXPmzMnzOWPGjFFSUpL7lpCQUODQAICSKyDAT9173K3X3k7Stl2/7ayZlu7SiBdPS3KoxwMt7A1YQG3bNVDVqiEa8eIZnTmbJUk6dvyS/jrpvJo0jVTtuuE2JyxcXq2xGDlypEaPHq2+fftKkho1aqSjR48qLi5OsbGxuT7H6XTK6XQWPCkAoNT42+T+eqDbMTXrfEx1a/vr55OXdCHD0tR3nlDlyuXtjlcgfn5lNP39pxXT51VFNDuiqBpO7TuYobCwIH3y7ydL/I6p1+NVsUhPT5ePj+dKDl9f35vuGF0AQMFUqhSiNV9P0KfL4rVl00FVqBikPn3bKCIyzO5oRrRqU0/x2/+hjxd8o4RjZzToydv0YO+WCirhm3nyw6ti0b17d02YMEERERFq0KCBtm7dqjfeeEODBw8urHwAgFLK6bxFD/ZppQf7tLI7SqG4NSxYTz39B7tjFDmvisXUqVP14osv6qmnntKpU6cUHh6uJ554Qi+99FJh5QMAACWIwyri04AlJycrJCREh0/MuClWCQEAUBqkJKer5m2PKykpScHBwXmO41ohAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjCljdwAAQN6yslz6cdcxORwONWhYXT4+/B5E8ebVJ7RGjRpyOBw5bkOGDCmsfABw0/rPis1q1mi47m3zV3Vo/YLubvKsvlq9w+5YwDV5VSzi4+N18uRJ923VqlWSpN69exdKOAC4WW3d8pMGPfKmmtS7qLWLq2n1J7fp9oh0PfLQ69qzO8HueECevNoUEhYW5nF/0qRJioqKUrt27YyGAoCb3fS3vlCtSD8tfr+qypRxSJJaNfdXnVYJmjFtpf4x9X9sTgjk7ob3sbh48aLmzp2rZ599Vg6HI89xmZmZyszMdN9PTk6+0VkCwE1j355j6tDa6S4VkuR0+qh9K6f27z1uYzLg2m54L6ClS5cqMTFRAwcOvOa4uLg4hYSEuG/Vq1e/0VkCwE2jWkQlxW+7JMuy3NNcLkvx2y6qWvWwazwTsNcNF4uZM2cqOjpa4eHh1xw3ZswYJSUluW8JCWwbBIDrefTxztq684KGvXBaJ05e1rHjl/TEyFPadzBTgx/rbHc8IE83tCnk6NGjWr16tRYvXnzdsU6nU06n80ZmAwA3rfb3NlLcqwM07sX5entWkiQpMNBPb/zzUbVoWcfmdEDebqhYzJo1S5UqVVLXrl1N5wEA/K/H/nyfej/UWuvX7ZKPj0Pt2jdUcEhZu2MB1+R1sXC5XJo1a5ZiY2NVpgzn1wKAwlQ+NFA9erWwOwaQb17vY7F69WodO3ZMgwcPLow8AACgBPN6lcN9993nsZcyAABANk46DwAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMMbrYnHixAk98sgjqlixogICAtSoUSNt2rSpMLIBAIASpow3g8+fP6/WrVurQ4cO+vzzzxUWFqYDBw4oNDS0sPIBAIASxKtiMXnyZFWvXl2zZs1yT6tZs+Y1n5OZmanMzEz3/eTkZC8jAgCAksKrTSHLly9X8+bN1bt3b1WqVElNmzbVu+++e83nxMXFKSQkxH2rXr16gQIDAIDiy6ti8dNPP2natGmqXbu2Vq5cqSeffFLDhg3TnDlz8nzOmDFjlJSU5L4lJCQUODQAACievNoU4nK51Lx5c02cOFGS1LRpU+3atUvvvPOOYmNjc32O0+mU0+kseFIAAFDsebXGomrVqqpfv77HtDvuuEPHjh0zGgoAAJRMXhWL1q1ba9++fR7T9u/fr8jISKOhAABAyeRVsXjmmWe0ceNGTZw4UQcPHtT8+fM1Y8YMDRkypLDyAQCAEsSrYnHXXXdpyZIl+vDDD9WwYUO98sorevPNNxUTE1NY+QAAQAni1c6bktStWzd169atMLIAAIASjmuFAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMMbra4UAAIDC8VPqHrsj5Ck1LTNf4ygWAADY7KfUPTqadlbfn42yO0qeMlMz8jWOYgEAgI2uLBUJJyuqmirbHSlXVnp6vsZRLAAAsMmVpWLHngBVSPNTWV+7U+XO90L+xlEsAACwQXap+PxIVR0/HqBqe8opyukvKZ/f4EUtM3+5KBYAgGIn+0u3NDubnq4fTkUpfV+Uqh27rChnOf2udhW7Y+Up/UJavsZRLAAAxcpPqXu05XSCDl5oJMfl2+2OU2iOnT4vxwk/lS0BpcIbFAsAQLGx9tdvdDY9XWt/aq9qqizL7kCFKGn7RYWn++hPDUtXeaJYAABsd/WREY4Tfjp/7LTdsQpVW+et+l3D0rGW4koUCwAo5orzSZNMubJUlNZf8jcLigUAFGPZv+R/zWpud5RCtSXB73+PjPBR41L6S/5mQbEAgGLKc/NA6b60k+NEyTgyAtdHsQCAYujqcxy0yCieZ2M05cSx05SKUoJiAQDFTPbhllee48DfWUxPmmQIpaL0oFgAKFFK+46M/7fpI6pUnuMApR/FAkCJkX2Og4MXGtkdpRCV58gIlGgUCwDFXkm5+qMJJ06cV9KZVLXN4sgIlEwUCwDFWm4nTiquV380oeyxy2rkvJVNHyixKBYAiq2rLyntc/Si2mYFq9he/dEA9qdASUexAEqom+vqj1U5xwFQQlAsgBLoyl/yXP0RQHFCsQBKmCvPcRCaWr9UX/2RUgGUPBQLoITw3Inxt3McqJRf/bGsxOGWQAlDsUCpcfOcOIlzHAAovrwqFi+//LLGjRvnMa1u3brau3ev0VCAN26Wqz8eSSx/xZERnOMAQPHk9RqLBg0aaPXq1f/3AmVY6QH7eO7EWNHuOIXm2OnzOp8YoGp7yrG/AYBizetWUKZMGVWpwn/UYL+rz3FQIS3N7kiFxiE/VTvmQ6kAUOx5XSwOHDig8PBw+fv7q2XLloqLi1NERESe4zMzM5WZmem+n5ycfGNJgSt4Xv0x+xwH/nbHKlxOUSoAFHteFYsWLVpo9uzZqlu3rk6ePKlx48apbdu22rVrl4KCgnJ9TlxcXI79MlC4boadGM+mp2vtT+05HBEAihmHZVk3fBh8YmKiIiMj9cYbb+jRRx/NdUxuayyqV6+uwydmKCi47I3OGnm4GXZkPJJ4/v+uGUGpAIAikX4hTb0f76KkpCQFBwfnOa5Ae16WL19ederU0cGDB/Mc43Q65XQ6CzIb5FP2JaXX/tTe7iiFjMMtAaC4KlCxSE1N1aFDh9S/f39TeXADcrv6Y23fULtjFZoTh0+rsZPDLQGgOPKqWDz33HPq3r27IiMj9fPPP2vs2LHy9fVVv379CisfroOrPwIAihOvisXx48fVr18/nT17VmFhYWrTpo02btyosLCwwspXIDfD1R8l6fMjVXX8OOc4AADYz6tisWDBgsLKYdzNdvVHLikNACgOSuVpM7n6IwAA9ih1xeLKIyNulqs/UioAAMWFbcXicNp+lfMxexjq1UdG8EseAICiZVux2JwSqQArwOhrHkkszyWlAQCwkW3FIuFsBTkzzJ55c8eBDC4pDQCAjWwrFid2pOmWALO7VVY7xuGWAADYybZiUf+0v/xNX42Sqz8CAGAr24pF46jKKhsQaNfsAQBAIfCxOwAAACg9KBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIwpULGYNGmSHA6HRowYYSgOAAAoyW64WMTHx2v69Olq3LixyTwAAKAEu6FikZqaqpiYGL377rsKDQ01nQkAAJRQN1QshgwZoq5du6pTp07XHZuZmank5GSPGwAAKJ3KePuEBQsWaMuWLYqPj8/X+Li4OI0bN87rYAAAoOTxao1FQkKChg8frnnz5snf3z9fzxkzZoySkpLct4SEhBsKCgAAij+v1lhs3rxZp06d0p133umelpWVpQ0bNuhf//qXMjMz5evr6/Ecp9Mpp9NpJi0AACjWvCoWHTt21M6dOz2mDRo0SPXq1dOoUaNylAoAAHBz8apYBAUFqWHDhh7TAgMDVbFixRzTAQDAzYczbwIAAGO8PirkauvWrTMQAwAAlAassQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQL5ln4hXWnpqXbHKBSWZSk5JUkZmRl2RwGAEq2MN4OnTZumadOm6ciRI5KkBg0a6KWXXlJ0dHRhZEMxcezEEc2YO0Vbd8VLkupGNdBjMU/rjtoNbU5mxnebNmj2R+/o+C/H5OPjqzZ3tddjjwxThfIV7Y4GACWOV2ssqlWrpkmTJmnz5s3atGmT7r33XvXo0UM//vhjYeWDzc4lntFfxj+lA7v3qZ7uVH011y8/ndSYicN0JOEnu+MVWPy27zRhygu68OtFNVQLRbka6If4/2rMhKd16dJFu+MBQInjVbHo3r27/vCHP6h27dqqU6eOJkyYoHLlymnjxo2FlQ82+2z1EmVkXNCdrnaq5qilcEcN3Wm10y0uPy36bL7d8Qps3qKZCnWE6XdWa1VxVFeko46auFrr+C/H9PUPa+2OBwAlzg3vY5GVlaUFCxYoLS1NLVu2zHNcZmamkpOTPW4oOfYe/FHlXWHyczjd03wdvqroqqK9B3bZmKzgLMvSwSP7FGbdJofD4Z4e5CivIN/y2n9oj43pAKBk8rpY7Ny5U+XKlZPT6dSf//xnLVmyRPXr189zfFxcnEJCQty36tWrFygwilZIcKgyfNJlWZbH9AuOVIUEl7cnlCEOh0PlAoN0QZ47pGZZl5VhpSs4KMSmZABQcnldLOrWratt27bp+++/15NPPqnY2Fjt3r07z/FjxoxRUlKS+5aQkFCgwCha97XrqhRXon7SbmVZWXJZLiVYh3TG+kVdOnS3O16BdenQXScch3Xa+lmWZemSdVF7tEVZytK9rbvYHQ8AShyvjgqRJD8/P91+++2SpGbNmik+Pl5TpkzR9OnTcx3vdDrldDpzfQzF3+8aNFfMA49q3uKZSnAckMPho0uui7q//R/VsU3JPxro4V6DdejIfm3d9V/5+Th12bosHx+HnnnseVWpFG53PAAocbwuFldzuVzKzMw0kQXF1MO9Bql9q876btMGXc66rLuatFStyNp2xzLC6efUK395Q7v2btOufdsVGBCoNi3u5VBTALhBXhWLMWPGKDo6WhEREUpJSdH8+fO1bt06rVy5srDyoZgIr1xNf+r6sN0xCoXD4VCjO5qq0R1N7Y4CACWeV8Xi1KlTGjBggE6ePKmQkBA1btxYK1euVOfOnQsrHwAAKEG8KhYzZ84srBwAAKAU4FohAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjCljd4DSwrIsHTy8T+eTz6lWRG3dWiHM7kgAABQ5r4pFXFycFi9erL179yogIECtWrXS5MmTVbdu3cLKVyKc+CVBE6f8VUeOH5Ik+Th81OmeP+ipgf9Pt5S5xeZ0AAAUHa82haxfv15DhgzRxo0btWrVKl26dEn33Xef0tLSCitfsXf58mX9ddIzOvPzaTVVG7XRH3S71UirN/xHH3zyrt3xAAAoUl6tsfjiiy887s+ePVuVKlXS5s2bdc899+T6nMzMTGVmZrrvJycn30DM4uv7rd/q1Nlf1EKdFOQoL0mKUG1dtDL02eoleuSBR+Xn57Q3JAAARaRAO28mJSVJkipUqJDnmLi4OIWEhLhv1atXL8gsi52Tvx7XLT5+7lKRrbxuVUbmBSWlJNqSCwAAO9xwsXC5XBoxYoRat26thg0b5jluzJgxSkpKct8SEhJudJbF0m1VquuS66JSrESP6ed1RgHOAIUElbclFwAAdrjho0KGDBmiXbt26ZtvvrnmOKfTKaez9G4KuOt3rVTl1nDtOveDarsaKVDBOqXjSnAc0J86P8xmEADATeWG1lgMHTpUK1as0Nq1a1WtWjXTmUqUMmXK6G+j/6Eq1apom77Vt/pch3x2q0v77nrkT/9jdzwAAIqUV2ssLMvS008/rSVLlmjdunWqWbNmYeUqUapWvk1T/jZTh48d1Pmkc6oZEaUK5W+1OxYAAEXOq2IxZMgQzZ8/X8uWLVNQUJB++eUXSVJISIgCAgIKJWBJ4XA4VCuytt0xAACwlVebQqZNm6akpCS1b99eVatWdd8++uijwsoHAABKEK83hQAAAOSFi5ABAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwxutisWHDBnXv3l3h4eFyOBxaunRpIcQCAAAlkdfFIi0tTU2aNNFbb71VGHkAAEAJVsbbJ0RHRys6OrowsgAAgBLO62LhrczMTGVmZrrvJyUlSZLSL6QV9qwBAIAh2d/blmVdc1yhF4u4uDiNGzcux/TY4Q8U9qwBAIBhKSkpCgkJyfNxh3W96nENDodDS5YsUc+ePfMcc/Uai8TEREVGRurYsWPXDFYSJScnq3r16kpISFBwcLDdcYxj+Uq20rx8pXnZJJavJCtNy2ZZllJSUhQeHi4fn7x30Sz0NRZOp1NOpzPH9JCQkBL/JuclODi41C6bxPKVdKV5+UrzskksX0lWWpYtPysEOI8FAAAwxus1FqmpqTp48KD7/uHDh7Vt2zZVqFBBERERRsMBAICSxetisWnTJnXo0MF9/9lnn5UkxcbGavbs2dd9vtPp1NixY3PdPFLSleZlk1i+kq40L19pXjaJ5SvJSvOy5aVAO28CAABciX0sAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxRVos3nrrLdWoUUP+/v5q0aKFfvjhh6KcfaHZsGGDunfvrvDwcDkcDi1dutTuSEbFxcXprrvuUlBQkCpVqqSePXtq3759dscyYtq0aWrcuLH7rHgtW7bU559/bnesQjNp0iQ5HA6NGDHC7ihGvPzyy3I4HB63evXq2R3LqBMnTuiRRx5RxYoVFRAQoEaNGmnTpk12xyqwGjVq5PjbORwODRkyxO5oRmRlZenFF19UzZo1FRAQoKioKL3yyivXvYBXaVBkxeKjjz7Ss88+q7Fjx2rLli1q0qSJunTpolOnThVVhEKTlpamJk2a6K233rI7SqFYv369hgwZoo0bN2rVqlW6dOmS7rvvPqWllfwr1FarVk2TJk3S5s2btWnTJt17773q0aOHfvzxR7ujGRcfH6/p06ercePGdkcxqkGDBjp58qT79s0339gdyZjz58+rdevWuuWWW/T5559r9+7dev311xUaGmp3tAKLj4/3+LutWrVKktS7d2+bk5kxefJkTZs2Tf/617+0Z88eTZ48WX//+981depUu6MVPquI3H333daQIUPc97Oysqzw8HArLi6uqCIUCUnWkiVL7I5RqE6dOmVJstavX293lEIRGhpqvffee3bHMColJcWqXbu2tWrVKqtdu3bW8OHD7Y5kxNixY60mTZrYHaPQjBo1ymrTpo3dMYrE8OHDraioKMvlctkdxYiuXbtagwcP9pj2wAMPWDExMTYlKjpFssbi4sWL2rx5szp16uSe5uPjo06dOum7774riggwKCkpSZJUoUIFm5OYlZWVpQULFigtLU0tW7a0O45RQ4YMUdeuXT3+DZYWBw4cUHh4uGrVqqWYmBgdO3bM7kjGLF++XM2bN1fv3r1VqVIlNW3aVO+++67dsYy7ePGi5s6dq8GDB8vhcNgdx4hWrVppzZo12r9/vyRp+/bt+uabbxQdHW1zssJX6Fc3laQzZ84oKytLlStX9pheuXJl7d27tygiwBCXy6URI0aodevWatiwod1xjNi5c6datmypjIwMlStXTkuWLFH9+vXtjmXMggULtGXLFsXHx9sdxbgWLVpo9uzZqlu3rk6ePKlx48apbdu22rVrl4KCguyOV2A//fSTpk2bpmeffVbPP/+84uPjNWzYMPn5+Sk2NtbueMYsXbpUiYmJGjhwoN1RjBk9erSSk5NVr149+fr6KisrSxMmTFBMTIzd0QpdkRQLlB5DhgzRrl27StV27Lp162rbtm1KSkrSJ598otjYWK1fv75UlIuEhAQNHz5cq1atkr+/v91xjLvy11/jxo3VokULRUZG6uOPP9ajjz5qYzIzXC6XmjdvrokTJ0qSmjZtql27dumdd94pVcVi5syZio6OVnh4uN1RjPn44481b948zZ8/Xw0aNNC2bds0YsQIhYeHl6q/XW6KpFjceuut8vX11a+//uox/ddff1WVKlWKIgIMGDp0qFasWKENGzaoWrVqdscxxs/PT7fffrskqVmzZoqPj9eUKVM0ffp0m5MV3ObNm3Xq1Cndeeed7mlZWVnasGGD/vWvfykzM1O+vr42JjSrfPnyqlOnjscVmEuyqlWr5ii4d9xxhxYtWmRTIvOOHj2q1atXa/HixXZHMWrkyJEaPXq0+vbtK0lq1KiRjh49qri4uFJfLIpkHws/Pz81a9ZMa9ascU9zuVxas2ZNqduWXRpZlqWhQ4dqyZIl+uqrr1SzZk27IxUql8ulzMxMu2MY0bFjR+3cuVPbtm1z35o3b66YmBht27atVJUKSUpNTdWhQ4dUtWpVu6MY0bp16xyHdu/fv1+RkZE2JTJv1qxZqlSpkrp27Wp3FKPS09Pl4+P5Fevr6yuXy2VToqJTZJtCnn32WcXGxqp58+a6++679eabbyotLU2DBg0qqgiFJjU11eMX0uHDh7Vt2zZVqFBBERERNiYzY8iQIZo/f76WLVumoKAg/fLLL5KkkJAQBQQE2JyuYMaMGaPo6GhFREQoJSVF8+fP17p167Ry5Uq7oxkRFBSUY1+YwMBAVaxYsVTsI/Pcc8+pe/fuioyM1M8//6yxY8fK19dX/fr1szuaEc8884xatWqliRMnqk+fPvrhhx80Y8YMzZgxw+5oRrhcLs2aNUuxsbEqU6Z0bZnv3r27JkyYoIiICDVo0EBbt27VG2+8ocGDB9sdrfAV5SEoU6dOtSIiIiw/Pz/r7rvvtjZu3FiUsy80a9eutSTluMXGxtodzYjclk2SNWvWLLujFdjgwYOtyMhIy8/PzwoLC7M6duxoffnll3bHKlSl6XDThx56yKpatarl5+dn3XbbbdZDDz1kHTx40O5YRn366adWw4YNLafTadWrV8+aMWOG3ZGMWblypSXJ2rdvn91RjEtOTraGDx9uRUREWP7+/latWrWsF154wcrMzLQ7WqFzWNZNcBowAABQJLhWCAAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGP+P4pdpVeqfWGhAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.3. Метод частичных наименьших квадратов (PLS, Partial Least Squares)\n",
        "\n",
        "#### Введение\n",
        "Метод частичных наименьших квадратов (PLS, Partial Least Squares) — это мощный инструмент многомерного анализа данных, который широко используется в задачах регрессии, классификации и снижения размерности. PLS особенно полезен, когда данные содержат большое количество коррелированных переменных, а количество наблюдений ограничено. Этот метод находит линейные комбинации исходных переменных (латентные переменные), которые максимизируют ковариацию между предикторами (X) и целевой переменной (Y).\n",
        "\n",
        "\n",
        "\n",
        "### 1. Основные понятия и идея метода\n",
        "Метод PLS объединяет идеи из метода главных компонент (PCA) и множественной линейной регрессии. В отличие от PCA, который максимизирует дисперсию только в пространстве предикторов X, PLS одновременно учитывает как дисперсию в X, так и связь между X и Y.\n",
        "\n",
        "#### 1.1. Постановка задачи\n",
        "Пусть:\n",
        "- $X \\in \\mathbb{R}^{n \\times p}$ — матрица предикторов (n наблюдений, p переменных),\n",
        "- $Y \\in \\mathbb{R}^{n \\times q}$ — матрица откликов (n наблюдений, q целевых переменных).\n",
        "\n",
        "Цель PLS — найти такие латентные переменные (компоненты) $T \\in \\mathbb{R}^{n \\times k}$, которые:\n",
        "1. Максимизируют ковариацию между X и Y.\n",
        "2. Позволяют выразить X и Y через линейные комбинации этих компонент.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Математическая основа PLS\n",
        "#### 2.1. Разложение матриц\n",
        "PLS строит разложение матриц X и Y в виде:\n",
        "$$\n",
        "X = T P^T + E,\n",
        "$$\n",
        "$$\n",
        "Y = T Q^T + F,\n",
        "$$\n",
        "где:\n",
        "- $T$ — матрица латентных переменных (компонент),\n",
        "- $P$ и $Q$ — матрицы нагрузок (loadings) для X и Y соответственно,\n",
        "- $E$ и $F$ — матрицы остатков.\n",
        "\n",
        "#### 2.2. Критерий оптимизации\n",
        "PLS ищет такие латентные переменные $T$, которые максимизируют ковариацию между X и Y. Для первой компоненты задача формулируется как:\n",
        "$$\n",
        "\\max_{w, c} \\text{Cov}(Xw, Yc),\n",
        "$$\n",
        "где:\n",
        "- $w \\in \\mathbb{R}^p$ — весовой вектор для X,\n",
        "- $c \\in \\mathbb{R}^q$ — весовой вектор для Y.\n",
        "\n",
        "Ограничения:\n",
        "$$\n",
        "\\|w\\| = 1, \\quad \\|c\\| = 1.\n",
        "$$\n",
        "\n",
        "#### 2.3. Итеративное построение компонент\n",
        "PLS строит компоненты итеративно. На каждом шаге:\n",
        "1. Находится первая пара весовых векторов $w_1$ и $c_1$, максимизирующая ковариацию.\n",
        "2. Вычисляется первая латентная переменная $t_1 = X w_1$.\n",
        "3. Матрицы X и Y проецируются на ортогональное дополнение к $t_1$, и процесс повторяется для следующих компонент.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Алгоритм PLS\n",
        "#### 3.1. Шаги алгоритма\n",
        "1. **Инициализация**: Центрирование и масштабирование матриц X и Y.\n",
        "2. **Построение компонент**:\n",
        "   - Для каждой компоненты $k = 1, 2, \\dots, K$:\n",
        "     a. Вычислить весовые векторы $w_k$ и $c_k$, решая задачу оптимизации:\n",
        "        $$\n",
        "        \\max_{w_k, c_k} \\text{Cov}(X_{k-1} w_k, Y_{k-1} c_k),\n",
        "        $$\n",
        "        где $X_{k-1}$ и $Y_{k-1}$ — остатки после построения предыдущих компонент.\n",
        "     b. Вычислить латентную переменную $t_k = X_{k-1} w_k$.\n",
        "     c. Вычислить нагрузки $p_k$ и $q_k$:\n",
        "        $$\n",
        "        p_k = \\frac{X_{k-1}^T t_k}{t_k^T t_k}, \\quad q_k = \\frac{Y_{k-1}^T t_k}{t_k^T t_k}.\n",
        "        $$\n",
        "     d. Обновить остатки:\n",
        "        $$\n",
        "        X_k = X_{k-1} - t_k p_k^T, \\quad Y_k = Y_{k-1} - t_k q_k^T.\n",
        "        $$\n",
        "3. **Завершение**: После построения K компонент, итоговые модели для X и Y записываются как:\n",
        "   $$\n",
        "   X = T P^T + E, \\quad Y = T Q^T + F.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### 4. Свойства PLS\n",
        "1. **Учет связи между X и Y**: PLS учитывает не только структуру данных в X, но и их связь с Y.\n",
        "2. **Устойчивость к мультиколлинеарности**: PLS эффективно работает даже при высокой корреляции между предикторами.\n",
        "3. **Интерпретируемость**: Латентные переменные и нагрузки позволяют интерпретировать вклад исходных переменных.\n",
        "\n",
        "\n",
        "\n",
        "### 5. Формулы для предсказания\n",
        "После построения модели PLS, предсказание для новых данных $X_{\\text{new}}$ выполняется следующим образом:\n",
        "1. Проецируем $X_{\\text{new}}$ на латентные переменные:\n",
        "   $$\n",
        "   T_{\\text{new}} = X_{\\text{new}} W (P^T W)^{-1},\n",
        "   $$\n",
        "   где $W$ — матрица весовых векторов.\n",
        "2. Вычисляем предсказание для Y:\n",
        "   $$\n",
        "   Y_{\\text{new}} = T_{\\text{new}} Q^T.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### 6. Преимущества и недостатки PLS\n",
        "#### Преимущества:\n",
        "- Эффективен при большом количестве переменных.\n",
        "- Устойчив к мультиколлинеарности.\n",
        "- Подходит для задач регрессии и классификации.\n",
        "\n",
        "#### Недостатки:\n",
        "- Требует настройки числа компонент.\n",
        "- Менее интерпретируем по сравнению с обычной линейной регрессией.\n",
        "\n",
        "Таким образом, метод частичных наименьших квадратов (PLS) является мощным инструментом для анализа многомерных данных, особенно в случаях, когда традиционные методы, такие как множественная линейная регрессия, не применимы. PLS позволяет эффективно снижать размерность данных, учитывая при этом связь между предикторами и целевой переменной.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}, \\quad Y = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3 \\\\\n",
        "4\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $X$ — матрица предикторов размером $4 \\times 3$,\n",
        "- $Y$ — вектор откликов размером $4 \\times 1$.\n",
        "\n",
        "Наша цель — построить модель PLS, которая связывает $X$ и $Y$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Центрирование данных\n",
        "Перед началом работы с PLS данные необходимо центрировать (вычесть среднее значение по каждому столбцу).\n",
        "\n",
        "1. Вычислим средние значения для $X$ и $Y$:\n",
        "   $$\n",
        "   \\bar{X} = \\begin{bmatrix} 5.5 & 6.5 & 7.5 \\end{bmatrix}, \\quad \\bar{Y} = 2.5.\n",
        "   $$\n",
        "\n",
        "2. Центрируем $X$ и $Y$:\n",
        "   $$\n",
        "   X_c = X - \\bar{X} = \\begin{bmatrix}\n",
        "   -4.5 & -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5 & 4.5\n",
        "   \\end{bmatrix},\n",
        "   $$\n",
        "   $$\n",
        "   Y_c = Y - \\bar{Y} = \\begin{bmatrix}\n",
        "   -1.5 \\\\\n",
        "   -0.5 \\\\\n",
        "   0.5 \\\\\n",
        "   1.5\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Построение первой компоненты\n",
        "#### 2.1. Вычисление весового вектора $w_1$\n",
        "PLS ищет такой весовой вектор $w_1$, который максимизирует ковариацию между $X_c$ и $Y_c$. Для первой компоненты:\n",
        "$$\n",
        "w_1 = \\frac{X_c^T Y_c}{\\|X_c^T Y_c\\|}.\n",
        "$$\n",
        "\n",
        "1. Вычислим $X_c^T Y_c$:\n",
        "   $$\n",
        "   X_c^T Y_c = \\begin{bmatrix}\n",
        "   -4.5 \\cdot (-1.5) + (-1.5) \\cdot (-0.5) + 1.5 \\cdot 0.5 + 4.5 \\cdot 1.5 \\\\\n",
        "   -4.5 \\cdot (-1.5) + (-1.5) \\cdot (-0.5) + 1.5 \\cdot 0.5 + 4.5 \\cdot 1.5 \\\\\n",
        "   -4.5 \\cdot (-1.5) + (-1.5) \\cdot (-0.5) + 1.5 \\cdot 0.5 + 4.5 \\cdot 1.5\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   15 \\\\\n",
        "   15 \\\\\n",
        "   15\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "2. Нормируем $w_1$:\n",
        "   $$\n",
        "   w_1 = \\frac{1}{\\sqrt{15^2 + 15^2 + 15^2}} \\begin{bmatrix} 15 \\\\ 15 \\\\ 15 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "#### 2.2. Вычисление латентной переменной $t_1$\n",
        "Латентная переменная $t_1$ вычисляется как проекция $X_c$ на $w_1$:\n",
        "$$\n",
        "t_1 = X_c w_1.\n",
        "$$\n",
        "\n",
        "1. Вычислим $t_1$:\n",
        "   $$\n",
        "   t_1 = \\begin{bmatrix}\n",
        "   -4.5 \\cdot \\frac{1}{\\sqrt{3}} + (-4.5) \\cdot \\frac{1}{\\sqrt{3}} + (-4.5) \\cdot \\frac{1}{\\sqrt{3}} \\\\\n",
        "   -1.5 \\cdot \\frac{1}{\\sqrt{3}} + (-1.5) \\cdot \\frac{1}{\\sqrt{3}} + (-1.5) \\cdot \\frac{1}{\\sqrt{3}} \\\\\n",
        "   1.5 \\cdot \\frac{1}{\\sqrt{3}} + 1.5 \\cdot \\frac{1}{\\sqrt{3}} + 1.5 \\cdot \\frac{1}{\\sqrt{3}} \\\\\n",
        "   4.5 \\cdot \\frac{1}{\\sqrt{3}} + 4.5 \\cdot \\frac{1}{\\sqrt{3}} + 4.5 \\cdot \\frac{1}{\\sqrt{3}}\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   -7.794 \\\\\n",
        "   -2.598 \\\\\n",
        "   2.598 \\\\\n",
        "   7.794\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "#### 2.3. Вычисление нагрузок $p_1$ и $q_1$\n",
        "Нагрузки $p_1$ и $q_1$ вычисляются как:\n",
        "$$\n",
        "p_1 = \\frac{X_c^T t_1}{t_1^T t_1}, \\quad q_1 = \\frac{Y_c^T t_1}{t_1^T t_1}.\n",
        "$$\n",
        "\n",
        "1. Вычислим $p_1$:\n",
        "   $$\n",
        "   p_1 = \\frac{1}{t_1^T t_1} \\begin{bmatrix}\n",
        "   -4.5 \\cdot (-7.794) + (-1.5) \\cdot (-2.598) + 1.5 \\cdot 2.598 + 4.5 \\cdot 7.794 \\\\\n",
        "   -4.5 \\cdot (-7.794) + (-1.5) \\cdot (-2.598) + 1.5 \\cdot 2.598 + 4.5 \\cdot 7.794 \\\\\n",
        "   -4.5 \\cdot (-7.794) + (-1.5) \\cdot (-2.598) + 1.5 \\cdot 2.598 + 4.5 \\cdot 7.794\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   1 \\\\\n",
        "   1 \\\\\n",
        "   1\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "2. Вычислим $q_1$:\n",
        "   $$\n",
        "   q_1 = \\frac{1}{t_1^T t_1} \\begin{bmatrix}\n",
        "   -1.5 \\cdot (-7.794) + (-0.5) \\cdot (-2.598) + 0.5 \\cdot 2.598 + 1.5 \\cdot 7.794\n",
        "   \\end{bmatrix} = 1.\n",
        "   $$\n",
        "\n",
        "#### 2.4. Обновление остатков\n",
        "Остатки $X_1$ и $Y_1$ вычисляются как:\n",
        "$$\n",
        "X_1 = X_c - t_1 p_1^T, \\quad Y_1 = Y_c - t_1 q_1.\n",
        "$$\n",
        "\n",
        "1. Вычислим $X_1$:\n",
        "   $$\n",
        "   X_1 = X_c - t_1 p_1^T = \\begin{bmatrix}\n",
        "   -4.5 & -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5 & 4.5\n",
        "   \\end{bmatrix} - \\begin{bmatrix}\n",
        "   -7.794 \\\\\n",
        "   -2.598 \\\\\n",
        "   2.598 \\\\\n",
        "   7.794\n",
        "   \\end{bmatrix} \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix}\n",
        "   0 & 0 & 0 \\\\\n",
        "   0 & 0 & 0 \\\\\n",
        "   0 & 0 & 0 \\\\\n",
        "   0 & 0 & 0\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "2. Вычислим $Y_1$:\n",
        "   $$\n",
        "   Y_1 = Y_c - t_1 q_1 = \\begin{bmatrix}\n",
        "   -1.5 \\\\\n",
        "   -0.5 \\\\\n",
        "   0.5 \\\\\n",
        "   1.5\n",
        "   \\end{bmatrix} - \\begin{bmatrix}\n",
        "   -7.794 \\\\\n",
        "   -2.598 \\\\\n",
        "   2.598 \\\\\n",
        "   7.794\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   6.294 \\\\\n",
        "   2.098 \\\\\n",
        "   -2.098 \\\\\n",
        "   -6.294\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Построение второй компоненты\n",
        "Поскольку $X_1$ стала нулевой матрицей, дальнейшие компоненты строить не нужно. Мы завершаем процесс.\n",
        "\n",
        "\n",
        "\n",
        "### Итоговая модель PLS\n",
        "Итоговая модель PLS выражается через латентные переменные и нагрузки:\n",
        "$$\n",
        "X_c = t_1 p_1^T, \\quad Y_c = t_1 q_1.\n",
        "$$\n",
        "\n",
        "Предсказание для новых данных $X_{\\text{new}}$ выполняется через проекцию на латентные переменные:\n",
        "$$\n",
        "Y_{\\text{new}} = \\bar{Y} + t_{\\text{new}} q_1.\n",
        "$$\n",
        "\n",
        "Таким образом, в этом примере мы построили первую компоненту PLS, которая полностью объясняет данные $X$. На практике, если данные более сложные, может потребоваться построение нескольких компонент.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Реализуем метод частичных наименьших квадратов (PLS) на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация PLS с нуля"
      ],
      "metadata": {
        "id": "IzI7KweLyaiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PLS:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components  # Количество компонент\n",
        "        self.weights = None  # Весовые векторы\n",
        "        self.loadings_p = None  # Нагрузки для X\n",
        "        self.loadings_q = None  # Нагрузки для Y\n",
        "        self.t = None  # Латентные переменные\n",
        "        self.X_mean = None  # Средние значения X\n",
        "        self.Y_mean = None  # Средние значения Y\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # Центрирование данных\n",
        "        self.X_mean = np.mean(X, axis=0)\n",
        "        self.Y_mean = np.mean(Y, axis=0)\n",
        "        X_c = X - self.X_mean\n",
        "        Y_c = Y - self.Y_mean\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((n_features, self.n_components))\n",
        "        self.loadings_p = np.zeros((n_features, self.n_components))\n",
        "        self.loadings_q = np.zeros((Y.shape[1], self.n_components))\n",
        "        self.t = np.zeros((n_samples, self.n_components))\n",
        "\n",
        "        for k in range(self.n_components):\n",
        "            # Шаг 1: Вычисление весового вектора w\n",
        "            w = (X_c.T @ Y_c) / np.linalg.norm(X_c.T @ Y_c)\n",
        "            w = w.flatten()  # Преобразуем в одномерный массив\n",
        "            self.weights[:, k] = w\n",
        "\n",
        "            # Шаг 2: Вычисление латентной переменной t\n",
        "            t = X_c @ w\n",
        "            self.t[:, k] = t\n",
        "\n",
        "            # Шаг 3: Вычисление нагрузок p и q\n",
        "            p = (X_c.T @ t) / (t.T @ t)\n",
        "            q = (Y_c.T @ t) / (t.T @ t)\n",
        "            self.loadings_p[:, k] = p\n",
        "            self.loadings_q[:, k] = q.flatten()  # Преобразуем в одномерный массив\n",
        "\n",
        "            # Шаг 4: Обновление остатков\n",
        "            X_c = X_c - t[:, np.newaxis] @ p[np.newaxis, :]\n",
        "            Y_c = Y_c - t[:, np.newaxis] @ q[np.newaxis, :]\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Центрирование новых данных\n",
        "        X_c = X - self.X_mean\n",
        "\n",
        "        # Проецирование на латентные переменные\n",
        "        t = X_c @ self.weights\n",
        "\n",
        "        # Предсказание Y\n",
        "        Y_pred = self.Y_mean + t @ self.loadings_q.T\n",
        "        return Y_pred\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "Y = np.array([1, 2, 3, 4]).reshape(-1, 1)\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "pls = PLS(n_components=1)\n",
        "pls.fit(X, Y)\n",
        "\n",
        "# Предсказание\n",
        "Y_pred = pls.predict(X)\n",
        "print(\"Предсказанные значения Y:\\n\", Y_pred)"
      ],
      "metadata": {
        "id": "ZRxqNFbm0FNh",
        "outputId": "8ddcc6f1-335b-43c1-cf1a-65cedf877210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные значения Y:\n",
            " [[1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn\n",
        "\n",
        "Библиотека scikit-learn предоставляет реализацию PLS в модуле sklearn.cross_decomposition."
      ],
      "metadata": {
        "id": "fvwOJThk0HDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "pls_sklearn = PLSRegression(n_components=1)\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "Y = np.array([1, 2, 3, 4]).reshape(-1, 1)\n",
        "\n",
        "pls_sklearn.fit(X, Y)\n",
        "\n",
        "# Предсказание\n",
        "Y_pred_sklearn = pls_sklearn.predict(X)\n",
        "print(\"Предсказанные значения Y (scikit-learn):\\n\", Y_pred_sklearn)"
      ],
      "metadata": {
        "id": "z8kFzVGa0NVB",
        "outputId": "daab8e9c-e135-47ce-ac3a-1a8a5266e2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные значения Y (scikit-learn):\n",
            " [[1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример снижения размерности"
      ],
      "metadata": {
        "id": "076IG77Q0auk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PLS:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components  # Количество компонент\n",
        "        self.weights = None  # Весовые векторы\n",
        "        self.loadings_p = None  # Нагрузки для X\n",
        "        self.loadings_q = None  # Нагрузки для Y\n",
        "        self.t = None  # Латентные переменные\n",
        "        self.X_mean = None  # Средние значения X\n",
        "        self.Y_mean = None  # Средние значения Y\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # Центрирование данных\n",
        "        self.X_mean = np.mean(X, axis=0)\n",
        "        self.Y_mean = np.mean(Y, axis=0)\n",
        "        X_c = X - self.X_mean\n",
        "        Y_c = Y - self.Y_mean\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((n_features, self.n_components))\n",
        "        self.loadings_p = np.zeros((n_features, self.n_components))\n",
        "        self.loadings_q = np.zeros((Y.shape[1], self.n_components))\n",
        "        self.t = np.zeros((n_samples, self.n_components))\n",
        "\n",
        "        for k in range(self.n_components):\n",
        "            # Шаг 1: Вычисление весового вектора w\n",
        "            cov = X_c.T @ Y_c\n",
        "            norm = np.linalg.norm(cov)\n",
        "            if norm < 1e-10:  # Проверка на близкую к нулю норму\n",
        "                print(f\"Норма близка к нулю на компоненте {k+1}. Прерывание.\")\n",
        "                break\n",
        "            w = cov / norm\n",
        "            w = w.flatten()  # Преобразуем в одномерный массив\n",
        "            self.weights[:, k] = w\n",
        "\n",
        "            # Шаг 2: Вычисление латентной переменной t\n",
        "            t = X_c @ w\n",
        "            self.t[:, k] = t\n",
        "\n",
        "            # Шаг 3: Вычисление нагрузок p и q\n",
        "            p = (X_c.T @ t) / (t.T @ t)\n",
        "            q = (Y_c.T @ t) / (t.T @ t)\n",
        "            self.loadings_p[:, k] = p\n",
        "            self.loadings_q[:, k] = q.flatten()  # Преобразуем в одномерный массив\n",
        "\n",
        "            # Шаг 4: Обновление остатков\n",
        "            X_c = X_c - t[:, np.newaxis] @ p[np.newaxis, :]\n",
        "            Y_c = Y_c - t[:, np.newaxis] @ q[np.newaxis, :]\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Центрирование новых данных\n",
        "        X_c = X - self.X_mean\n",
        "\n",
        "        # Проецирование на латентные переменные\n",
        "        return X_c @ self.weights\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "Y = np.array([1, 2, 3, 4]).reshape(-1, 1)  # Целевая переменная\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "pls = PLS(n_components=2)\n",
        "pls.fit(X, Y)\n",
        "\n",
        "# Преобразуем данные в латентное пространство\n",
        "X_transformed = pls.transform(X)\n",
        "print(\"Преобразованные данные (латентные переменные):\\n\", X_transformed)"
      ],
      "metadata": {
        "id": "QlnkSJfK1Xjp",
        "outputId": "7dec7b2c-bd47-4e49-d8d4-dbba27f530ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Норма близка к нулю на компоненте 2. Прерывание.\n",
            "Преобразованные данные (латентные переменные):\n",
            " [[-7.79422863  0.        ]\n",
            " [-2.59807621  0.        ]\n",
            " [ 2.59807621  0.        ]\n",
            " [ 7.79422863  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "Y = np.array([1, 2, 3, 4]).reshape(-1, 1)  # Целевая переменная\n",
        "\n",
        "# Масштабирование и центрирование данных\n",
        "scaler = StandardScaler(with_std=False)  # Только центрирование\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Y_scaled = scaler.fit_transform(Y)\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "pls_sklearn = PLSRegression(n_components=2)\n",
        "pls_sklearn.fit(X_scaled, Y_scaled)\n",
        "\n",
        "# Преобразуем данные в латентное пространство\n",
        "X_transformed_sklearn = pls_sklearn.transform(X_scaled)\n",
        "print(\"Преобразованные данные (scikit-learn):\\n\", X_transformed_sklearn)"
      ],
      "metadata": {
        "id": "EKA2-lMo1Fm2",
        "outputId": "c3818505-0aa8-4246-dfd8-e0eb2825dde4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Преобразованные данные (scikit-learn):\n",
            " [[-2.01246118  0.        ]\n",
            " [-0.67082039  0.        ]\n",
            " [ 0.67082039  0.        ]\n",
            " [ 2.01246118  0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cross_decomposition/_pls.py:348: UserWarning: y residual is constant at iteration 1\n",
            "  warnings.warn(f\"y residual is constant at iteration {k}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.4 Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Обобщенный дискриминантный анализ (GDA, Generalized Discriminant Analysis) — это мощный метод многомерного статистического анализа, который расширяет классический линейный дискриминантный анализ (LDA) для работы с нелинейно разделимыми данными. В отличие от LDA, который предполагает линейную разделимость классов, GDA использует ядерные методы (kernel methods) для отображения данных в пространство более высокой размерности, где они становятся линейно разделимыми. Это позволяет применять линейные методы классификации и снижения размерности в задачах, где данные имеют сложную нелинейную структуру.\n",
        "\n",
        "В данной лекции мы подробно рассмотрим математические основы GDA, его применение для снижения размерности, а также использование в задачах регрессии.\n",
        "\n",
        "\n",
        "\n",
        "### 1. Математические основы GDA\n",
        "\n",
        "#### 1.1 Линейный дискриминантный анализ (LDA)\n",
        "\n",
        "LDA — это метод, который ищет линейные комбинации признаков, наилучшим образом разделяющие классы. Он предполагает, что данные распределены по многомерному нормальному распределению с одинаковой ковариационной матрицей для всех классов. Цель LDA — максимизировать отношение межклассовой дисперсии к внутриклассовой дисперсии.\n",
        "\n",
        "Формально, для двух классов LDA максимизирует критерий:\n",
        "$$\n",
        "J(\\mathbf{w}) = \\frac{\\mathbf{w}^T \\mathbf{S}_B \\mathbf{w}}{\\mathbf{w}^T \\mathbf{S}_W \\mathbf{w}},\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{w}$ — вектор весов,\n",
        "- $\\mathbf{S}_B$ — межклассовая матрица рассеяния,\n",
        "- $\\mathbf{S}_W$ — внутриклассовая матрица рассеяния.\n",
        "\n",
        "Решение задачи LDA сводится к нахождению собственных векторов матрицы $\\mathbf{S}_W^{-1} \\mathbf{S}_B$.\n",
        "\n",
        "#### 1.2 Обобщенный дискриминантный анализ (GDA)\n",
        "\n",
        "GDA расширяет LDA, используя ядерные функции для отображения данных в пространство более высокой размерности $\\mathcal{F}$. Пусть $\\phi(\\mathbf{x})$ — это отображение исходного признакового пространства в новое пространство $\\mathcal{F}$. В этом пространстве задача LDA формулируется как:\n",
        "$$\n",
        "J(\\mathbf{w}) = \\frac{\\mathbf{w}^T \\mathbf{S}_B^\\phi \\mathbf{w}}{\\mathbf{w}^T \\mathbf{S}_W^\\phi \\mathbf{w}},\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{S}_B^\\phi$ — межклассовая матрица рассеяния в пространстве $\\mathcal{F}$,\n",
        "- $\\mathbf{S}_W^\\phi$ — внутриклассовая матрица рассеяния в пространстве $\\mathcal{F}$.\n",
        "\n",
        "#### 1.3 Ядерные функции\n",
        "\n",
        "Ядерная функция $K(\\mathbf{x}_i, \\mathbf{x}_j)$ позволяет вычислять скалярное произведение в пространстве $\\mathcal{F}$ без явного вычисления отображения $\\phi(\\mathbf{x})$:\n",
        "$$\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle.\n",
        "$$\n",
        "Примеры ядерных функций:\n",
        "- Линейное ядро: $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j$,\n",
        "- Полиномиальное ядро: $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d$,\n",
        "- Радиальное базисное ядро (RBF): $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$.\n",
        "\n",
        "#### 1.4 Решение задачи GDA\n",
        "\n",
        "В GDA решение ищется в виде линейной комбинации ядерных функций:\n",
        "$$\n",
        "\\mathbf{w} = \\sum_{i=1}^N \\alpha_i \\phi(\\mathbf{x}_i),\n",
        "$$\n",
        "где $\\alpha_i$ — коэффициенты, которые необходимо найти.\n",
        "\n",
        "Подставляя это выражение в критерий $J(\\mathbf{w})$, получаем задачу оптимизации:\n",
        "$$\n",
        "J(\\boldsymbol{\\alpha}) = \\frac{\\boldsymbol{\\alpha}^T \\mathbf{K}_B \\boldsymbol{\\alpha}}{\\boldsymbol{\\alpha}^T \\mathbf{K}_W \\boldsymbol{\\alpha}},\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{K}_B$ — межклассовая матрица рассеяния, выраженная через ядерную матрицу,\n",
        "- $\\mathbf{K}_W$ — внутриклассовая матрица рассеяния, выраженная через ядерную матрицу.\n",
        "\n",
        "Решение этой задачи сводится к нахождению собственных векторов и собственных значений обобщенной задачи на собственные значения:\n",
        "$$\n",
        "\\mathbf{K}_B \\boldsymbol{\\alpha} = \\lambda \\mathbf{K}_W \\boldsymbol{\\alpha}.\n",
        "$$\n",
        "\n",
        "#### 1.5 Классификация\n",
        "\n",
        "После нахождения коэффициентов $\\boldsymbol{\\alpha}$ классификация нового объекта $\\mathbf{x}$ выполняется следующим образом:\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^N \\alpha_i K(\\mathbf{x}_i, \\mathbf{x}) + b,\n",
        "$$\n",
        "где $b$ — смещение, которое может быть найдено из условий классификации.\n",
        "\n",
        "\n",
        "\n",
        "### 2. GDA для снижения размерности\n",
        "\n",
        "GDA может использоваться не только для классификации, но и для снижения размерности данных. В этом случае цель GDA — найти проекцию данных в подпространство меньшей размерности, которое сохраняет максимальную информацию о разделении классов.\n",
        "\n",
        "#### 2.1 Алгоритм снижения размерности с использованием GDA\n",
        "\n",
        "1. **Отображение данных**: Используя ядерную функцию, данные отображаются в пространство $\\mathcal{F}$.\n",
        "2. **Решение задачи GDA**: Находятся собственные векторы $\\boldsymbol{\\alpha}$, соответствующие наибольшим собственным значениям задачи $\\mathbf{K}_B \\boldsymbol{\\alpha} = \\lambda \\mathbf{K}_W \\boldsymbol{\\alpha}$.\n",
        "3. **Проекция данных**: Данные проецируются на подпространство, заданное найденными собственными векторами.\n",
        "\n",
        "Результатом является набор признаков меньшей размерности, которые сохраняют информацию о разделении классов.\n",
        "\n",
        "\n",
        "\n",
        "### 3. GDA для задачи регрессии\n",
        "\n",
        "Хотя GDA чаще используется для классификации и снижения размерности, его можно адаптировать и для задач регрессии. В этом случае цель — предсказать непрерывную целевую переменную на основе входных признаков.\n",
        "\n",
        "#### 3.1 Адаптация GDA для регрессии\n",
        "\n",
        "1. **Отображение данных**: Используя ядерную функцию, данные отображаются в пространство $\\mathcal{F}$.\n",
        "2. **Решение задачи регрессии**: В пространстве $\\mathcal{F}$ строится линейная модель регрессии:\n",
        "   $$\n",
        "   y = \\mathbf{w}^T \\phi(\\mathbf{x}) + b,\n",
        "   $$\n",
        "   где $\\mathbf{w}$ — вектор весов, $b$ — смещение.\n",
        "3. **Оптимизация**: Коэффициенты $\\mathbf{w}$ и $b$ находятся путем минимизации функции потерь, например, среднеквадратичной ошибки (MSE).\n",
        "\n",
        "#### 3.2 Преимущества GDA в регрессии\n",
        "\n",
        "- **Гибкость**: GDA позволяет моделировать сложные нелинейные зависимости между признаками и целевой переменной.\n",
        "- **Ядерные методы**: Использование ядерных функций позволяет избежать явного вычисления отображения в пространство высокой размерности.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Преимущества и недостатки GDA\n",
        "\n",
        "#### Преимущества:\n",
        "1. **Гибкость**: GDA может работать с нелинейно разделимыми данными.\n",
        "2. **Эффективность**: Использование ядерных функций позволяет избежать явного вычисления отображения в пространство высокой размерности.\n",
        "3. **Универсальность**: GDA может использоваться для классификации, снижения размерности и регрессии.\n",
        "\n",
        "#### Недостатки:\n",
        "1. **Вычислительная сложность**: GDA требует вычисления ядерной матрицы, что может быть затратно для больших наборов данных.\n",
        "2. **Выбор ядра**: Результаты GDA сильно зависят от выбора ядерной функции и её параметров.\n",
        "3. **Интерпретируемость**: Результаты GDA сложнее интерпретировать по сравнению с линейными методами.\n",
        "\n",
        "Таким образом, обобщенный дискриминантный анализ (GDA) — это мощный инструмент для работы с нелинейно разделимыми данными. Он расширяет возможности классического LDA, используя ядерные методы для отображения данных в пространство более высокой размерности. GDA может применяться для классификации, снижения размерности и даже регрессии, что делает его универсальным методом в машинном обучении. Однако успешное применение GDA требует тщательного выбора ядерной функции и учёта вычислительной сложности метода.\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что эти точки принадлежат двум классам:\n",
        "- Класс 1: первые две точки ($[1, 2, 3]$ и $[4, 5, 6]$),\n",
        "- Класс 2: последние две точки ($[7, 8, 9]$ и $[10, 11, 12]$).\n",
        "\n",
        "Цель — снизить размерность данных с 3D до 1D с использованием GDA.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Выбор ядерной функции\n",
        "\n",
        "Для простоты выберем линейное ядро:\n",
        "$$\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление ядерной матрицы\n",
        "\n",
        "Ядерная матрица $K$ вычисляется как попарное скалярное произведение всех точек данных:\n",
        "$$\n",
        "K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j).\n",
        "$$\n",
        "\n",
        "Для нашего набора данных:\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "\\mathbf{x}_1^T \\mathbf{x}_1 & \\mathbf{x}_1^T \\mathbf{x}_2 & \\mathbf{x}_1^T \\mathbf{x}_3 & \\mathbf{x}_1^T \\mathbf{x}_4 \\\\\n",
        "\\mathbf{x}_2^T \\mathbf{x}_1 & \\mathbf{x}_2^T \\mathbf{x}_2 & \\mathbf{x}_2^T \\mathbf{x}_3 & \\mathbf{x}_2^T \\mathbf{x}_4 \\\\\n",
        "\\mathbf{x}_3^T \\mathbf{x}_1 & \\mathbf{x}_3^T \\mathbf{x}_2 & \\mathbf{x}_3^T \\mathbf{x}_3 & \\mathbf{x}_3^T \\mathbf{x}_4 \\\\\n",
        "\\mathbf{x}_4^T \\mathbf{x}_1 & \\mathbf{x}_4^T \\mathbf{x}_2 & \\mathbf{x}_4^T \\mathbf{x}_3 & \\mathbf{x}_4^T \\mathbf{x}_4\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Вычислим каждое скалярное произведение:\n",
        "$$\n",
        "\\mathbf{x}_1^T \\mathbf{x}_1 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14, \\\\\n",
        "\\mathbf{x}_1^T \\mathbf{x}_2 = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32, \\\\\n",
        "\\mathbf{x}_1^T \\mathbf{x}_3 = 1 \\cdot 7 + 2 \\cdot 8 + 3 \\cdot 9 = 7 + 16 + 27 = 50, \\\\\n",
        "\\mathbf{x}_1^T \\mathbf{x}_4 = 1 \\cdot 10 + 2 \\cdot 11 + 3 \\cdot 12 = 10 + 22 + 36 = 68, \\\\\n",
        "\\mathbf{x}_2^T \\mathbf{x}_2 = 4^2 + 5^2 + 6^2 = 16 + 25 + 36 = 77, \\\\\n",
        "\\mathbf{x}_2^T \\mathbf{x}_3 = 4 \\cdot 7 + 5 \\cdot 8 + 6 \\cdot 9 = 28 + 40 + 54 = 122, \\\\\n",
        "\\mathbf{x}_2^T \\mathbf{x}_4 = 4 \\cdot 10 + 5 \\cdot 11 + 6 \\cdot 12 = 40 + 55 + 72 = 167, \\\\\n",
        "\\mathbf{x}_3^T \\mathbf{x}_3 = 7^2 + 8^2 + 9^2 = 49 + 64 + 81 = 194, \\\\\n",
        "\\mathbf{x}_3^T \\mathbf{x}_4 = 7 \\cdot 10 + 8 \\cdot 11 + 9 \\cdot 12 = 70 + 88 + 108 = 266, \\\\\n",
        "\\mathbf{x}_4^T \\mathbf{x}_4 = 10^2 + 11^2 + 12^2 = 100 + 121 + 144 = 365.\n",
        "$$\n",
        "\n",
        "Таким образом, ядерная матрица:\n",
        "$$\n",
        "K = \\begin{bmatrix}\n",
        "14 & 32 & 50 & 68 \\\\\n",
        "32 & 77 & 122 & 167 \\\\\n",
        "50 & 122 & 194 & 266 \\\\\n",
        "68 & 167 & 266 & 365\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Вычисление межклассовой и внутриклассовой матриц рассеяния\n",
        "\n",
        "#### 3.1 Межклассовая матрица рассеяния $\\mathbf{K}_B$\n",
        "\n",
        "Межклассовая матрица рассеяния вычисляется как:\n",
        "$$\n",
        "\\mathbf{K}_B = \\sum_{c=1}^C n_c (\\mathbf{m}_c - \\mathbf{m})(\\mathbf{m}_c - \\mathbf{m})^T,\n",
        "$$\n",
        "где:\n",
        "- $C$ — количество классов,\n",
        "- $n_c$ — количество точек в классе $c$,\n",
        "- $\\mathbf{m}_c$ — средний вектор класса $c$ в пространстве $\\mathcal{F}$,\n",
        "- $\\mathbf{m}$ — общий средний вектор всех данных в пространстве $\\mathcal{F}$.\n",
        "\n",
        "Для нашего примера:\n",
        "- Класс 1: $\\mathbf{m}_1 = \\frac{1}{2} (\\mathbf{x}_1 + \\mathbf{x}_2) = \\frac{1}{2} ([1, 2, 3] + [4, 5, 6]) = [2.5, 3.5, 4.5]$,\n",
        "- Класс 2: $\\mathbf{m}_2 = \\frac{1}{2} (\\mathbf{x}_3 + \\mathbf{x}_4) = \\frac{1}{2} ([7, 8, 9] + [10, 11, 12]) = [8.5, 9.5, 10.5]$,\n",
        "- Общий средний вектор: $\\mathbf{m} = \\frac{1}{4} (\\mathbf{x}_1 + \\mathbf{x}_2 + \\mathbf{x}_3 + \\mathbf{x}_4) = \\frac{1}{4} ([1, 2, 3] + [4, 5, 6] + [7, 8, 9] + [10, 11, 12]) = [5.5, 6.5, 7.5]$.\n",
        "\n",
        "Теперь вычислим $\\mathbf{K}_B$:\n",
        "$$\n",
        "\\mathbf{K}_B = 2 (\\mathbf{m}_1 - \\mathbf{m})(\\mathbf{m}_1 - \\mathbf{m})^T + 2 (\\mathbf{m}_2 - \\mathbf{m})(\\mathbf{m}_2 - \\mathbf{m})^T.\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "$$\n",
        "\\mathbf{m}_1 - \\mathbf{m} = [2.5, 3.5, 4.5] - [5.5, 6.5, 7.5] = [-3, -3, -3], \\\\\n",
        "\\mathbf{m}_2 - \\mathbf{m} = [8.5, 9.5, 10.5] - [5.5, 6.5, 7.5] = [3, 3, 3].\n",
        "$$\n",
        "\n",
        "Тогда:\n",
        "$$\n",
        "\\mathbf{K}_B = 2 \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix} \\begin{bmatrix} -3 & -3 & -3 \\end{bmatrix} + 2 \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 3 & 3 & 3 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Вычислим внешние произведения:\n",
        "$$\n",
        "\\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix} \\begin{bmatrix} -3 & -3 & -3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}, \\\\\n",
        "\\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 3 & 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Таким образом:\n",
        "$$\n",
        "\\mathbf{K}_B = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} + 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### 3.2 Внутриклассовая матрица рассеяния $\\mathbf{K}_W$\n",
        "\n",
        "Внутриклассовая матрица рассеяния вычисляется как:\n",
        "$$\n",
        "\\mathbf{K}_W = \\sum_{c=1}^C \\sum_{i \\in c} (\\mathbf{x}_i - \\mathbf{m}_c)(\\mathbf{x}_i - \\mathbf{m}_c)^T.\n",
        "$$\n",
        "\n",
        "Для нашего примера:\n",
        "$$\n",
        "\\mathbf{K}_W = \\sum_{i=1}^2 (\\mathbf{x}_i - \\mathbf{m}_1)(\\mathbf{x}_i - \\mathbf{m}_1)^T + \\sum_{i=3}^4 (\\mathbf{x}_i - \\mathbf{m}_2)(\\mathbf{x}_i - \\mathbf{m}_2)^T.\n",
        "$$\n",
        "\n",
        "Вычислим каждое слагаемое:\n",
        "$$\n",
        "\\mathbf{x}_1 - \\mathbf{m}_1 = [1, 2, 3] - [2.5, 3.5, 4.5] = [-1.5, -1.5, -1.5], \\\\\n",
        "\\mathbf{x}_2 - \\mathbf{m}_1 = [4, 5, 6] - [2.5, 3.5, 4.5] = [1.5, 1.5, 1.5], \\\\\n",
        "\\mathbf{x}_3 - \\mathbf{m}_2 = [7, 8, 9] - [8.5, 9.5, 10.5] = [-1.5, -1.5, -1.5], \\\\\n",
        "\\mathbf{x}_4 - \\mathbf{m}_2 = [10, 11, 12] - [8.5, 9.5, 10.5] = [1.5, 1.5, 1.5].\n",
        "$$\n",
        "\n",
        "Теперь вычислим внешние произведения:\n",
        "$$\n",
        "(\\mathbf{x}_1 - \\mathbf{m}_1)(\\mathbf{x}_1 - \\mathbf{m}_1)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}, \\\\\n",
        "(\\mathbf{x}_2 - \\mathbf{m}_1)(\\mathbf{x}_2 - \\mathbf{m}_1)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}, \\\\\n",
        "(\\mathbf{x}_3 - \\mathbf{m}_2)(\\mathbf{x}_3 - \\mathbf{m}_2)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}, \\\\\n",
        "(\\mathbf{x}_4 - \\mathbf{m}_2)(\\mathbf{x}_4 - \\mathbf{m}_2)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Таким образом:\n",
        "$$\n",
        "\\mathbf{K}_W = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Решение обобщенной задачи на собственные значения\n",
        "\n",
        "Теперь решим задачу:\n",
        "$$\n",
        "\\mathbf{K}_B \\boldsymbol{\\alpha} = \\lambda \\mathbf{K}_W \\boldsymbol{\\alpha}.\n",
        "$$\n",
        "\n",
        "Подставим $\\mathbf{K}_B$ и $\\mathbf{K}_W$:\n",
        "$$\n",
        "\\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix} \\boldsymbol{\\alpha} = \\lambda \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} \\boldsymbol{\\alpha}.\n",
        "$$\n",
        "\n",
        "Упростим уравнение, разделив обе части на 9:\n",
        "$$\n",
        "\\begin{bmatrix} 4 & 4 & 4 \\\\ 4 & 4 & 4 \\\\ 4 & 4 & 4 \\end{bmatrix} \\boldsymbol{\\alpha} = \\lambda \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} \\boldsymbol{\\alpha}.\n",
        "$$\n",
        "\n",
        "Это уравнение имеет нетривиальное решение только если $\\lambda = 4$. Собственный вектор $\\boldsymbol{\\alpha}$ может быть любым вектором, пропорциональным $[1, 1, 1]$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Проекция данных\n",
        "\n",
        "Используем найденный собственный вектор $\\boldsymbol{\\alpha} = [1, 1, 1]$ для проекции данных в 1D-пространство. Проекция вычисляется как:\n",
        "$$\n",
        "y_i = \\sum_{j=1}^4 \\alpha_j K(\\mathbf{x}_j, \\mathbf{x}_i).\n",
        "$$\n",
        "\n",
        "Для нашего примера:\n",
        "$$\n",
        "y_1 = 1 \\cdot 14 + 1 \\cdot 32 + 1 \\cdot 50 + 1 \\cdot 68 = 164, \\\\\n",
        "y_2 = 1 \\cdot 32 + 1 \\cdot 77 + 1 \\cdot 122 + 1 \\cdot 167 = 398, \\\\\n",
        "y_3 = 1 \\cdot 50 + 1 \\cdot 122 + 1 \\cdot 194 + 1 \\cdot 266 = 632, \\\\\n",
        "y_4 = 1 \\cdot 68 + 1 \\cdot 167 + 1 \\cdot 266 + 1 \\cdot 365 = 866.\n",
        "$$\n",
        "\n",
        "Таким образом, проекция данных в 1D-пространство:\n",
        "$$\n",
        "\\mathbf{y} = [164, 398, 632, 866].\n",
        "$$\n",
        "\n",
        "Таким образом, мы успешно применили GDA для снижения размерности данных с 3D до 1D. Проекция данных сохранила информацию о разделении классов, что делает GDA полезным инструментом для анализа многомерных данных.\n",
        "\n",
        "Реализуем пример GDA для задачи снижения размерности на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация с нуля\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZsvcBfG26jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GDA:\n",
        "    def __init__(self, kernel='linear'):\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def _linear_kernel(self, X1, X2):\n",
        "        return np.dot(X1, X2.T)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.classes = np.unique(y)\n",
        "        self.n_classes = len(self.classes)\n",
        "        self.n_features = X.shape[1]  # Количество признаков\n",
        "\n",
        "        # Вычисляем межклассовую и внутриклассовую матрицы рассеяния\n",
        "        self.S_B = np.zeros((self.n_features, self.n_features), dtype=np.float64)\n",
        "        self.S_W = np.zeros((self.n_features, self.n_features), dtype=np.float64)\n",
        "\n",
        "        overall_mean = np.mean(X, axis=0)\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            mean_c = np.mean(X_c, axis=0)\n",
        "            n_c = X_c.shape[0]\n",
        "\n",
        "            # Межклассовая матрица рассеяния\n",
        "            diff = mean_c - overall_mean\n",
        "            self.S_B += n_c * np.outer(diff, diff)\n",
        "\n",
        "            # Внутриклассовая матрица рассеяния\n",
        "            for x in X_c:\n",
        "                diff = x - mean_c\n",
        "                self.S_W += np.outer(diff, diff)\n",
        "\n",
        "        # Решаем обобщенную задачу на собственные значения\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(np.linalg.pinv(self.S_W) @ self.S_B)\n",
        "        self.eigenvalues = eigenvalues\n",
        "        self.eigenvectors = eigenvectors\n",
        "\n",
        "        # Сортируем собственные векторы по убыванию собственных значений\n",
        "        idx = np.argsort(eigenvalues)[::-1]\n",
        "        self.eigenvalues = eigenvalues[idx]\n",
        "        self.eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "    def transform(self, X, n_components=1):\n",
        "        # Проецируем данные на собственные векторы\n",
        "        projection = np.dot(X, self.eigenvectors[:, :n_components])\n",
        "        return projection\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "], dtype=np.float64)  # Убедимся, что входные данные имеют тип float64\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "gda = GDA()\n",
        "gda.fit(X, y)\n",
        "\n",
        "# Снижение размерности до 1D\n",
        "X_transformed = gda.transform(X, n_components=1)\n",
        "print(\"Проекция данных (GDA с нуля):\")\n",
        "print(X_transformed)"
      ],
      "metadata": {
        "id": "Hoz-qzVB45Ot",
        "outputId": "e8dafbc8-fb26-4431-8765-d375dbbf6b62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (GDA с нуля):\n",
            "[[ 3.46410162+0.j]\n",
            " [ 8.66025404+0.j]\n",
            " [13.85640646+0.j]\n",
            " [19.05255888+0.j]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn\n",
        "\n",
        "В scikit-learn нет прямой реализации GDA, но мы можем использовать KernelPCA (ядровый метод главных компонент) для аналогичной задачи."
      ],
      "metadata": {
        "id": "LCU6L-JB5CfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "\n",
        "# Пример использования KernelPCA\n",
        "kpca = KernelPCA(n_components=1, kernel='linear')\n",
        "X_transformed_kpca = kpca.fit_transform(X)\n",
        "\n",
        "print(\"Проекция данных (KernelPCA из scikit-learn):\")\n",
        "print(X_transformed_kpca)"
      ],
      "metadata": {
        "id": "M9k02lQP5Dik",
        "outputId": "fa51eac3-7ceb-4950-d1cf-161631748526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекция данных (KernelPCA из scikit-learn):\n",
            "[[ 7.79422863]\n",
            " [ 2.59807621]\n",
            " [-2.59807621]\n",
            " [-7.79422863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.5 Канонический корреляционный анализ (CCA, Canonical Correlation Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Канонический корреляционный анализ (Canonical Correlation Analysis, CCA) — это метод многомерного статистического анализа, который позволяет исследовать взаимосвязи между двумя наборами переменных. Этот метод является обобщением множественной корреляции и используется для нахождения таких линейных комбинаций переменных из каждого набора, которые максимизируют корреляцию между этими комбинациями. CCA находит применение в различных областях, таких как машинное обучение, нейробиология, экономика, биоинформатика и других, где требуется анализ сложных взаимосвязей между многомерными данными.\n",
        "\n",
        "Основная идея CCA заключается в том, чтобы найти пары линейных комбинаций (канонических переменных) для двух наборов переменных, которые имеют максимальную корреляцию между собой. Это позволяет выявить скрытые структуры и взаимосвязи между наборами данных, которые могут быть неочевидны при анализе отдельных переменных.\n",
        "\n",
        "\n",
        "\n",
        "#### 1. Основные понятия и постановка задачи\n",
        "\n",
        "Предположим, у нас есть два набора переменных:\n",
        "- $\\mathbf{X} = (X_1, X_2, \\dots, X_p)$ — первый набор из $p$ переменных,\n",
        "- $\\mathbf{Y} = (Y_1, Y_2, \\dots, Y_q)$ — второй набор из $q$ переменных.\n",
        "\n",
        "Эти наборы могут представлять, например, характеристики объектов в двух различных пространствах (например, психологические тесты и физиологические показатели).\n",
        "\n",
        "Цель CCA — найти такие линейные комбинации переменных из каждого набора:\n",
        "- $U = \\mathbf{a}^T \\mathbf{X} = a_1 X_1 + a_2 X_2 + \\dots + a_p X_p$,\n",
        "- $V = \\mathbf{b}^T \\mathbf{Y} = b_1 Y_1 + b_2 Y_2 + \\dots + b_q Y_q$,\n",
        "\n",
        "где:\n",
        "- $\\mathbf{a} = (a_1, a_2, \\dots, a_p)^T$ — вектор весов для первого набора переменных $\\mathbf{X}$,\n",
        "- $\\mathbf{b} = (b_1, b_2, \\dots, b_q)^T$ — вектор весов для второго набора переменных $\\mathbf{Y}$.\n",
        "\n",
        "Эти линейные комбинации $U$ и $V$ называются **каноническими переменными**. Задача CCA заключается в нахождении таких векторов $\\mathbf{a}$ и $\\mathbf{b}$, чтобы корреляция между $U$ и $V$ была максимальной.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Математическая формулировка задачи\n",
        "\n",
        "Корреляция между каноническими переменными $U$ и $V$ выражается как:\n",
        "$$\n",
        "\\rho = \\text{corr}(U, V) = \\frac{\\text{cov}(U, V)}{\\sqrt{\\text{var}(U) \\cdot \\text{var}(V)}},\n",
        "$$\n",
        "где:\n",
        "- $\\text{cov}(U, V)$ — ковариация между $U$ и $V$,\n",
        "- $\\text{var}(U)$ — дисперсия $U$,\n",
        "- $\\text{var}(V)$ — дисперсия $V$.\n",
        "\n",
        "Выразим эти величины через ковариационные матрицы:\n",
        "- $\\text{cov}(U, V) = \\mathbf{a}^T \\Sigma_{XY} \\mathbf{b}$, где $\\Sigma_{XY}$ — кросс-ковариационная матрица между $\\mathbf{X}$ и $\\mathbf{Y}$,\n",
        "- $\\text{var}(U) = \\mathbf{a}^T \\Sigma_{XX} \\mathbf{a}$, где $\\Sigma_{XX}$ — ковариационная матрица для $\\mathbf{X}$,\n",
        "- $\\text{var}(V) = \\mathbf{b}^T \\Sigma_{YY} \\mathbf{b}$, где $\\Sigma_{YY}$ — ковариационная матрица для $\\mathbf{Y}$.\n",
        "\n",
        "Таким образом, задача CCA формулируется как максимизация корреляции $\\rho$:\n",
        "$$\n",
        "\\rho = \\max_{\\mathbf{a}, \\mathbf{b}} \\frac{\\mathbf{a}^T \\Sigma_{XY} \\mathbf{b}}{\\sqrt{(\\mathbf{a}^T \\Sigma_{XX} \\mathbf{a}) (\\mathbf{b}^T \\Sigma_{YY} \\mathbf{b})}}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Решение задачи CCA\n",
        "\n",
        "Для решения задачи максимизации корреляции используется метод множителей Лагранжа. Введем функцию Лагранжа:\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{a}, \\mathbf{b}, \\lambda_1, \\lambda_2) = \\mathbf{a}^T \\Sigma_{XY} \\mathbf{b} - \\frac{\\lambda_1}{2} (\\mathbf{a}^T \\Sigma_{XX} \\mathbf{a} - 1) - \\frac{\\lambda_2}{2} (\\mathbf{b}^T \\Sigma_{YY} \\mathbf{b} - 1),\n",
        "$$\n",
        "где $\\lambda_1$ и $\\lambda_2$ — множители Лагранжа. Условия $\\mathbf{a}^T \\Sigma_{XX} \\mathbf{a} = 1$ и $\\mathbf{b}^T \\Sigma_{YY} \\mathbf{b} = 1$ вводятся для нормировки, чтобы избежать тривиальных решений.\n",
        "\n",
        "Дифференцируя $\\mathcal{L}$ по $\\mathbf{a}$ и $\\mathbf{b}$, получаем систему уравнений:\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\Sigma_{XY} \\mathbf{b} = \\lambda_1 \\Sigma_{XX} \\mathbf{a}, \\\\\n",
        "\\Sigma_{YX} \\mathbf{a} = \\lambda_2 \\Sigma_{YY} \\mathbf{b}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Учитывая, что $\\Sigma_{YX} = \\Sigma_{XY}^T$, и предполагая $\\lambda_1 = \\lambda_2 = \\lambda$, систему можно переписать в виде:\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\Sigma_{XX}^{-1} \\Sigma_{XY} \\mathbf{b} = \\lambda \\mathbf{a}, \\\\\n",
        "\\Sigma_{YY}^{-1} \\Sigma_{YX} \\mathbf{a} = \\lambda \\mathbf{b}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Подставляя второе уравнение в первое, получаем:\n",
        "$$\n",
        "\\Sigma_{XX}^{-1} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} \\mathbf{a} = \\lambda^2 \\mathbf{a}.\n",
        "$$\n",
        "\n",
        "Это уравнение является задачей на собственные значения для матрицы $\\Sigma_{XX}^{-1} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX}$. Собственные значения $\\lambda^2$ соответствуют квадратам канонических корреляций, а собственные векторы $\\mathbf{a}$ и $\\mathbf{b}$ — весам канонических переменных.\n",
        "\n",
        "\n",
        "\n",
        "#### 4. Канонические переменные и корреляции\n",
        "\n",
        "Решив задачу на собственные значения, получаем:\n",
        "- Канонические корреляции $\\rho_1, \\rho_2, \\dots, \\rho_k$, где $k = \\min(p, q)$,\n",
        "- Пары канонических переменных $(U_1, V_1), (U_2, V_2), \\dots, (U_k, V_k)$.\n",
        "\n",
        "Каждая пара канонических переменных $(U_i, V_i)$ соответствует своей канонической корреляции $\\rho_i$, причем $\\rho_1 \\geq \\rho_2 \\geq \\dots \\geq \\rho_k$. Первая пара канонических переменных $(U_1, V_1)$ имеет максимальную корреляцию $\\rho_1$, вторая пара $(U_2, V_2)$ — следующую по величине корреляцию $\\rho_2$, и так далее.\n",
        "\n",
        "\n",
        "\n",
        "#### 5. Интерпретация результатов\n",
        "\n",
        "1. **Канонические корреляции**: Показывают силу связи между соответствующими парами канонических переменных. Чем ближе $\\rho_i$ к 1, тем сильнее связь между $U_i$ и $V_i$.\n",
        "2. **Канонические веса**: Векторы $\\mathbf{a}$ и $\\mathbf{b}$ интерпретируются как вклады исходных переменных в канонические переменные. Они показывают, какие переменные из каждого набора наиболее значимы для формирования канонической связи.\n",
        "3. **Канонические переменные**: Линейные комбинации исходных переменных, которые максимизируют корреляцию между наборами. Они могут использоваться для дальнейшего анализа или визуализации.\n",
        "\n",
        "\n",
        "\n",
        "#### 6. Ограничения и предпосылки CCA\n",
        "\n",
        "1. **Линейность**: CCA предполагает линейную зависимость между наборами переменных. Если связь нелинейна, результаты могут быть неинформативными.\n",
        "2. **Многомерная нормальность**: Для корректного применения CCA желательно, чтобы данные были многомерно нормально распределены. Это важно для статистической значимости результатов.\n",
        "3. **Отсутствие мультиколлинеарности**: Высокая корреляция внутри наборов переменных (мультиколлинеарность) может привести к неустойчивости результатов и затруднить интерпретацию канонических весов.\n",
        "\n",
        "Таким образом, канонический корреляционный анализ — мощный инструмент для исследования взаимосвязей между двумя наборами переменных. Он позволяет выявить скрытые структуры в данных и используется в различных приложениях, где требуется анализ многомерных зависимостей. Понимание математической основы CCA важно для корректного применения метода и интерпретации результатов. CCA является важным методом в арсенале многомерного анализа и находит применение в задачах, где необходимо исследовать сложные взаимосвязи между группами переменных.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что у нас есть два набора переменных $\\mathbf{X}$ и $\\mathbf{Y}$, где $\\mathbf{X}$ — это первые два столбца матрицы $X$, а $\\mathbf{Y}$ — третий столбец:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "4 & 5 \\\\\n",
        "7 & 8 \\\\\n",
        "10 & 11\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{Y} = \\begin{bmatrix}\n",
        "3 \\\\\n",
        "6 \\\\\n",
        "9 \\\\\n",
        "12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Наша цель — найти такие линейные комбинации $U = \\mathbf{a}^T \\mathbf{X}$ и $V = \\mathbf{b}^T \\mathbf{Y}$, чтобы корреляция между $U$ и $V$ была максимальной.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Вычисление ковариационных матриц\n",
        "\n",
        "1. **Центрирование данных**: Вычтем средние значения из каждой переменной.\n",
        "\n",
        "   Средние значения для $\\mathbf{X}$:\n",
        "$$\n",
        "   \\bar{X}_1 = \\frac{1 + 4 + 7 + 10}{4} = 5.5, \\quad \\bar{X}_2 = \\frac{2 + 5 + 8 + 11}{4} = 6.5\n",
        "$$\n",
        "\n",
        "   Среднее значение для $\\mathbf{Y}$:\n",
        "$$\n",
        "   \\bar{Y} = \\frac{3 + 6 + 9 + 12}{4} = 7.5\n",
        "$$\n",
        "\n",
        "   Центрированные данные:\n",
        "$$\n",
        "   \\mathbf{X}_c = \\begin{bmatrix}\n",
        "   -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5\n",
        "   \\end{bmatrix}, \\quad\n",
        "   \\mathbf{Y}_c = \\begin{bmatrix}\n",
        "   -4.5 \\\\\n",
        "   -1.5 \\\\\n",
        "   1.5 \\\\\n",
        "   4.5\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Ковариационные матрицы**:\n",
        "   - $\\Sigma_{XX} = \\frac{1}{n-1} \\mathbf{X}_c^T \\mathbf{X}_c$,\n",
        "   - $\\Sigma_{YY} = \\frac{1}{n-1} \\mathbf{Y}_c^T \\mathbf{Y}_c$,\n",
        "   - $\\Sigma_{XY} = \\frac{1}{n-1} \\mathbf{X}_c^T \\mathbf{Y}_c$.\n",
        "\n",
        "   Вычислим:\n",
        "$$\n",
        "   \\Sigma_{XX} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "   -4.5 & -1.5 & 1.5 & 4.5\n",
        "   \\end{bmatrix}\n",
        "   \\begin{bmatrix}\n",
        "   -4.5 & -4.5 \\\\\n",
        "   -1.5 & -1.5 \\\\\n",
        "   1.5 & 1.5 \\\\\n",
        "   4.5 & 4.5\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   15 & 15 \\\\\n",
        "   15 & 15\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "   \\Sigma_{YY} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   -4.5 & -1.5 & 1.5 & 4.5\n",
        "   \\end{bmatrix}\n",
        "   \\begin{bmatrix}\n",
        "   -4.5 \\\\\n",
        "   -1.5 \\\\\n",
        "   1.5 \\\\\n",
        "   4.5\n",
        "   \\end{bmatrix} = 15\n",
        "$$\n",
        "\n",
        "$$\n",
        "   \\Sigma_{XY} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "   -4.5 & -1.5 & 1.5 & 4.5\n",
        "   \\end{bmatrix}\n",
        "   \\begin{bmatrix}\n",
        "   -4.5 \\\\\n",
        "   -1.5 \\\\\n",
        "   1.5 \\\\\n",
        "   4.5\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   15 \\\\\n",
        "   15\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Решение задачи на собственные значения\n",
        "\n",
        "Задача CCA сводится к нахождению собственных значений и собственных векторов матрицы:\n",
        "$$\n",
        "\\Sigma_{XX}^{-1} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX}.\n",
        "$$\n",
        "\n",
        "1. Вычислим $\\Sigma_{XX}^{-1}$:\n",
        "$$\n",
        "   \\Sigma_{XX} = \\begin{bmatrix}\n",
        "   15 & 15 \\\\\n",
        "   15 & 15\n",
        "   \\end{bmatrix}, \\quad\n",
        "   \\Sigma_{XX}^{-1} = \\frac{1}{15 \\cdot 15 - 15 \\cdot 15} \\begin{bmatrix}\n",
        "   15 & -15 \\\\\n",
        "   -15 & 15\n",
        "   \\end{bmatrix} = \\text{не существует (матрица вырождена)}.\n",
        "$$\n",
        "\n",
        "   Поскольку матрица $\\Sigma_{XX}$ вырождена, мы не можем продолжить вычисления в этом примере. Это связано с тем, что данные в $\\mathbf{X}$ линейно зависимы (второй столбец является линейной функцией первого).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Устранение линейной зависимости\n",
        "\n",
        "Чтобы устранить линейную зависимость, изменим матрицу $\\mathbf{X}$, сделав её столбцы линейно независимыми. Например:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "4 & 3 \\\\\n",
        "7 & 5 \\\\\n",
        "10 & 6\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь столбцы $\\mathbf{X}$ линейно независимы. Повторим вычисления.\n",
        "\n",
        "1. **Центрирование данных**:\n",
        "$$\n",
        "   \\bar{X}_1 = 5.5, \\quad \\bar{X}_2 = 4\n",
        "$$\n",
        "$$\n",
        "   \\mathbf{X}_c = \\begin{bmatrix}\n",
        "   -4.5 & -2 \\\\\n",
        "   -1.5 & -1 \\\\\n",
        "   1.5 & 1 \\\\\n",
        "   4.5 & 2\n",
        "   \\end{bmatrix}, \\quad\n",
        "   \\mathbf{Y}_c = \\begin{bmatrix}\n",
        "   -5 \\\\\n",
        "   -2 \\\\\n",
        "   2 \\\\\n",
        "   5\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Ковариационные матрицы**:\n",
        "$$\n",
        "   \\Sigma_{XX} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "   -2 & -1 & 1 & 2\n",
        "   \\end{bmatrix}\n",
        "   \\begin{bmatrix}\n",
        "   -4.5 & -2 \\\\\n",
        "   -1.5 & -1 \\\\\n",
        "   1.5 & 1 \\\\\n",
        "   4.5 & 2\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   15 & 7.5 \\\\\n",
        "   7.5 & 3.33\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "   \\Sigma_{XY} = \\frac{1}{3} \\begin{bmatrix}\n",
        "   -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "   -2 & -1 & 1 & 2\n",
        "   \\end{bmatrix}\n",
        "   \\begin{bmatrix}\n",
        "   -5 \\\\\n",
        "   -2 \\\\\n",
        "   2 \\\\\n",
        "   5\n",
        "   \\end{bmatrix} = \\begin{bmatrix}\n",
        "   17 \\\\\n",
        "   8.33\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. **Решение задачи на собственные значения**:\n",
        "   Теперь матрица $\\Sigma_{XX}$ не вырождена, и мы можем вычислить $\\Sigma_{XX}^{-1}$:\n",
        "$$\n",
        "   \\Sigma_{XX}^{-1} = \\frac{1}{15 \\cdot 3.33 - 7.5 \\cdot 7.5} \\begin{bmatrix}\n",
        "   3.33 & -7.5 \\\\\n",
        "   -7.5 & 15\n",
        "   \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "   Далее можно продолжить вычисления для нахождения канонических корреляций.\n",
        "\n",
        "Такми образом, этот пример показывает, как важно устранять линейную зависимость в данных перед применением CCA. В исправленной версии мы успешно применили метод, сделав матрицу $\\mathbf{X}$ линейно независимой.\n",
        "\n",
        "\n",
        "\n",
        "Реализуем канонический корреляционный анализ (CCA) на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация CCA с нуля\n"
      ],
      "metadata": {
        "id": "JK9ScLCe5qr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CCA:\n",
        "    def __init__(self):\n",
        "        self.a = None  # Коэффициенты для X\n",
        "        self.b = None  # Коэффициенты для Y\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # Центрирование данных\n",
        "        X_centered = X - np.mean(X, axis=0)\n",
        "        Y_centered = Y - np.mean(Y, axis=0)\n",
        "\n",
        "        # Вычисление ковариационных матриц\n",
        "        Sigma_XX = (X_centered.T @ X_centered) / (X.shape[0] - 1)\n",
        "        Sigma_YY = (Y_centered.T @ Y_centered) / (Y.shape[0] - 1)\n",
        "        Sigma_XY = (X_centered.T @ Y_centered) / (X.shape[0] - 1)\n",
        "\n",
        "        # Вычисление обратных матриц\n",
        "        Sigma_XX_inv = np.linalg.inv(Sigma_XX)\n",
        "        Sigma_YY_inv = np.linalg.inv(Sigma_YY)\n",
        "\n",
        "        # Решение задачи на собственные значения\n",
        "        M = Sigma_XX_inv @ Sigma_XY @ Sigma_YY_inv @ Sigma_XY.T\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(M)\n",
        "\n",
        "        # Нахождение канонических коэффициентов\n",
        "        idx = np.argmax(eigenvalues)  # Индекс максимального собственного значения\n",
        "        self.a = eigenvectors[:, idx]  # Коэффициенты для X\n",
        "        self.b = Sigma_YY_inv @ Sigma_XY.T @ self.a  # Коэффициенты для Y\n",
        "\n",
        "    def transform(self, X, Y):\n",
        "        # Преобразование данных\n",
        "        U = X @ self.a\n",
        "        V = Y @ self.b\n",
        "        return U, V\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([[1, 2], [4, 3], [7, 5], [10, 6]])\n",
        "Y = np.array([[3], [6], [10], [13]])\n",
        "\n",
        "cca = CCA()\n",
        "cca.fit(X, Y)\n",
        "U, V = cca.transform(X, Y)\n",
        "\n",
        "print(\"Канонические переменные U:\", U)\n",
        "print(\"Канонические переменные V:\", V)"
      ],
      "metadata": {
        "id": "yB4Y4fta8CHO",
        "outputId": "b69e7b1b-d3f1-44e3-9b61-79ae3f43a9f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Канонические переменные U: [ 2.21880078  4.71495167  8.04315285 10.53930373]\n",
            "Канонические переменные V: [ 2.49615088  4.99230177  8.32050294 10.81665383]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn\n",
        "\n",
        "Библиотека scikit-learn предоставляет встроенный класс CCA для выполнения канонического корреляционного анализа."
      ],
      "metadata": {
        "id": "AclsuASa8Fqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import CCA\n",
        "\n",
        "# Пример данных\n",
        "X = np.array([[1, 2], [4, 3], [7, 5], [10, 6]])\n",
        "Y = np.array([[3], [6], [10], [13]])\n",
        "\n",
        "# Создание и обучение модели CCA\n",
        "cca = CCA(n_components=1)\n",
        "cca.fit(X, Y)\n",
        "\n",
        "# Преобразование данных\n",
        "U, V = cca.transform(X, Y)\n",
        "\n",
        "print(\"Канонические переменные U (scikit-learn):\", U)\n",
        "print(\"Канонические переменные V (scikit-learn):\", V)"
      ],
      "metadata": {
        "id": "mEpwVjDr8J4g",
        "outputId": "ca1b862b-1097-4b14-855c-08e872fbabad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Канонические переменные U (scikit-learn): [[-1.58113883]\n",
            " [-0.63245553]\n",
            " [ 0.63245553]\n",
            " [ 1.58113883]]\n",
            "Канонические переменные V (scikit-learn): [[-1.13714707]\n",
            " [-0.45485883]\n",
            " [ 0.45485883]\n",
            " [ 1.13714707]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Канонические переменные $U$ и $V$, полученные в результате канонического корреляционного анализа (CCA), представляют собой линейные комбинации исходных переменных $\\mathbf{X}$ и $\\mathbf{Y}$, которые максимизируют корреляцию между двумя наборами данных. Давайте разберем, что означают эти результаты и о чем они говорят.\n",
        "\n",
        "\n",
        "\n",
        "### Интерпретация результатов\n",
        "\n",
        "#### 1. **Канонические переменные $U$ и $V$**:\n",
        "   - $U$ — это линейная комбинация переменных из набора $\\mathbf{X}$.\n",
        "   - $V$ — это линейная комбинация переменных из набора $\\mathbf{Y}$.\n",
        "\n",
        "   В вашем случае:\n",
        "$$\n",
        "   U = \\begin{bmatrix}\n",
        "   -1.58113883 \\\\\n",
        "   -0.63245553 \\\\\n",
        "   0.63245553 \\\\\n",
        "   1.58113883\n",
        "   \\end{bmatrix}, \\quad\n",
        "   V = \\begin{bmatrix}\n",
        "   -1.13714707 \\\\\n",
        "   -0.45485883 \\\\\n",
        "   0.45485883 \\\\\n",
        "   1.13714707\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "   Эти значения показывают, как каждая точка данных (строка в $\\mathbf{X}$ и $\\mathbf{Y}$) проецируется на канонические оси.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. **Корреляция между $U$ и $V$**:\n",
        "   CCA находит такие линейные комбинации $U$ и $V$, чтобы корреляция между ними была максимальной. В данном случае:\n",
        "   - Корреляция между $U$ и $V$ будет близка к 1 (или -1, если зависимость обратная), так как CCA максимизирует эту корреляцию.\n",
        "\n",
        "   Это означает, что $U$ и $V$ сильно связаны между собой.\n",
        "\n",
        "\n",
        "\n",
        "#### 3. **Смысл коэффициентов**:\n",
        "   Коэффициенты $\\mathbf{a}$ и $\\mathbf{b}$, которые используются для вычисления $U$ и $V$, показывают вклад каждой исходной переменной в канонические переменные:\n",
        "   - $U = \\mathbf{a}^T \\mathbf{X}$\n",
        "   - $V = \\mathbf{b}^T \\mathbf{Y}$\n",
        "\n",
        "   Например, если $\\mathbf{a} = [a_1, a_2]$, то:\n",
        "$$\n",
        "   U = a_1 \\cdot X_1 + a_2 \\cdot X_2\n",
        "$$\n",
        "   Аналогично для $V$.\n",
        "\n",
        "\n",
        "\n",
        "#### 4. **О чем говорят значения $U$ и $V$**:\n",
        "   - Значения $U$ и $V$ показывают, как каждая точка данных (строка в $\\mathbf{X}$ и $\\mathbf{Y}$) проецируется на канонические оси.\n",
        "   - Если $U$ и $V$ имеют одинаковый знак и близкие значения для одной и той же точки данных, это означает, что между $\\mathbf{X}$ и $\\mathbf{Y}$ существует сильная связь.\n",
        "   - В вашем случае значения $U$ и $V$ для каждой точки данных пропорциональны, что подтверждает сильную корреляцию между $\\mathbf{X}$ и $\\mathbf{Y}$.\n",
        "\n",
        "\n",
        "\n",
        "#### 5. **Пример интерпретации для первой точки данных**:\n",
        "   Для первой точки данных:\n",
        "$$\n",
        "   U_1 = -1.58113883, \\quad V_1 = -1.13714707\n",
        "$$\n",
        "   Это означает:\n",
        "   - Оба значения отрицательные, что указывает на согласованное поведение $\\mathbf{X}$ и $\\mathbf{Y}$ для этой точки.\n",
        "   - Значения $U$ и $V$ пропорциональны, что подтверждает сильную корреляцию.\n",
        "\n",
        "\n",
        "\n",
        "### Практическое применение\n",
        "\n",
        "1. **Анализ взаимосвязи**:\n",
        "   - CCA позволяет понять, как два набора переменных $\\mathbf{X}$ и $\\mathbf{Y}$ связаны между собой.\n",
        "   - В вашем случае результаты показывают, что между $\\mathbf{X}$ и $\\mathbf{Y}$ существует сильная линейная зависимость.\n",
        "\n",
        "2. **Снижение размерности**:\n",
        "   - Канонические переменные $U$ и $V$ можно использовать для представления данных в более низкоразмерном пространстве, сохраняя при этом максимальную корреляцию между наборами.\n",
        "\n",
        "3. **Прогнозирование**:\n",
        "   - Если известны значения $\\mathbf{X}$, можно использовать канонические переменные для прогнозирования значений $\\mathbf{Y}$, и наоборот.\n",
        "\n",
        "\n",
        "Таким образом, результаты CCA показывают, что между наборами переменных $\\mathbf{X}$ и $\\mathbf{Y}$ существует сильная линейная зависимость. Канонические переменные $U$ и $V$ позволяют выразить эту зависимость в компактной форме и могут быть использованы для анализа, снижения размерности или прогнозирования."
      ],
      "metadata": {
        "id": "B3vhMAJ78v7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.6 Контролируемый метод главных компонент (Supervised PCA, Supervised Principal Components Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Метод главных компонент (Principal Component Analysis, PCA) — это один из самых известных и широко используемых методов снижения размерности данных. Его основная идея заключается в том, чтобы найти такие направления в данных (главные компоненты), которые объясняют максимальную дисперсию. Однако классический PCA является **неконтролируемым** методом, то есть он не учитывает информацию о целевой переменной (например, метки классов в задачах классификации или значения целевой переменной в задачах регрессии). Это может быть недостатком, если наша цель — сохранить или выделить информацию, которая важна для предсказания целевой переменной.\n",
        "\n",
        "**Контролируемый метод главных компонент (Supervised PCA)** — это расширение классического PCA, которое учитывает информацию о целевой переменной. В отличие от классического PCA, Supervised PCA ищет такие направления в данных, которые не только объясняют максимальную дисперсию, но и максимально связаны с целевой переменной. Это делает метод особенно полезным в задачах машинного обучения, где важно сохранить связь между признаками и целевой переменной.\n",
        "\n",
        "\n",
        "\n",
        "#### 1. Постановка задачи\n",
        "\n",
        "Предположим, у нас есть:\n",
        "- Матрица признаков $X \\in \\mathbb{R}^{n \\times p}$, где:\n",
        "  - $n$ — количество объектов (наблюдений),\n",
        "  - $p$ — количество признаков (переменных).\n",
        "- Вектор целевой переменной $y \\in \\mathbb{R}^{n}$ (для задачи регрессии) или матрица меток классов $Y \\in \\mathbb{R}^{n \\times k}$ (для задачи классификации, где $k$ — количество классов).\n",
        "\n",
        "Цель Supervised PCA — найти такие линейные комбинации признаков (главные компоненты), которые:\n",
        "1. Объясняют максимальную дисперсию в данных (как в классическом PCA).\n",
        "2. Максимально коррелируют с целевой переменной $y$ или метками классов $Y$.\n",
        "\n",
        "Таким образом, Supervised PCA объединяет в себе идеи снижения размерности и учета целевой переменной.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Математическая формулировка\n",
        "\n",
        "##### 2.1. Классический PCA\n",
        "Для понимания Supervised PCA начнем с классического PCA. В классическом PCA мы ищем такие направления (главные компоненты), которые максимизируют дисперсию данных. Это эквивалентно решению следующей задачи оптимизации:\n",
        "\n",
        "$$\n",
        "\\max_{w} \\text{Var}(Xw) = \\max_{w} w^T \\Sigma w,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w \\in \\mathbb{R}^{p}$ — вектор весов, определяющий направление главной компоненты,\n",
        "- $\\Sigma = X^T X$ — ковариационная матрица данных.\n",
        "\n",
        "Ограничение: $w^T w = 1$ (нормировка вектора весов).\n",
        "\n",
        "Решение этой задачи сводится к нахождению собственных векторов ковариационной матрицы $\\Sigma$, соответствующих наибольшим собственным значениям.\n",
        "\n",
        "##### 2.2. Supervised PCA\n",
        "В Supervised PCA мы добавляем информацию о целевой переменной. Для этого вводим дополнительное условие, которое учитывает связь между признаками и целевой переменной. Формально задача оптимизации принимает вид:\n",
        "\n",
        "$$\n",
        "\\max_{w} \\left( \\text{Var}(Xw) + \\lambda \\cdot \\text{Corr}(Xw, y) \\right),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\text{Var}(Xw)$ — дисперсия проекции данных на направление $w$,\n",
        "- $\\text{Corr}(Xw, y)$ — корреляция между проекцией данных и целевой переменной,\n",
        "- $\\lambda$ — гиперпараметр, регулирующий вклад целевой переменной.\n",
        "\n",
        "Ограничение: $w^T w = 1$.\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Решение задачи Supervised PCA\n",
        "\n",
        "##### 3.1. Преобразование задачи\n",
        "Чтобы решить задачу Supervised PCA, переформулируем её в виде задачи нахождения собственных векторов. Для этого введем матрицу $Q$, которая учитывает как ковариацию данных, так и связь с целевой переменной:\n",
        "\n",
        "$$\n",
        "Q = X^T X + \\lambda X^T y y^T X.\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $X^T X$ — ковариационная матрица данных,\n",
        "- $X^T y y^T X$ — матрица, которая учитывает связь между признаками и целевой переменной.\n",
        "\n",
        "Теперь задача оптимизации принимает вид:\n",
        "\n",
        "$$\n",
        "\\max_{w} w^T Q w,\n",
        "$$\n",
        "\n",
        "с ограничением $w^T w = 1$.\n",
        "\n",
        "##### 3.2. Нахождение главных компонент\n",
        "Решение задачи оптимизации сводится к нахождению собственных векторов матрицы $Q$. Главные компоненты $w_1, w_2, \\dots, w_k$ соответствуют собственным векторам, связанным с наибольшими собственными значениями.\n",
        "\n",
        "Таким образом, Supervised PCA находит такие направления в данных, которые:\n",
        "1. Объясняют максимальную дисперсию.\n",
        "2. Максимально связаны с целевой переменной.\n",
        "\n",
        "\n",
        "\n",
        "#### 4. Оптимизация задачи Supervised PCA\n",
        "\n",
        "Теперь, когда мы сформулировали задачу Supervised PCA в виде задачи оптимизации, важно понять, как её можно эффективно решить. Задача оптимизации принимает следующий вид:\n",
        "\n",
        "$$\n",
        "\\max_{w} w^T Q w,\n",
        "$$\n",
        "\n",
        "с ограничением $w^T w = 1$, где $Q = X^T X + \\lambda X^T y y^T X$.\n",
        "\n",
        "Эта задача является задачей нахождения **собственных векторов** матрицы $Q$, соответствующих наибольшим собственным значениям. Давайте разберем, как это делается.\n",
        "\n",
        "\n",
        "\n",
        "#### 4.1. Решение через спектральное разложение\n",
        "\n",
        "Задача оптимизации, которую мы получили, является классической задачей нахождения **собственных векторов** матрицы $Q$. Формально, нам нужно решить следующее уравнение:\n",
        "\n",
        "$$\n",
        "Q w = \\alpha w,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w$ — собственный вектор матрицы $Q$,\n",
        "- $\\alpha$ — соответствующее собственное значение.\n",
        "\n",
        "Собственные векторы $w_1, w_2, \\dots, w_k$, соответствующие наибольшим собственным значениям $\\alpha_1, \\alpha_2, \\dots, \\alpha_k$, и будут искомыми главными компонентами.\n",
        "\n",
        "##### Шаги решения:\n",
        "1. **Вычисление матрицы $Q$**:\n",
        "   $$\n",
        "   Q = X^T X + \\lambda X^T y y^T X.\n",
        "   $$\n",
        "   Здесь $X^T X$ — ковариационная матрица данных, а $X^T y y^T X$ — матрица, учитывающая связь с целевой переменной.\n",
        "\n",
        "2. **Спектральное разложение матрицы $Q$**:\n",
        "   Находим собственные значения и собственные векторы матрицы $Q$. Это можно сделать с помощью численных методов, таких как:\n",
        "   - Метод Якоби,\n",
        "   - QR-алгоритм,\n",
        "   - Сингулярное разложение (SVD).\n",
        "\n",
        "3. **Выбор главных компонент**:\n",
        "   Собственные векторы, соответствующие наибольшим собственным значениям, выбираются в качестве главных компонент. Например, если мы хотим снизить размерность до $k$, то выбираем $k$ собственных векторов с наибольшими собственными значениями.\n",
        "\n",
        "\n",
        "\n",
        "#### 4.2. Альтернативный подход: использование сингулярного разложения (SVD)\n",
        "\n",
        "Вместо явного вычисления матрицы $Q$ и её спектрального разложения, можно использовать **сингулярное разложение (SVD)** для более эффективного решения задачи.\n",
        "\n",
        "##### Шаги решения:\n",
        "1. **Построение расширенной матрицы**:\n",
        "   Создаем расширенную матрицу $\\tilde{X}$, которая объединяет информацию о данных и целевой переменной:\n",
        "   $$\n",
        "   \\tilde{X} = \\begin{bmatrix} X \\\\ \\sqrt{\\lambda} y^T X \\end{bmatrix}.\n",
        "   $$\n",
        "   Здесь $\\sqrt{\\lambda} y^T X$ — это \"взвешенная\" информация о целевой переменной.\n",
        "\n",
        "2. **Применение SVD**:\n",
        "   Выполняем сингулярное разложение матрицы $\\tilde{X}$:\n",
        "   $$\n",
        "   \\tilde{X} = U S V^T,\n",
        "   $$\n",
        "   где:\n",
        "   - $U$ — левые сингулярные векторы,\n",
        "   - $S$ — диагональная матрица сингулярных значений,\n",
        "   - $V$ — правые сингулярные векторы.\n",
        "\n",
        "3. **Выбор главных компонент**:\n",
        "   Правые сингулярные векторы $V$, соответствующие наибольшим сингулярным значениям, будут искомыми главными компонентами.\n",
        "\n",
        "\n",
        "\n",
        "#### 4.3. Практические аспекты оптимизации\n",
        "\n",
        "##### 4.3.1. Вычислительная сложность\n",
        "- Вычисление матрицы $Q$ требует $O(np^2)$ операций (где $n$ — количество объектов, $p$ — количество признаков).\n",
        "- Спектральное разложение матрицы $Q$ требует $O(p^3)$ операций.\n",
        "- Использование SVD может быть более эффективным, особенно для больших данных, так как современные алгоритмы SVD (например, randomized SVD) позволяют работать с большими матрицами.\n",
        "\n",
        "##### 4.3.2. Выбор гиперпараметра $\\lambda$\n",
        "Гиперпараметр $\\lambda$ регулирует баланс между объяснением дисперсии данных и учетом целевой переменной. Для выбора оптимального значения $\\lambda$ можно использовать:\n",
        "- **Кросс-валидацию**: Разделение данных на обучающую и тестовую выборки и оценка качества модели для разных значений $\\lambda$.\n",
        "- **Градиентный спуск**: Если задача формулируется как оптимизация некоторой функции потерь, можно использовать методы градиентного спуска для настройки $\\lambda$.\n",
        "\n",
        "##### 4.3.3. Регуляризация\n",
        "В случае, если матрица $Q$ оказывается плохо обусловленной (например, при большом количестве признаков), можно добавить регуляризацию. Например, можно использовать $L_2$-регуляризацию, добавив к матрице $Q$ диагональную матрицу:\n",
        "$$\n",
        "Q_{\\text{рег}} = Q + \\gamma I,\n",
        "$$\n",
        "где $\\gamma$ — параметр регуляризации, $I$ — единичная матрица.\n",
        "\n",
        "\n",
        "\n",
        "#### 5. Интерпретация Supervised PCA\n",
        "\n",
        "##### 5.1. Учет целевой переменной\n",
        "Supervised PCA учитывает информацию о целевой переменной, что делает его более подходящим для задач машинного обучения, таких как классификация и регрессия. В отличие от классического PCA, который фокусируется только на дисперсии данных, Supervised PCA находит такие направления, которые полезны для предсказания целевой переменной.\n",
        "\n",
        "##### 5.2. Роль гиперпараметра $\\lambda$\n",
        "Гиперпараметр $\\lambda$ регулирует баланс между объяснением дисперсии данных и учетом целевой переменной:\n",
        "- При $\\lambda = 0$ Supervised PCA сводится к классическому PCA.\n",
        "- При увеличении $\\lambda$ большее внимание уделяется связи с целевой переменной.\n",
        "\n",
        "Выбор оптимального значения $\\lambda$ может быть выполнен с помощью кросс-валидации или других методов настройки гиперпараметров.\n",
        "\n",
        "\n",
        "\n",
        "#### 6. Преимущества и недостатки\n",
        "\n",
        "##### Преимущества:\n",
        "1. **Учет целевой переменной**: Supervised PCA находит такие компоненты, которые полезны для предсказания целевой переменной, что делает его более эффективным в задачах машинного обучения.\n",
        "2. **Интерпретируемость**: Компоненты, найденные с помощью Supervised PCA, могут быть более интерпретируемыми, если целевая переменная имеет смысловую нагрузку.\n",
        "\n",
        "##### Недостатки:\n",
        "1. **Зависимость от гиперпараметра**: Введение гиперпараметра $\\lambda$ требует дополнительной настройки, что может быть трудоемким.\n",
        "2. **Чувствительность к шуму**: Если целевая переменная содержит ошибки или шум, это может негативно сказаться на качестве найденных компонент.\n",
        "\n",
        "\n",
        "\n",
        "#### 7. Заключение\n",
        "\n",
        "Supervised PCA представляет собой мощный инструмент для снижения размерности данных с учетом целевой переменной. Он объединяет в себе преимущества классического PCA и способность учитывать информацию о целевой переменной, что делает его особенно полезным в задачах машинного обучения. Однако, как и любой метод, Supervised PCA имеет свои ограничения, такие как необходимость настройки гиперпараметров и чувствительность к шуму в данных.\n",
        "\n",
        "Таким образом, Supervised PCA предоставляет гибкий и мощный инструмент для снижения размерности данных с учетом целевой переменной, что делает его особенно полезным в задачах машинного обучения.\n",
        "\n",
        "\n",
        "Рассмотрим конкретный числовой пример применения **Supervised PCA** для набора данных из 4 точек в 3D-пространстве. Мы будем шаг за шагом выполнять все необходимые вычисления, чтобы найти главные компоненты с учетом целевой переменной.\n",
        "\n",
        "\n",
        "\n",
        "### Исходные данные\n",
        "\n",
        "Матрица признаков $X$ и вектор целевой переменной $y$:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}, \\quad\n",
        "y = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3 \\\\\n",
        "4\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $X$ — матрица признаков размером $4 \\times 3$ (4 объекта, 3 признака),\n",
        "- $y$ — вектор целевой переменной размером $4 \\times 1$.\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 1: Центрирование данных\n",
        "\n",
        "Перед применением PCA (и Supervised PCA) данные необходимо центрировать, чтобы каждая переменная имела нулевое среднее значение. Для этого вычтем из каждого столбца $X$ его среднее значение.\n",
        "\n",
        "Средние значения по каждому признаку:\n",
        "$$\n",
        "\\text{mean}(X) = \\begin{bmatrix}\n",
        "\\frac{1 + 4 + 7 + 10}{4} \\\\\n",
        "\\frac{2 + 5 + 8 + 11}{4} \\\\\n",
        "\\frac{3 + 6 + 9 + 12}{4}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "5.5 \\\\\n",
        "6.5 \\\\\n",
        "7.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Центрированная матрица $X_c$:\n",
        "$$\n",
        "X_c = X - \\text{mean}(X) = \\begin{bmatrix}\n",
        "1 - 5.5 & 2 - 6.5 & 3 - 7.5 \\\\\n",
        "4 - 5.5 & 5 - 6.5 & 6 - 7.5 \\\\\n",
        "7 - 5.5 & 8 - 6.5 & 9 - 7.5 \\\\\n",
        "10 - 5.5 & 11 - 6.5 & 12 - 7.5\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-4.5 & -4.5 & -4.5 \\\\\n",
        "-1.5 & -1.5 & -1.5 \\\\\n",
        "1.5 & 1.5 & 1.5 \\\\\n",
        "4.5 & 4.5 & 4.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "\n",
        "Ковариационная матрица $\\Sigma$ для центрированных данных вычисляется по формуле:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n-1} X_c^T X_c,\n",
        "$$\n",
        "где $n = 4$ — количество объектов.\n",
        "\n",
        "Вычислим $X_c^T X_c$:\n",
        "$$\n",
        "X_c^T X_c = \\begin{bmatrix}\n",
        "-4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "-4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "-4.5 & -1.5 & 1.5 & 4.5\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "-4.5 & -4.5 & -4.5 \\\\\n",
        "-1.5 & -1.5 & -1.5 \\\\\n",
        "1.5 & 1.5 & 1.5 \\\\\n",
        "4.5 & 4.5 & 4.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Результат:\n",
        "$$\n",
        "X_c^T X_c = \\begin{bmatrix}\n",
        "67.5 & 67.5 & 67.5 \\\\\n",
        "67.5 & 67.5 & 67.5 \\\\\n",
        "67.5 & 67.5 & 67.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим ковариационную матрицу $\\Sigma$:\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3} \\begin{bmatrix}\n",
        "67.5 & 67.5 & 67.5 \\\\\n",
        "67.5 & 67.5 & 67.5 \\\\\n",
        "67.5 & 67.5 & 67.5\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "22.5 & 22.5 & 22.5 \\\\\n",
        "22.5 & 22.5 & 22.5 \\\\\n",
        "22.5 & 22.5 & 22.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 3: Учет целевой переменной\n",
        "\n",
        "В Supervised PCA мы добавляем информацию о целевой переменной. Для этого вводим матрицу $Q$, которая учитывает как ковариацию данных, так и связь с целевой переменной:\n",
        "$$\n",
        "Q = \\Sigma + \\lambda X_c^T y y^T X_c,\n",
        "$$\n",
        "где $\\lambda$ — гиперпараметр, регулирующий вклад целевой переменной. Для простоты выберем $\\lambda = 1$.\n",
        "\n",
        "Сначала вычислим $X_c^T y$:\n",
        "$$\n",
        "X_c^T y = \\begin{bmatrix}\n",
        "-4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "-4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
        "-4.5 & -1.5 & 1.5 & 4.5\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3 \\\\\n",
        "4\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "-4.5 \\cdot 1 + (-1.5) \\cdot 2 + 1.5 \\cdot 3 + 4.5 \\cdot 4 \\\\\n",
        "-4.5 \\cdot 1 + (-1.5) \\cdot 2 + 1.5 \\cdot 3 + 4.5 \\cdot 4 \\\\\n",
        "-4.5 \\cdot 1 + (-1.5) \\cdot 2 + 1.5 \\cdot 3 + 4.5 \\cdot 4\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "9 \\\\\n",
        "9 \\\\\n",
        "9\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим $X_c^T y y^T X_c$:\n",
        "$$\n",
        "X_c^T y y^T X_c = \\begin{bmatrix}\n",
        "9 \\\\\n",
        "9 \\\\\n",
        "9\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "9 & 9 & 9\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "81 & 81 & 81 \\\\\n",
        "81 & 81 & 81 \\\\\n",
        "81 & 81 & 81\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим матрицу $Q$:\n",
        "$$\n",
        "Q = \\Sigma + \\lambda X_c^T y y^T X_c = \\begin{bmatrix}\n",
        "22.5 & 22.5 & 22.5 \\\\\n",
        "22.5 & 22.5 & 22.5 \\\\\n",
        "22.5 & 22.5 & 22.5\n",
        "\\end{bmatrix} + \\begin{bmatrix}\n",
        "81 & 81 & 81 \\\\\n",
        "81 & 81 & 81 \\\\\n",
        "81 & 81 & 81\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "103.5 & 103.5 & 103.5 \\\\\n",
        "103.5 & 103.5 & 103.5 \\\\\n",
        "103.5 & 103.5 & 103.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 4: Нахождение собственных векторов\n",
        "\n",
        "Теперь найдем собственные векторы матрицы $Q$. Для этого решим уравнение:\n",
        "$$\n",
        "Q w = \\alpha w,\n",
        "$$\n",
        "где $w$ — собственный вектор, $\\alpha$ — собственное значение.\n",
        "\n",
        "Матрица $Q$ имеет вид:\n",
        "$$\n",
        "Q = \\begin{bmatrix}\n",
        "103.5 & 103.5 & 103.5 \\\\\n",
        "103.5 & 103.5 & 103.5 \\\\\n",
        "103.5 & 103.5 & 103.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Заметим, что все строки матрицы $Q$ одинаковы, поэтому она имеет только одно ненулевое собственное значение. Собственный вектор, соответствующий этому значению, будет:\n",
        "$$\n",
        "w = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Нормируем этот вектор:\n",
        "$$\n",
        "w = \\frac{1}{\\sqrt{3}} \\begin{bmatrix}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Шаг 5: Интерпретация результата\n",
        "\n",
        "Главная компонента $w$ указывает направление, которое объясняет максимальную дисперсию в данных и максимально связано с целевой переменной $y$. В данном случае это направление $\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T$, что означает, что все признаки вносят одинаковый вклад в главную компоненту.\n",
        "\n",
        "\n",
        "Такми образом, мы рассмотрели пошаговый пример применения Supervised PCA для небольшого набора данных. Основные шаги включали центрирование данных, вычисление ковариационной матрицы, учет целевой переменной и нахождение собственных векторов. Этот пример демонстрирует, как Supervised PCA может быть использован для снижения размерности данных с учетом целевой переменной.\n",
        "\n",
        "\n",
        "Реализуем пример Supervised PCA на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация с нуля\n",
        "\n",
        "Создадим класс SupervisedPCA, который будет выполнять все шаги, описанные в примере.\n",
        "\n"
      ],
      "metadata": {
        "id": "hhcFYcRU8wcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SupervisedPCA:\n",
        "    def __init__(self, lambda_=1):\n",
        "        self.lambda_ = lambda_  # Гиперпараметр, регулирующий вклад целевой переменной\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели Supervised PCA.\n",
        "\n",
        "        Параметры:\n",
        "        X: Матрица признаков (n_samples, n_features).\n",
        "        y: Вектор целевой переменной (n_samples,).\n",
        "        \"\"\"\n",
        "        # Шаг 1: Центрирование данных\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean_\n",
        "\n",
        "        # Шаг 2: Вычисление ковариационной матрицы\n",
        "        n_samples = X.shape[0]\n",
        "        cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n",
        "\n",
        "        # Шаг 3: Учет целевой переменной\n",
        "        Xty = X_centered.T @ y\n",
        "        Q = cov_matrix + self.lambda_ * (Xty @ Xty.T)\n",
        "\n",
        "        # Шаг 4: Нахождение собственных векторов\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(Q)\n",
        "        self.components_ = eigenvectors[:, np.argmax(eigenvalues)]  # Главная компонента\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Преобразование данных в новое пространство.\n",
        "\n",
        "        Параметры:\n",
        "        X: Матрица признаков (n_samples, n_features).\n",
        "\n",
        "        Возвращает:\n",
        "        X_transformed: Преобразованные данные (n_samples, 1).\n",
        "        \"\"\"\n",
        "        X_centered = X - self.mean_\n",
        "        return X_centered @ self.components_\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([1, 2, 3, 4])\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "spca = SupervisedPCA(lambda_=1)\n",
        "spca.fit(X, y)\n",
        "\n",
        "# Преобразуем данные\n",
        "X_transformed = spca.transform(X)\n",
        "print(\"Преобразованные данные (с нуля):\\n\", X_transformed)"
      ],
      "metadata": {
        "id": "e_Oxabpm_Ssg",
        "outputId": "4d90cc2d-1e20-444a-86d1-14dd782d4723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Преобразованные данные (с нуля):\n",
            " [-7.79422863 -2.59807621  2.59807621  7.79422863]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения\n",
        "\n",
        "Для реализации Supervised PCA можно использовать библиотеку scikit-learn. Хотя в scikit-learn нет встроенной реализации Supervised PCA, мы можем использовать PCA и добавить информацию о целевой переменной вручную."
      ],
      "metadata": {
        "id": "p-WBWK0X_bPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Центрирование данных\n",
        "scaler = StandardScaler(with_std=False)\n",
        "X_centered = scaler.fit_transform(X)\n",
        "\n",
        "# Добавление информации о целевой переменной\n",
        "lambda_ = 1\n",
        "Xty = X_centered.T @ y\n",
        "Q = X_centered.T @ X_centered + lambda_ * (Xty @ Xty.T)\n",
        "\n",
        "# Нахождение собственных векторов\n",
        "eigenvalues, eigenvectors = np.linalg.eig(Q)\n",
        "principal_component = eigenvectors[:, np.argmax(eigenvalues)]\n",
        "\n",
        "# Преобразование данных\n",
        "X_transformed = X_centered @ principal_component\n",
        "print(\"Преобразованные данные (scikit-learn):\\n\", X_transformed)"
      ],
      "metadata": {
        "id": "UdrYiZkS_cY-",
        "outputId": "b26b78d8-a1b3-4b5b-f4cf-a3152829fad1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Преобразованные данные (scikit-learn):\n",
            " [-7.79422863 -2.59807621  2.59807621  7.79422863]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты, которые были получены:\n",
        "\n",
        "```\n",
        "Преобразованные данные (scikit-learn):\n",
        " [-7.79422863 -2.59807621  2.59807621  7.79422863]\n",
        "```\n",
        "\n",
        "представляют собой проекции исходных данных на **главную компоненту**, найденную с помощью метода Supervised PCA. Ниже приведено объяснение, что это означает и как интерпретировать эти значения.\n",
        "\n",
        "\n",
        "\n",
        "### 1. **Что такое преобразованные данные?**\n",
        "\n",
        "Преобразованные данные — это новые значения, полученные после проецирования исходных данных на направление, заданное главной компонентой. В Supervised PCA это направление учитывает:\n",
        "- Максимальную дисперсию данных (как в классическом PCA).\n",
        "- Максимальную связь с целевой переменной $y$.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Интерпретация значений**\n",
        "\n",
        "Каждое значение в результате соответствует проекции одной из исходных точек данных на главную компоненту. В данном случае:\n",
        "\n",
        "- Исходные данные:\n",
        "$$\n",
        "  X = \\begin{bmatrix}\n",
        "  1 & 2 & 3 \\\\\n",
        "  4 & 5 & 6 \\\\\n",
        "  7 & 8 & 9 \\\\\n",
        "  10 & 11 & 12\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- Преобразованные данные:\n",
        "$$\n",
        "  \\text{Преобразованные данные} = \\begin{bmatrix}\n",
        "  -7.79422863 \\\\\n",
        "  -2.59807621 \\\\\n",
        "  2.59807621 \\\\\n",
        "  7.79422863\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Каждое значение в преобразованных данных соответствует одной строке исходной матрицы $X$. Например:\n",
        "- Первая строка $[1, 2, 3]$ проецируется на значение \\(-7.79422863\\).\n",
        "- Вторая строка $[4, 5, 6]$ проецируется на значение \\(-2.59807621\\).\n",
        "- И так далее.\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Что означают эти значения?**\n",
        "\n",
        "- **Знак числа**:\n",
        "  - Отрицательные значения (например, \\(-7.79422863\\)) указывают на то, что точка данных находится в направлении, противоположном главной компоненте.\n",
        "  - Положительные значения (например, \\(7.79422863\\)) указывают на то, что точка данных находится в направлении главной компоненты.\n",
        "\n",
        "- **Абсолютное значение числа**:\n",
        "  - Чем больше абсолютное значение, тем дальше точка данных от центра (среднего значения) вдоль главной компоненты.\n",
        "  - Например, \\(-7.79422863\\) и \\(7.79422863\\) — это самые удаленные точки от центра, а \\(-2.59807621\\) и \\(2.59807621\\) — ближе к центру.\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Главная компонента**\n",
        "\n",
        "Главная компонента, найденная методом Supervised PCA, — это направление в пространстве признаков, которое:\n",
        "1. Объясняет максимальную дисперсию данных.\n",
        "2. Максимально связано с целевой переменной $y$.\n",
        "\n",
        "В данном случае главная компонента направлена вдоль вектора $\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T$. Это означает, что все признаки (столбцы матрицы $X$) вносят одинаковый вклад в главную компоненту.\n",
        "\n",
        "\n",
        "\n",
        "### 5. **Связь с исходными данными**\n",
        "\n",
        "Исходные данные $X$ представляют собой точки в 3D-пространстве. После применения Supervised PCA эти точки проецируются на одномерное пространство (главную компоненту). Преобразованные данные — это координаты точек вдоль этой главной компоненты.\n",
        "\n",
        "- Точки, которые были далеко от центра в исходном пространстве, остаются далеко от центра и в преобразованном пространстве.\n",
        "- Точки, которые были близко к центру, остаются близко к центру.\n",
        "\n",
        "\n",
        "\n",
        "### 6. **Пример визуализации**\n",
        "\n",
        "Можно представить, что исходные данные — это 4 точки в 3D-пространстве, расположенные вдоль прямой линии. После применения Supervised PCA эти точки проецируются на одну прямую (главную компоненту). Преобразованные данные — это координаты этих точек на этой прямой.\n",
        "\n",
        "\n",
        "\n",
        "### 7. **Практическое применение**\n",
        "\n",
        "Преобразованные данные могут быть использованы для:\n",
        "- Снижения размерности данных (например, для визуализации).\n",
        "- Улучшения качества моделей машинного обучения, так как главная компонента учитывает связь с целевой переменной.\n",
        "\n",
        "\n",
        "\n",
        "### 8. **Почему значения могут отличаться от других реализаций?**\n",
        "\n",
        "Если значения отличаются от результатов других реализаций, это может быть связано с:\n",
        "1. Нормировкой главной компоненты (например, на длину вектора).\n",
        "2. Разными значениями гиперпараметра $\\lambda$.\n",
        "3. Ошибками в вычислениях.\n",
        "\n",
        "Для корректного сравнения необходимо убедиться, что:\n",
        "- Гиперпараметр $\\lambda$ одинаков в обеих реализациях.\n",
        "- Данные центрированы одинаково.\n",
        "- Главная компонента нормирована на единичную длину.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, преобразованные данные — это проекции исходных точек на главную компоненту, которая учитывает как дисперсию данных, так и связь с целевой переменной. Эти значения могут быть использованы для анализа данных, визуализации или построения моделей машинного обучения."
      ],
      "metadata": {
        "id": "PLGYUyrS_WKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.7 Дискриминантный анализ Фишера (FDA, Fisher Discriminant Analysis)\n",
        "\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "Сегодня мы рассмотрим один из классических методов машинного обучения и многомерного статистического анализа — **дискриминантный анализ Фишера (FDA)**, также известный как **линейный дискриминантный анализ Фишера**. Этот метод был предложен Рональдом Фишером в 1936 году и до сих пор активно используется в задачах классификации и снижения размерности данных. Основная цель FDA — найти такое направление в пространстве признаков, которое наилучшим образом разделяет данные, принадлежащие разным классам.\n",
        "\n",
        "FDA особенно полезен, когда мы работаем с многомерными данными, и нам нужно выделить наиболее информативные признаки для разделения классов. В отличие от других методов, FDA не просто снижает размерность, но и делает это таким образом, чтобы максимизировать разделимость классов.\n",
        "\n",
        "\n",
        "\n",
        "#### Основная идея метода\n",
        "\n",
        "Представьте, что у нас есть набор данных, где каждый объект описывается множеством признаков (например, рост, вес, возраст и т.д.), и эти объекты принадлежат одному из нескольких классов (например, \"здоровые\" и \"больные\"). Наша задача — найти такое направление в пространстве признаков, при котором проекции объектов из разных классов будут максимально разделены.\n",
        "\n",
        "Для этого FDA использует два ключевых понятия:\n",
        "1. **Межклассовый разброс (Between-class scatter)** — насколько далеко друг от друга находятся центры классов.\n",
        "2. **Внутриклассовый разброс (Within-class scatter)** — насколько компактно расположены объекты внутри каждого класса.\n",
        "\n",
        "FDA ищет проекцию, которая **максимизирует отношение межклассового разброса к внутриклассовому разбросу**. Другими словами, мы хотим, чтобы центры классов были как можно дальше друг от друга, а объекты внутри каждого класса — как можно ближе друг к другу.\n",
        "\n",
        "\n",
        "\n",
        "#### Математическая постановка задачи\n",
        "\n",
        "Давайте формализуем задачу. Пусть у нас есть набор данных $X = \\{x_1, x_2, \\dots, x_n\\}$, где каждый $x_i$ — это вектор признаков размерности $d$. Данные принадлежат одному из $C$ классов. Обозначим количество элементов в классе $c$ как $n_c$, а общее количество элементов как $n = \\sum_{c=1}^C n_c$.\n",
        "\n",
        "1. **Средние значения классов**:\n",
        "   Для каждого класса $c$ вычислим среднее значение:\n",
        "   $$\n",
        "   \\mu_c = \\frac{1}{n_c} \\sum_{x_i \\in \\text{класс } c} x_i\n",
        "   $$\n",
        "   Это среднее значение представляет собой \"центр масс\" класса $c$.\n",
        "\n",
        "   Также вычислим общее среднее значение по всем данным:\n",
        "   $$\n",
        "   \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
        "   $$\n",
        "   Это среднее значение всех объектов, независимо от их класса.\n",
        "\n",
        "2. **Матрица внутриклассового разброса (Within-class scatter matrix)**:\n",
        "   Матрица $S_W$ измеряет, насколько объекты внутри каждого класса разбросаны вокруг своего среднего значения:\n",
        "   $$\n",
        "   S_W = \\sum_{c=1}^C \\sum_{x_i \\in \\text{класс } c} (x_i - \\mu_c)(x_i - \\mu_c)^T\n",
        "   $$\n",
        "   Здесь $(x_i - \\mu_c)$ — это отклонение объекта от среднего значения своего класса, а $(x_i - \\mu_c)^T$ — транспонированный вектор. Таким образом, $S_W$ суммирует разброс всех объектов внутри каждого класса.\n",
        "\n",
        "3. **Матрица межклассового разброса (Between-class scatter matrix)**:\n",
        "   Матрица $S_B$ измеряет, насколько далеко друг от друга находятся средние значения разных классов:\n",
        "   $$\n",
        "   S_B = \\sum_{c=1}^C n_c (\\mu_c - \\mu)(\\mu_c - \\mu)^T\n",
        "   $$\n",
        "   Здесь $(\\mu_c - \\mu)$ — это отклонение среднего значения класса $c$ от общего среднего значения, а $n_c$ — количество объектов в классе $c$. Таким образом, $S_B$ учитывает как расстояние между классами, так и количество объектов в каждом классе.\n",
        "\n",
        "4. **Целевая функция**:\n",
        "   FDA ищет такой вектор $w$, который максимизирует отношение межклассового разброса к внутриклассовому разбросу в проекции:\n",
        "   $$\n",
        "   J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
        "   $$\n",
        "   Здесь $w$ — это направление, на которое проецируются данные. Чем больше значение $J(w)$, тем лучше разделение классов в проекции.\n",
        "\n",
        "\n",
        "\n",
        "#### Решение задачи оптимизации\n",
        "\n",
        "Теперь нам нужно найти вектор $w$, который максимизирует целевую функцию $J(w)$. Для этого воспользуемся методом Лагранжа.\n",
        "\n",
        "1. **Условие оптимальности**:\n",
        "   Оптимальное направление $w$ удовлетворяет условию:\n",
        "   $$\n",
        "   S_B w = \\lambda S_W w\n",
        "   $$\n",
        "   Это обобщенная задача на собственные значения, где $\\lambda$ — собственное значение.\n",
        "\n",
        "2. **Решение через обобщенные собственные значения**:\n",
        "   Если матрица $S_W$ невырождена (т.е. обратима), то оптимальное направление $w$ можно найти как собственный вектор, соответствующий наибольшему собственному значению матрицы $S_W^{-1} S_B$:\n",
        "   $$\n",
        "   S_W^{-1} S_B w = \\lambda w\n",
        "   $$\n",
        "   Таким образом, задача сводится к нахождению собственных значений и собственных векторов матрицы $S_W^{-1} S_B$.\n",
        "\n",
        "3. **Многоклассовый случай**:\n",
        "   Если число классов $C > 2$, FDA ищет несколько направлений $w_1, w_2, \\dots, w_{C-1}$, которые максимизируют отношение $J(w)$. Эти направления являются собственными векторами, соответствующими наибольшим $C-1$ собственным значениям матрицы $S_W^{-1} S_B$.\n",
        "\n",
        "\n",
        "\n",
        "#### Проецирование данных\n",
        "\n",
        "После нахождения оптимальных направлений $w_1, w_2, \\dots, w_{C-1}$, данные можно спроецировать на новое подпространство:\n",
        "$$\n",
        "y_i = W^T x_i\n",
        "$$\n",
        "где $W = [w_1, w_2, \\dots, w_{C-1}]$ — матрица, столбцы которой являются найденными направлениями, а $y_i$ — проекция данных в новое подпространство. Это подпространство имеет размерность $C-1$, что позволяет значительно снизить размерность данных, сохраняя при этом их разделимость.\n",
        "\n",
        "\n",
        "\n",
        "#### Преимущества и ограничения FDA\n",
        "\n",
        "1. **Преимущества**:\n",
        "   - FDA эффективно снижает размерность данных, сохраняя информацию о разделимости классов.\n",
        "   - Метод хорошо работает, когда классы линейно разделимы.\n",
        "   - FDA может быть использован как для классификации, так и для визуализации данных.\n",
        "\n",
        "2. **Ограничения**:\n",
        "   - FDA предполагает, что данные распределены нормально, что может не выполняться на практике.\n",
        "   - Метод чувствителен к выбросам.\n",
        "   - FDA не учитывает нелинейные зависимости между признаками.\n",
        "\n",
        "Таким образом, дискриминантный анализ Фишера — это мощный и интуитивно понятный метод, который позволяет эффективно разделять классы в многомерных данных. Он основан на простой, но глубокой идее: найти направление, которое максимизирует разделимость классов, минимизируя при этом разброс внутри классов. Однако, как и любой метод, FDA имеет свои ограничения, которые важно учитывать при его применении.\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим набор данных из 4 точек в 3D-пространстве:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Предположим, что первые две точки принадлежат классу 1, а последние две — классу 2. Наша задача — применить дискриминантный анализ Фишера (FDA) для нахождения направления $w$, которое наилучшим образом разделяет эти два класса.\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 1: Вычисление средних значений классов\n",
        "\n",
        "1. **Класс 1**:\n",
        "   $$\n",
        "   \\mu_1 = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\right) = \\frac{1}{2} \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix} = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "2. **Класс 2**:\n",
        "   $$\n",
        "   \\mu_2 = \\frac{1}{2} \\left( \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\frac{1}{2} \\begin{bmatrix} 17 \\\\ 19 \\\\ 21 \\end{bmatrix} = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "3. **Общее среднее значение**:\n",
        "   $$\n",
        "   \\mu = \\frac{1}{4} \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} + \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\right) = \\frac{1}{4} \\begin{bmatrix} 22 \\\\ 26 \\\\ 30 \\end{bmatrix} = \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 2: Вычисление матрицы внутриклассового разброса $S_W$\n",
        "\n",
        "Матрица внутриклассового разброса $S_W$ вычисляется как сумма разбросов внутри каждого класса.\n",
        "\n",
        "1. **Разброс для класса 1**:\n",
        "   $$\n",
        "   S_{W1} = \\sum_{x_i \\in \\text{класс 1}} (x_i - \\mu_1)(x_i - \\mu_1)^T\n",
        "   $$\n",
        "   Для первой точки:\n",
        "   $$\n",
        "   (x_1 - \\mu_1) = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix}\n",
        "   $$\n",
        "   $$\n",
        "   (x_1 - \\mu_1)(x_1 - \\mu_1)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "   Для второй точки:\n",
        "   $$\n",
        "   (x_2 - \\mu_1) = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix}\n",
        "   $$\n",
        "   $$\n",
        "   (x_2 - \\mu_1)(x_2 - \\mu_1)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "   Суммируем:\n",
        "   $$\n",
        "   S_{W1} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "2. **Разброс для класса 2**:\n",
        "   $$\n",
        "   S_{W2} = \\sum_{x_i \\in \\text{класс 2}} (x_i - \\mu_2)(x_i - \\mu_2)^T\n",
        "   $$\n",
        "   Для третьей точки:\n",
        "   $$\n",
        "   (x_3 - \\mu_2) = \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} - \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix} = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix}\n",
        "   $$\n",
        "   $$\n",
        "   (x_3 - \\mu_2)(x_3 - \\mu_2)^T = \\begin{bmatrix} -1.5 \\\\ -1.5 \\\\ -1.5 \\end{bmatrix} \\begin{bmatrix} -1.5 & -1.5 & -1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "   Для четвертой точки:\n",
        "   $$\n",
        "   (x_4 - \\mu_2) = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} - \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix}\n",
        "   $$\n",
        "   $$\n",
        "   (x_4 - \\mu_2)(x_4 - \\mu_2)^T = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 1.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 1.5 & 1.5 \\end{bmatrix} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "   Суммируем:\n",
        "   $$\n",
        "   S_{W2} = \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\\\ 2.25 & 2.25 & 2.25 \\end{bmatrix} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "3. **Общая матрица внутриклассового разброса $S_W$**:\n",
        "   $$\n",
        "   S_W = S_{W1} + S_{W2} = \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} + \\begin{bmatrix} 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\\\ 4.5 & 4.5 & 4.5 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 3: Вычисление матрицы межклассового разброса $S_B$\n",
        "\n",
        "Матрица межклассового разброса $S_B$ вычисляется как:\n",
        "$$\n",
        "S_B = \\sum_{c=1}^C n_c (\\mu_c - \\mu)(\\mu_c - \\mu)^T\n",
        "$$\n",
        "\n",
        "1. **Для класса 1**:\n",
        "   $$\n",
        "   (\\mu_1 - \\mu) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix}\n",
        "   $$\n",
        "   $$\n",
        "   (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = \\begin{bmatrix} -3 \\\\ -3 \\\\ -3 \\end{bmatrix} \\begin{bmatrix} -3 & -3 & -3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "   $$\n",
        "   Учитывая $n_1 = 2$:\n",
        "   $$\n",
        "   n_1 (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "2. **Для класса 2**:\n",
        "   $$\n",
        "   (\\mu_2 - \\mu) = \\begin{bmatrix} 8.5 \\\\ 9.5 \\\\ 10.5 \\end{bmatrix} - \\begin{bmatrix} 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix}\n",
        "   $$\n",
        "   $$\n",
        "   (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 3 & 3 & 3 \\end{bmatrix} = \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix}\n",
        "   $$\n",
        "   Учитывая $n_2 = 2$:\n",
        "   $$\n",
        "   n_2 (\\mu_2 - \\mu)(\\mu_2 - \\mu)^T = 2 \\begin{bmatrix} 9 & 9 & 9 \\\\ 9 & 9 & 9 \\\\ 9 & 9 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "3. **Общая матрица межклассового разброса $S_B$**:\n",
        "   $$\n",
        "   S_B = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} + \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix} = \\begin{bmatrix} 36 & 36 & 36 \\\\ 36 & 36 & 36 \\\\ 36 & 36 & 36 \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "\n",
        "\n",
        "#### Шаг 4: Решение задачи на собственные значения\n",
        "\n",
        "Теперь нам нужно решить обобщенную задачу на собственные значения:\n",
        "$$\n",
        "S_W^{-1} S_B w = \\lambda w\n",
        "$$\n",
        "\n",
        "1. **Обратная матрица $S_W^{-1}$**:\n",
        "   Матрица $S_W$ вырождена (ее определитель равен нулю), поэтому обратной матрицы не существует. Это означает, что в данном примере FDA не может быть применен напрямую.\n",
        "\n",
        "Таким образом, в данном примере мы столкнулись с проблемой вырожденности матрицы внутриклассового разброса $S_W$, что делает невозможным применение FDA в чистом виде. Это подчеркивает важность проверки условий применимости метода, таких как невырожденность $S_W$ и линейная разделимость классов. В реальных задачах для преодоления этой проблемы можно использовать регуляризацию или другие методы снижения размерности.\n",
        "\n",
        "Реализуем пример дискриминантного анализа Фишера (FDA) на Python. Сначала создадим класс с нуля, а затем воспользуемся готовым решением из библиотеки scikit-learn.\n",
        "\n",
        "1. Реализация FDA с нуля\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3If_lp3BAGr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FisherDiscriminantAnalysis:\n",
        "    def __init__(self, alpha=1e-6):\n",
        "        self.w = None  # Направление проекции\n",
        "        self.alpha = alpha  # Параметр регуляризации\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение модели: нахождение направления w.\n",
        "        :param X: Матрица данных (n_samples, n_features).\n",
        "        :param y: Вектор меток классов (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        classes = np.unique(y)\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        # Вычисление средних значений для каждого класса\n",
        "        class_means = np.array([X[y == c].mean(axis=0) for c in classes])\n",
        "\n",
        "        # Общее среднее значение\n",
        "        global_mean = X.mean(axis=0)\n",
        "\n",
        "        # Матрица внутриклассового разброса (Within-class scatter matrix)\n",
        "        S_W = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            class_scatter = np.zeros((n_features, n_features))\n",
        "            for sample in X[y == c]:\n",
        "                sample, mean = sample.reshape(n_features, 1), mean.reshape(n_features, 1)\n",
        "                class_scatter += (sample - mean).dot((sample - mean).T)\n",
        "            S_W += class_scatter\n",
        "\n",
        "        # Регуляризация: добавляем alpha * I к S_W\n",
        "        S_W += self.alpha * np.eye(n_features)\n",
        "\n",
        "        # Матрица межклассового разброса (Between-class scatter matrix)\n",
        "        S_B = np.zeros((n_features, n_features))\n",
        "        for c, mean in zip(classes, class_means):\n",
        "            n_c = X[y == c].shape[0]\n",
        "            mean, global_mean = mean.reshape(n_features, 1), global_mean.reshape(n_features, 1)\n",
        "            S_B += n_c * (mean - global_mean).dot((mean - global_mean).T)\n",
        "\n",
        "        # Решение обобщенной задачи на собственные значения\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "        # Выбор направления, соответствующего наибольшему собственному значению\n",
        "        self.w = eigenvectors[:, np.argmax(eigenvalues)].real\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Проецирование данных на найденное направление.\n",
        "        :param X: Матрица данных (n_samples, n_features).\n",
        "        :return: Проекции данных (n_samples,).\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "fda = FisherDiscriminantAnalysis(alpha=1e-6)\n",
        "fda.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_projected = fda.transform(X)\n",
        "print(\"Проекции данных:\", X_projected)"
      ],
      "metadata": {
        "id": "gSejcMz_BG8c",
        "outputId": "60e695df-b1ab-45e7-db2c-7dd60c65b6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекции данных: [ -3.46410161  -8.66025404 -13.85640646 -19.05255888]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Использование готового решения из scikit-learn\n",
        "\n",
        "Библиотека scikit-learn предоставляет реализацию линейного дискриминантного анализа (LDA), который является обобщением FDA для многоклассового случая."
      ],
      "metadata": {
        "id": "2o7EtptIBQPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Пример использования\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "y = np.array([0, 0, 1, 1])  # Метки классов\n",
        "\n",
        "# Создаем и обучаем модель\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X, y)\n",
        "\n",
        "# Проецируем данные\n",
        "X_projected_sklearn = lda.transform(X)\n",
        "print(\"Проекции данных (scikit-learn):\", X_projected_sklearn)"
      ],
      "metadata": {
        "id": "88hvHzC8BRU3",
        "outputId": "47607221-c08b-4330-e379-c100297b02f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проекции данных (scikit-learn): [[-2.12132034]\n",
            " [-0.70710678]\n",
            " [ 0.70710678]\n",
            " [ 2.12132034]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат, который был получен:\n",
        "\n",
        "```\n",
        "Проекции данных (scikit-learn): [[-2.12132034]\n",
        "                                 [-0.70710678]\n",
        "                                 [ 0.70710678]\n",
        "                                 [ 2.12132034]]\n",
        "```\n",
        "\n",
        "представляет собой проекции исходных данных на направление, найденное методом **линейного дискриминантного анализа (LDA)**. Ниже приведено объяснение, что это означает и как интерпретировать эти значения.\n",
        "\n",
        "\n",
        "\n",
        "### 1. **Что такое проекции?**\n",
        "\n",
        "Метод LDA (или FDA) находит направление $w$ в пространстве признаков, которое наилучшим образом разделяет классы. После нахождения этого направления данные проецируются на него. Проекция — это результат скалярного произведения вектора данных $x_i$ на вектор направления $w$:\n",
        "\n",
        "$$\n",
        "y_i = x_i \\cdot w\n",
        "$$\n",
        "\n",
        "Каждое значение $y_i$ — это координата точки $x_i$ на новой оси (направлении $w$).\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Интерпретация результата**\n",
        "\n",
        "Исходные данные:\n",
        "```\n",
        "X = [\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "]\n",
        "```\n",
        "\n",
        "Метки классов:\n",
        "```\n",
        "y = [0, 0, 1, 1]\n",
        "```\n",
        "\n",
        "Результат проекции:\n",
        "```\n",
        "[[-2.12132034]\n",
        " [-0.70710678]\n",
        " [ 0.70710678]\n",
        " [ 2.12132034]]\n",
        "```\n",
        "\n",
        "#### Что это означает?\n",
        "\n",
        "1. **Разделение классов**:\n",
        "   - Первые два значения (`-2.12132034` и `-0.70710678`) соответствуют объектам класса 0.\n",
        "   - Последние два значения (`0.70710678` и `2.12132034`) соответствуют объектам класса 1.\n",
        "\n",
        "   Видно, что проекции объектов разных классов разделены: отрицательные значения для класса 0 и положительные для класса 1.\n",
        "\n",
        "2. **Расстояние между классами**:\n",
        "   - Проекции объектов класса 0 находятся в диапазоне от `-2.12` до `-0.71`.\n",
        "   - Проекции объектов класса 1 находятся в диапазоне от `0.71` до `2.12`.\n",
        "\n",
        "   Это означает, что LDA успешно нашла направление, которое разделяет классы.\n",
        "\n",
        "3. **Масштаб значений**:\n",
        "   - Значения проекций зависят от масштаба данных и направления $w$. В данном случае они находятся в диапазоне от `-2.12` до `2.12`.\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Как использовать эти проекции?**\n",
        "\n",
        "1. **Классификация**:\n",
        "   - После получения проекций можно использовать простое правило для классификации. Например:\n",
        "     - Если проекция $y_i < 0$, то объект принадлежит классу 0.\n",
        "     - Если проекция $y_i > 0$, то объект принадлежит классу 1.\n",
        "\n",
        "2. **Визуализация**:\n",
        "   - Если данные многомерные (например, 3D, как в данном случае), проекция на одну ось позволяет визуализировать их в одномерном пространстве. Это упрощает анализ данных.\n",
        "\n",
        "3. **Снижение размерности**:\n",
        "   - LDA используется для снижения размерности данных. В данном случае размерность была уменьшена с 3D до 1D, при этом сохранена информация о разделимости классов.\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Почему именно такие числа?**\n",
        "\n",
        "Числа `-2.12132034`, `-0.70710678`, `0.70710678` и `2.12132034` — это результат проекции данных на направление $w$, которое LDA нашла как оптимальное для разделения классов. Эти значения зависят от:\n",
        "- Исходных данных $X$.\n",
        "- Меток классов $y$.\n",
        "- Направления $w$, которое максимизирует отношение межклассового разброса к внутриклассовому разбросу.\n",
        "\n",
        "\n",
        "\n",
        "### 5. **Пример интерпретации**\n",
        "\n",
        "Предположим, что данные описывают рост и вес людей, и требуется разделить их на две группы: \"низкие\" и \"высокие\". После применения LDA были получены проекции:\n",
        "- Для \"низких\": `-2.12` и `-0.71`.\n",
        "- Для \"высоких\": `0.71` и `2.12`.\n",
        "\n",
        "Это означает, что:\n",
        "- Отрицательные значения соответствуют \"низким\".\n",
        "- Положительные значения соответствуют \"высоким\".\n",
        "- Чем больше абсолютное значение проекции, тем сильнее объект принадлежит своему классу.\n",
        "\n",
        "\n",
        "Таким образом, результат проекции данных показывает, что LDA успешно разделила классы. Отрицательные значения соответствуют одному классу, а положительные — другому. Эти проекции можно использовать для классификации, визуализации или дальнейшего анализа данных."
      ],
      "metadata": {
        "id": "mEPv5jSDBK7-"
      }
    }
  ]
}