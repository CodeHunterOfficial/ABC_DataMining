{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEwgOAOlro1sudHY5oXM7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/%D0%A0%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%9F%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%BD%D1%83%D1%82%D1%8B%D0%B9_%D1%83%D1%80%D0%BE%D0%B2%D0%B5%D0%BD%D1%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Регрессионный анализ. Продвинутый уровень"
      ],
      "metadata": {
        "id": "Fngb4dt1yiwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Линейная регрессия\n",
        "\n",
        "**Регрессия** — одна из трех основных задач машинного обучения наряду с классификацией и кластеризацией, предназначенная для предсказания числовых значений целевой переменной на основе одной или нескольких независимых переменных. Цель линейной регрессии — установить линейную зависимость между признаками и целевой переменной, чтобы сделать прогноз для новых данных.\n",
        "\n",
        "### Типы линейной регрессии\n",
        "\n",
        "1. **Простая линейная регрессия**: Модель содержит одну независимую переменную.\n",
        "2. **Множественная линейная регрессия**: Модель содержит несколько независимых переменных.\n",
        "\n",
        "Линейная регрессия является одним из самых простых и популярных методов регрессии, благодаря понятности и простоте интерпретации. Модель линейной регрессии подбирает коэффициенты (веса) для независимых переменных, минимизируя среднеквадратичную ошибку между предсказанными и фактическими значениями.\n",
        "\n",
        "### Определение задачи регрессии\n",
        "\n",
        "Задача предсказания значения целевой переменной $t$ на основе вектора признаков $x$ называется задачей регрессии. Для этого входные признаки $x$ (например, характеристики дома) используются для предсказания целевого значения $t$ (например, цена дома). Линейная регрессия сводится к построению \"линии наилучшего соответствия\", проходящей через набор точек данных.\n",
        "\n",
        "## Данные\n",
        "\n",
        "Модель линейной регрессии обучается на **датасете** — наборе данных, содержащем информацию о независимых переменных и целевой переменной. Датасет состоит из наблюдений $(x, t)$, где:\n",
        "- $x$ — вектор предикторов (признаков),\n",
        "- $t$ — целевая переменная.\n",
        "\n",
        "### Разделение данных\n",
        "\n",
        "Часто данные делят на три подмножества:\n",
        "1. **Обучающая выборка (Training set)**: используется для подбора параметров модели.\n",
        "2. **Валидационная выборка (Validation set)**: служит для настройки гиперпараметров и предотвращения переобучения.\n",
        "3. **Тестовая выборка (Test set)**: предназначена для окончательной оценки модели после завершения обучения.\n",
        "\n",
        "Рекомендуемые соотношения деления:\n",
        "- Обучающая выборка — 60-80%\n",
        "- Валидационная выборка — 10-20%\n",
        "- Тестовая выборка — 10-20%\n",
        "\n",
        "## Постановка задачи регрессии\n",
        "\n",
        "Пусть значение целевой переменной $t$ для вектора $x$ описывается детерминированной функцией $y(x, w)$ с добавлением гауссовского шума $\\varepsilon$:\n",
        "$$\n",
        "t = y(x, w) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
        "$$\n",
        "Здесь:\n",
        "- $y(x, w)$ — детерминированная функция, зависящая от параметров $w$,\n",
        "- $\\varepsilon$ — нормально распределенный шум с дисперсией $\\sigma^2$.\n"
      ],
      "metadata": {
        "id": "Jq8t9ocE9kJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Нормально распределенный шум: теоретические основы\n",
        "\n",
        "Термин \"нормально распределенный шум\" описывает случайную величину $\\varepsilon$, подчиняющуюся нормальному (гауссовскому) распределению. Математически это записывается как $\\varepsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$, где $\\mu$ представляет математическое ожидание, а $\\sigma^2$ — дисперсию. В контексте моделирования обычно предполагается, что $\\mu = 0$, если не указано иное.\n",
        "\n",
        "#### Функция плотности вероятности\n",
        "\n",
        "Плотность вероятности нормально распределенной случайной величины задается выражением:\n",
        "\n",
        "$$\n",
        "f(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\varepsilon - \\mu)^2}{2\\sigma^2}\\right),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\mu$ определяет положение центра распределения,\n",
        "- $\\sigma^2$ характеризует разброс значений вокруг центра,\n",
        "- $\\exp(x)$ обозначает экспоненциальную функцию ($e^x$).\n",
        "\n",
        "При $\\mu = 0$ формула упрощается до:\n",
        "\n",
        "$$\n",
        "f(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\varepsilon^2}{2\\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "#### Интерпретация в моделях\n",
        "\n",
        "Нормально распределенный шум часто используется для моделирования случайных возмущений в данных. Например, в задачах регрессии наблюдаемая переменная $y$ может быть представлена как:\n",
        "\n",
        "$$\n",
        "y = f(x) + \\varepsilon,\n",
        "$$\n",
        "\n",
        "где $f(x)$ — детерминированная компонента, а $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ — случайный шум, добавляемый к модели.\n",
        "\n",
        "#### Свойства параметров распределения\n",
        "\n",
        "1. **Математическое ожидание ($\\mu$):**\n",
        "   Параметр $\\mu$ определяет смещение распределения относительно нуля. Если $\\mu = 0$, распределение становится симметричным относительно начала координат.\n",
        "\n",
        "2. **Дисперсия ($\\sigma^2$):**\n",
        "   Дисперсия $\\sigma^2$ влияет на ширину \"колоколообразной\" кривой. Увеличение $\\sigma^2$ приводит к более широкому разбросу значений случайной величины.\n",
        "\n",
        "#### Оценка параметров по выборке\n",
        "\n",
        "Если доступна выборка значений $\\{\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_n\\}$, параметры $\\mu$ и $\\sigma^2$ могут быть оценены следующим образом:\n",
        "\n",
        "1. **Оценка математического ожидания ($\\mu$):**\n",
        "   $$\n",
        "   \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n \\varepsilon_i.\n",
        "   $$\n",
        "\n",
        "2. **Оценка дисперсии ($\\sigma^2$):**\n",
        "   $$\n",
        "   \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (\\varepsilon_i - \\hat{\\mu})^2.\n",
        "   $$\n",
        "\n",
        "Таким образом, нормальное распределение является фундаментальным инструментом в статистике и машинном обучении благодаря своей универсальности и удобству анализа. Его плотность вероятности полностью определяется двумя параметрами: математическим ожиданием $\\mu$ и дисперсией $\\sigma^2$.\n",
        "\n",
        "Финальная формула плотности вероятности имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{f(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\varepsilon - \\mu)^2}{2\\sigma^2}\\right).}\n",
        "$$"
      ],
      "metadata": {
        "id": "m9qwugmkP9MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Продолжение\n",
        "**Вероятностная интерпретация модели** заключается в том, что вероятность наблюдения конкретного значения $t$ при известных $x$, $w$ и $\\sigma^2$ описывается нормальным распределением:\n",
        "$$\n",
        "p(t|x, w, \\sigma^2) = \\mathcal{N}(y(x, w), \\sigma^2)\n",
        "$$\n",
        "В большинстве случаев предполагается, что дисперсия ошибки $\\sigma^2$ постоянна (гомоскедастичность). Если дисперсия зависит от значений $x$ — это **гетероскедастичность**. В случае гетероскедастичности применяют методы, учитывающие изменяющуюся дисперсию, например, взвешенную регрессию.\n",
        "\n",
        "### Проверка гомоскедастичности и гетероскедастичности\n",
        "\n",
        "**Визуальные методы**:  \n",
        "- **График остатков** — если остатки распределены случайно и равномерно вокруг оси, то присутствует гомоскедастичность. Отклонения указывают на гетероскедастичность.\n",
        "- **Q-Q график** — помогает оценить нормальность распределения остатков.\n",
        "\n",
        "**Статистические тесты**:  \n",
        "- **Тест Бреуша-Пагана** и **тест Голдфелда-Квандта** помогают проверить гомоскедастичность на статистически значимом уровне.\n",
        "  \n",
        "  \n",
        "## Построение функции $y(x, w)$\n",
        "\n",
        "Задача линейной регрессии заключается в построении функции вида:\n",
        "$$\n",
        "y(x, w) \\to \\mathbb{R}\n",
        "$$\n",
        "где $w$ — вектор параметров функции, а $x$ — вектор характеристик (признаков) объекта.\n",
        "\n",
        "Наиболее часто используемая форма линейной функции:\n",
        "$$\n",
        "y(x, w) = w_0 + w_1 x_1 + \\ldots + w_D x_D = w_0 + \\sum_{i=1}^D w_i \\cdot x_i\n",
        "$$\n",
        "где:\n",
        "- $y(x, w)$ — предсказанное значение целевой переменной (таргета),\n",
        "- $x = (x_1, \\ldots, x_D)$ — вектор признаков,\n",
        "- $w_1, \\ldots, w_D, w_0$ — параметры модели.\n",
        "\n",
        "Признаки также называют **фичами** (от англ. features). Вектор $w = (w_1, \\ldots, w_D)$ часто называют вектором **весов**, так как модель можно интерпретировать как взвешенную сумму признаков объекта. Число $w_0$ называется **свободным членом** или **сдвигом** (bias).\n",
        "\n",
        "**Примечание.** Свободный член $w_0$ иногда опускают, добавляя ко всем $x_i$ признак, тождественно равный единице. В таком случае роль $w_0$ будет выполнять соответствующий вес:\n",
        "$$\n",
        "\\begin{pmatrix}x_{1} & \\ldots & x_{D} \\end{pmatrix}\\cdot\\begin{pmatrix}w_1\\\\ \\vdots \\\\ w_D\\end{pmatrix} + w_0 =\n",
        "\\begin{pmatrix}1 & x_{1} & \\ldots & x_{D} \\end{pmatrix}\\cdot\\begin{pmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Базисные функции (Basis Functions)\n",
        "\n",
        "Базисные функции в линейной регрессии представляют собой преобразования, которые применяются к исходным признакам (входным переменным), чтобы перенести их в новое пространство признаков. Это позволяет модели лучше улавливать сложные зависимости между входными данными и целевой переменной, обеспечивая более точные предсказания. Введение базисных функций в линейную модель особенно полезно, когда между исходными признаками и целевой переменной нет простой линейной зависимости. С их помощью можно расширить пространство признаков и создать более точную и гибкую модель.\n",
        "\n",
        "### Модель линейной регрессии с базисными функциями\n",
        "\n",
        "Для включения базисных функций модель линейной регрессии может быть записана следующим образом:\n",
        "$$\n",
        "y(x, w) = w_0 + \\sum_{j=1}^{M-1} w_j \\cdot \\phi_j(x) = w^T \\phi(x),\n",
        "$$\n",
        "где вектор $\\phi(x)$ определяется как:\n",
        "$$\n",
        "\\phi(x) = (\\phi_0(x), \\phi_1(x), \\ldots, \\phi_{M-1}(x))^T,\n",
        "$$\n",
        "при этом $\\phi_0(x) = 1$, что позволяет учесть свободный член модели $w_0$.\n",
        "\n",
        "Параметры модели $w$ также представлены вектором:\n",
        "$$\n",
        "w = (w_0, w_1, \\ldots, w_{M-1})^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Давайте разберем несколько конкретных примеров для $ w^T \\phi(x) $, чтобы лучше понять, как это работает. Мы рассмотрим разные случаи: линейную модель, полиномиальную модель и модель с нелинейными базисными функциями.\n",
        "\n",
        "\n",
        "\n",
        "### Пример 1: Линейная модель\n",
        "#### Дано:\n",
        "- Входное значение: $ x = 2 $,\n",
        "- Базисные функции: $\\phi_0(x) = 1$, $\\phi_1(x) = x$,\n",
        "- Параметры модели: $ w = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $.\n",
        "\n",
        "#### Вектор $\\phi(x)$:\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x) \\\\\n",
        "\\phi_1(x)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "x\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Вычисление $ w^T \\phi(x) $:\n",
        "$$\n",
        "w^T \\phi(x) =\n",
        "\\begin{bmatrix} 3 & 4 \\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n",
        "= 3 \\cdot 1 + 4 \\cdot 2 = 3 + 8 = 11.\n",
        "$$\n",
        "\n",
        "#### Результат:\n",
        "$$\n",
        "y(x, w) = 11.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Пример 2: Полиномиальная модель (квадратичная)\n",
        "#### Дано:\n",
        "- Входное значение: $ x = -1 $,\n",
        "- Базисные функции: $\\phi_0(x) = 1$, $\\phi_1(x) = x$, $\\phi_2(x) = x^2$,\n",
        "- Параметры модели: $ w = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix} $.\n",
        "\n",
        "#### Вектор $\\phi(x)$:\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x) \\\\\n",
        "\\phi_1(x) \\\\\n",
        "\\phi_2(x)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "x \\\\\n",
        "x^2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "-1 \\\\\n",
        "(-1)^2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "-1 \\\\\n",
        "1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Вычисление $ w^T \\phi(x) $:\n",
        "$$\n",
        "w^T \\phi(x) =\n",
        "\\begin{bmatrix} 1 & -2 & 3 \\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix}\n",
        "= 1 \\cdot 1 + (-2) \\cdot (-1) + 3 \\cdot 1 = 1 + 2 + 3 = 6.\n",
        "$$\n",
        "\n",
        "#### Результат:\n",
        "$$\n",
        "y(x, w) = 6.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Пример 3: Модель с нелинейными базисными функциями\n",
        "#### Дано:\n",
        "- Входное значение: $ x = \\pi/4 $ (угол в радианах),\n",
        "- Базисные функции: $\\phi_0(x) = 1$, $\\phi_1(x) = \\sin(x)$, $\\phi_2(x) = \\cos(x)$,\n",
        "- Параметры модели: $ w = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} $.\n",
        "\n",
        "#### Вектор $\\phi(x)$:\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x) \\\\\n",
        "\\phi_1(x) \\\\\n",
        "\\phi_2(x)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "\\sin(x) \\\\\n",
        "\\cos(x)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "\\sin(\\pi/4) \\\\\n",
        "\\cos(\\pi/4)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "\\sqrt{2}/2 \\\\\n",
        "\\sqrt{2}/2\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Вычисление $ w^T \\phi(x) $:\n",
        "$$\n",
        "w^T \\phi(x) =\n",
        "\\begin{bmatrix} 2 & -1 & 3 \\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} 1 \\\\ \\sqrt{2}/2 \\\\ \\sqrt{2}/2 \\end{bmatrix}\n",
        "= 2 \\cdot 1 + (-1) \\cdot \\frac{\\sqrt{2}}{2} + 3 \\cdot \\frac{\\sqrt{2}}{2}.\n",
        "$$\n",
        "\n",
        "Упрощаем:\n",
        "$$\n",
        "w^T \\phi(x) = 2 - \\frac{\\sqrt{2}}{2} + \\frac{3\\sqrt{2}}{2} = 2 + \\frac{2\\sqrt{2}}{2} = 2 + \\sqrt{2}.\n",
        "$$\n",
        "\n",
        "#### Результат:\n",
        "$$\n",
        "y(x, w) = 2 + \\sqrt{2}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Пример 4: Множественные наблюдения\n",
        "#### Дано:\n",
        "- Входные значения: $ x_1 = 1 $, $ x_2 = 2 $, $ x_3 = 3 $,\n",
        "- Базисные функции: $\\phi_0(x) = 1$, $\\phi_1(x) = x$, $\\phi_2(x) = x^2$,\n",
        "- Параметры модели: $ w = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $.\n",
        "\n",
        "#### Матрица $\\Phi$:\n",
        "Для каждого $x_i$ вычисляем $\\phi(x_i)$:\n",
        "$$\n",
        "\\phi(x_1) =\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}, \\quad\n",
        "\\phi(x_2) =\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "4\n",
        "\\end{bmatrix}, \\quad\n",
        "\\phi(x_3) =\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "3 \\\\\n",
        "9\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Матрица $\\Phi$:\n",
        "$$\n",
        "\\Phi =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 2 & 4 \\\\\n",
        "1 & 3 & 9\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Вычисление прогнозов:\n",
        "$$\n",
        "y = \\Phi w =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1 \\\\\n",
        "1 & 2 & 4 \\\\\n",
        "1 & 3 & 9\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Выполняем умножение:\n",
        "$$\n",
        "y =\n",
        "\\begin{bmatrix}\n",
        "1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 3 \\\\\n",
        "1 \\cdot 1 + 2 \\cdot 2 + 4 \\cdot 3 \\\\\n",
        "1 \\cdot 1 + 3 \\cdot 2 + 9 \\cdot 3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "6 \\\\\n",
        "17 \\\\\n",
        "34\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Результат:\n",
        "$$\n",
        "y =\n",
        "\\begin{bmatrix}\n",
        "6 \\\\\n",
        "17 \\\\\n",
        "34\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итог\n",
        "Примеры показывают, как $ w^T \\phi(x) $ используется для вычисления прогнозов в разных случаях. Важно помнить:\n",
        "- $ w $ — вектор параметров (столбец или строка, в зависимости от контекста),\n",
        "- $ \\phi(x) $ — вектор базисных функций (всегда столбец).\n",
        "\n",
        "\n",
        "### Зачем нужны базисные функции?\n",
        "\n",
        "Использование базисных функций позволяет решать несколько важных задач:\n",
        "\n",
        "1. **Моделирование сложных зависимостей.** Базисные функции помогают выявить сложные нелинейные зависимости между входными признаками и целевой переменной, делая модель более гибкой.\n",
        "   \n",
        "2. **Повышение точности предсказаний.** Применение различных базисных функций увеличивает способность модели подстраиваться под данные, улучшая точность прогнозов.\n",
        "   \n",
        "3. **Снижение ошибки модели.** Введение базисных функций способствует уменьшению ошибки, позволяя более точно описывать закономерности в данных.\n",
        "   \n",
        "4. **Улучшение обобщающей способности.** Базисные функции позволяют модели лучше обобщать на новые данные, особенно если исходные признаки не подчиняются простой линейной зависимости.\n",
        "\n",
        "### Примеры базисных функций\n",
        "\n",
        "Существует несколько распространённых типов базисных функций, полезных для разных задач линейной регрессии:\n",
        "\n",
        "1. **Полиномиальные базисные функции.** Полиномы позволяют моделировать нелинейные зависимости, добавляя степени признаков в качестве новых базисных функций:\n",
        "$$\n",
        "   \\phi(x) = (1, x, x^2, x^3, \\ldots).\n",
        "$$\n",
        "\n",
        "2. **Радиально-базисные функции (RBF).** Эти функции полезны для аппроксимации сложных нелинейных зависимостей, так как каждая RBF сосредоточена вокруг определённого значения признака:\n",
        "$$\n",
        "   \\phi(x) = \\left(e^{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}}, e^{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}}, \\ldots\\right),\n",
        "$$\n",
        "   где $\\mu_i$ — центр функции, а $\\sigma$ — параметр ширины функции.\n",
        "\n",
        "3. **Сплайн-функции.** Сплайны делят пространство признаков на интервалы и строят отдельные полиномы для каждого из них, плавно соединяя их между собой. Они полезны для моделирования зависимостей, изменяющихся в разных диапазонах данных:\n",
        "$$\n",
        "   \\phi(x) = \\begin{cases}\n",
        "      p_1(x), & x < k_1 \\\\\n",
        "      p_2(x), & k_1 \\leq x < k_2 \\\\\n",
        "      \\dots \\\\\n",
        "      p_n(x), & x \\geq k_{n-1}\n",
        "   \\end{cases}\n",
        "$$\n",
        "   где $k_i$ — узлы, а $p_i(x)$ — полиномы на каждом интервале.\n",
        "\n",
        "4. **Синусоидальные базисные функции.** Для анализа периодических данных используются синусоидальные функции, особенно полезные для моделирования сезонных или циклических процессов:\n",
        "$$\n",
        "   \\phi(x) = (\\sin(\\omega_1 x), \\cos(\\omega_1 x), \\sin(\\omega_2 x), \\cos(\\omega_2 x), \\ldots),\n",
        "$$\n",
        "   где $\\omega_i$ — частоты, задающие периодичность.\n",
        "\n",
        "5. **Логистические базисные функции.** Эти функции подходят для задач, где зависимость резко изменяется при переходе через определённые значения, например, для биологических или социальных процессов. Функция имеет вид:\n",
        "$$\n",
        "   \\phi(x) = \\frac{1}{1 + e^{-(x - \\mu)/\\sigma}},\n",
        "$$\n",
        "   где $\\mu$ — центр, а $\\sigma$ — параметр, регулирующий крутизну перехода.\n",
        "\n",
        "Эти базисные функции дают модели возможность учитывать разнообразные типы зависимостей, от полиномиальных до периодических, локальных и пороговых.\n",
        "\n",
        "### Варианты моделей с базисными функциями\n",
        "\n",
        "Рассмотрим примеры применения базисных функций в линейной регрессии.\n",
        "\n",
        "#### 1. Линейная регрессия с исходными признаками\n",
        "\n",
        "В стандартной линейной регрессии в качестве базисных функций можно использовать сами исходные признаки. Для признаков $x_1, x_2, \\ldots, x_D$ базисные функции определяются так:\n",
        "$$\n",
        "\\phi_1(x) = x_1, \\quad \\phi_2(x) = x_2, \\quad \\ldots, \\quad \\phi_D(x) = x_D.\n",
        "$$\n",
        "Здесь базисные функции представляют исходные переменные, а модель остаётся линейной по параметрам $w$.\n",
        "\n",
        "#### 2. Полиномиальная регрессия с одной переменной\n",
        "\n",
        "Полиномиальная регрессия расширяет линейную модель, добавляя степени исходной переменной в качестве базисных функций. Это позволяет учитывать нелинейные зависимости от переменной:\n",
        "$$\n",
        "\\phi_1(x) = x, \\quad \\phi_2(x) = x^2, \\quad \\ldots, \\quad \\phi_K(x) = x^K.\n",
        "$$\n",
        "Каждая степень переменной $x$ становится новой базисной функцией, увеличивая степень нелинейности модели.\n",
        "\n",
        "#### 3. Регрессия с радиально-базисными функциями (RBF-регрессия)\n",
        "\n",
        "Радиально-базисные функции обеспечивают моделирование локальных зависимостей. Каждая RBF-функция определяется центром $\\mu_i$ и шириной $\\sigma$, создавая «пик» вблизи $\\mu_i$. Это полезно для задач с локальными изменениями зависимостей:\n",
        "$$\n",
        "\\phi(x) = \\left(e^{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}}, e^{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}}, \\ldots, e^{-\\frac{(x - \\mu_K)^2}{2\\sigma^2}}\\right).\n",
        "$$\n",
        "RBF-регрессия особенно эффективна для аппроксимации данных, где зависимость меняется локально, как в задачах географического моделирования.\n",
        "\n",
        "#### 4. Сплайн-регрессия\n",
        "\n",
        "Сплайн-регрессия использует полиномы, которые адаптируются в каждом из заданных интервалов (узлов). Это помогает избежать чрезмерного роста значений на краях, что часто наблюдается в полиномиальных моделях. В сплайн-регрессии на каждом интервале применяется свой полином, плавно соединённый с соседними:\n",
        "$$\n",
        "\\phi(x) = \\begin{cases}\n",
        "      p_1(x), & x < k_1 \\\\\n",
        "      p_2(x), & k_1 \\leq x < k_2 \\\\\n",
        "      \\dots \\\\\n",
        "      p_n(x), & x \\geq k_{n-1}\n",
        "   \\end{cases}.\n",
        "$$\n",
        "Сплайн-регрессия подходит для данных, где зависимость может значительно изменяться в разных диапазонах.\n",
        "\n",
        "#### 5. Синусоидальные базисные функции\n",
        "\n",
        "Для задач анализа временных рядов или периодических данных полезны синусоидальные функции. Эти базисные функции захватывают повторяющиеся тренды, такие как сезонные изменения:\n",
        "$$\n",
        "\\phi(x) = (\\sin(\\omega_1 x), \\cos(\\omega_1 x), \\sin(\\omega_2 x), \\cos(\\omega_2 x), \\ldots),\n",
        "$$\n",
        "где $\\omega_i$ — частоты, задающие периодичность.\n",
        "\n",
        "#### 6. Регрессия с логистическими базисными функциями\n",
        "\n",
        "Логистические базисные функции применяются для задач, в которых зависимость резко изменяется при переходе через пороговые значения. Они полезны для моделирования плав\n",
        "\n",
        "ных переходов, характерных для социальных или биологических процессов:\n",
        "$$\n",
        "\\phi(x) = \\frac{1}{1 + e^{-(x - \\mu)/\\sigma}}.\n",
        "$$\n",
        "Логистические функции позволяют модели плавно описывать скачки или пороговые эффекты, делая её гибкой для работы с нелинейными данными.\n",
        "Таким образом, выбор подходящих базисных функций позволяет адаптировать линейную регрессию под задачи с различной степенью сложности. Полиномиальные, радиально-базисные, сплайновые, синусоидальные и логистические функции создают разнообразные способы моделирования зависимости, от простой до сильно нелинейной.\n",
        "\n",
        "\n",
        "## Матрица плана (Design Matrix)\n",
        "\n",
        "Матрица плана, или матрица признаков, обозначаемая как $X$, представляет собой таблицу значений всех независимых переменных (факторов или признаков) для каждого наблюдения в модели линейной регрессии. Каждая строка этой матрицы соответствует одному наблюдению (например, объекту или записи в выборке), а каждый столбец — отдельному фактору, который может быть представлен в исходном виде или преобразован базисной функцией.\n",
        "\n",
        "### Структура матрицы плана\n",
        "\n",
        "Для классической линейной регрессии матрица плана $X$ имеет размерность $n \\times (p+1)$, где $n$ — количество наблюдений, а $p$ — количество факторов (включая свободный член). Структура стандартной матрицы плана выглядит так:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "где первый столбец состоит из единиц (это bias term, или смещение), представляющий свободный член $w_0$ в модели, а остальные столбцы содержат значения факторов для каждого наблюдения.\n",
        "\n",
        "### Функции и задачи матрицы плана\n",
        "\n",
        "Матрица плана играет ключевую роль в линейной регрессии, облегчая расчётные операции и позволяя использовать методы линейной алгебры для построения модели. Основные её функции:\n",
        "\n",
        "1. **Оценка параметров модели.** С помощью матрицы плана можно вычислить параметры $w$ линейной регрессии с использованием метода наименьших квадратов, минимизируя отклонения между реальными значениями целевой переменной и предсказанными значениями модели. Оценка параметров $w$ производится по формуле:\n",
        "$$\n",
        "   \\hat{w} = (X^T X)^{-1} X^T y,\n",
        "$$\n",
        "   где $y$ — вектор значений целевой переменной, $X^T$ — транспонированная матрица плана. Позже мы подробно рассмотрим, как получить эту оценку.\n",
        "\n",
        "2. **Учет множества факторов.** Матрица плана позволяет модели регрессии учитывать несколько факторов одновременно, что повышает её гибкость и даёт возможность рассматривать влияние различных переменных на целевую переменную.\n",
        "\n",
        "3. **Проведение статистических тестов.** Через матрицу плана можно проводить статистические тесты, чтобы оценить значимость коэффициентов регрессионной модели, определяя, какие из факторов оказывают статистически значимое влияние на целевую переменную.\n",
        "\n",
        "### Матрица плана с базисными функциями\n",
        "\n",
        "В случае использования базисных функций структура матрицы плана изменяется, поскольку каждый фактор может быть представлен нелинейными преобразованиями, что позволяет модели захватывать сложные зависимости между признаками и целевой переменной. В этом случае матрица плана, также называемая матрицей базисных функций и обозначаемая как $\\Phi(x)$, имеет следующий вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) = \\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\dots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\dots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\dots & \\phi_p(x_n)\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- каждая строка соответствует одному наблюдению $x_i$,\n",
        "- каждый столбец представляет значение базисной функции $\\phi_j(x_i)$, применённой к признаку $x_i$.\n",
        "\n",
        "### Примеры матрицы плана с базисными функциями\n",
        "\n",
        "#### Полиномиальные базисные функции\n",
        "\n",
        "Для полиномиальной регрессии, где базисные функции представляют собой степени переменной $x$, матрица плана будет выглядеть так:\n",
        "\n",
        "$$\n",
        "\\Phi(x) = \\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\dots & x_1^p \\\\\n",
        "1 & x_2 & x_2^2 & \\dots & x_2^p \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & \\dots & x_n^p\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Радиально-базисные функции (RBF)\n",
        "\n",
        "В случае радиально-базисных функций, которые обычно используют для захвата локальных зависимостей в данных, каждая базисная функция фокусируется на определённой точке $\\mu_j$, и матрица плана принимает вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) = \\begin{bmatrix}\n",
        "e^{-\\frac{(x_1 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_1 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_1 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "e^{-\\frac{(x_2 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_2 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_2 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "e^{-\\frac{(x_n - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_n - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_n - \\mu_p)^2}{2\\sigma^2}}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь $\\mu_j$ — центры радиально-базисных функций, а $\\sigma$ регулирует их ширину.\n",
        "\n",
        "### Преимущества матрицы плана с базисными функциями\n",
        "\n",
        "Использование матрицы плана с базисными функциями даёт несколько преимуществ:\n",
        "\n",
        "1. **Моделирование нелинейных зависимостей.** Преобразованные признаки позволяют регрессионной модели захватывать нелинейные зависимости между факторами и целевой переменной, делая её более гибкой и подходящей для моделирования сложных данных.\n",
        "\n",
        "2. **Расширение возможностей анализа.** Базисные функции позволяют строить более сложные и точные модели, которые можно адаптировать для задач с различными типами данных и зависимостей.\n",
        "\n",
        "3. **Повышение точности модели.** С добавлением базисных функций модель способна лучше подстраиваться под закономерности в данных, улучшая точность прогнозирования и обобщающую способность.\n",
        "\n",
        "Таким образом, матрица плана с базисными функциями является мощным инструментом при построении моделей линейной регрессии, позволяя учитывать различные нелинейные преобразования исходных данных и повышать качество модели за счёт лучшего описания сложных зависимостей.\n",
        "\n",
        "\n",
        "\n",
        "## Функция правдоподобия (Likelihood)\n",
        "\n",
        "Функция правдоподобия — это мера того, насколько хорошо параметры модели $w$ объясняют наблюдаемые данные $t$ при известных значениях входных переменных $X$. В линейной регрессии обычно предполагается, что ошибки модели распределены нормально с математическим ожиданием 0 и дисперсией $\\sigma^2$.\n"
      ],
      "metadata": {
        "id": "_J2xCc7YP9X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Пример с непрерывным распределением\n",
        "\n",
        "Теперь, переходя от теории к практике, давайте рассмотрим несколько примеров. Один из них будет для дискретного случая, а другой — для непрерывного. Суть примеров заключается в том, что мы попытаемся найти оценки параметров распределений методом максимального правдоподобия (ММП).\n",
        "\n",
        "Для первого примера возьмем **нормальное распределение**, которое имеет два параметра: математическое ожидание $\\mu$ и дисперсию $\\sigma^2$. Убедимся, что оценки максимального правдоподобия для этих параметров имеют вид:\n",
        "$$\n",
        "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2,\n",
        "$$\n",
        "где $\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ — выборочное среднее.\n",
        "\n",
        "Таким образом, параметры $\\mu$ и $\\sigma^2$ будут оцениваться как вектор $(\\hat{\\mu}, \\hat{\\sigma}^2)$.\n",
        "\n",
        "\n",
        "#### 1. Определяем функцию плотности\n",
        "\n",
        "Функция плотности вероятности нормального распределения $N(\\mu, \\sigma^2)$ задается формулой:\n",
        "$$\n",
        "f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right),\n",
        "$$\n",
        "где:\n",
        "- $\\mu$ — математическое ожидание (среднее значение),\n",
        "- $\\sigma^2$ — дисперсия,\n",
        "- $\\sigma > 0$ — стандартное отклонение.\n",
        "\n",
        "#### 2. Формулируем функцию правдоподобия\n",
        "\n",
        "Пусть у нас есть выборка $x_1, x_2, \\dots, x_n$, состоящая из $n$ независимых наблюдений, взятых из нормального распределения $N(\\mu, \\sigma^2)$. Функция правдоподобия $L(\\mu, \\sigma^2)$ определяется как совместная плотность вероятности всех наблюдений:\n",
        "$$\n",
        "L(\\mu, \\sigma^2) = \\prod_{i=1}^n f(x_i; \\mu, \\sigma^2).\n",
        "$$\n",
        "\n",
        "Подставляя выражение для $f(x_i; \\mu, \\sigma^2)$, получаем:\n",
        "$$\n",
        "L(\\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right).\n",
        "$$\n",
        "\n",
        "Это можно записать в более компактной форме:\n",
        "$$\n",
        "L(\\mu, \\sigma^2) = \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Логарифмическая функция правдоподобия\n",
        "\n",
        "Чтобы упростить вычисления, переходим к логарифмической функции правдоподобия:\n",
        "$$\n",
        "\\ln L(\\mu, \\sigma^2) = \\ln\\left[\\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right)\\right].\n",
        "$$\n",
        "\n",
        "Раскрываем логарифм:\n",
        "$$\n",
        "\\ln L(\\mu, \\sigma^2) = n \\ln\\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
        "$$\n",
        "\n",
        "Упрощаем первое слагаемое:\n",
        "$$\n",
        "\\ln L(\\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### 4. Находим оценки максимального правдоподобия\n",
        "\n",
        "##### Оценка для $\\mu$\n",
        "Чтобы найти $\\hat{\\mu}$, максимизируем $\\ln L(\\mu, \\sigma^2)$ по $\\mu$. Для этого берем частную производную по $\\mu$ и приравниваем ее к нулю:\n",
        "$$\n",
        "\\frac{\\partial \\ln L(\\mu, \\sigma^2)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) = 0.\n",
        "$$\n",
        "\n",
        "Отсюда:\n",
        "$$\n",
        "\\sum_{i=1}^n (x_i - \\mu) = 0 \\quad \\Rightarrow \\quad \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i.\n",
        "$$\n",
        "\n",
        "Таким образом, оценка для $\\mu$:\n",
        "$$\n",
        "\\boxed{\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i.}\n",
        "$$\n",
        "\n",
        "##### Оценка для $\\sigma^2$\n",
        "Чтобы найти $\\hat{\\sigma}^2$, максимизируем $\\ln L(\\mu, \\sigma^2)$ по $\\sigma^2$. Для этого берем частную производную по $\\sigma^2$ и приравниваем ее к нулю:\n",
        "$$\n",
        "\\frac{\\partial \\ln L(\\mu, \\sigma^2)}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^n (x_i - \\mu)^2 = 0.\n",
        "$$\n",
        "\n",
        "Умножаем на $2(\\sigma^2)^2$:\n",
        "$$\n",
        "-n\\sigma^2 + \\sum_{i=1}^n (x_i - \\mu)^2 = 0.\n",
        "$$\n",
        "\n",
        "Отсюда:\n",
        "$$\n",
        "\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
        "$$\n",
        "\n",
        "Подставляем $\\mu = \\hat{\\mu}$:\n",
        "$$\n",
        "\\boxed{\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2.}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итоговые результаты\n",
        "Оценки максимального правдоподобия для параметров нормального распределения:\n",
        "$$\n",
        "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2.\n",
        "$$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "84vyFfp0pTyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Правдоподобие для одного наблюдения\n",
        "\n",
        "Предположим, что каждое наблюдение $t_i$ подчиняется нормальному распределению с математическим ожиданием, заданным линейной моделью $y(x_i, w) = w^T \\phi(x_i)$, и дисперсией $\\sigma^2$. Тогда правдоподобие для одного наблюдения $t_i$, при известных значениях $x_i$ и параметрах модели $w$, выражается через функцию плотности вероятности нормального распределения:\n",
        "\n",
        "$$\n",
        "L(t_i | x_i, w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - y(x_i, w))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Интуитивное объяснение формулы\n",
        "\n",
        "- Значение правдоподобия тем выше, чем ближе наблюдаемое значение $t_i$ к предсказанному значению $y(x_i, w)$. Это связано с тем, что отклонение $(t_i - y(x_i, w))^2$ оказывает экспоненциальное влияние на величину правдоподобия.\n",
        "- Если наблюдаемое значение $t_i$ значительно отличается от предсказанного, то экспонента быстро уменьшается, что приводит к снижению правдоподобия.\n",
        "- Множитель $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ является нормировочным коэффициентом. Он обеспечивает выполнение условия, согласно которому интеграл функции плотности вероятности по всем возможным значениям $t_i$ равен единице.\n",
        "\n",
        "\n",
        "\n",
        "### Применение правдоподобия\n",
        "\n",
        "Правдоподобие играет ключевую роль в задачах статистического вывода. Вот несколько примеров его использования:\n",
        "\n",
        "1. **Метод максимального правдоподобия (MLE)**:  \n",
        "   Этот метод позволяет найти оптимальные параметры модели $w$, которые максимизируют правдоподобие данных. Другими словами, мы ищем такие параметры, при которых наблюдаемые данные наиболее вероятны.\n",
        "\n",
        "2. **Байесовский вывод**:  \n",
        "   В байесовском подходе правдоподобие используется как часть формулы Байеса. Оно комбинируется с априорным распределением параметров для вычисления их апостериорного распределения.\n",
        "\n",
        "3. **Оценка качества модели**:  \n",
        "   Правдоподобие также может служить мерой того, насколько хорошо модель соответствует данным. Чем выше суммарное правдоподобие для всех наблюдений, тем лучше модель описывает данные.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, правдоподобие для одного наблюдения $t_i$ показывает, насколько вероятно получить это значение при заданных параметрах модели ($x_i$, $w$, $\\sigma^2$). Оно основано на предположении нормальности распределения $t_i$ с центром в предсказанном значении $y(x_i, w)$ и дисперсией $\\sigma^2$.\n",
        "\n",
        "$$\n",
        "\\boxed{L(t_i | x_i, w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - y(x_i, w))^2}{2\\sigma^2}}.}\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d7lDv8o0pUGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Пример"
      ],
      "metadata": {
        "id": "JPr7VZqxYt0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Генерация данных\n",
        "np.random.seed(42)  # Для воспроизводимости результатов\n",
        "x = np.linspace(0, 10, 100)  # Входные данные (равномерно распределенные)\n",
        "true_w = 2.5  # Истинный вес линейной модели\n",
        "true_b = 1.0  # Истинное смещение\n",
        "sigma = 1.5  # Стандартное отклонение шума (дисперсия sigma^2)\n",
        "\n",
        "# Наблюдаемые значения t_i = y(x_i) + шум\n",
        "t = true_w * x + true_b + np.random.normal(0, sigma, size=x.shape)\n",
        "\n",
        "# 2. Построение модели (линейная регрессия)\n",
        "# Предположим, что мы знаем истинные параметры w и b\n",
        "predicted_mean = true_w * x + true_b  # Среднее значение (предсказания модели)\n",
        "\n",
        "# 3. Доверительный интервал (на основе дисперсии)\n",
        "# Интервал ±2 стандартных отклонения охватывает ~95% данных для нормального распределения\n",
        "lower_bound = predicted_mean - 2 * sigma\n",
        "upper_bound = predicted_mean + 2 * sigma\n",
        "\n",
        "# 4. Визуализация\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Наблюдаемые данные\n",
        "plt.scatter(x, t, color='blue', label='Наблюдаемые данные', alpha=0.6)\n",
        "\n",
        "# Среднее значение (предсказания модели)\n",
        "plt.plot(x, predicted_mean, color='red', label='Среднее значение (предсказания)', linewidth=2)\n",
        "\n",
        "# Доверительный интервал\n",
        "plt.fill_between(x, lower_bound, upper_bound, color='orange', alpha=0.3, label='Доверительный интервал (±2σ)')\n",
        "\n",
        "# Настройки графика\n",
        "plt.title('Пример среднего значения и дисперсии в регрессии', fontsize=16)\n",
        "plt.xlabel('Входные данные (x)', fontsize=14)\n",
        "plt.ylabel('Целевая переменная (t)', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(alpha=0.4)\n",
        "\n",
        "# Отображение графика\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "dqHkH3ghZUMb",
        "outputId": "be531fc9-a2be-4bcd-deed-abf413963622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Доверительный интервал\n",
        "\n",
        "**Доверительный интервал** — это статистическая оценка, которая определяет диапазон значений, внутри которого с заданной вероятностью (уровнем доверия) находится истинное значение оцениваемого параметра или предсказания. В контексте регрессионного анализа доверительный интервал характеризует неопределенность модели, обусловленную наличием случайных ошибок (шума) в данных, и позволяет количественно оценить точность предсказаний.\n",
        "\n",
        "### Методология вычисления\n",
        "\n",
        "При условии нормального распределения ошибок модели доверительный интервал строится на основе математического ожидания (предсказанного значения) и стандартного отклонения ($\\sigma$), которое является квадратным корнем из дисперсии ошибок. Для нормального распределения:\n",
        "- Интервал $\\pm 1\\sigma$ покрывает приблизительно **68%** наблюдений.\n",
        "- Интервал $\\pm 2\\sigma$ охватывает около **95%** наблюдений.\n",
        "- Интервал $\\pm 3\\sigma$ включает примерно **99.7%** наблюдений.\n",
        "\n",
        "Формально, доверительный интервал для предсказанного значения $y(x)$ может быть выражен как:\n",
        "$$\n",
        "[\\hat{y}(x) - k \\cdot \\sigma, \\hat{y}(x) + k \\cdot \\sigma],\n",
        "$$\n",
        "где:\n",
        "- $\\hat{y}(x)$ — предсказанное значение (математическое ожидание),\n",
        "  \n",
        " В случае линейной регрессии:\n",
        "$$\n",
        "\\hat{y}(x) = w^T \\phi(x),\n",
        "$$\n",
        "где:\n",
        "- $w$ — вектор параметров модели,\n",
        "- $\\phi(x) $ — преобразование входных данных (например, базисные функции).\n",
        "\n",
        "-  стандартное отклонение ошибок $$\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (t_i - \\hat{y}(x_i))^2}$$,\n",
        "- $k$ — коэффициент, зависящий от уровня доверия (например, $k = 2$ для 95%-го доверительного интервала).\n",
        "\n",
        "### Значимость доверительного интервала\n",
        "\n",
        "1. **Оценка точности модели**:\n",
        "   Ширина доверительного интервала напрямую связана с точностью модели. Узкий интервал указывает на высокую точность предсказаний, тогда как широкий интервал свидетельствует о значительной неопределенности, вызванной либо недостаточным объемом данных, либо высоким уровнем шума.\n",
        "\n",
        "2. **Учет стохастической природы данных**:\n",
        "   Реальные данные часто содержат случайные флуктуации, которые невозможно полностью устранить. Доверительный интервал позволяет формализовать эту неопределенность и предоставляет количественную меру разброса данных вокруг предсказанного значения.\n",
        "\n",
        "3. **Принятие решений на основе модели**:\n",
        "   В прикладных задачах доверительные интервалы играют ключевую роль при принятии решений. Например, в прогнозировании спроса или финансовых показателей интервалы позволяют оценить возможные границы изменений и минимизировать риски.\n",
        "\n",
        "### Интерпретация графика\n",
        "\n",
        "На графике, построенном ранее:\n",
        "- Красная линия соответствует **математическому ожиданию** (предсказаниям модели).\n",
        "- Оранжевая область отражает **доверительный интервал** ($\\pm 2\\sigma$), который с вероятностью 95% содержит истинные значения целевой переменной.\n",
        "\n",
        "Если наблюдаемые данные (синие точки) находятся внутри оранжевой области, это подтверждает согласованность модели с данными. Точки, выходящие за пределы доверительного интервала, могут быть классифицированы как выбросы или указывать на систематические ошибки модели.\n",
        "\n",
        "### Формализация\n",
        "\n",
        "Математически доверительный интервал связан с вероятностной интерпретацией:\n",
        "$$\n",
        "P(\\text{истинное значение} \\in [\\hat{y}(x) - k \\cdot \\sigma, \\hat{y}(x) + k \\cdot \\sigma]) = 1 - \\alpha,\n",
        "$$\n",
        "где:\n",
        "- $1 - \\alpha$ — уровень доверия (например, 0.95 для 95%-го доверительного интервала),\n",
        "- $\\alpha$ — допустимая вероятность ошибки.\n",
        "\n",
        "Допустимая вероятность ошибки ($\\alpha$) определяет уровень значимости, связанный с доверительным интервалом. Уровень доверия ($1 - \\alpha$) указывает вероятность того, что истинное значение находится в пределах доверительного интервала.\n",
        "\n",
        "Например:\n",
        "- Для 95%-го доверительного интервала: $\\alpha = 0.05$, $1 - \\alpha = 0.95$.\n",
        "- Для 99%-го доверительного интервала: $\\alpha = 0.01$, $1 - \\alpha = 0.99$.\n",
        "\n",
        "### Дополнение\n",
        "\n",
        "Важно отметить, что доверительный интервал не является единственной мерой неопределенности. В некоторых случаях, особенно при работе с малыми выборками или нестандартными распределениями, могут использоваться альтернативные методы, такие как бутстреп-интервалы или байесовские доверительные интервалы. Эти методы позволяют более гибко учитывать особенности данных и модели.\n",
        "\n",
        "Кроме того, при интерпретации доверительных интервалов следует учитывать, что они отражают только случайную составляющую неопределенности. Систематические ошибки, связанные с неправильной спецификацией модели или смещением данных, не учитываются в доверительных интервалах и требуют отдельного анализа.\n",
        "\n"
      ],
      "metadata": {
        "id": "oKYMDjjBdFNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Давайте разберем этот пример более подробно и приведем числовой пример для функции правдоподобия одного наблюдения в случае нормального распределения.\n",
        "\n",
        "\n",
        "\n",
        "### Пример: Нормальное распределение\n",
        "\n",
        "Предположим, что у нас есть одно наблюдение $t_i = 5$, которое следует нормальному распределению с математическим ожиданием, заданным линейной моделью:\n",
        "\n",
        "$$\n",
        "y(x_i, w) = w^T \\phi(x_i),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $x_i$ — входное значение,\n",
        "- $w$ — параметры модели,\n",
        "- $\\phi(x_i)$ — базисные функции (например, полиномиальные или другие).\n",
        "\n",
        "Для простоты предположим, что:\n",
        "- $x_i = 2$,\n",
        "- $w = [1, 2]^T$ (вектор параметров),\n",
        "- $\\phi(x_i) = [1, x_i]^T$ (линейная базисная функция),\n",
        "- $\\sigma^2 = 1$ (дисперсия шума).\n",
        "\n",
        "Тогда линейная модель принимает вид:\n",
        "$$\n",
        "y(x_i, w) = w^T \\phi(x_i) = [1, 2] \\cdot [1, 2]^T = 1 \\cdot 1 + 2 \\cdot 2 = 5.\n",
        "$$\n",
        "\n",
        "Таким образом, математическое ожидание $y(x_i, w) = 5$.\n",
        "\n",
        "#### Функция правдоподобия:\n",
        "Функция правдоподобия для одного наблюдения $t_i$ задается формулой плотности вероятности нормального распределения:\n",
        "$$\n",
        "L(t_i | x_i, w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - y(x_i, w))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "Подставим известные значения:\n",
        "- $t_i = 5$,\n",
        "- $y(x_i, w) = 5$,\n",
        "- $\\sigma^2 = 1$.\n",
        "\n",
        "Тогда:\n",
        "$$\n",
        "L(t_i | x_i, w) = \\frac{1}{\\sqrt{2\\pi \\cdot 1}} \\cdot e^{-\\frac{(5 - 5)^2}{2 \\cdot 1}}.\n",
        "$$\n",
        "\n",
        "Упростим выражение:\n",
        "1. Вычислим экспоненту:\n",
        "   $$\n",
        "   e^{-\\frac{(5 - 5)^2}{2}} = e^{-\\frac{0}{2}} = e^0 = 1.\n",
        "   $$\n",
        "\n",
        "2. Вычислим нормировочный множитель:\n",
        "   $$\n",
        "   \\frac{1}{\\sqrt{2\\pi \\cdot 1}} = \\frac{1}{\\sqrt{2\\pi}} \\approx 0.3989.\n",
        "   $$\n",
        "\n",
        "Итоговое значение правдоподобия:\n",
        "$$\n",
        "L(t_i | x_i, w) = 0.3989 \\cdot 1 = 0.3989.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Анализ:\n",
        "В данном случае наблюдение $t_i = 5$ полностью соответствует математическому ожиданию $y(x_i, w) = 5$, поэтому правдоподобие достигает своего максимального значения для данного $\\sigma^2$. Если бы $t_i$ отклонялось от $y(x_i, w)$, то правдоподобие уменьшалось бы из-за экспоненциального члена в формуле.\n",
        "\n",
        "\n",
        "\n",
        "### Обобщение:\n",
        "Если бы $t_i \\neq y(x_i, w)$, например, $t_i = 6$, то правдоподобие уменьшилось бы. Рассчитаем это для сравнения:\n",
        "\n",
        "1. Подставим $t_i = 6$:\n",
        "   $$\n",
        "   L(t_i | x_i, w) = \\frac{1}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{(6 - 5)^2}{2}}.\n",
        "   $$\n",
        "\n",
        "2. Вычислим экспоненту:\n",
        "   $$\n",
        "   e^{-\\frac{(6 - 5)^2}{2}} = e^{-\\frac{1}{2}} \\approx e^{-0.5} \\approx 0.6065.\n",
        "   $$\n",
        "\n",
        "3. Умножим на нормировочный множитель:\n",
        "   $$\n",
        "   L(t_i | x_i, w) = 0.3989 \\cdot 0.6065 \\approx 0.242.\n",
        "   $$\n",
        "\n",
        "Таким образом, правдоподобие уменьшилось, так как $t_i = 6$ дальше от математического ожидания $y(x_i, w) = 5$.\n",
        "\n",
        "Таким образом, функция правдоподобия показывает, насколько вероятно наблюдение $t_i$ при заданных параметрах модели $w$ и дисперсии $\\sigma^2$. В данном примере мы увидели, что:\n",
        "- При $t_i = 5$ правдоподобие максимально: $\\boxed{0.3989}$,\n",
        "- При $t_i = 6$ правдоподобие уменьшается: $\\boxed{0.242}$.\n",
        "\n",
        "### Полная функция правдоподобия\n",
        "\n",
        "Функция правдоподобия для всей выборки из $n$ наблюдений — это произведение правдоподобий для каждого наблюдения:\n",
        "\n",
        "$$\n",
        "L(t | X, w) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - y(x_i, w))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "Подставив $y(x_i, w) = w^T \\phi(x_i)$, мы получаем:\n",
        "\n",
        "$$\n",
        "L(t | X, w, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "### Логарифмирование функции правдоподобия\n",
        "\n",
        "Вместо максимизации функции правдоподобия $L(t | X, w, \\sigma^2)$ мы можем максимизировать её логарифм, что значительно упрощает вычисления. Логарифмирование преобразует произведение в сумму, что делает анализ и оптимизацию более управляемыми.\n",
        "\n",
        "Рассчитаем логарифм функции правдоподобия:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\ln \\left( \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}} \\right).\n",
        "$$\n",
        "\n",
        "Применяя свойство логарифма $\\ln (a \\cdot b) = \\ln a + \\ln b$, получаем:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}} \\right).\n",
        "$$\n",
        "\n",
        "Теперь можем разложить логарифм произведения на сумму логарифмов:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\left( \\ln \\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\ln e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}} \\right).\n",
        "$$\n",
        "\n",
        "### Применение свойств логарифма\n",
        "\n",
        "Воспользуемся двумя свойствами логарифма:\n",
        "\n",
        "1. $\\ln \\frac{1}{a} = -\\ln a$,\n",
        "2. $\\ln e^x = x$.\n",
        "\n",
        "Используя эти свойства, получаем:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\left( -\\ln \\sqrt{2\\pi\\sigma^2} - \\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2} \\right).\n",
        "$$\n",
        "\n",
        "Теперь разложим первый член более подробно:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\left( -\\frac{1}{2} \\ln (2\\pi\\sigma^2) - \\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2} \\right).\n",
        "$$\n",
        "\n",
        "Раскрывая сумму, можно представить результат в виде двух отдельных слагаемых:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = -\\frac{n}{2} \\ln (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "### Окончательная логарифмическая функция правдоподобия\n",
        "\n",
        "Итак, логарифм функции правдоподобия для линейной регрессии принимает следующий вид:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = -\\frac{n}{2} \\ln (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "Эта функция называется **логарифмической функцией правдоподобия**, и её удобно использовать для оптимизации параметров $w$ и $\\sigma^2$ модели линейной регрессии, так как задача максимизации этой функции эквивалентна минимизации функции потерь:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "Такое логарифмическое представление упрощает работу с функцией правдоподобия, особенно при использовании методов оптимизации, таких как градиентный спуск или аналитические методы для нахождения оптимальных параметров модели.\n",
        "\n",
        "### Оптимизация параметров модели\n",
        "\n",
        "Для нахождения оценок параметров $w$ и дисперсии ошибок $\\sigma^2$ максимизируют логарифмическую функцию правдоподобия. Так как логарифмическая функция правдоподобия является убывающей функцией ошибки модели, это эквивалентно минимизации суммы квадратов отклонений, или функции потерь, для линейной регрессии.\n",
        "\n",
        "В частности, целевая функция для минимизации, полученная из логарифмической функции правдоподобия, имеет вид:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "### Преимущества логарифмирования функции правдоподобия\n",
        "\n",
        "1. **Превращение произведения в сумму**: Логарифм позволяет преобразовать произведение плотностей вероятности для каждого наблюдения в сумму логарифмов. Это упрощает вычисления и делает задачу более устойчивой.\n",
        "\n",
        "2. **Удобство для градиентного спуска и аналитического решения**: В большинстве задач, включая линейную регрессию, логарифмированная функция правдоподобия легче оптимизируется, так как её градиенты имеют более простую форму. Это упрощает поиск оптимальных параметров $w$.\n",
        "\n",
        "3. **Стабильность при вычислениях**: Логарифмирование снижает риск ошибки округления при работе с очень малыми вероятностями, что может быть важно при большом объёме данных.\n",
        "\n",
        "Таким образом, логарифмическая функция правдоподобия является важным инструментом для максимизации правдоподобия в линейной регрессии, позволяя находить такие значения параметров $w$, которые обеспечивают наилучшее объяснение имеющихся данных в рамках предположений модели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Минимизация целевой функции\n",
        "\n",
        "Целевая функция (или функционал) в задачах оптимизации — это функция, которую необходимо минимизировать или максимизировать в зависимости от поставленной задачи. В контексте линейной регрессии целевая функция служит критерием для оценки того, насколько хорошо модель соответствует данным.\n",
        "\n",
        "### Целевая функция линейной регрессии\n",
        "\n",
        "В линейной регрессии целевая функция, отражающая ошибку предсказания, обычно представляется следующим образом:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $t_i$ — фактическое значение зависимой переменной для $i$-го наблюдения,\n",
        "- $w$ — вектор параметров модели (коэффициентов),\n",
        "- $\\phi(x_i)$ — вектор базисных функций для $i$-го наблюдения.\n",
        "\n",
        "Минимизация этой целевой функции позволяет найти оптимальные коэффициенты модели, которые наилучшим образом соответствуют данным, тем самым создавая наилучшую прямую (или плоскость в случае многомерной регрессии), описывающую зависимость между независимыми и зависимой переменными.\n",
        "\n",
        "### Важность минимизации целевой функции\n",
        "\n",
        "Минимизация целевой функции в линейной регрессии критически важна для нахождения значений коэффициентов, которые оптимально объясняют зависимую переменную на основе предикторов. Это позволяет строить модели, способные эффективно прогнозировать или анализировать зависимую переменную.\n",
        "\n",
        "### Отличие целевой функции от функции потерь\n",
        "\n",
        "Хотя целевая функция и функция потерь могут совпадать в некоторых случаях, они имеют разные роли в контексте оптимизации:\n",
        "\n",
        "- **Целевая функция** — это общее понятие, описывающее функцию, которую необходимо минимизировать или максимизировать. В случае линейной регрессии целевая функция направлена на нахождение наилучших параметров модели для объяснения зависимости между переменными.\n",
        "  \n",
        "- **Функция потерь** — это более специфический термин, который обычно используется для оценки ошибок предсказания модели. Она показывает, насколько сильно предсказанные значения отличаются от фактических. В линейной регрессии функция потерь часто представляется в виде средней квадратичной ошибки (MSE), которая фактически является целевой функцией.\n",
        "\n",
        "Таким образом, целевая функция может быть функцией потерь, но не всегда. В других задачах оптимизации целевая функция может иметь другие формы, в зависимости от контекста и требований.\n",
        "\n",
        "### Методы оптимизации\n",
        "\n",
        "Для минимизации целевой функции в линейной регрессии можно использовать различные методы оптимизации:\n",
        "\n",
        "- **Метод наименьших квадратов (МНК)** — находит коэффициенты, минимизируя сумму квадратов ошибок.\n",
        "- **Градиентный спуск** — итеративный метод, который обновляет параметры модели в направлении антиградиента функции потерь.\n",
        "- **Нормальное уравнение** — аналитический метод, позволяющий находить оптимальные параметры без необходимости итераций.\n",
        "\n",
        "После минимизации целевой функции и нахождения оптимальных значений коэффициентов модель становится более точной и эффективной в предсказании зависимой переменной на основе предикторов.\n",
        "\n",
        "## Функция потерь\n",
        "\n",
        "**Функция потерь** — это мера ошибок предсказания модели. В задаче регрессии такой мерой может служить расстояние между предсказанным значением $f(x)$ и его фактическим значением $y$.\n",
        "\n",
        "Одной из распространенных функций потерь является **средняя квадратичная ошибка** (MSE), которая совпадает с нашей целевой функцией:\n",
        "\n",
        "$$\n",
        "\\text{loss} = E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения,\n",
        "- $f(x_i) = w^T \\phi(x_i)$ — предсказанное значение зависимой переменной, полученное с использованием модели с параметрами $w$ и базисными функциями $\\phi$.\n",
        "\n",
        "### Оптимизация функции потерь\n",
        "\n",
        "Поиск оптимальных параметров модели сводится к задаче нахождения минимума функции потерь. Это можно сделать с использованием различных алгоритмов оптимизации, в зависимости от требований к вычислительной эффективности и точности. Например, при использовании градиентного спуска параметры обновляются следующим образом:\n",
        "\n",
        "$$\n",
        "w^{(t+1)} = w^{(t)} - \\eta \\nabla E(w^{(t)}),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w^{(t)}$ — текущее значение параметров,\n",
        "- $\\eta$ — скорость обучения,\n",
        "- $\\nabla E(w^{(t)})$ — градиент функции потерь по параметрам.\n",
        "\n",
        "Подробно рассмотрим метод градиентного спуска в следующих разделах.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Поиск локального минимума\n",
        "\n",
        "Рассмотрим задачу нахождения локального минимума функции потерь (ошибки) $ E(w) $ для простой линейной регрессии. Необходимым (но недостаточным) условием для нахождения локального минимума дифференцируемой функции является равенство нулю всех её частных производных по параметрам модели:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "\\begin{cases}\n",
        "\\displaystyle \\frac{\\partial E(w)}{\\partial w_0} = 0, \\\\\n",
        "\\displaystyle \\frac{\\partial E(w)}{\\partial w_1} = 0,\n",
        "\\end{cases}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ w_0 $ — смещение (bias),\n",
        "- $ w_1 $ — вес признака,\n",
        "- $ E(w) $ — функция потерь (например, среднеквадратичная ошибка, MSE).\n",
        "\n",
        "Это условие означает, что градиент функции потерь $ E(w) $ должен быть равен нулю:\n",
        "$$\n",
        "\\nabla E(w) = \\begin{bmatrix}\n",
        "\\frac{\\partial E(w)}{\\partial w_0} \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_1}\n",
        "\\end{bmatrix} = \\mathbf{0}.\n",
        "$$\n",
        "\n",
        "Однако выполнение этого условия гарантирует только наличие **критической точки** (которая может быть локальным минимумом, максимумом или седловой точкой). Чтобы убедиться, что найденная критическая точка является локальным минимумом, необходимо проверить достаточные условия, например:\n",
        "1. Положительную определённость гессиана $ H(w) $ (матрицы вторых производных):\n",
        "   $$\n",
        "   H(w) = \\begin{bmatrix}\n",
        "   \\frac{\\partial^2 E(w)}{\\partial w_0^2} & \\frac{\\partial^2 E(w)}{\\partial w_0 \\partial w_1} \\\\\n",
        "   \\frac{\\partial^2 E(w)}{\\partial w_1 \\partial w_0} & \\frac{\\partial^2 E(w)}{\\partial w_1^2}\n",
        "   \\end{bmatrix}.\n",
        "   $$\n",
        "   Если все собственные значения гессиана положительны, то критическая точка является локальным минимумом.\n",
        "\n",
        "2. Или использовать численные методы (например, метод градиентного спуска), чтобы подтвердить сходимость к минимуму.\n",
        "\n",
        "\n",
        "\n",
        "### Особенности MSE для линейной регрессии\n",
        "\n",
        "В случае среднеквадратичной ошибки (MSE) для линейной регрессии функция потерь имеет вид:\n",
        "$$\n",
        "E(w) = \\frac{1}{2n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right)^2,\n",
        "$$\n",
        "где:\n",
        "- $ n $ — количество наблюдений,\n",
        "- $ x_i $ — значения признаков,\n",
        "- $ y_i $ — истинные значения целевой переменной,\n",
        "- $ w_0 $ и $ w_1 $ — параметры модели.\n",
        "\n",
        "Функция $ E(w) $ является **квадратичной** относительно параметров $ w_0 $ и $ w_1 $. Это означает, что её поверхность ошибок представляет собой параболоид, который имеет **единственную глобальную точку минимума**. Таким образом:\n",
        "- Локальный минимум всегда совпадает с глобальным минимумом.\n",
        "- Достаточные условия (например, положительная определённость гессиана) выполняются автоматически, так как гессиан для квадратичной функции всегда положительно определён.\n"
      ],
      "metadata": {
        "id": "IYaUc-ZMYunm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Метод наименьших квадратов\n",
        "\n",
        "Для простой линейной регрессии функция потерь обычно определяется как среднеквадратичная ошибка (Mean Squared Error, MSE) и имеет следующий вид:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{n} \\left( y_i - (w_0 + w_1 x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $E(w)$ — функция потерь, которую мы стремимся минимизировать при настройке параметров модели.\n",
        "- $w$ — вектор параметров модели. В данном случае $w_0$ и $w_1$ — это коэффициенты регрессии, которые мы хотим определить.\n",
        "- $n$ — количество наблюдений в нашем наборе данных.\n",
        "- $y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения.\n",
        "- $x_i$ — значение независимой переменной для $i$-го наблюдения.\n",
        "- Формула $y_i - (w_0 + w_1 x_i)$ представляет собой разницу между фактическим и предсказанным значением зависимой переменной для $i$-го наблюдения.\n",
        "- Квадрат этой разницы используется для учета как положительных, так и отрицательных отклонений от фактического значения.\n",
        "- Сумма квадратов ошибок (среднеквадратичная ошибка, MSE) дает общую меру того, насколько хорошо модель соответствует данным.\n",
        "\n",
        "Наша цель заключается в подборе значений параметров $w_0$ и $w_1$ так, чтобы минимизировать эту функцию потерь, обеспечивая наилучшее соответствие модели данным.\n",
        "\n",
        "### Оптимизация методом наименьших квадратов\n",
        "\n",
        "Чтобы найти значения параметров $w_0$ и $w_1$, минимизирующие функцию потерь, мы можем использовать метод наименьших квадратов, который включает в себя следующие шаги:\n",
        "\n",
        "1. **Запись функции потерь:** Определяем функцию потерь $E(w)$ как среднеквадратичную ошибку.\n",
        "\n",
        "2. **Вычисление частных производных:** Вычисляем частные производные функции потерь по параметрам $w_0$ и $w_1$:\n",
        "\n",
        "#### 1. Вычисление производных функции потерь по параметрам $w_0$ и $w_1$\n",
        "\n",
        "Для нахождения оптимальных значений параметров $w_0$ и $w_1$ необходимо вычислить частные производные функции потерь $E(w)$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_0} = -\\sum_{i=1}^{n} (y_i - (w_0 + w_1 x_i)),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_1} = -\\sum_{i=1}^{n} x_i (y_i - (w_0 + w_1 x_i)).\n",
        "$$\n",
        "\n",
        "#### 2. Приравнивание производных к нулю и решение системы уравнений\n",
        "\n",
        "После вычисления производных, приравниваем их к нулю и решаем систему уравнений:\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^{n} (y_i - (w_0 + w_1 x_i)) = 0,\n",
        "$$\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^{n} x_i (y_i - (w_0 + w_1 x_i)) = 0.\n",
        "$$\n",
        "\n",
        "Упростив эти уравнения, получаем систему нормальных уравнений:\n",
        "\n",
        "1. $\\sum y_i = n w_0 + w_1 \\sum x_i$\n",
        "\n",
        "2. $\\sum x_i y_i = w_0 \\sum x_i + w_1 \\sum x_i^2$\n",
        "\n",
        "#### 3. Нахождение оптимальных значений $w_0$ и $w_1$\n",
        "\n",
        "Решив полученную систему уравнений, можно найти оптимальные значения параметров $w_0$ и $w_1$, которые минимизируют функцию потерь MSE:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{n \\sum_{i=1}^{n} x_i y_i - \\left( \\sum_{i=1}^{n} x_i \\right) \\left( \\sum_{i=1}^{n} y_i \\right)}{n \\sum_{i=1}^{n} x_i^2 - \\left( \\sum_{i=1}^{n} x_i \\right)^2},\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_0 = \\frac{\\sum_{i=1}^{n} y_i - w_1 \\left( \\sum_{i=1}^{n} x_i \\right)}{n}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Упрощение через ковариацию и дисперсию\n",
        "\n",
        "#### 1. Наклон ($w_1$)\n",
        "Формула для наклона $w_1$ в терминах ковариации и дисперсии выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\text{Cov}(x, y)$ — ковариация между переменными $x$ и $y$, которая измеряет линейную связь между ними;\n",
        "- $\\text{Var}(x)$ — дисперсия переменной $x$, которая измеряет разброс значений $x$ вокруг их среднего.\n",
        "\n",
        "Ковариация и дисперсия определяются как:\n",
        "\n",
        "$$\n",
        "\\text{Cov}(x, y) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}(x) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\bar{x}$ — среднее значение $x$,\n",
        "- $\\bar{y}$ — среднее значение $y$.\n",
        "\n",
        "Таким образом, формула для $w_1$ становится:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}.\n",
        "$$\n",
        "\n",
        "#### 2. Свободный член ($w_0$)\n",
        "Свободный член $w_0$ можно записать через средние значения $x$ и $y$:\n",
        "\n",
        "$$\n",
        "w_0 = \\bar{y} - w_1 \\bar{x},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\bar{x}$ — среднее значение $x$,\n",
        "- $\\bar{y}$ — среднее значение $y$,\n",
        "- $w_1$ — уже вычисленный наклон.\n",
        "\n",
        "\n",
        "\n",
        "### Интерпретация формул\n",
        "\n",
        "1. **Наклон ($w_1$):**\n",
        "   - Наклон пропорционален ковариации между $x$ и $y$, что показывает, насколько сильно $y$ зависит от $x$.\n",
        "   - Деление на дисперсию $x$ нормализует эту зависимость, чтобы учесть масштаб переменной $x$.\n",
        "\n",
        "2. **Свободный член ($w_0$):**\n",
        "   - Свободный член гарантирует, что линия регрессии проходит через точку $(\\bar{x}, \\bar{y})$, то есть через центр масс данных.\n",
        "\n",
        "\n",
        "\n",
        "### Итоговые формулы\n",
        "\n",
        "Используя ковариацию и дисперсию, получаем:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2},\n",
        "}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "w_0 = \\bar{y} - w_1 \\bar{x}.\n",
        "}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Преимущества такой записи\n",
        "\n",
        "1. **Интуитивность:**\n",
        "   - Ковариация и дисперсия имеют ясный статистический смысл, что делает формулы более понятными.\n",
        "   - Формула для $w_1$ показывает, насколько сильно $y$ зависит от $x$, учитывая их совместное изменение.\n",
        "\n",
        "2. **Удобство вычислений:**\n",
        "   - Если данные уже стандартизированы (например, если $x$ и $y$ имеют нулевое среднее), формулы становятся еще проще.\n",
        "\n",
        "3. **Обобщение:**\n",
        "   - Эти формулы легко обобщаются на случай множественной линейной регрессии, где вместо скалярных величин используются матрицы и векторы.\n",
        "\n",
        "\n",
        "\n",
        "### Пример использования\n",
        "\n",
        "Предположим, у нас есть следующие данные:\n",
        "\n",
        "| $x$ | $y$ |\n",
        "|-----|-----|\n",
        "| 1   | 2   |\n",
        "| 2   | 4   |\n",
        "| 3   | 6   |\n",
        "\n",
        "1. Вычислим средние значения:\n",
        "   $$\n",
        "   \\bar{x} = \\frac{1 + 2 + 3}{3} = 2, \\quad \\bar{y} = \\frac{2 + 4 + 6}{3} = 4.\n",
        "   $$\n",
        "\n",
        "2. Вычислим ковариацию:\n",
        "   $$\n",
        "   \\text{Cov}(x, y) = \\frac{(1-2)(2-4) + (2-2)(4-4) + (3-2)(6-4)}{3} = \\frac{(-1)(-2) + 0 + (1)(2)}{3} = \\frac{4}{3}.\n",
        "   $$\n",
        "\n",
        "3. Вычислим дисперсию:\n",
        "   $$\n",
        "   \\text{Var}(x) = \\frac{(1-2)^2 + (2-2)^2 + (3-2)^2}{3} = \\frac{1 + 0 + 1}{3} = \\frac{2}{3}.\n",
        "   $$\n",
        "\n",
        "4. Найдем наклон:\n",
        "   $$\n",
        "   w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)} = \\frac{\\frac{4}{3}}{\\frac{2}{3}} = 2.\n",
        "   $$\n",
        "\n",
        "5. Найдем свободный член:\n",
        "   $$\n",
        "   w_0 = \\bar{y} - w_1 \\bar{x} = 4 - 2 \\cdot 2 = 0.\n",
        "   $$\n",
        "\n",
        "Таким образом, уравнение регрессии:\n",
        "$$\n",
        "y = 2x.\n",
        "$$\n",
        "\n",
        "Для повышения вычислительной эффективности часто используют матричный подход для определения модели линейной регрессии и выполнения последующего анализа.\n",
        "\n",
        "\n",
        "### Матричный подход в линейной регрессии\n",
        "\n",
        "Уравнения простой линейной регрессии можно записать в матричной форме следующим образом:\n",
        "\n",
        "$$\n",
        "Y = Xw + \\varepsilon,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$ — вектор зависимой переменной.\n",
        "- $X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$ — матрица независимых переменных, где первый столбец состоит из единиц для учета свободного члена.\n",
        "- $w = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$ — вектор коэффициентов модели.\n",
        "- $\\varepsilon = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}$ — вектор ошибок модели.\n",
        "\n",
        "### Связи между матрицами\n",
        "\n",
        "Ранее мы нашли соотношения:\n",
        "\n",
        "1. $\\sum y_i = n w_0 + w_1 \\sum x_i$\n",
        "\n",
        "2. $\\sum x_i y_i = w_0 \\sum x_i + w_1 \\sum x_i^2$\n",
        "\n",
        "Известно, что:\n",
        "\n",
        "$$\n",
        "X^T \\cdot X =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & \\cdots & 1 \\\\\n",
        "x_1 & x_2 & \\cdots & x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_n\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "n & \\sum x_i \\\\\n",
        "\\sum x_i & \\sum x_i^2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "X^T \\cdot Y =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & \\cdots & 1 \\\\\n",
        "x_1 & x_2 & \\cdots & x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\sum y_i \\\\\n",
        "\\sum x_i y_i\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, уравнения (1) и (2) эквивалентны следующей матричной формуле:\n",
        "\n",
        "$$\n",
        "X^T Y = X^T X w\n",
        "$$\n",
        "\n",
        "Отсюда мы можем получить:\n",
        "\n",
        "$$\n",
        "w = (X^T \\cdot X)^{-1} \\cdot X^T Y\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$ — вектор коэффициентов модели (размерность $2 \\times 1$, где первый элемент — свободный член, а второй — коэффициент при независимой переменной),\n",
        "- $X$ — матрица независимых переменных (размерность $n \\times 2$),\n",
        "- $Y$ — вектор зависимой переменной (размерность $n \\times 1$),\n",
        "- $X^T$ — транспонированная матрица $X$,- $(X^T \\cdot X)^{-1}$ — обратная матрица от произведения транспонированной матрицы $X$ на матрицу $X$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DamxOjwJet3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Функции базиса\n",
        "\n",
        "Напомним, что ранее мы вводили функции базиса, но не использовали их на практике:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\ldots & \\phi_p(x_n)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Общая формула целевой функции для многомерной линейной регрессии записывается следующим образом:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right)^2\n",
        "$$\n",
        "\n",
        "\n",
        "где:\n",
        "- $ t_i $ — истинное значение целевой переменной для объекта $ i $,\n",
        "- $ w $ — вектор параметров модели,\n",
        "- $ \\Phi(x_i) $ — вектор значений базисных функций для объекта $ x_i $,\n",
        "- $ w^T \\Phi(x_i) $ — предсказанное значение для объекта $ x_i $.\n",
        "\n",
        "\n",
        "### 2. Цель: найти градиент $ \\nabla_w E $\n",
        "\n",
        "Градиент функции потерь $ E(w) $ по вектору $ w $ определяется как вектор частных производных:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\begin{bmatrix}\n",
        "\\frac{\\partial E(w)}{\\partial w_0} \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_1} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial E(w)}{\\partial w_p}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Мы будем находить каждую компоненту этого вектора.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Расчет производной по $ w_k $\n",
        "\n",
        "Рассмотрим одну компоненту $ \\frac{\\partial E(w)}{\\partial w_k} $. Для этого возьмем производную от $ E(w) $ по $ w_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\left( \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right)^2 \\right).\n",
        "$$\n",
        "\n",
        "#### a) Применяем правило дифференцирования сложной функции\n",
        "\n",
        "Внутренняя функция — это квадрат разности $ (t_i - w^T \\Phi(x_i))^2 $. Внешняя функция — это сумма с коэффициентом $ \\frac{1}{2} $. Производная квадрата равна удвоенному выражению под знаком квадрата. Таким образом:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right) \\cdot \\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\Phi(x_i) \\right).\n",
        "$$\n",
        "\n",
        "Упрощаем коэффициент $ \\frac{1}{2} \\cdot 2 = 1 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right) \\cdot \\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\Phi(x_i) \\right).\n",
        "$$\n",
        "\n",
        "#### b) Находим производную внутреннего выражения\n",
        "\n",
        "Выражение $ t_i - w^T \\Phi(x_i) $ зависит от $ w_k $ через линейную комбинацию $ w^T \\Phi(x_i) $. Распишем его:\n",
        "\n",
        "$$\n",
        "w^T \\Phi(x_i) = \\sum_{j=0}^{p} w_j \\phi_j(x_i),\n",
        "$$\n",
        "\n",
        "где $ \\phi_j(x_i) $ — значение $ j $-й базисной функции на объекте $ x_i $. Тогда:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\Phi(x_i) \\right) = \\frac{\\partial}{\\partial w_k} \\left( t_i - \\sum_{j=0}^{p} w_j \\phi_j(x_i) \\right).\n",
        "$$\n",
        "\n",
        "Заметим, что $ t_i $ не зависит от $ w_k $, а производная линейной комбинации $ \\sum_{j=0}^{p} w_j \\phi_j(x_i) $ по $ w_k $ равна $ \\phi_k(x_i) $ (только слагаемое с $ w_k $ зависит от $ w_k $). Следовательно:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_k} \\left( t_i - w^T \\Phi(x_i) \\right) = -\\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Подставляем это в выражение для $ \\frac{\\partial E(w)}{\\partial w_k} $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right) \\cdot (-\\phi_k(x_i)).\n",
        "$$\n",
        "\n",
        "#### c) Выносим знак минус из суммы\n",
        "\n",
        "Знак минус можно вынести за сумму, так как он является константой:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = -\\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right) \\cdot \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Теперь перепишем выражение под суммой, чтобы избавиться от лишнего минуса. Заметим, что:\n",
        "\n",
        "$$\n",
        "t_i - w^T \\Phi(x_i) = -(w^T \\Phi(x_i) - t_i).\n",
        "$$\n",
        "\n",
        "Подставим это в выражение для частной производной:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = -\\sum_{i=1}^{N} \\big( -(w^T \\Phi(x_i) - t_i) \\big) \\cdot \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "Упростим двойной минус:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( w^T \\Phi(x_i) - t_i \\right) \\cdot \\phi_k(x_i).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 4. Переход к векторной форме\n",
        "\n",
        "Теперь соберем все частные производные $ \\frac{\\partial E(w)}{\\partial w_k} $ в один вектор $ \\nabla_w E $. Заметим, что:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = \\sum_{i=1}^{N} \\left( w^T \\Phi(x_i) - t_i \\right) \\cdot \\phi_k(x_i),\n",
        "$$\n",
        "\n",
        "где $ \\phi_k(x_i) $ — это $ k $-й элемент вектора $ \\Phi(x_i) $. Таким образом, можно записать:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\sum_{i=1}^{N} \\left( w^T \\Phi(x_i) - t_i \\right) \\cdot \\Phi(x_i).\n",
        "$$\n",
        "\n",
        "В матричной форме это выражение принимает вид:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\Phi^T (\\Phi W - T),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ \\Phi $ — матрица базисных функций размера $ N \\times (p+1) $,\n",
        "- $ W $ — вектор параметров модели размера $ (p+1) \\times 1 $,\n",
        "- $ T $ — вектор истинных значений размера $ N \\times 1 $.\n",
        "\n",
        "Окончательно, градиент функции потерь $ E(w) $ по вектору параметров $ w $ имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{\\nabla_w E = \\Phi^T (\\Phi W - T)}.\n",
        "$$\n",
        "\n",
        "\n",
        "###Вариант 1\n",
        "\n",
        "\n",
        "Чтобы найти оптимальные значения $ W $, приравниваем градиент к нулю:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = 0.\n",
        "$$\n",
        "\n",
        "Подставляем выражение для градиента:\n",
        "\n",
        "$$\n",
        "\\Phi^T (\\Phi W - T) = 0.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 2. Раскрываем скобки\n",
        "\n",
        "Раскроем скобки в выражении:\n",
        "\n",
        "$$\n",
        "\\Phi^T \\Phi W - \\Phi^T T = 0.\n",
        "$$\n",
        "\n",
        "Переносим второе слагаемое в правую часть:\n",
        "\n",
        "$$\n",
        "\\Phi^T \\Phi W = \\Phi^T T.\n",
        "$$\n",
        "\n",
        "\n",
        "### 3. Умножение обеих частей на $(\\Phi^T \\Phi)^{-1}$\n",
        "\n",
        "Предположим, что матрица $\\Phi^T \\Phi$ обратима (это выполняется, если ранг матрицы $\\Phi$ равен $ p+1 $). Умножим обе части уравнения слева на $(\\Phi^T \\Phi)^{-1}$:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} \\Phi^T \\Phi W = (\\Phi^T \\Phi)^{-1} \\Phi^T T.\n",
        "$$\n",
        "\n",
        "Упрощаем левую часть, так как $(\\Phi^T \\Phi)^{-1} \\Phi^T \\Phi = I$ (единичная матрица):\n",
        "\n",
        "$$\n",
        "W = (\\Phi^T \\Phi)^{-1} \\Phi^T T.\n",
        "$$\n",
        "\n",
        "\n",
        "### 4. Финальное выражение\n",
        "\n",
        "Таким образом, мы получили формулу для оптимальных коэффициентов $ W $:\n",
        "\n",
        "$$\n",
        "\\boxed{W = (\\Phi^T \\Phi)^{-1} \\Phi^T T.}\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ W $ — вектор параметров модели (размер $ (p+1) \\times 1 $),\n",
        "- $ \\Phi $ — матрица базисных функций (размер $ N \\times (p+1) $),\n",
        "- $ T $ — вектор истинных значений (размер $ N \\times 1 $).\n",
        "\n",
        "\n",
        "\n",
        "### 5. Проверка транспонирования\n",
        "\n",
        "Ваш текст содержит рассуждения о транспонировании, но они избыточны. Давайте убедимся, что формула корректна без лишних преобразований.\n",
        "\n",
        "#### a) Размерности матриц\n",
        "- Матрица $\\Phi^T \\Phi$ имеет размер $ (p+1) \\times (p+1) $, так как $\\Phi^T$ — это матрица размера $ (p+1) \\times N $, а $\\Phi$ — матрица размера $ N \\times (p+1) $.\n",
        "- Матрица $(\\Phi^T \\Phi)^{-1}$ также имеет размер $ (p+1) \\times (p+1) $.\n",
        "- Произведение $(\\Phi^T \\Phi)^{-1} \\Phi^T$ имеет размер $ (p+1) \\times N $.\n",
        "- Вектор $ T $ имеет размер $ N \\times 1 $, поэтому произведение $(\\Phi^T \\Phi)^{-1} \\Phi^T T$ имеет размер $ (p+1) \\times 1 $, что совпадает с размером $ W $.\n",
        "\n",
        "#### b) Транспонирование не требуется\n",
        "Формула $ W = (\\Phi^T \\Phi)^{-1} \\Phi^T T $ уже записана в правильной форме. Нет необходимости транспонировать $ T $ или другие матрицы, если $ T $ изначально является вектором-столбцом.\n",
        "\n",
        "\n",
        "\n",
        "### 6. Заключение\n",
        "\n",
        "Итоговая формула для коэффициентов линейной регрессии, минимизирующих функцию потерь методом наименьших квадратов, имеет вид:\n",
        "\n",
        "$$\n",
        "\\boxed{W = (\\Phi^T \\Phi)^{-1} \\Phi^T T.}\n",
        "$$\n",
        "\n",
        "Эта формула позволяет эффективно вычислить оптимальные параметры модели, используя матричные операции.\n",
        "\n"
      ],
      "metadata": {
        "id": "7laR3e072zS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Вариант 2\n",
        "\n",
        "Чтобы найти коэффициенты $w$, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = 0\n",
        "$$\n",
        "\n",
        "Если транспонировать обе стороны данного уравнения, результат останется тем же. Транспонирование не изменяет смысла уравнения:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = 0^T\n",
        "$$\n",
        "\n",
        "Таким образом, мы получаем:\n",
        "\n",
        "$$\n",
        "\\nabla_w E^T = \\sum_{i=1}^{N} (w^T \\Phi(x_i) - t_i) \\cdot \\Phi(x_i)^T = \\sum_{i=1}^{N} \\left( w^T \\Phi(x_i) \\cdot \\Phi(x_i)^T - t_i \\cdot \\Phi(x_i)^T \\right)\n",
        "$$\n",
        "\n",
        "Теперь можем записать это в более развернутом виде:\n",
        "\n",
        "$$\n",
        "\\nabla_w E^T = w^T \\cdot \\sum_{i=1}^{N} \\Phi(x_i) \\cdot \\Phi(x_i)^T - \\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T\n",
        "$$\n",
        "\n",
        "### 1. Рассмотрим матрицу $\\sum_{i=1}^{N} \\Phi(x_i) \\cdot \\Phi(x_i)^T$\n",
        "\n",
        "Здесь мы умножаем вектор-столбец на вектор-строку. Если их размерности совпадают, получится матрица.\n",
        "\n",
        "В данном случае, если $\\Phi(x_i)$ — это вектор-столбец размерности $m \\times 1$, а $\\Phi(x_i)^T$ — это вектор-строка размерности $1 \\times m$, то их произведение даст матрицу размерности $m \\times m$:\n",
        "\n",
        "$$\n",
        "\\Phi(x_i) \\cdot \\Phi(x_i)^T =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i) \\\\\n",
        "\\phi_1(x_i) \\\\\n",
        "\\vdots \\\\\n",
        "\\phi_p(x_i)\n",
        "\\end{bmatrix} \\cdot\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i) & \\phi_1(x_i) & \\ldots & \\phi_p(x_i)\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i)^2 & \\phi_0(x_i) \\phi_1(x_i) & \\ldots & \\phi_0(x_i) \\phi_p(x_i) \\\\\n",
        "\\phi_1(x_i) \\phi_0(x_i) & \\phi_1(x_i)^2 & \\ldots & \\phi_1(x_i) \\phi_p(x_i) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_p(x_i) \\phi_0(x_i) & \\phi_p(x_i) \\phi_1(x_i) & \\ldots & \\phi_p(x_i)^2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, результат произведения вектора-столбца на вектор-строку дает квадратную матрицу:\n",
        "\n",
        "$$\n",
        "\\Phi(x_i) \\cdot \\Phi(x_i)^T = \\Phi \\cdot \\Phi^T\n",
        "$$\n",
        "\n",
        "### 2. Рассмотрим $\\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T$\n",
        "\n",
        "С одной стороны, можем записать:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T = \\left(\\sum_{i=1}^{N} t_i \\cdot \\phi_0(x_i), \\sum_{i=1}^{N} t_i \\cdot \\phi_1(x_i), \\ldots\\right)\n",
        "$$\n",
        "\n",
        "С другой стороны, это выражение можно представить в матричной форме:\n",
        "\n",
        "$$\n",
        "(t_1, t_2, \\ldots, t_N) \\cdot\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\ldots & \\phi_p(x_n)\n",
        "\\end{bmatrix} = \\left(\\sum_{i=1}^{N} t_i \\cdot \\phi_0(x_i), \\sum_{i=1}^{N} t_i \\cdot \\phi_1(x_i), \\ldots\\right)\n",
        "$$\n",
        "\n",
        "Следовательно:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T = t^T \\cdot \\Phi\n",
        "$$\n",
        "\n",
        "### 3. Подставляем выражения в градиент\n",
        "\n",
        "Таким образом, подставляя полученные результаты, мы имеем:\n",
        "\n",
        "$$\n",
        "\\nabla_w E^T = w^T \\Phi^T \\Phi - t^T \\Phi^T\n",
        "$$\n",
        "\n",
        "Теперь можем записать уравнение, приравняв его к нулю:\n",
        "\n",
        "$$\n",
        "w^T \\Phi^T \\Phi - t^T \\Phi^T = 0\n",
        "$$\n",
        "\n",
        "Отсюда следует:\n",
        "\n",
        "$$\n",
        "w^T \\Phi^T \\Phi = t^T \\Phi^T\n",
        "$$\n",
        "\n",
        "Умножим обе части последнего равенства на $(\\Phi^T \\Phi)^{-1}$:\n",
        "\n",
        "$$\n",
        "w^T \\Phi^T \\Phi \\cdot (\\Phi^T \\Phi)^{-1} = t^T \\Phi^T \\cdot (\\Phi^T \\Phi)^{-1}\n",
        "$$\n",
        "\n",
        "После сокращения и некоторых преобразований получим:\n",
        "\n",
        "$$\n",
        "w^T = t^T \\Phi^T \\cdot (\\Phi^T \\Phi)^{-1}\n",
        "$$\n",
        "\n",
        "### 4. Применение свойств транспонирования\n",
        "\n",
        "Чтобы решить уравнение $x^T = A$, где $x$ и $A$ — матрицы, необходимо транспонировать обе стороны уравнения. После транспонирования получаем:\n",
        "\n",
        "$$\n",
        "(x^T)^T = A^T\n",
        "$$\n",
        "\n",
        "Так как транспонирование дважды возвращает исходную матрицу, то:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "x = A^T\n",
        "$$\n",
        "\n",
        "### 5. Финальное выражение для коэффициентов\n",
        "\n",
        "Таким образом, получаем:\n",
        "\n",
        "$$\n",
        "w = (t^T \\Phi^T \\cdot (\\Phi^T \\Phi)^{-1})^T\n",
        "$$\n",
        "\n",
        "Из курса линейной алгебры известно, что:\n",
        "\n",
        "1. $(A \\cdot B)^T = B^T \\cdot A^T$ (транспонирование произведения матриц меняет порядок умножения).\n",
        "2. $(A^T)^{-1} = (A^{-1})^T$ (транспонирование обратной матрицы равно обратной транспонированной матрице).\n",
        "\n",
        "Теперь рассмотрим ваше выражение:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t^T\n",
        "$$\n",
        "\n",
        "Если $t$ — это вектор-столбец, то $t^T$ — это вектор-строка. Однако в контексте задачи линейной регрессии, $t$ обычно представляет собой вектор-столбец целевых значений, и тогда $t^T$ — это вектор-строка.\n",
        "\n",
        "Если вы хотите выразить $w$ через $\\Phi$ и $t$, то правильное выражение для вектора весов $w$ в задаче линейной регрессии будет:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
        "$$\n",
        "\n",
        "Здесь $t$ — это вектор-столбец, и $w$ также будет вектором-столбцом.\n",
        "\n",
        "Итак, правильное выражение:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
        "$$\n",
        "\n",
        "Если же $t$ — это вектор-строка, то вам нужно будет транспонировать его, чтобы получить вектор-столбец, и тогда выражение будет:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t^T\n",
        "$$\n",
        "\n",
        "Но в стандартной постановке задачи линейной регрессии $t$ — это вектор-столбец, и транспонирование не требуется.\n",
        "\n",
        "Таким образом, исправленный текст будет выглядеть следующим образом:\n",
        "\n",
        "Из курса линейной алгебры известно, что:\n",
        "\n",
        "$$\n",
        "(A \\cdot B)^T = B^T \\cdot A^T \\quad \\text{и} \\quad (A^T)^{-1} = (A^{-1})^T\n",
        "$$\n",
        "\n",
        "Тогда у нас будет:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
        "$$\n",
        "\n",
        "где $t$ — вектор-столбец целевых значений.\n",
        "\n",
        "Таким образом, мы вывели формулы для коэффициентов линейной регрессии в общем виде, используя метод наименьших квадратов и матричный подход. Это позволяет эффективно рассчитывать оптимальные параметры модели, минимизируя ошибку между предсказанными значениями и реальными наблюдениями.\n",
        "\n"
      ],
      "metadata": {
        "id": "hZY2fq0S5F4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Вопрос на подумать\n",
        "\n",
        "*Для вычисления $w$ нам приходится обращать (квадратную) матрицу $\\Phi^T \\Phi$, что возможно только если она невырожденна. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?*\n",
        "\n",
        "### Ответ\n",
        "\n",
        "С точки зрения линейной алгебры, для вещественной матрицы $\\Phi$ ранги матриц $\\Phi$ и $\\Phi^T \\Phi$ совпадают. Матрица $\\Phi^T \\Phi$ будет невырожденной, то есть обратимой, тогда и только тогда, когда её ранг равен числу её столбцов, что также равно числу столбцов матрицы $\\Phi$. Иными словами, формула регрессии поломается, только если столбцы матрицы $\\Phi$ линейно зависимы.\n",
        "\n",
        "В данной ситуации столбцы матрицы $\\Phi$ представляют собой признаки (фичи) нашего набора данных. Если признаки линейно зависимы, это значит, что один или несколько признаков могут быть выражены как линейные комбинации других признаков. Это приводит к проблеме избыточности информации. Когда признаки линейно зависимы, необходимо оставить только линейно независимые признаки, так как зависимые признаки не добавляют новой информации и лишь усложняют вычисления. Таким образом, чтобы избежать вырождения, мы обычно следим за тем, чтобы признаки были линейно независимыми.\n",
        "\n",
        "Однако на практике часто встречается ситуация, когда признаки приближённо линейно зависимы, особенно если их много. В таких случаях матрица $\\Phi^T \\Phi$ будет близка к вырожденной, что приводит к ряду вычислительных проблем. Как мы увидим далее, такие случаи плохо сказываются на численной устойчивости решения и могут потребовать дополнительных методов для его стабилизации.\n",
        "\n",
        "## Вычислительная сложность аналитического решения\n",
        "\n",
        "Аналитическое решение задачи линейной регрессии имеет вычислительную сложность $O(D^2 N + D^3)$, где:\n",
        "- $N$ — это количество объектов в выборке,\n",
        "- $D$ — это количество признаков у каждого объекта.\n",
        "\n",
        "### Разбор компонентов вычислительной сложности\n",
        "\n",
        "1. **Слагаемое $O(D^2 N)$** отвечает за сложность перемножения матриц $\\Phi^T$ и $\\Phi$. Для нахождения матрицы $\\Phi^T \\Phi$ требуется выполнить $N$ умножений для каждого из $D \\times D$ элементов.\n",
        "\n",
        "2. **Слагаемое $O(D^3)$** обусловлено сложностью обращения матрицы $\\Phi^T \\Phi$. Поскольку эта матрица размерности $D \\times D$, стандартные методы обращения, такие как метод Гаусса или метод разложения Холецкого, требуют $O(D^3)$ операций.\n",
        "\n",
        "Чтобы минимизировать вычислительные затраты, не рекомендуется перемножать матрицы в выражении $(\\Phi^T \\Phi)^{-1} \\cdot \\Phi^T$ целиком. Вместо этого целесообразно сначала умножить вектор $t$ на $\\Phi^T$, а затем результат умножить на $(\\Phi^T \\Phi)^{-1}$. Такой подход позволяет значительно снизить объём вычислений и уменьшает необходимость хранения матрицы $(\\Phi^T \\Phi)^{-1} \\cdot \\Phi^T$ в памяти, что особенно важно при больших размерах данных.\n",
        "\n",
        "### Ускорение вычислений\n",
        "\n",
        "Вычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц, такие как метод Штрассена или метод Винограда, которые снижают степень вычислительной сложности. Кроме того, для больших матриц эффективны итерационные методы обращения, такие как метод Якоби или метод сопряжённых градиентов, которые позволяют обойтись без явного обращения матрицы, что также повышает стабильность численных решений.\n",
        "\n",
        "## Проблемы «точного» решения\n",
        "\n",
        "Для нахождения коэффициентов регрессии требуется обращение матрицы $\\Phi^T \\Phi$, что порождает несколько серьёзных проблем:\n",
        "\n",
        "1. **Сложность обращения больших матриц.**  \n",
        "   Обращение больших матриц вычислительно затратно, а в задачах машинного обучения часто необходимо работать с датасетами, содержащими миллионы точек. В таких случаях обращение может потребовать значительных вычислительных ресурсов, и для оптимизации приходится применять приближённые или итерационные методы.\n",
        "\n",
        "2. **Плохая обусловленность матрицы $\\Phi^T \\Phi$.**  \n",
        "   Даже если матрица $\\Phi^T \\Phi$ является обратимой, она может быть плохо обусловлена. Это происходит особенно часто, когда количество признаков велико. В такой ситуации некоторые признаки могут быть почти линейно зависимы от других, что приводит к тому, что $\\Phi^T \\Phi$ становится близкой к вырожденной. Плохая обусловленность означает, что малые изменения в данных могут привести к большим изменениям в итоговом решении $w$, что делает его численно неустойчивым.\n",
        "\n",
        "   Например, малое изменение в целевом векторе $t$ может привести к значительным изменениям в коэффициентах $w$. Это явление возникает из-за того, что погрешность результата будет зависеть от квадрата обусловленности матрицы $\\Phi$. В итоге решение становится ненадёжным: незначительные погрешности данных могут вызвать серьёзные ошибки в предсказаниях модели.\n",
        "\n",
        "3. **Численные ошибки при вычислении.**  \n",
        "   Плохая обусловленность ведет к накоплению численных ошибок. Даже при использовании высокоточных вычислений, ошибки округления при обращении матрицы $\\Phi^T \\Phi$ могут существенно исказить решение. Эта проблема особенно критична для систем с большой размерностью, где погрешности округления накапливаются в каждой операции и могут искажать конечный результат.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, обращение матрицы $\\Phi^T \\Phi$ при решении задачи линейной регрессии связано с рядом вычислительных и численных трудностей, особенно в случае больших данных и многомерных признаков. Для их преодоления разработаны методы регуляризации, такие как метод ридж-регрессии (регуляризация Тихонова), позволяющие стабилизировать решение путем добавления регуляризующего члена к матрице $\\Phi^T \\Phi$. Этот подход помогает улучшить обусловленность матрицы и снизить зависимость от линейных зависимостей между признаками, что в свою очередь делает модель более устойчивой к численным ошибкам и повышает её обобщающую способность.\n",
        "\n",
        "\n",
        "\n",
        "### Пара слов про число обусловленности\n",
        "\n",
        "С математической точки зрения, число обусловленности матрицы $\\Phi$ — это показатель, отражающий, насколько различаются масштабы её собственных значений. Упрощая, число обусловленности матрицы $\\Phi$ можно рассматривать как корень из отношения наибольшего и наименьшего собственных значений матрицы $\\Phi^T \\Phi$. Иными словами, оно показывает, насколько разного масштаба бывают собственные значения этой матрицы.\n",
        "\n",
        "Если рассмотреть $L^2$-норму ошибки предсказания как функцию от коэффициентов, то линии уровня этой функции представляют собой эллипсоиды. Форма этих эллипсоидов определяется квадратичной формой, заданной матрицей $\\Phi^T \\Phi$. Вытянутость этих эллипсоидов говорит о том, насколько сильно отличаются величины собственных значений $\\Phi^T \\Phi$, что и выражает число обусловленности. Высокое число обусловленности может сигнализировать о том, что матрица плохо обусловлена, что, в свою очередь, может привести к численным проблемам при вычислении.\n",
        "\n",
        "### Подробнее\n",
        "\n",
        "Проблемы с численной устойчивостью и высокой сложностью вычислений не означают, что «точное» решение необходимо отбросить. Существуют несколько методов для улучшения численных свойств решения. Однако для их полного понимания необходимо знание сингулярного разложения. Если оно вам не знакомо, рекомендуется сначала изучить этот материал и затем вернуться к данной теме.\n",
        "\n",
        "#### Метод 1: Использование QR-разложения\n",
        "\n",
        "Первый способ улучшения решения заключается в применении QR-разложения матрицы $X$. QR-разложение — это представление матрицы $X$ в виде произведения $X = QR$, где:\n",
        "- $Q$ — матрица с ортогональными столбцами (то есть $Q^T Q = E$),\n",
        "- $R$ — квадратная верхнетреугольная матрица.\n",
        "\n",
        "Подставив это разложение в исходное уравнение для $w$, получим:\n",
        "\n",
        "$$\n",
        "w = ((QR)^T QR)^{-1} (QR)^T y\n",
        "$$\n",
        "\n",
        "Раскроем выражение, используя свойства ортогональной матрицы $Q$:\n",
        "\n",
        "$$\n",
        "w = (R^T Q^T Q R)^{-1} R^T Q^T y = (R^T R)^{-1} R^T Q^T y = R^{-1} R^{-T} R^T Q^T y = R^{-1} Q^T y\n",
        "$$\n",
        "\n",
        "Здесь мы использовали тот факт, что $(R^T R)^{-1} = R^{-1} R^{-T}$. Полученное выражение значительно проще, поскольку обращение верхнетреугольной матрицы $R$ сводится к решению системы уравнений с верхнетреугольной левой частью, что позволяет выполнять вычисления быстрее и с меньшей численной погрешностью. Погрешность вычисления $w$ при этом будет зависеть от числа обусловленности матрицы $\\Phi$, а QR-разложение, как правило, обладает хорошей численной устойчивостью, что делает его применение предпочтительным для улучшения численных свойств решения.\n",
        "\n",
        "#### Метод 2: Использование псевдообратной матрицы через сингулярное разложение\n",
        "\n",
        "Другим подходом является применение псевдообратной матрицы, построенной с помощью сингулярного разложения. Пусть\n",
        "\n",
        "$$\n",
        "A = U \\underbrace{\\mathrm{diag}(\\sigma_1, \\ldots, \\sigma_r)}_{=\\Sigma} V^T\n",
        "$$\n",
        "\n",
        "— это усечённое сингулярное разложение матрицы $A$, где $r$ — ранг $A$. Здесь:\n",
        "- $\\Sigma$ — диагональная матрица, содержащая ненулевые сингулярные значения $\\sigma_i$,\n",
        "- $U$ и $V$ — ортогональные матрицы, такие что $U^T U = E$ и $V^T V = E$.\n",
        "\n",
        "В таком случае $w$ можно выразить следующим образом:\n",
        "\n",
        "$$\n",
        "w = (V \\Sigma U^T U \\Sigma V^T)^{-1} V \\Sigma U^T y\n",
        "$$\n",
        "\n",
        "Используем свойства ортогональных матриц и диагональной матрицы $\\Sigma$. Поскольку $V \\Sigma^{-2} V^T \\cdot V \\Sigma^2 V^T = E$, верно, что\n",
        "\n",
        "$$\n",
        "(V \\Sigma^2 V^T)^{-1} = V \\Sigma^{-2} V^T,\n",
        "$$\n",
        "\n",
        "что позволяет упростить выражение:\n",
        "\n",
        "$$\n",
        "w = V \\Sigma^{-2} V^T \\Sigma U^T y = V \\Sigma^{-1} U^T y\n",
        "$$\n",
        "\n",
        "Применение сингулярного разложения гарантирует численную устойчивость решения, так как сингулярное разложение хорошо себя ведёт при вычислениях, даже если матрица плохо обусловлена. Это улучшает численные свойства решения и делает его более надёжным.\n",
        "\n",
        "Тем не менее, несмотря на преимущества сингулярного разложения, вычислительная сложность остаётся значительной, особенно для больших матриц. Плохая обусловленность матрицы $X$ всё равно окажет влияние, хоть и в меньшей степени, на точность вычислений.\n",
        "\n",
        "Хотя эти методы помогают улучшить численную устойчивость и стабилизировать решение, они не могут полностью устранить проблемы, связанные с вырожденностью или плохой обусловленностью матрицы $X$. Поэтому нет необходимости останавливаться на «точных» решениях, которые всё равно никогда не будут полностью точными в условиях реальных данных. В следующих разделах мы рассмотрим альтернативные подходы к решению задачи, которые не полагаются на обращение матрицы и могут дать более надёжные результаты в условиях плохой обусловленности.\n",
        "\n",
        "\n",
        "\n",
        "### Почему градиент указывает направление наибольшего возрастания функции?\n",
        "\n",
        "**Что такое градиент функции?**\n",
        "\n",
        "Градиент функции многих переменных — это вектор, составленный из частных производных этой функции по каждой переменной. Он показывает направление наибольшего возрастания функции в данной точке и его величина указывает на скорость изменения функции вдоль этого направления. Градиент используется в задачах оптимизации для нахождения экстремумов, так как именно он указывает направление наискорейшего увеличения функции.\n",
        "\n",
        "Для функции $f(x_1, x_2, \\dots, x_n)$ градиент обозначается символом $\\nabla f$ и формируется следующим образом:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
        "$$\n",
        "\n",
        "Этот вектор указывает направление наибольшего возрастания функции в данной точке.\n",
        "\n",
        "**Что такое производная по направлению?**\n",
        "\n",
        "Производная по направлению — это производная функции вдоль заданного вектора. Пусть $\\vec{u} = (u_1, u_2, \\dots, u_n)$ — нормированный вектор (то есть $||\\vec{u}|| = 1$), вдоль которого берется производная функции $f(x_1, x_2, \\dots, x_n)$. Норма вектора $\\vec{u}$ определяется как:\n",
        "\n",
        "$$\n",
        "||\\vec{u}|| = \\sqrt{u_1^2 + u_2^2 + \\dots + u_n^2}\n",
        "$$\n",
        "\n",
        "Производная функции $f$ по направлению вектора $\\vec{u}$ определяется как предел:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\frac{f(\\vec{x} + h \\cdot \\vec{u}) - f(\\vec{x})}{h}\n",
        "$$\n",
        "\n",
        "где $\\vec{x} + h \\cdot \\vec{u} = (x_1 + h \\cdot u_1, x_2 + h \\cdot u_2, \\dots, x_n + h \\cdot u_n)$. Эта формула показывает изменение функции вдоль направления, заданного вектором $\\vec{u}$.\n",
        "\n",
        "### Доказательство, что градиент указывает направление наибольшего возрастания функции\n",
        "\n",
        "Рассмотрим производную функции $f$ по направлению вектора $\\vec{u}$:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\frac{f(x_1 + h \\cdot u_1, x_2 + h \\cdot u_2, \\dots, x_n + h \\cdot u_n) - f(x_1, x_2, \\dots, x_n)}{h}\n",
        "$$\n",
        "\n",
        "Мы можем разложить эту производную, выделяя отдельные слагаемые для каждого индекса:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\sum_{i=1}^{n} \\frac{f(x_1, \\dots, x_i + h \\cdot u_i, \\dots, x_n) - f(x_1, \\dots, x_n)}{h}\n",
        "$$\n",
        "\n",
        "Эта сумма почти представляет собой выражение для частной производной функции. Если умножить каждый член на $u_i$ и затем разделить на $u_i$, мы получим следующее:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\sum_{i=1}^{n} \\frac{f(x_1, \\dots, x_i + h \\cdot u_i, \\dots, x_n) - f(x_1, \\dots, x_n)}{h \\cdot u_i} \\cdot u_i = \\nabla f^T \\cdot \\vec{u}\n",
        "$$\n",
        "\n",
        "Скалярное произведение двух векторов $\\vec{a}$ и $\\vec{b}$ можно выразить как:\n",
        "\n",
        "$$\n",
        "\\vec{a}^T \\cdot \\vec{b} = ||\\vec{a}|| \\cdot ||\\vec{b}|| \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "где $\\theta$ — угол между векторами. Таким образом, для градиента мы имеем:\n",
        "\n",
        "$$\n",
        "\\nabla f^T \\cdot \\vec{u} = ||\\nabla f|| \\cdot \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "Производная по направлению отражает скорость возрастания функции в заданном направлении $\\vec{u}$. Чтобы максимизировать скорость роста функции, вектор $\\vec{u}$ должен быть направлен так, чтобы угол $\\theta = 0$, то есть векторы $\\nabla f$ и $\\vec{u}$ должны совпадать по направлению. Таким образом, направление градиента действительно является направлением наибольшего возрастания функции.\n",
        "\n",
        "### Ортогональность градиента к линиям уровня\n",
        "\n",
        "Градиент также обладает свойством ортогональности к линиям уровня функции, что важно для визуализации процесса оптимизации. Доказательство этого свойства выглядит следующим образом.\n",
        "\n",
        "Пусть $x_0$ — некоторая точка, и $S(x_0) = \\{ x \\in \\mathbb{R}^d \\mid f(x) = f(x_0) \\}$ — линия уровня функции $f$, соответствующая этой точке. Разложим функцию в ряд Тейлора на линии уровня в окрестности точки $x_0$:\n",
        "\n",
        "$$\n",
        "f(x_0 + \\varepsilon) = f(x_0) + \\langle \\nabla f, \\varepsilon \\rangle + o(||\\varepsilon||),\n",
        "$$\n",
        "\n",
        "где $x_0 + \\varepsilon \\in S(x_0)$. Поскольку $f(x_0 + \\varepsilon) = f(x_0)$ (это свойство линии уровня), получаем:\n",
        "\n",
        "$$\n",
        "\\langle \\nabla f, \\varepsilon \\rangle = o(||\\varepsilon||).\n",
        "$$\n",
        "\n",
        "Разделив обе части на $||\\varepsilon||$, имеем:\n",
        "\n",
        "$$\n",
        "\\left\\langle \\nabla f, \\frac{\\varepsilon}{||\\varepsilon||} \\right\\rangle = o(1).\n",
        "$$\n",
        "\n",
        "При стремлении $||\\varepsilon||$ к нулю вектор $\\frac{\\varepsilon}{||\\varepsilon||}$ будет стремиться к касательной к линии уровня в точке $x_0$. В пределе мы получаем, что градиент ортогонален этой касательной.\n",
        "\n",
        "Таким образом, градиент функции является мощным инструментом в математике и оптимизации, указывая направление наибольшего возрастания функции и демонстрируя важные свойства, такие как ортогональность к линиям уровня. Понимание этих концепций позволяет более эффективно решать задачи оптимизации и анализировать поведение многомерных функций.\n",
        "\n",
        "\n",
        "Чтобы лучше понять концепцию градиента и его применение, давайте рассмотрим несколько конкретных примеров.\n",
        "\n",
        "### Пример 1: Градиент функции двух переменных\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "f(x, y) = x^2 + y^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Градиент функции $f$ будет вычисляться как:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
        "$$\n",
        "\n",
        "Найдём частные производные:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 2y\n",
        "$$\n",
        "\n",
        "Таким образом, градиент равен:\n",
        "\n",
        "$$\n",
        "\\nabla f = (2x, 2y)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент показывает направление наибольшего возрастания функции. Например, в точке $(1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla f(1, 1) = (2 \\cdot 1, 2 \\cdot 1) = (2, 2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1)$ функция $f$ возрастает быстрее всего в направлении вектора $(2, 2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для данной функции представляют собой круги, заданные уравнением $x^2 + y^2 = c$. Градиент будет направлен от центра этих кругов к их границе, что подтверждает, что он ортогонален линиям уровня.\n",
        "\n",
        "### Пример 2: Градиент функции с ограничениями\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "g(x, y) = 4 - x^2 - y^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Найдём градиент функции $g$:\n",
        "\n",
        "$$\n",
        "\\nabla g = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right) = (-2x, -2y)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент в точке $(1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla g(1, 1) = (-2 \\cdot 1, -2 \\cdot 1) = (-2, -2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1)$ функция $g$ убывает быстрее всего в направлении вектора $(-2, -2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для этой функции будут представлять собой окружности, заданные уравнением $x^2 + y^2 = c$, что также подтверждает, что градиент направлен внутрь окружностей, оставаясь ортогональным им.\n",
        "\n",
        "### Пример 3: Градиент в многомерной функции\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "h(x, y, z) = x^2 + y^2 + z^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Найдём градиент функции $h$:\n",
        "\n",
        "$$\n",
        "\\nabla h = \\left( \\frac{\\partial h}{\\partial x}, \\frac{\\partial h}{\\partial y}, \\frac{\\partial h}{\\partial z} \\right) = (2x, 2y, 2z)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент в точке $(1, 1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla h(1, 1, 1) = (2 \\cdot 1, 2 \\cdot 1, 2 \\cdot 1) = (2, 2, 2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1, 1)$ функция $h$ возрастает быстрее всего в направлении вектора $(2, 2, 2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для функции $h$ будут представлять собой сферы, заданные уравнением $x^2 + y^2 + z^2 = c$. Градиент будет направлен от центра сфер к их поверхности, что также подтверждает его ортогональность к линиям уровня.\n",
        "\n",
        "### Пример 4: Градиент в задачах оптимизации\n",
        "\n",
        "Предположим, мы хотим минимизировать функцию потерь в регрессионной модели, заданной следующей функцией:\n",
        "\n",
        "$$\n",
        "L(w) = \\sum_{i=1}^{n} (y_i - (w_1 x_{1i} + w_2 x_{2i}))^2\n",
        "$$\n",
        "\n",
        "где $w = (w_1, w_2)$ — вектор весов, $x_{1i}, x_{2i}$ — входные данные, а $y_i$ — целевые значения.\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Градиент функции потерь будет иметь вид:\n",
        "\n",
        "$$\n",
        "\\nabla L(w) = \\left( \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2} \\right)\n",
        "$$\n",
        "\n",
        "При нахождении производных получаем:\n",
        "\n",
        "$$\n",
        "\\nabla L(w) = \\left( -2 \\sum_{i=1}^{n} x_{1i} (y_i - (w_1 x_{1i} + w_2 x_{2i})), -2 \\sum_{i=1}^{n} x_{2i} (y_i - (w_1 x_{1i} + w_2 x_{2i})) \\right)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент указывает на то, как изменить веса $w_1$ и $w_2$, чтобы минимизировать функцию потерь $L(w)$. Направление, указанное градиентом, показывает, как нужно корректировать веса, чтобы достигнуть оптимального результата.\n",
        "\n",
        "Эти примеры демонстрируют, как градиент помогает понять поведение многомерных функций, указывая направление наибольшего возрастания и предоставляя полезные инструменты для задач оптимизации. Понимание этих концепций позволяет более эффективно решать практические задачи в математике и машинном обучении. Если у вас есть вопросы или нужны дополнительные примеры, дайте знать!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Пример применения\n",
        "\n",
        "Рассмотрим простой пример с использованием метода наименьших квадратов для линейной регрессии, основанный на наборе данных, где мы исследуем влияние цены на объем продаж.\n",
        "\n",
        "#### Данные\n",
        "\n",
        "Предположим, у нас есть следующие данные о ценах и объемах продаж:\n",
        "\n",
        "| Цена (X) | Объем продаж (Y) |\n",
        "|----------|------------------|\n",
        "| 10       | 100              |\n",
        "| 20       | 150              |\n",
        "| 30       | 200              |\n",
        "| 40       | 300              |\n",
        "| 50       | 400              |\n",
        "\n",
        "#### 1. Определим функции базиса\n",
        "\n",
        "Мы будем использовать линейные функции базиса:\n",
        "\n",
        "- $\\phi_0(x) = 1$ (константа)\n",
        "- $\\phi_1(x) = x$ (линейная функция)\n",
        "\n",
        "#### 2. Сформируем матрицу функций базиса\n",
        "\n",
        "Для нашей выборки получаем матрицу:\n",
        "\n",
        "$$\n",
        "\\Phi =\n",
        "\\begin{bmatrix}\n",
        "1 & 10 \\\\\n",
        "1 & 20 \\\\\n",
        "1 & 30 \\\\\n",
        "1 & 40 \\\\\n",
        "1 & 50\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### 3. Определим вектор целевой переменной\n",
        "\n",
        "Вектор целевой переменной:\n",
        "\n",
        "$$\n",
        "t =\n",
        "\\begin{bmatrix}\n",
        "100 \\\\\n",
        "150 \\\\\n",
        "200 \\\\\n",
        "300 \\\\\n",
        "400\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### 4. Подставим данные в формулу\n",
        "\n",
        "Теперь можем найти коэффициенты $w$:\n",
        "\n",
        "1. Вычислим $\\Phi^T \\Phi$:\n",
        "\n",
        "$$\n",
        "\\Phi^T \\Phi =\n",
        "\\begin{bmatrix}\n",
        "5 & 150 \\\\\n",
        "150 & 3850\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Найдем обратную матрицу $(\\Phi^T \\Phi)^{-1}$:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} = \\frac{1}{(5)(3850) - (150)(150)}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "После вычислений получаем:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} = \\frac{1}{(19250 - 22500)}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "= \\frac{1}{-3250}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "-1.1846 & 0.04615 \\\\\n",
        "0.04615 & -0.0015385\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. Вычислим $\\Phi^T t$:\n",
        "\n",
        "$$\n",
        "\\Phi^T t =\n",
        "\\begin{bmatrix}\n",
        "1300 \\\\\n",
        "28500\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Теперь можем вычислить $w$:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
        "$$\n",
        "\n",
        "#### 5. Получение коэффициентов\n",
        "\n",
        "После подстановки значений мы получаем:\n",
        "\n",
        "$$\n",
        "w \\approx\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Интерпретация коэффициентов\n",
        "\n",
        "Полученные коэффициенты означают следующее:\n",
        "\n",
        "- $w_0 = 0$: При цене 0 объем продаж также равен 0, что логично.\n",
        "- $w_1 = 8$: Это значит, что на каждый доллар увеличения цены объем продаж увеличивается на 8 единиц.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JKu4jJJQ3xvL"
      }
    }
  ]
}