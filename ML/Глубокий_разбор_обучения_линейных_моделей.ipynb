{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM34g1zCzLYYc+548JG25bA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/%D0%93%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%B8%D0%B9_%D1%80%D0%B0%D0%B7%D0%B1%D0%BE%D1%80_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D1%85_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Алгоритм Forward Pass и Backpropagation для Простой Линейной Регрессии\n",
        "\n",
        "\n",
        "\n",
        "## **Введение**\n",
        "\n",
        "Давайте рассмотрим алгоритм **Forward Pass** и **Backpropagation** для простой линейной регрессии. Мы будем работать с моделью, которая имеет одну входную переменную $x$, один вес $w$, и смещение (bias) $b$. Выход модели будет предсказанием $y_{\\text{pred}}$, а целевая переменная — это $y_{\\text{true}}$.\n",
        "\n",
        "\n",
        "\n",
        "## **1. Forward Pass**\n",
        "\n",
        "Forward Pass представляет собой процесс вычисления предсказания модели на основе входных данных. Этот этап включает выполнение следующих шагов:\n",
        "\n",
        "### 1.1. Определение модели\n",
        "Модель линейной регрессии задается уравнением:\n",
        "$$\n",
        "y_{\\text{pred}} = w \\cdot x + b\n",
        "$$\n",
        "где:\n",
        "- $x$ — входное значение,\n",
        "- $w$ — вес,\n",
        "- $b$ — смещение (bias),\n",
        "- $y_{\\text{pred}}$ — предсказанное значение.\n",
        "\n",
        "### 1.2. Вычисление предсказания\n",
        "Подставляем значения $x$, $w$, и $b$ в уравнение модели:\n",
        "$$\n",
        "y_{\\text{pred}} = w \\cdot x + b\n",
        "$$\n",
        "\n",
        "### 1.3. Вычисление функции потерь\n",
        "Функция потерь (Loss Function) измеряет, насколько предсказанное значение $y_{\\text{pred}}$ отличается от истинного значения $y_{\\text{true}}$. Для линейной регрессии часто используется **Mean Squared Error (MSE)**:\n",
        "$$\n",
        "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "$$\n",
        "Здесь множитель $\\frac{1}{2}$ добавляется для удобства при дифференцировании.\n",
        "\n",
        "Теперь мы знаем, как вычислить предсказание модели и функцию потерь.\n",
        "\n",
        "\n",
        "\n",
        "## **2. Backpropagation**\n",
        "\n",
        "Backpropagation представляет собой процесс вычисления градиентов функции потерь по параметрам модели ($w$ и $b$), чтобы обновить их с помощью алгоритма градиентного спуска.\n",
        "\n",
        "### 2.1. Вычисление градиента функции потерь по $y_{\\text{pred}}$\n",
        "Начнем с функции потерь:\n",
        "$$\n",
        "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "$$\n",
        "Берем производную $L$ по $y_{\\text{pred}}$:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = (y_{\\text{pred}} - y_{\\text{true}})\n",
        "$$\n",
        "\n",
        "### 2.2. Вычисление градиента $y_{\\text{pred}}$ по параметрам $w$ и $b$\n",
        "Из уравнения модели:\n",
        "$$\n",
        "y_{\\text{pred}} = w \\cdot x + b\n",
        "$$\n",
        "Берем частные производные:\n",
        "$$\n",
        "\\frac{\\partial y_{\\text{pred}}}{\\partial w} = x, \\quad \\frac{\\partial y_{\\text{pred}}}{\\partial b} = 1\n",
        "$$\n",
        "\n",
        "### 2.3. Вычисление градиентов функции потерь по параметрам $w$ и $b$\n",
        "Используем цепное правило:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial w}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial b}\n",
        "$$\n",
        "\n",
        "Подставляем значения:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot x\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y_{\\text{true}})\n",
        "$$\n",
        "\n",
        "### 2.4. Обновление параметров\n",
        "Используем градиентный спуск для обновления параметров $w$ и $b$. Формулы обновления:\n",
        "$$\n",
        "w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "$$\n",
        "b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "где $\\alpha$ — скорость обучения (learning rate).\n",
        "\n",
        "\n",
        "\n",
        "## **3. Итоговый алгоритм**\n",
        "\n",
        "### 3.1. Forward Pass:\n",
        "1. Вычислить предсказание:\n",
        "$$\n",
        "   y_{\\text{pred}} = w \\cdot x + b\n",
        "$$\n",
        "2. Вычислить функцию потерь:\n",
        "$$\n",
        "   L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "$$\n",
        "\n",
        "### 3.2. Backpropagation:\n",
        "1. Вычислить градиент функции потерь по $y_{\\text{pred}}$:\n",
        "$$\n",
        "   \\frac{\\partial L}{\\partial y_{\\text{pred}}} = (y_{\\text{pred}} - y_{\\text{true}})\n",
        "$$\n",
        "2. Вычислить градиенты по параметрам:\n",
        "   - Градиент по $w$:\n",
        "$$\n",
        "     \\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot x\n",
        "$$\n",
        "   - Градиент по $b$:\n",
        "$$\n",
        "     \\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y_{\\text{true}})\n",
        "$$\n",
        "3. Обновить параметры:\n",
        "   - Обновление $w$:\n",
        "$$\n",
        "     w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "   - Обновление $b$:\n",
        "$$\n",
        "     b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## **4. Пример численных вычислений**\n",
        "\n",
        "Пусть:\n",
        "- $x = 2$,\n",
        "- $y_{\\text{true}} = 5$,\n",
        "- Начальные параметры: $w = 1$, $b = 0$,\n",
        "- Скорость обучения: $\\alpha = 0.1$.\n",
        "\n",
        "### 4.1. Forward Pass:\n",
        "1. Вычислим предсказание:\n",
        "$$\n",
        "   y_{\\text{pred}} = w \\cdot x + b = 1 \\cdot 2 + 0 = 2\n",
        "$$\n",
        "2. Вычислим функцию потерь:\n",
        "$$\n",
        "   L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2 = \\frac{1}{2} (2 - 5)^2 = \\frac{1}{2} \\cdot 9 = 4.5\n",
        "$$\n",
        "\n",
        "### 4.2. Backpropagation:\n",
        "1. Вычислим градиент функции потерь по $y_{\\text{pred}}$:\n",
        "$$\n",
        "   \\frac{\\partial L}{\\partial y_{\\text{pred}}} = y_{\\text{pred}} - y_{\\text{true}} = 2 - 5 = -3\n",
        "$$\n",
        "2. Вычислим градиенты по параметрам:\n",
        "   - Градиент по $w$:\n",
        "$$\n",
        "     \\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot x = -3 \\cdot 2 = -6\n",
        "$$\n",
        "   - Градиент по $b$:\n",
        "$$\n",
        "     \\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y_{\\text{true}}) = -3\n",
        "$$\n",
        "3. Обновим параметры:\n",
        "   - Обновление $w$:\n",
        "$$\n",
        "     w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} = 1 - 0.1 \\cdot (-6) = 1 + 0.6 = 1.6\n",
        "$$\n",
        "   - Обновление $b$:\n",
        "$$\n",
        "     b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b} = 0 - 0.1 \\cdot (-3) = 0 + 0.3 = 0.3\n",
        "$$\n",
        "\n",
        "После одного шага градиентного спуска новые значения параметров: $w = 1.6$, $b = 0.3$.\n",
        "\n",
        "\n",
        "\n",
        "## **5. Ответ**\n",
        "$$\n",
        "\\boxed{\n",
        "\\text{Forward Pass: } y_{\\text{pred}} = w \\cdot x + b, \\quad L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
        "}\n",
        "$$\n",
        "$$\n",
        "\\boxed{\n",
        "\\text{Backpropagation: } \\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot x, \\quad \\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y_{\\text{true}})\n",
        "}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "5IraO0pElA8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Настройки графиков\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(42)\n",
        "x = np.array([1, 2, 3, 4, 5])  # Входные данные\n",
        "y_true = 3 * x + 2 + np.random.normal(0, 1, size=x.shape)  # Истинные значения (линейная зависимость + шум)\n",
        "\n",
        "# Параметры модели\n",
        "w = 0.0  # Начальное значение веса\n",
        "b = 0.0  # Начальное значение смещения\n",
        "learning_rate = 0.01  # Скорость обучения\n",
        "epochs = 100  # Количество итераций\n",
        "\n",
        "# Для визуализации\n",
        "loss_history = []  # История значений функции потерь\n",
        "w_history = []  # История значений w\n",
        "b_history = []  # История значений b\n",
        "\n",
        "# Функция для вычисления предсказания и функции потерь\n",
        "def compute_loss(x, y_true, w, b):\n",
        "    y_pred = w * x + b\n",
        "    loss = np.mean((y_pred - y_true) ** 2) / 2\n",
        "    return y_pred, loss\n",
        "\n",
        "# Градиентный спуск\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    y_pred, loss = compute_loss(x, y_true, w, b)\n",
        "\n",
        "    # Сохраняем историю\n",
        "    loss_history.append(loss)\n",
        "    w_history.append(w)\n",
        "    b_history.append(b)\n",
        "\n",
        "    # Backpropagation (вычисление градиентов)\n",
        "    dL_dy_pred = (y_pred - y_true)  # Градиент по y_pred\n",
        "    dL_dw = np.mean(dL_dy_pred * x)  # Градиент по w\n",
        "    dL_db = np.mean(dL_dy_pred)  # Градиент по b\n",
        "\n",
        "    # Обновление параметров\n",
        "    w = w - learning_rate * dL_dw\n",
        "    b = b - learning_rate * dL_db\n",
        "\n",
        "# Визуализация\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "# 1. График изменения функции потерь\n",
        "axes[0].plot(loss_history, label=\"Loss\", color=\"blue\")\n",
        "axes[0].set_title(\"Loss over epochs\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "\n",
        "# 2. График изменения параметра w\n",
        "axes[1].plot(w_history, label=\"w\", color=\"orange\")\n",
        "axes[1].set_title(\"Weight (w) over epochs\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"w\")\n",
        "axes[1].legend()\n",
        "\n",
        "# 3. График изменения параметра b\n",
        "axes[2].plot(b_history, label=\"b\", color=\"green\")\n",
        "axes[2].set_title(\"Bias (b) over epochs\")\n",
        "axes[2].set_xlabel(\"Epoch\")\n",
        "axes[2].set_ylabel(\"b\")\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Дополнительная визуализация: аппроксимация данных моделью\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y_true, label=\"True data\", color=\"green\")  # Исходные данные\n",
        "\n",
        "# Отображение модели на нескольких шагах\n",
        "for epoch in [0, 10, 50, epochs - 1]:\n",
        "    w_epoch = w_history[epoch]\n",
        "    b_epoch = b_history[epoch]\n",
        "    y_pred = w_epoch * x + b_epoch\n",
        "    plt.plot(x, y_pred, label=f\"Epoch {epoch}\", linestyle=\"--\")\n",
        "\n",
        "plt.title(\"Model fit over epochs\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Финальные значения параметров\n",
        "print(f\"Final weight (w): {w:.4f}\")\n",
        "print(f\"Final bias (b): {b:.4f}\")"
      ],
      "metadata": {
        "id": "fGM48I_5lU7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подбор начальных значений весов и смещений, скорости обучения, а также реализация ранней остановки в линейной регрессии\n",
        "\n",
        "\n",
        "\n",
        "## **1. Подбор начальных значений весов ($w$) и смещения ($b$)**\n",
        "\n",
        "### 1.1. Почему важно правильно выбирать начальные значения?\n",
        "Начальные значения параметров модели ($w$ и $b$) влияют на:\n",
        "- **Скорость сходимости**: Если начальные значения слишком велики или малы, градиентный спуск может сходиться медленно.\n",
        "- **Устойчивость алгоритма**: Неправильные начальные значения могут привести к застреванию в локальных минимумах (особенно для сложных моделей).\n",
        "- **Качество результата**: Хорошая инициализация помогает модели быстрее найти правильное решение.\n",
        "\n",
        "### 1.2. Методы подбора начальных значений\n",
        "\n",
        "#### 1.2.1. Нулевые начальные значения\n",
        "Простейший подход — использовать нулевые начальные значения:\n",
        "$$\n",
        "w = 0, \\quad b = 0\n",
        "$$\n",
        "**Пример кода**:\n"
      ],
      "metadata": {
        "id": "Wd_JI_qwlQo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Начальные значения\n",
        "w = 0.0\n",
        "b = 0.0\n",
        "\n",
        "print(f\"Initial w: {w}, Initial b: {b}\")"
      ],
      "metadata": {
        "id": "A7O8YFvgm3TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.2.2. Случайные начальные значения\n",
        "Веса инициализируются случайными значениями:\n",
        "$$\n",
        "w \\sim \\mathcal{N}(0, \\sigma^2), \\quad b \\sim \\mathcal{N}(0, \\sigma^2)\n",
        "$$\n",
        "где $\\mathcal{N}(0, \\sigma^2)$ — нормальное распределение с математическим ожиданием $0$ и дисперсией $\\sigma^2$.\n",
        "\n",
        "**Пример кода**:\n"
      ],
      "metadata": {
        "id": "4DtG2M3Wm3aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Случайные начальные значения\n",
        "np.random.seed(42)\n",
        "w = np.random.randn() * 0.01  # Малое случайное значение\n",
        "b = np.random.randn() * 0.01\n",
        "\n",
        "print(f\"Random initial w: {w:.4f}, Random initial b: {b:.4f}\")"
      ],
      "metadata": {
        "id": "Cmb5b1dbm7ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.2.3. Инициализация на основе априорных знаний\n",
        "Если у вас есть информация о данных или задаче, вы можете задать начальные значения на основе этой информации.\n",
        "\n",
        "**Пример кода**:\n"
      ],
      "metadata": {
        "id": "OgTJ0Ehgm7tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Априорные знания\n",
        "w = 3.0  # Предполагаемый коэффициент\n",
        "b = 0.0  # Предполагаемое смещение\n",
        "\n",
        "print(f\"Prior knowledge initial w: {w}, Prior knowledge initial b: {b}\")"
      ],
      "metadata": {
        "id": "1EDlZMyFnB_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## **2. Подбор скорости обучения ($\\alpha$)**\n",
        "\n",
        "### 2.1. Что такое скорость обучения?\n",
        "Скорость обучения ($\\alpha$) — это параметр, который контролирует размер шага при обновлении весов:\n",
        "$$\n",
        "w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}, \\quad b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "- **Слишком большая скорость обучения**:\n",
        "  - Градиентный спуск может \"перепрыгивать\" минимум.\n",
        "  - Функция потерь может расходиться.\n",
        "- **Слишком маленькая скорость обучения**:\n",
        "  - Градиентный спуск сходится медленно.\n",
        "  - Требуется больше итераций для достижения оптимума.\n",
        "\n",
        "### 2.2. Как подбирать скорость обучения?\n",
        "\n",
        "#### 2.2.1. Эмпирический подход\n",
        "- Начните с небольших значений ($\\alpha = 0.001, 0.01, 0.1$).\n",
        "- Постройте график изменения функции потерь. Если потери растут, уменьшите $\\alpha$. Если потери уменьшаются слишком медленно, увеличьте $\\alpha$.\n",
        "\n",
        "**Пример кода**:"
      ],
      "metadata": {
        "id": "A9818tTqnCH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(42)\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y_true = 3 * x + 2 + np.random.normal(0, 1, size=x.shape)\n",
        "\n",
        "# Разные значения скорости обучения\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.002, 0.003]\n",
        "loss_histories = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    w, b = 0.0, 0.0\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(100):\n",
        "        y_pred = w * x + b\n",
        "        loss = np.mean((y_pred - y_true) ** 2) / 2\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Обновление параметров\n",
        "        dL_dw = np.mean((y_pred - y_true) * x)\n",
        "        dL_db = np.mean(y_pred - y_true)\n",
        "        w -= lr * dL_dw\n",
        "        b -= lr * dL_db\n",
        "\n",
        "    loss_histories.append(loss_history)\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.plot(loss_histories[i], label=f\"Learning Rate = {lr}\")\n",
        "\n",
        "plt.title(\"Effect of Learning Rate on Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AhXP_BKtnHEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 2.2.2. Автоматический подбор\n",
        "Используйте методы автоматического подбора, такие как `Grid Search` или `Random Search`.\n",
        "\n",
        "**Пример кода**:"
      ],
      "metadata": {
        "id": "I2ROSbTtnHNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "# Генерация данных\n",
        "X = x.reshape(-1, 1)\n",
        "y = y_true\n",
        "\n",
        "# Создание модели\n",
        "model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
        "\n",
        "# Подбор скорости обучения\n",
        "param_grid = {\"eta0\": [0.001, 0.01, 0.1]}\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"neg_mean_squared_error\")\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(f\"Best learning rate: {grid_search.best_params_['eta0']}\")"
      ],
      "metadata": {
        "id": "TZ1tyW1vpFr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## **3. Ранняя остановка (Early Stopping)**\n",
        "\n",
        "### 3.1. Что такое ранняя остановка?\n",
        "Ранняя остановка — это техника, которая прекращает обучение, когда улучшение модели становится незначительным. Это помогает предотвратить переобучение и сэкономить вычислительные ресурсы.\n",
        "\n",
        "### 3.2. Как реализовать раннюю остановку?\n",
        "\n",
        "Для реализации ранней остановки используется валидационный набор данных:\n",
        "1. Разделите данные на обучающую и валидационную выборки.\n",
        "2. Отслеживайте значение функции потерь на валидационной выборке.\n",
        "3. Если потери на валидационной выборке начинают увеличиваться (или перестают уменьшаться), прекратите обучение.\n",
        "\n",
        "**Пример кода**:"
      ],
      "metadata": {
        "id": "L3-Mso0EpF1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(42)\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y_true = 3 * x + 2 + np.random.normal(0, 1, size=x.shape)\n",
        "\n",
        "# Разделение данных на обучающую и валидационную выборки\n",
        "train_ratio = 0.8\n",
        "split_index = int(len(x) * train_ratio)\n",
        "x_train, x_val = x[:split_index], x[split_index:]\n",
        "y_train, y_val = y_true[:split_index], y_true[split_index:]\n",
        "\n",
        "# Параметры модели\n",
        "w = 0.0  # Начальное значение веса\n",
        "b = 0.0  # Начальное значение смещения\n",
        "learning_rate = 0.01  # Скорость обучения\n",
        "max_epochs = 500  # Максимальное количество эпох\n",
        "patience = 10  # Количество эпох для ранней остановки\n",
        "\n",
        "# Для визуализации\n",
        "train_loss_history = []  # История значений функции потерь на обучающей выборке\n",
        "val_loss_history = []  # История значений функции потерь на валидационной выборке\n",
        "best_loss = float('inf')  # Лучшее значение потерь\n",
        "epochs_without_improvement = 0  # Счетчик эпох без улучшений\n",
        "\n",
        "# Градиентный спуск с ранней остановкой\n",
        "for epoch in range(max_epochs):\n",
        "    # Forward pass (обучающая выборка)\n",
        "    y_pred_train = w * x_train + b  # Предсказание на обучающей выборке\n",
        "    train_loss = np.mean((y_pred_train - y_train) ** 2) / 2  # Функция потерь (MSE)\n",
        "\n",
        "    # Forward pass (валидационная выборка)\n",
        "    y_pred_val = w * x_val + b  # Предсказание на валидационной выборке\n",
        "    val_loss = np.mean((y_pred_val - y_val) ** 2) / 2  # Функция потерь (MSE)\n",
        "\n",
        "    # Сохраняем историю потерь\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append(val_loss)\n",
        "\n",
        "    # Проверка улучшения (ранняя остановка)\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    # Ранняя остановка\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "    # Backpropagation (вычисление градиентов)\n",
        "    dL_dw_train = np.mean((y_pred_train - y_train) * x_train)  # Градиент по w\n",
        "    dL_db_train = np.mean(y_pred_train - y_train)  # Градиент по b\n",
        "\n",
        "    # Обновление параметров\n",
        "    w = w - learning_rate * dL_dw_train\n",
        "    b = b - learning_rate * dL_db_train\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. График изменения функции потерь\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_history, label=\"Training Loss\", color=\"blue\")\n",
        "plt.plot(val_loss_history, label=\"Validation Loss\", color=\"orange\")\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# 2. График предсказаний модели\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(x, y_true, label=\"True data\", color=\"green\")  # Исходные данные\n",
        "plt.plot(x, w * x + b, label=f\"Model: y = {w:.2f}x + {b:.2f}\", color=\"red\")  # Линия модели\n",
        "plt.title(\"Model fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Финальные значения параметров\n",
        "print(f\"Final weight (w): {w:.4f}\")\n",
        "print(f\"Final bias (b): {b:.4f}\")"
      ],
      "metadata": {
        "id": "7e7EXXBFpN4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте разберем этот код пошагово, чтобы понять, как он работает. Этот код реализует обучение модели линейной регрессии с использованием градиентного спуска и ранней остановки (Early Stopping). Мы также визуализируем процесс обучения.\n",
        "\n",
        "\n",
        "\n",
        "### **1. Генерация данных**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(42)\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y_true = 3 * x + 2 + np.random.normal(0, 1, size=x.shape)\n",
        "```\n",
        "\n",
        "- `x` — это входные данные (одномерный массив значений).\n",
        "- `y_true` — это целевые значения, которые создаются по формуле:\n",
        "  $$\n",
        "  y_{\\text{true}} = 3 \\cdot x + 2 + \\text{шум}\n",
        "  $$\n",
        "  где шум добавляется с помощью `np.random.normal(0, 1)`, чтобы сделать данные более реалистичными.\n",
        "- `np.random.seed(42)` фиксирует случайное число для воспроизводимости результатов.\n",
        "\n",
        "\n",
        "\n",
        "### **2. Разделение данных на обучающую и валидационную выборки**\n",
        "\n",
        "```python\n",
        "train_ratio = 0.8\n",
        "split_index = int(len(x) * train_ratio)\n",
        "x_train, x_val = x[:split_index], x[split_index:]\n",
        "y_train, y_val = y_true[:split_index], y_true[split_index:]\n",
        "```\n",
        "\n",
        "- Данные разделяются на обучающую и валидационную выборки в соотношении 80% / 20%.\n",
        "- `split_index` определяет индекс, по которому происходит разделение.\n",
        "- `x_train` и `y_train` — это обучающие данные.\n",
        "- `x_val` и `y_val` — это валидационные данные.\n",
        "\n",
        "\n",
        "### **3. Инициализация параметров модели**\n",
        "\n",
        "```python\n",
        "w = 0.0  # Начальное значение веса\n",
        "b = 0.0  # Начальное значение смещения\n",
        "learning_rate = 0.01  # Скорость обучения\n",
        "max_epochs = 500  # Максимальное количество эпох\n",
        "patience = 10  # Количество эпох для ранней остановки\n",
        "```\n",
        "\n",
        "- `w` и `b` — начальные значения параметров модели ($w$ — вес, $b$ — смещение).\n",
        "- `learning_rate` — скорость обучения, которая контролирует размер шага при обновлении параметров.\n",
        "- `max_epochs` — максимальное количество итераций (эпох) обучения.\n",
        "- `patience` — количество эпох, после которых обучение прекращается, если потери на валидации не улучшаются.\n",
        "\n",
        "\n",
        "\n",
        "### **4. Обучение модели**\n",
        "\n",
        "#### 4.1. Forward Pass (вычисление предсказаний и функции потерь)\n",
        "\n",
        "```python\n",
        "for epoch in range(max_epochs):\n",
        "    # Forward pass (обучающая выборка)\n",
        "    y_pred_train = w * x_train + b  # Предсказание на обучающей выборке\n",
        "    train_loss = np.mean((y_pred_train - y_train) ** 2) / 2  # Функция потерь (MSE)\n",
        "    \n",
        "    # Forward pass (валидационная выборка)\n",
        "    y_pred_val = w * x_val + b  # Предсказание на валидационной выборке\n",
        "    val_loss = np.mean((y_pred_val - y_val) ** 2) / 2  # Функция потерь (MSE)\n",
        "```\n",
        "\n",
        "- На каждой эпохе:\n",
        "  - Вычисляются предсказания модели для обучающих и валидационных данных:\n",
        "    $$\n",
        "    y_{\\text{pred}} = w \\cdot x + b\n",
        "    $$\n",
        "  - Вычисляется функция потерь (Mean Squared Error, MSE):\n",
        "    $$\n",
        "    L = \\frac{1}{2} \\cdot \\text{mean}((y_{\\text{pred}} - y_{\\text{true}})^2)\n",
        "    $$\n",
        "\n",
        "#### 4.2. Сохранение истории потерь\n",
        "\n",
        "```python\n",
        "    # Сохраняем историю потерь\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append(val_loss)\n",
        "```\n",
        "\n",
        "- Значения функции потерь сохраняются для последующей визуализации.\n",
        "\n",
        "#### 4.3. Проверка улучшения и ранняя остановка\n",
        "\n",
        "```python\n",
        "    # Проверка улучшения (ранняя остановка)\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "    \n",
        "    # Ранняя остановка\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "```\n",
        "\n",
        "- Если потери на валидации уменьшились, обновляем `best_loss` и сбрасываем счетчик `epochs_without_improvement`.\n",
        "- Если потери на валидации не улучшаются в течение `patience` эпох, обучение прекращается.\n",
        "\n",
        "#### 4.4. Backpropagation (вычисление градиентов и обновление параметров)\n",
        "\n",
        "```python\n",
        "    # Backpropagation (вычисление градиентов)\n",
        "    dL_dw_train = np.mean((y_pred_train - y_train) * x_train)  # Градиент по w\n",
        "    dL_db_train = np.mean(y_pred_train - y_train)  # Градиент по b\n",
        "    \n",
        "    # Обновление параметров\n",
        "    w = w - learning_rate * dL_dw_train\n",
        "    b = b - learning_rate * dL_db_train\n",
        "```\n",
        "\n",
        "- Вычисляются градиенты функции потерь по параметрам $w$ и $b$:\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial w} = \\text{mean}((y_{\\text{pred}} - y_{\\text{true}}) \\cdot x)\n",
        "  $$\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial b} = \\text{mean}(y_{\\text{pred}} - y_{\\text{true}})\n",
        "  $$\n",
        "- Параметры обновляются с помощью правила градиентного спуска:\n",
        "  $$\n",
        "  w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}, \\quad b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "### **5. Визуализация**\n",
        "\n",
        "#### 5.1. График изменения функции потерь\n",
        "\n",
        "```python\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_history, label=\"Training Loss\", color=\"blue\")\n",
        "plt.plot(val_loss_history, label=\"Validation Loss\", color=\"orange\")\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "```\n",
        "\n",
        "- Первый график показывает, как меняются потери на обучающей и валидационной выборках с каждой эпохой.\n",
        "\n",
        "#### 5.2. График предсказаний модели\n",
        "\n",
        "```python\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(x, y_true, label=\"True data\", color=\"green\")  # Исходные данные\n",
        "plt.plot(x, w * x + b, label=f\"Model: y = {w:.2f}x + {b:.2f}\", color=\"red\")  # Линия модели\n",
        "plt.title(\"Model fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "```\n",
        "\n",
        "- Второй график показывает исходные данные и аппроксимирующую линию модели.\n",
        "\n",
        "\n",
        "\n",
        "### **6. Финальные значения параметров**\n",
        "\n",
        "```python\n",
        "print(f\"Final weight (w): {w:.4f}\")\n",
        "print(f\"Final bias (b): {b:.4f}\")\n",
        "```\n",
        "\n",
        "- После завершения обучения выводятся финальные значения параметров $w$ и $b$.\n",
        "\n",
        "\n",
        "### **Как это работает?**\n",
        "\n",
        "1. **Генерация данных**: Создаются искусственные данные с линейной зависимостью и шумом.\n",
        "2. **Обучение модели**:\n",
        "   - На каждой эпохе вычисляются предсказания и потери.\n",
        "   - Параметры модели обновляются с помощью градиентного спуска.\n",
        "   - Если потери на валидации не улучшаются в течение `patience` эпох, обучение прекращается.\n",
        "3. **Визуализация**:\n",
        "   - Строится график изменения потерь.\n",
        "   - Строится график, показывающий, как модель аппроксимирует данные.\n",
        "\n",
        "\n",
        "\n",
        "### **Пример вывода**\n",
        "\n",
        "1. **Графики**:\n",
        "   - Первый график покажет, как потери на обучающей и валидационной выборках уменьшаются.\n",
        "   - Второй график покажет, как модель приближается к данным.\n",
        "\n",
        "2. **Финальные значения**:\n",
        "   ```\n",
        "   Early stopping at epoch 120\n",
        "   Final weight (w): 2.9987\n",
        "   Final bias (b): 1.9995\n",
        "   ```\n",
        "\n",
        "Эти значения близки к истинным параметрам ($w = 3$, $b = 2$), что подтверждает корректность работы алгоритма.\n"
      ],
      "metadata": {
        "id": "-WR1K2_Trrki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## **4. Заключение**\n",
        "\n",
        "1. **Подбор начальных значений**:\n",
        "   - Для линейной регрессии используйте нулевые или малые случайные значения для $w$ и $b$.\n",
        "   - Для сложных моделей используйте специальные методы инициализации.\n",
        "\n",
        "2. **Подбор скорости обучения**:\n",
        "   - Начните с небольших значений ($\\alpha = 0.001, 0.01, 0.1$).\n",
        "   - Используйте методы автоматического подбора, если необходимо.\n",
        "\n",
        "3. **Ранняя остановка**:\n",
        "   - Позволяет предотвратить переобучение и сэкономить вычислительные ресурсы.\n",
        "   - Реализуется через отслеживание функции потерь на валидационной выборке.\n",
        "\n"
      ],
      "metadata": {
        "id": "QhmUEDAJpOAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Подбор начальных значений весов и смещений в машинном обучении\n",
        "\n",
        "## Введение\n",
        "\n",
        "Подбор начальных значений весов ($W$) и смещений ($b$) является важным шагом в задачах машинного обучения. Этот процесс особенно критичен для моделей, которые зависят от градиентного спуска, таких как линейная регрессия, логистическая регрессия и нейронные сети. Неправильно выбранные начальные значения могут замедлить или даже полностью нарушить процесс обучения.\n",
        "\n",
        "В этой лекции мы рассмотрим:\n",
        "1. Простую линейную регрессию.\n",
        "2. Методы подбора начальных значений.\n",
        "3. Примеры на практике.\n",
        "4. Обобщение для более сложных моделей.\n",
        "\n",
        "\n",
        "\n",
        "## 1. Линейная регрессия\n",
        "\n",
        "### 1.1. Основная формула\n",
        "Линейная регрессия моделирует зависимость между входными данными ($X$) и выходными данными ($y$) с помощью линейной функции:\n",
        "$$\n",
        "\\hat{y} = W \\cdot X + b\n",
        "$$\n",
        "где:\n",
        "- $W$ — вектор весов,\n",
        "- $b$ — смещение (bias),\n",
        "- $\\hat{y}$ — предсказание модели.\n",
        "\n",
        "Цель обучения — найти такие $W$ и $b$, чтобы минимизировать функцию потерь, например, среднеквадратичную ошибку (MSE):\n",
        "$$\n",
        "L(W, b) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "### 1.2. Алгоритм градиентного спуска\n",
        "Градиентный спуск обновляет параметры $W$ и $b$ следующим образом:\n",
        "$$\n",
        "W := W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "$$\n",
        "b := b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "где $\\alpha$ — скорость обучения (learning rate).\n",
        "\n",
        "Для вычисления градиентов:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\cdot X_i\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)\n",
        "$$\n",
        "\n",
        "### 1.3. Зачем нужны начальные значения?\n",
        "Если начальные значения $W$ и $b$ выбраны плохо:\n",
        "- Градиенты могут быть слишком маленькими (затухание) или слишком большими (взрыв).\n",
        "- Алгоритм может застрять в плохом локальном минимуме.\n",
        "- Скорость сходимости может быть крайне медленной.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Методы подбора начальных значений\n",
        "\n",
        "### 2.1. Нулевые начальные значения\n",
        "Простейший подход — установить все веса и смещения равными нулю:\n",
        "$$\n",
        "W = 0, \\quad b = 0\n",
        "$$\n",
        "\n",
        "Однако этот метод имеет серьёзные недостатки:\n",
        "- Все предсказания будут одинаковыми ($\\hat{y} = b$), что приводит к симметрии.\n",
        "- Градиенты не смогут \"разорвать\" эту симметрию, и обучение будет неэффективным.\n",
        "\n",
        "**Вывод:** Нулевая инициализация практически никогда не используется.\n",
        "\n",
        "\n",
        "\n",
        "### 2.2. Случайные начальные значения\n",
        "Более распространённый подход — инициализировать веса случайными числами. Например:\n",
        "$$\n",
        "W \\sim U(-a, a) \\quad \\text{или} \\quad W \\sim N(0, \\sigma^2)\n",
        "$$\n",
        "где $U(-a, a)$ — равномерное распределение на интервале $[-a, a]$, а $N(0, \\sigma^2)$ — нормальное распределение с математическим ожиданием 0 и дисперсией $\\sigma^2$.\n",
        "\n",
        "#### Пример:\n",
        "Пусть у нас есть линейная регрессия с одним признаком ($X$). Мы можем инициализировать $W$ и $b$ следующим образом:\n",
        "$$\n",
        "W \\sim U(-0.1, 0.1), \\quad b \\sim U(-0.1, 0.1)\n",
        "$$\n",
        "\n",
        "#### Проблемы:\n",
        "- Если значения слишком маленькие, градиенты могут затухать.\n",
        "- Если значения слишком большие, градиенты могут взрываться.\n",
        "\n",
        "\n",
        "\n",
        "### 2.3. Инициализация согласно размерности данных\n",
        "Для более эффективного обучения можно учитывать размерность входных данных. Например:\n",
        "- Если входные данные имеют размерность $d$, то веса можно инициализировать так:\n",
        "$$\n",
        "W \\sim N\\left(0, \\frac{1}{\\sqrt{d}}\\right)\n",
        "$$\n",
        "\n",
        "#### Пример:\n",
        "Пусть $d = 10$. Тогда:\n",
        "$$\n",
        "W \\sim N\\left(0, \\frac{1}{\\sqrt{10}}\\right) = N(0, 0.316)\n",
        "$$\n",
        "\n",
        "Этот подход помогает сохранить стабильность активаций и градиентов.\n",
        "\n",
        "\n",
        "\n",
        "### 2.4. Метод Ксавьера (Xavier Initialization)\n",
        "Метод Ксавьера был разработан для задач глубокого обучения, но его принципы применимы и к линейной регрессии.\n",
        "\n",
        "#### Формула:\n",
        "Для слоя с $n_{in}$ входами и $n_{out}$ выходами веса инициализируются следующим образом:\n",
        "$$\n",
        "W \\sim U\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)\n",
        "$$\n",
        "или\n",
        "$$\n",
        "W \\sim N\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right)\n",
        "$$\n",
        "\n",
        "#### Пример:\n",
        "Пусть $n_{in} = 5$, $n_{out} = 1$. Тогда:\n",
        "$$\n",
        "W \\sim U\\left(-\\sqrt{\\frac{6}{5 + 1}}, \\sqrt{\\frac{6}{5 + 1}}\\right) = U(-0.775, 0.775)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 2.5. Метод Хе (He Initialization)\n",
        "Метод Хе был разработан для сетей с функциями активации ReLU, но его также можно адаптировать для линейной регрессии.\n",
        "\n",
        "#### Формула:\n",
        "Для слоя с $n_{in}$ входами веса инициализируются следующим образом:\n",
        "$$\n",
        "W \\sim N\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)\n",
        "$$\n",
        "\n",
        "#### Пример:\n",
        "Пусть $n_{in} = 5$. Тогда:\n",
        "$$\n",
        "W \\sim N\\left(0, \\sqrt{\\frac{2}{5}}\\right) = N(0, 0.632)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 3. Примеры на практике\n",
        "\n",
        "### Пример 1: Линейная регрессия с нулевой инициализацией\n",
        "\n",
        "\n",
        "### **1. Исходные данные**\n",
        "У нас есть следующие данные:\n",
        "$$\n",
        "X = [1, 2, 3, 4], \\quad y = [2, 4, 6, 8]\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $ X $ — это входные значения (признаки).\n",
        "- $ y $ — это целевые значения (метки).\n",
        "\n",
        "Модель линейной регрессии имеет вид:\n",
        "$$\n",
        "\\hat{y} = W \\cdot X + b\n",
        "$$\n",
        "где:\n",
        "- $ W $ — вес (наклон прямой),\n",
        "- $ b $ — смещение (свободный член),\n",
        "- $ \\hat{y} $ — предсказанные значения.\n",
        "\n",
        "\n",
        "\n",
        "### **2. Инициализация параметров**\n",
        "Инициализируем веса модели:\n",
        "$$\n",
        "W = 0, \\quad b = 0\n",
        "$$\n",
        "\n",
        "Подставим их в уравнение модели:\n",
        "$$\n",
        "\\hat{y} = W \\cdot X + b = 0 \\cdot X + 0 = 0\n",
        "$$\n",
        "\n",
        "Таким образом, начальные предсказания модели для всех точек равны:\n",
        "$$\n",
        "\\hat{y} = [0, 0, 0, 0]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **3. Вычисление ошибки**\n",
        "Определим функцию потерь как среднеквадратичную ошибку (MSE):\n",
        "$$\n",
        "L = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "Подставим известные значения:\n",
        "- $ n = 4 $ (количество точек),\n",
        "- $ \\hat{y} = [0, 0, 0, 0] $,\n",
        "- $ y = [2, 4, 6, 8] $.\n",
        "\n",
        "Вычислим ошибку для каждой точки:\n",
        "$$\n",
        "(\\hat{y}_i - y_i)^2 = (0 - 2)^2 = 4, \\quad (0 - 4)^2 = 16, \\quad (0 - 6)^2 = 36, \\quad (0 - 8)^2 = 64\n",
        "$$\n",
        "\n",
        "Сумма квадратов ошибок:\n",
        "$$\n",
        "\\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = 4 + 16 + 36 + 64 = 120\n",
        "$$\n",
        "\n",
        "Функция потерь:\n",
        "$$\n",
        "L = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\frac{1}{4} \\cdot 120 = 30\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **4. Вычисление градиентов**\n",
        "Градиенты функции потерь по параметрам $ W $ и $ b $ вычисляются следующим образом:\n",
        "\n",
        "#### Градиент по $ W $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\cdot X_i\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "- $ n = 4 $,\n",
        "- $ \\hat{y}_i - y_i = [0 - 2, 0 - 4, 0 - 6, 0 - 8] = [-2, -4, -6, -8] $,\n",
        "- $ X = [1, 2, 3, 4] $.\n",
        "\n",
        "Вычислим сумму:\n",
        "$$\n",
        "\\sum_{i=1}^n (\\hat{y}_i - y_i) \\cdot X_i = (-2) \\cdot 1 + (-4) \\cdot 2 + (-6) \\cdot 3 + (-8) \\cdot 4\n",
        "$$\n",
        "$$\n",
        "= -2 - 8 - 18 - 32 = -60\n",
        "$$\n",
        "\n",
        "Градиент:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{2}{4} \\cdot (-60) = -30\n",
        "$$\n",
        "\n",
        "#### Градиент по $ b $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)\n",
        "$$\n",
        "\n",
        "Подставим значения:\n",
        "- $ n = 4 $,\n",
        "- $ \\hat{y}_i - y_i = [-2, -4, -6, -8] $.\n",
        "\n",
        "Вычислим сумму:\n",
        "$$\n",
        "\\sum_{i=1}^n (\\hat{y}_i - y_i) = -2 - 4 - 6 - 8 = -20\n",
        "$$\n",
        "\n",
        "Градиент:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{4} \\cdot (-20) = -10\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **5. Обновление параметров**\n",
        "Обновим параметры $ W $ и $ b $ с использованием правила градиентного спуска:\n",
        "$$\n",
        "W := W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "$$\n",
        "b := b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "Пусть $ \\alpha $ (скорость обучения) равна $ 0.1 $. Тогда:\n",
        "$$\n",
        "W := 0 - 0.1 \\cdot (-30) = 0 + 3 = 3\n",
        "$$\n",
        "$$\n",
        "b := 0 - 0.1 \\cdot (-10) = 0 + 1 = 1\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **6. Результат**\n",
        "После одного шага градиентного спуска:\n",
        "$$\n",
        "W = 3, \\quad b = 1\n",
        "$$\n",
        "\n",
        "Новая модель:\n",
        "$$\n",
        "\\hat{y} = W \\cdot X + b = 3 \\cdot X + 1\n",
        "$$\n",
        "\n",
        "Проверим новые предсказания:\n",
        "$$\n",
        "\\hat{y} = [3 \\cdot 1 + 1, 3 \\cdot 2 + 1, 3 \\cdot 3 + 1, 3 \\cdot 4 + 1] = [4, 7, 10, 13]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### **Ответ:**\n",
        "После первого шага градиентного спуска:\n",
        "$$\n",
        "\\boxed{W = 3, \\, b = 1}\n",
        "$$\n",
        "\n",
        "### Пример 2: Линейная регрессия со случайной инициализацией\n",
        "Инициализируем $W \\sim U(-0.1, 0.1)$, $b \\sim U(-0.1, 0.1)$. Повторяем те же шаги.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Обобщение для более сложных моделей\n",
        "\n",
        "Для нейронных сетей принципы инициализации остаются теми же, но учитываются дополнительные факторы:\n",
        "- Размерность входных данных.\n",
        "- Функция активации.\n",
        "- Глубина сети.\n",
        "\n",
        "Основные методы:\n",
        "- **Xavier Initialization** — для сигмоидных и гиперболических тангенсов.\n",
        "- **He Initialization** — для ReLU и его вариантов.\n",
        "\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Подбор начальных значений весов и смещений — это ключевой этап построения эффективной модели. Для линейной регрессии достаточно использовать случайную инициализацию с учётом размерности данных. Для более сложных моделей рекомендуется применять специализированные методы, такие как Xavier или He Initialization.\n",
        "\n"
      ],
      "metadata": {
        "id": "qvLlZm3Sr4ax"
      }
    }
  ]
}