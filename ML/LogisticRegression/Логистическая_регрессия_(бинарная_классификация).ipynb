{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO82G/br0bxiHq++VETpPhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/ML/LogisticRegression/%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F_(%D0%B1%D0%B8%D0%BD%D0%B0%D1%80%D0%BD%D0%B0%D1%8F_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Логистическая регрессия (бинарная классификация)\n",
        "\n",
        "## 1. Введение\n",
        "\n",
        "Логистическая регрессия представляет собой параметрический метод машинного обучения, предназначенный для решения задач бинарной классификации. Данный подход позволяет моделировать вероятность принадлежности объекта к одному из двух возможных классов на основе заданного набора признаков.\n",
        "\n",
        "### Обозначения и постановка задачи\n",
        "\n",
        "Пусть задано множество объектов, каждый из которых описывается $n$-мерным вектором признаков:\n",
        "$$\n",
        "\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T \\in \\mathbb{R}^n,\n",
        "$$\n",
        "где $n$ — количество информативных признаков, характеризующих объект.\n",
        "\n",
        "Целевая переменная $y$ принимает значения из множества $\\{0, 1\\}$, где:\n",
        "- $y = 1$ обозначает принадлежность объекта к положительному (целевому) классу;\n",
        "- $y = 0$ соответствует отрицательному классу.\n",
        "\n",
        "Задача заключается в построении функции, которая для заданного входного вектора $\\mathbf{x}$ возвращает оценку условной вероятности принадлежности объекта к положительному классу:\n",
        "$$\n",
        "p(\\mathbf{x}) = P(y=1 \\mid \\mathbf{x}).\n",
        "$$\n",
        "\n",
        "Соответственно, вероятность принадлежности объекта к отрицательному классу выражается как:\n",
        "$$\n",
        "P(y=0 \\mid \\mathbf{x}) = 1 - P(y=1 \\mid \\mathbf{x}) = 1 - p(\\mathbf{x}).\n",
        "$$\n",
        "\n",
        "Таким образом, модель позволяет не только предсказывать класс объекта, но и оценивать уверенность в этом предсказании через вероятностную интерпретацию.\n",
        "\n",
        "\n",
        "### Модель логистической регрессии\n",
        "\n",
        "Для оценки указанной вероятности используется следующая модель:\n",
        "$$\n",
        "p(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b),\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{w} = (w_1, w_2, \\dots, w_n)^T \\in \\mathbb{R}^n$ — вектор весовых коэффициентов (параметров модели);\n",
        "- $b \\in \\mathbb{R}$ — свободный член (bias), обеспечивающий сдвиг гиперплоскости;\n",
        "- $\\sigma(z)$ — сигмоидная функция, определяемая как:\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
        "$$\n",
        "\n",
        "Величина $z = \\mathbf{w}^T \\mathbf{x} + b$ называется **логитом** (по позже рассмотрим его по подробнее) и представляет собой нелинейно преобразованный линейный прогноз модели. Применение сигмоиды обеспечивает отображение вещественного числа $z \\in \\mathbb{R}$ в интервал $(0, 1)$, что делает возможной интерпретацию выхода модели как вероятности.\n",
        "\n",
        "### Предсказание класса\n",
        "\n",
        "На основе полученной вероятности $p(\\mathbf{x})$ осуществляется предсказание класса $\\hat{y} \\in \\{0, 1\\}$, обычно по пороговой функции:\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } p(\\mathbf{x}) \\geq 0.5 \\\\\n",
        "0, & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Таким образом:\n",
        "- $y$ — истинное значение целевой переменной;\n",
        "- $p(\\mathbf{x})$ — модельная оценка вероятности принадлежности к классу $1$;\n",
        "- $\\hat{y}$ — предсказанное значение класса;\n",
        "- $z$ — линейная комбинация признаков, используемая для вычисления $p(\\mathbf{x})$.\n",
        "\n",
        "\n",
        "## 2. Вероятностная интерпретация модели\n",
        "\n",
        "В отличие от обычной линейной регрессии, выход которой может принимать любые вещественные значения, **логистическая регрессия** предназначена для решения задач **бинарной классификации** и выдает на выходе **вероятность принадлежности объекта к одному из классов**, что делает её вывод интерпретируемым и полезным в практике.\n",
        "\n",
        "Для описания этой вероятности предполагается, что при заданном входном векторе $\\mathbf{x}$ целевая переменная $y$ подчиняется **распределению Бернулли**, поскольку возможны лишь два исхода: $y = 1$ или $y = 0$. Тогда вероятность можно записать следующим образом:\n",
        "\n",
        "$$\n",
        "P(y \\mid \\mathbf{x}) =\n",
        "\\begin{cases}\n",
        "p(\\mathbf{x}), & \\text{если } y = 1, \\\\\n",
        "1 - p(\\mathbf{x}), & \\text{если } y = 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Здесь $p(\\mathbf{x})$ — это вероятность того, что объект с признаками $\\mathbf{x}$ принадлежит к положительному классу ($y=1$). Эта вероятность моделируется с помощью **сигмоидной функции**, зависящей от параметров модели:\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b),\n",
        "$$\n",
        "\n",
        "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, а $\\mathbf{w}$ и $b$ — параметры модели (веса и смещение).\n",
        "\n",
        "Это выражение можно переписать в более компактной и универсальной форме:\n",
        "\n",
        "$$\n",
        "P(y \\mid \\mathbf{x}; \\mathbf{w}, b) = \\left( \\sigma(\\mathbf{w}^T \\mathbf{x} + b) \\right)^y \\cdot \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\\right)^{1 - y}.\n",
        "$$\n",
        "\n",
        "Такая форма удобна тем, что объединяет оба случая — $y=1$ и $y=0$ — в одно выражение. При подстановке конкретного значения $y$ один из множителей становится равным единице, и остаётся только вероятность соответствующего класса:\n",
        "- если $y=1$, то формула возвращает $p(\\mathbf{x})$,\n",
        "- если $y=0$, то — $1 - p(\\mathbf{x})$.\n",
        "\n",
        "Это представление особенно ценно при построении **функции правдоподобия**, которая используется для оценки неизвестных параметров модели методом **максимального правдоподобия**. Логарифмирование этой функции приводит к стандартной **функции потерь логистической регрессии**, которую можно эффективно минимизировать градиентными методами.\n",
        "\n",
        "Таким образом, вероятностная интерпретация модели позволяет не только строго математически обосновать выбор функции потерь, но и получить интерпретируемый результат — оценку вероятности принадлежности к каждому из классов.\n",
        "\n",
        "## 3. Логит-преобразование и его связь с сигмоидой\n",
        "\n",
        "Одним из ключевых понятий в логистической регрессии является **логит-преобразование** (или **логарифм шансов**), определяемое как:\n",
        "\n",
        "$$\n",
        "\\text{logit}(p) = \\ln\\left( \\frac{p}{1 - p} \\right), \\quad p \\in (0, 1).\n",
        "$$\n",
        "\n",
        "Функция $\\text{logit}(p)$ отображает вероятность $p$ на всю область вещественных чисел $\\mathbb{R}$, что позволяет использовать линейные методы моделирования.\n",
        "\n",
        "Существует взаимно однозначное соответствие между логитом и сигмоидой. Если положить $p = \\sigma(z)$, то можно показать, что:\n",
        "\n",
        "$$\n",
        "z = \\text{logit}(p).\n",
        "$$\n",
        "\n",
        "**Доказательство:**\n",
        "\n",
        "Пусть $p = \\sigma(z) = \\dfrac{1}{1 + e^{-z}}$. Тогда:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p &= \\frac{1}{1 + e^{-z}} \\\\\n",
        "\\Rightarrow 1 + e^{-z} &= \\frac{1}{p} \\\\\n",
        "\\Rightarrow e^{-z} &= \\frac{1 - p}{p} \\\\\n",
        "\\Rightarrow -z &= \\ln\\left( \\frac{1 - p}{p} \\right) \\\\\n",
        "\\Rightarrow z &= \\ln\\left( \\frac{p}{1 - p} \\right) = \\text{logit}(p).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Таким образом, сигмоида и логит являются обратными друг к другу функциями:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{logit}(p) = \\ln\\left( \\frac{p}{1 - p} \\right), \\quad \\text{причём } \\sigma(\\text{logit}(p)) = p, \\; \\text{logit}(\\sigma(z)) = z.\n",
        "$$\n",
        "\n",
        "## 4. Линейная зависимость в пространстве логитов\n",
        "\n",
        "Логистическая регрессия предполагает, что логит вероятности принадлежности к положительному классу линейно зависит от входных признаков:\n",
        "\n",
        "$$\n",
        "\\text{logit}\\left(P(y=1 \\mid \\mathbf{x})\\right) = \\mathbf{w}^T \\mathbf{x} + b.\n",
        "$$\n",
        "\n",
        "Применяя обратное преобразование (сигмоиду), получаем:\n",
        "\n",
        "$$\n",
        "P(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b).\n",
        "$$\n",
        "\n",
        "Таким образом, модель устанавливает **линейную зависимость в пространстве логитов**, а сигмоида обеспечивает корректную интерпретацию результата как вероятности.\n",
        "\n",
        "## 5. Пример преобразования\n",
        "\n",
        "Рассмотрим конкретный пример. Пусть модель предсказывает вероятность $p = 0.8$. Вычислим соответствующее значение логита:\n",
        "\n",
        "$$\n",
        "\\text{logit}(0.8) = \\ln\\left( \\frac{0.8}{1 - 0.8} \\right) = \\ln\\left( \\frac{0.8}{0.2} \\right) = \\ln(4) \\approx 1.386.\n",
        "$$\n",
        "\n",
        "Обратное преобразование:\n",
        "\n",
        "$$\n",
        "\\sigma(1.386) = \\frac{1}{1 + e^{-1.386}} \\approx 0.8.\n",
        "$$\n",
        "\n",
        "Этот пример демонстрирует, как происходит переход между вероятностным и линейным представлениями в логистической регрессии.\n",
        "\n",
        "## 6. Сводка основных функций\n",
        "\n",
        "| Функция      | Обозначение          | Область определения         | Область значений     | Свойства |\n",
        "|--------------|----------------------|-----------------------------|-----------------------|----------|\n",
        "| **Сигмоида** | $\\sigma(z)$        | $z \\in \\mathbb{R}$         | $\\sigma(z) \\in (0, 1)$ | $\\sigma(\\text{logit}(p)) = p$ |\n",
        "| **Логит**    | $\\text{logit}(p)$  | $p \\in (0, 1)$             | $\\text{logit}(p) \\in \\mathbb{R}$ | $\\text{logit}(\\sigma(z)) = z$ |\n"
      ],
      "metadata": {
        "id": "iQwOESVpPS0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сигмоидная функция: определение, свойства и производная\n",
        "\n",
        "\n",
        "\n",
        "## 1. Определение сигмоидной функции\n",
        "\n",
        "**Сигмоидная функция** (или **логистическая функция**) — это математическая функция вида:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad z \\in \\mathbb{R}.\n",
        "$$\n",
        "\n",
        "Она преобразует любое вещественное число $z$ в интервал $(0, 1)$, что делает её удобной для моделирования вероятностей.\n",
        "\n",
        "\n",
        "\n",
        "## 2. График сигмоиды\n",
        "\n",
        "Функция имеет характерную **S-образную форму**:\n",
        "- При $z \\to +\\infty$, $\\sigma(z) \\to 1$,\n",
        "- При $z \\to -\\infty$, $\\sigma(z) \\to 0$,\n",
        "- В точке $z = 0$: $\\sigma(0) = \\frac{1}{1 + 1} = 0.5$.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Основные свойства сигмоидной функции\n",
        "\n",
        "| Свойство | Описание |\n",
        "|---------|----------|\n",
        "| **Область определения** | Все вещественные числа: $z \\in \\mathbb{R}$ |\n",
        "| **Область значений** | Интервал $(0, 1)$ |\n",
        "| **Непрерывность** | Функция непрерывна на всей области определения |\n",
        "| **Монотонность** | Строго возрастает |\n",
        "| **Дифференцируемость** | Бесконечно дифференцируема |\n",
        "| **Симметричность относительно точки (0, 0.5)** | $\\sigma(-z) = 1 - \\sigma(z)$ |\n",
        "\n",
        "\n",
        "\n",
        "## 4. Производная сигмоидной функции\n",
        "\n",
        "Сигмоида обладает важным свойством: её производная выражается через саму себя.\n",
        "\n",
        "### Формула производной:\n",
        "\n",
        "$$\n",
        "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
        "$$\n",
        "\n",
        "### Вывод производной:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\sigma(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
        "\\Rightarrow \\sigma'(z) &= \\frac{d}{dz} \\left( \\frac{1}{1 + e^{-z}} \\right) \\\\\n",
        "&= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\n",
        "&= \\frac{1}{1 + e^{-z}} \\cdot \\left(1 - \\frac{1}{1 + e^{-z}}\\right) \\\\\n",
        "&= \\sigma(z)(1 - \\sigma(z)).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Это свойство делает сигмоиду удобной для использования в градиентных методах оптимизации, например, при обучении логистической регрессии или нейронных сетей.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Применение сигмоидной функции\n",
        "\n",
        "Сигмоида широко используется в машинном обучении:\n",
        "- **Логистическая регрессия**: для моделирования вероятности принадлежности к классу.\n",
        "- **Нейронные сети**: как функция активации (хотя сейчас часто заменяется ReLU и его аналогами).\n",
        "- **Бинарная классификация**: отображает выход модели в интервал [0, 1], что можно интерпретировать как вероятность.\n",
        "\n",
        "\n",
        "## 6. Преимущества и недостатки\n",
        "\n",
        "\n",
        "| Плюсы                                 | Минусы                                      |\n",
        "|--------------------------------------|---------------------------------------------|\n",
        "| Выход в интервале (0, 1) — удобно для вероятностей | Проблема затухающих градиентов при больших |z| |\n",
        "| Непрерывная и дифференцируемая       | Не симметрична относительно нуля (в отличие от tanh) |\n",
        "| Легко вычисляется и дифференцируется | Может приводить к исчезновению градиентов в глубоких сетях |\n",
        "\n"
      ],
      "metadata": {
        "id": "vMxkzng4jdoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "sQlbAZ1VsY8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 7. Альтернативы сигмоиде\n",
        "\n",
        "В современных задачах сигмоида часто заменяется другими функциями активации, такими как:\n",
        "- **ReLU**: $f(z) = \\max(0, z)$,\n",
        "- **tanh**: $\\tanh(z) = 2\\sigma(2z) - 1$,\n",
        "- **Softmax**: для многоклассовой классификации.\n"
      ],
      "metadata": {
        "id": "txUjWw5LsZJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Определим сигмоидную функцию и её производную\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Создаём массив значений z\n",
        "z = np.linspace(-10, 10, 400)\n",
        "\n",
        "# Вычисляем значения сигмоиды и её производной\n",
        "y = sigmoid(z)\n",
        "dy = sigmoid_derivative(z)\n",
        "\n",
        "# Строим график\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, y, label='σ(z) — сигмоида', color='blue')\n",
        "plt.plot(z, dy, label=\"σ'(z) — производная\", color='red', linestyle='--')\n",
        "\n",
        "# Оформление графика\n",
        "plt.title('Сигмоидная функция и её производная')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('σ(z)')\n",
        "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
        "plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Показываем график\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JpN24FBHjhcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Метод максимального правдоподобия (MLE)\n",
        "\n",
        "Пусть у нас имеется обучающая выборка из $ n $ независимых наблюдений:\n",
        "\n",
        "$$\n",
        "D = \\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^n,\n",
        "$$\n",
        "\n",
        "где $ \\mathbf{x}^{(i)} \\in \\mathbb{R}^d $ — вектор признаков объекта $ i $, а $ y^{(i)} \\in \\{0, 1\\} $ — соответствующая метка класса.\n",
        "\n",
        "Цель модели — оценить вероятность принадлежности объекта к положительному классу:\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}^{(i)}) = P(y^{(i)} = 1 \\mid \\mathbf{x}^{(i)}; \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b),\n",
        "$$\n",
        "\n",
        "где $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ — сигмоидная функция, а $ \\mathbf{w}, b $ — параметры модели.\n",
        "\n",
        "### Функция правдоподобия\n",
        "\n",
        "Поскольку задача является бинарной классификацией, вероятностное распределение целевой переменной задаётся как **распределение Бернулли**:\n",
        "\n",
        "$$\n",
        "P(y^{(i)} \\mid \\mathbf{x}^{(i)}; \\mathbf{w}, b) =\n",
        "\\begin{cases}\n",
        "p(\\mathbf{x}^{(i)}), & \\text{если } y^{(i)} = 1, \\\\\n",
        "1 - p(\\mathbf{x}^{(i)}), & \\text{если } y^{(i)} = 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Это выражение можно записать компактно:\n",
        "\n",
        "$$\n",
        "P(y^{(i)} \\mid \\mathbf{x}^{(i)}; \\mathbf{w}, b) = \\left( \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)^{y^{(i)}} \\cdot \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)\\right)^{1 - y^{(i)}}.\n",
        "$$\n",
        "\n",
        "Функция правдоподобия всей выборки с учетом предположения о независимости и одинаковой распределённости (i.i.d.) наблюдений:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{w}, b) = \\prod_{i=1}^{n} P(y^{(i)} \\mid \\mathbf{x}^{(i)}; \\mathbf{w}, b).\n",
        "$$\n",
        "\n",
        "Подставляя формулу выше:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{w}, b) = \\prod_{i=1}^{n} \\left[ \\left( \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)^{y^{(i)}} \\cdot \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)\\right)^{1 - y^{(i)}} \\right].\n",
        "$$\n",
        "\n",
        "### Логарифмическая функция правдоподобия\n",
        "\n",
        "Для удобства оптимизации перейдём к **логарифму функции правдоподобия**:\n",
        "\n",
        "$$\n",
        "\\ell(\\mathbf{w}, b) = \\log L(\\mathbf{w}, b) = \\sum_{i=1}^{n} \\log \\left[ \\left( \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)^{y^{(i)}} \\cdot \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)\\right)^{1 - y^{(i)}} \\right].\n",
        "$$\n",
        "\n",
        "Используем свойства логарифма:\n",
        "\n",
        "$$\n",
        "\\ell(\\mathbf{w}, b) = \\sum_{i=1}^{n} \\left[ y^{(i)} \\log \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) + (1 - y^{(i)}) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)\\right) \\right].\n",
        "$$\n",
        "\n",
        "Эта функция называется **логарифмом правдоподобия** (log-likelihood), и её максимизация позволяет найти наилучшие значения параметров $\\mathbf{w}, b$, которые лучше всего объясняют данные.\n",
        "\n",
        "\n",
        "### Оптимизация\n",
        "\n",
        "Максимизация логарифма правдоподобия $\\ell(\\mathbf{w}, b)$ означает нахождение таких параметров модели $\\mathbf{w}$ и $b$, которые делают обучающие данные максимально вероятными при заданной гипотезе модели. Формально задача выглядит так:\n",
        "\n",
        "$$\n",
        "\\max_{\\mathbf{w}, b} \\ell(\\mathbf{w}, b).\n",
        "$$\n",
        "\n",
        "Однако большинство численных методов оптимизации, таких как **градиентный спуск**, изначально предназначены для **минимизации функций**. Чтобы воспользоваться этими алгоритмами, мы переходим к эквивалентной задаче: **минимизации отрицательного значения логарифма правдоподобия**.\n",
        "\n",
        "#### Почему это можно сделать?\n",
        "\n",
        "Заметим следующее: если одно число больше другого, то его отрицательное значение будет меньше. То есть, если\n",
        "\n",
        "$$\n",
        "\\ell(\\mathbf{w}_1, b_1) > \\ell(\\mathbf{w}_2, b_2),\n",
        "$$\n",
        "\n",
        "то автоматически выполняется:\n",
        "\n",
        "$$\n",
        "-\\ell(\\mathbf{w}_1, b_1) < -\\ell(\\mathbf{w}_2, b_2).\n",
        "$$\n",
        "\n",
        "Это свойство позволяет нам утверждать, что **точка, в которой $\\ell(\\mathbf{w}, b)$ достигает максимума, совпадает с точкой, в которой $-\\ell(\\mathbf{w}, b)$ достигает минимума**.\n",
        "\n",
        "Таким образом, задачи:\n",
        "\n",
        "- $\\displaystyle \\max_{\\mathbf{w}, b} \\ell(\\mathbf{w}, b)$\n",
        "- и $\\displaystyle \\min_{\\mathbf{w}, b} \\left(-\\ell(\\mathbf{w}, b)\\right)$\n",
        "\n",
        "— **математически эквивалентны**.\n",
        "\n",
        "\n",
        "\n",
        "#### Функция потерь (бинарная кросс-энтропия)\n",
        "\n",
        "Поэтому мы можем ввести новую функцию — **функцию потерь**, равную отрицательному значению логарифма правдоподобия:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}, b) = -\\ell(\\mathbf{w}, b) = -\\sum_{i=1}^{n} \\left[ y^{(i)} \\log \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) + (1 - y^{(i)}) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b)\\right) \\right].\n",
        "$$\n",
        "\n",
        "Эта функция называется **бинарной кросс-энтропией** или **логистической функцией потерь**.\n",
        "\n",
        "Её минимизация позволяет найти такие значения параметров $\\mathbf{w}, b$, при которых модель наиболее точно предсказывает метки классов на обучающей выборке.\n",
        "\n",
        "\n",
        "\n",
        "#### Важное свойство: выпуклость\n",
        "\n",
        "Одним из ключевых преимуществ этой функции потерь является её **выпуклость относительно параметров $\\mathbf{w}$ и $b$** (при фиксированных значениях $\\mathbf{x}^{(i)}$).\n",
        "\n",
        "##### Что такое выпуклая функция?\n",
        "\n",
        "Функция называется **выпуклой**, если для любых двух точек $\\mathbf{w}_1, b_1$ и $\\mathbf{w}_2, b_2$ из области определения и любого числа $\\lambda \\in [0, 1]$ выполняется неравенство:\n",
        "\n",
        "$$\n",
        "J(\\lambda (\\mathbf{w}_1, b_1) + (1 - \\lambda)(\\mathbf{w}_2, b_2)) \\leq \\lambda J(\\mathbf{w}_1, b_1) + (1 - \\lambda) J(\\mathbf{w}_2, b_2).\n",
        "$$\n",
        "\n",
        "Графически это означает, что график функции \"не проваливается\" ниже соединяющего отрезка между двумя точками.\n",
        "\n",
        "##### Почему это важно?\n",
        "\n",
        "Выпуклость гарантирует:\n",
        "- Существование **единственного глобального минимума**,\n",
        "- Отсутствие \"проблемных\" локальных минимумов,\n",
        "- Быструю и стабильную сходимость градиентных методов.\n",
        "\n",
        "В случае логистической регрессии **бинарная кросс-энтропия является выпуклой функцией**, что делает её идеальной целевой функцией для оптимизации.\n",
        "\n",
        "\n",
        "\n",
        "### ✅ Итог\n",
        "\n",
        "- Максимизация $\\ell(\\mathbf{w}, b)$ эквивалентна минимизации $-\\ell(\\mathbf{w}, b)$, потому что при изменении знака порядок значений инвертируется, и максимум превращается в минимум.\n",
        "- Полученная функция $J(\\mathbf{w}, b)$ называется **бинарной кросс-энтропией** и используется как **функция потерь** в логистической регрессии.\n",
        "- Эта функция является **выпуклой**, что обеспечивает хорошие свойства сходимости при использовании численных методов оптимизации, таких как градиентный спуск.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "77rrG1WOP5Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Сигмоидная функция\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Функция правдоподобия (log-likelihood)\n",
        "def log_likelihood(X, y, w, b):\n",
        "    z = X * w + b\n",
        "    log_lik = np.sum(y * np.log(sigmoid(z) + 1e-15) + (1 - y) * np.log(1 - sigmoid(z) + 1e-15))\n",
        "    return log_lik\n",
        "\n",
        "# Функция потерь — бинарная кросс-энтропия\n",
        "def binary_cross_entropy(X, y, w, b):\n",
        "    return -log_likelihood(X, y, w, b)\n",
        "\n",
        "# Генерация синтетических данных\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-3, 3, 10).reshape(-1, 1)\n",
        "y = np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1])\n",
        "\n",
        "# Обучение модели для сравнения оптимального веса\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "optimal_w = model.coef_[0][0]\n",
        "optimal_b = model.intercept_[0]\n",
        "\n",
        "# Варьируем параметр w, фиксируем b = 0\n",
        "w_values = np.linspace(-5, 5, 400)\n",
        "l_values = []\n",
        "j_values = []\n",
        "\n",
        "for w in w_values:\n",
        "    l = log_likelihood(X.flatten(), y, w, b=0)\n",
        "    j = binary_cross_entropy(X.flatten(), y, w, b=0)\n",
        "    l_values.append(l)\n",
        "    j_values.append(j)\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Левый график: log-likelihood\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(w_values, l_values, label=r'$\\ell(w, b=0)$')\n",
        "plt.axvline(optimal_w, color='r', linestyle='--', label='Оптимальный w (sklearn)')\n",
        "plt.title('Логарифм правдоподобия $\\ell(w, b=0)$')\n",
        "plt.xlabel('Вес w')\n",
        "plt.ylabel(r'$\\ell(w)$')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Правый график: бинарная кросс-энтропия\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(w_values, j_values, label=r'$J(w, b=0)$')\n",
        "plt.axvline(optimal_w, color='r', linestyle='--', label='Оптимальный w (sklearn)')\n",
        "plt.title('Бинарная кросс-энтропия $J(w, b=0)$')\n",
        "plt.xlabel('Вес w')\n",
        "plt.ylabel(r'$J(w)$')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I-RUPdC8Ulj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Градиентный спуск в логистической регрессии\n",
        "\n",
        "## 4.1. Функция потерь (бинарная кросс-энтропия)\n",
        "\n",
        "Как было показано ранее, функция потерь для модели бинарной логистической регрессии определяется как **бинарная кросс-энтропия**:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}, b) = -\\sum_{i=1}^{n} \\left[ y^{(i)} \\log \\sigma(z^{(i)}) + (1 - y^{(i)}) \\log (1 - \\sigma(z^{(i)})) \\right],\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ z^{(i)} = \\mathbf{w}^T \\mathbf{x}^{(i)} + b $ — линейный выход модели перед применением сигмоиды,\n",
        "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ — сигмоидная функция.\n",
        "\n",
        "Целью обучения модели является минимизация этой функции потерь по параметрам модели $\\mathbf{w}$ и $b$. Для этого используется метод градиентного спуска, требующий вычисления **градиента функции потерь**:\n",
        "\n",
        "$$\n",
        "\\nabla J(\\mathbf{w}, b) = \\left( \\frac{\\partial J}{\\partial w_1}, \\dots, \\frac{\\partial J}{\\partial w_d}, \\frac{\\partial J}{\\partial b} \\right)^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4.2. Вспомогательные формулы\n",
        "\n",
        "### 4.2.1. Производная сигмоидной функции\n",
        "\n",
        "Для последующего дифференцирования функции потерь понадобится выражение для производной сигмоидной функции:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{тогда}\n",
        "$$\n",
        "$$\n",
        "\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z)).\n",
        "$$\n",
        "\n",
        "Это свойство будет использоваться при нахождении частных производных функции потерь по параметрам модели.\n",
        "\n",
        "\n",
        "\n",
        "## 4.3. Дифференцирование функции потерь\n",
        "\n",
        "Рассмотрим отдельное слагаемое в сумме функции потерь:\n",
        "\n",
        "$$\n",
        "J_i = - \\left[ y^{(i)} \\log \\sigma(z^{(i)}) + (1 - y^{(i)}) \\log (1 - \\sigma(z^{(i)})) \\right].\n",
        "$$\n",
        "\n",
        "Вычислим его производную по $z^{(i)}$, используя цепное правило дифференцирования:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J_i}{\\partial z^{(i)}} =\n",
        "- \\left[ \\frac{y^{(i)}}{\\sigma(z^{(i)})} \\cdot \\frac{d\\sigma}{dz^{(i)}} + \\frac{(1 - y^{(i)}) \\cdot (-1)}{1 - \\sigma(z^{(i)})} \\cdot \\frac{d\\sigma}{dz^{(i)}} \\right]\n",
        "$$\n",
        "\n",
        "Подставим выражение для производной сигмоиды:\n",
        "\n",
        "$$\n",
        "\\frac{d\\sigma}{dz^{(i)}} = \\sigma(z^{(i)})(1 - \\sigma(z^{(i)})),\n",
        "$$\n",
        "\n",
        "получаем:\n",
        "$$\n",
        "\\frac{\\partial J_i}{\\partial z^{(i)}} =\n",
        "- \\left[ \\frac{y^{(i)}}{\\sigma(z^{(i)})} \\cdot \\sigma(z^{(i)})(1 - \\sigma(z^{(i)})) - \\frac{(1 - y^{(i)})}{1 - \\sigma(z^{(i)})} \\cdot \\sigma(z^{(i)})(1 - \\sigma(z^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "Упрощаем:\n",
        "$$\n",
        "\\frac{\\partial J_i}{\\partial z^{(i)}} = - \\left[ y^{(i)}(1 - \\sigma(z^{(i)})) - (1 - y^{(i)})\\sigma(z^{(i)}) \\right] = \\sigma(z^{(i)}) - y^{(i)}.\n",
        "$$\n",
        "\n",
        "Таким образом, производная одного слагаемого по $z^{(i)}$ равна:\n",
        "$$\n",
        "\\frac{\\partial J_i}{\\partial z^{(i)}} = \\hat{y}^{(i)} - y^{(i)},\n",
        "$$\n",
        "где $\\hat{y}^{(i)} = \\sigma(z^{(i)})$ — предсказанная моделью вероятность принадлежности объекта к положительному классу.\n",
        "\n",
        "\n",
        "\n",
        "## 4.4. Частные производные функции потерь по параметрам модели\n",
        "\n",
        "Применим цепное правило для вычисления частных производных функции потерь по каждому из параметров модели:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^{n} \\frac{\\partial J_i}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial w_j}, \\quad\n",
        "\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{n} \\frac{\\partial J_i}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial b}.\n",
        "$$\n",
        "\n",
        "Заметим, что:\n",
        "$$\n",
        "z^{(i)} = \\mathbf{w}^T \\mathbf{x}^{(i)} + b = \\sum_{j=1}^{d} w_j x_j^{(i)} + b.\n",
        "$$\n",
        "\n",
        "Следовательно:\n",
        "$$\n",
        "\\frac{\\partial z^{(i)}}{\\partial w_j} = x_j^{(i)}, \\quad\n",
        "\\frac{\\partial z^{(i)}}{\\partial b} = 1.\n",
        "$$\n",
        "\n",
        "Подставляем:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}, \\quad\n",
        "\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4.5. Градиент функции потерь\n",
        "\n",
        "Объединяя все компоненты, получаем выражение для градиента функции потерь:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}, b} J(\\mathbf{w}, b) =\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) x_1^{(i)} \\\\\n",
        "\\vdots \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) x_d^{(i)} \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})\n",
        "\\end{bmatrix}\n",
        "= \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) \\cdot\n",
        "\\begin{bmatrix}\n",
        "x_1^{(i)} \\\\\n",
        "\\vdots \\\\\n",
        "x_d^{(i)} \\\\\n",
        "1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Можно также записать в более компактном виде, если объединить веса и свободный член $b$ в единый вектор:\n",
        "$$\n",
        "\\tilde{\\mathbf{w}} = \\begin{bmatrix} \\mathbf{w} \\\\ b \\end{bmatrix}, \\quad\n",
        "\\tilde{\\mathbf{x}}^{(i)} = \\begin{bmatrix} \\mathbf{x}^{(i)} \\\\ 1 \\end{bmatrix},\n",
        "$$\n",
        "тогда:\n",
        "$$\n",
        "\\nabla_{\\tilde{\\mathbf{w}}} J(\\tilde{\\mathbf{w}}) = \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\tilde{\\mathbf{x}}^{(i)}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4.6. Алгоритм обновления параметров (градиентный спуск)\n",
        "\n",
        "На каждом шаге итерационного процесса параметры модели обновляются следующим образом:\n",
        "\n",
        "$$\n",
        "\\tilde{\\mathbf{w}} := \\tilde{\\mathbf{w}} - \\alpha \\cdot \\nabla_{\\tilde{\\mathbf{w}}} J(\\tilde{\\mathbf{w}}),\n",
        "$$\n",
        "или покомпонентно:\n",
        "$$\n",
        "w_j := w_j - \\alpha \\cdot \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}, \\quad j = 1, \\dots, d,\n",
        "$$\n",
        "$$\n",
        "b := b - \\alpha \\cdot \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}),\n",
        "$$\n",
        "где $\\alpha > 0$ — положительное число, называемое **скоростью обучения** или **шагом градиентного спуска**.\n",
        "\n",
        "Процесс повторяется до достижения заданной точности или максимального числа итераций.\n"
      ],
      "metadata": {
        "id": "aCmpp3ZOT0Zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ 4.7. Реализация на Python"
      ],
      "metadata": {
        "id": "iSddsOCnYKVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Сигмоидная функция\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Функция потерь: бинарная кросс-энтропия\n",
        "def compute_loss(y, y_pred):\n",
        "    epsilon = 1e-15  # чтобы избежать log(0)\n",
        "    loss = -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
        "    return loss\n",
        "\n",
        "# Градиентный спуск\n",
        "def logistic_regression(X, y, lr=0.01, n_iters=1000):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0\n",
        "    losses = []\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        # Линейный вывод модели\n",
        "        linear_model = np.dot(X, weights) + bias\n",
        "        # Применяем сигмоиду — получаем вероятности классов\n",
        "        y_pred = sigmoid(linear_model)\n",
        "\n",
        "        # Вычисляем градиенты\n",
        "        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "        db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "        # Обновляем параметры\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        # Вычисляем и сохраняем ошибку\n",
        "        loss = compute_loss(y, y_pred)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Loss = {loss:.4f}\")\n",
        "\n",
        "    return weights, bias, losses\n",
        "\n",
        "# Визуализация разделяющей границы и графика потерь\n",
        "def plot_decision_boundary(X, y, weights, bias):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    probs = sigmoid(np.dot(grid, weights) + bias)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    Z = preds.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Разделяющая граница\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0')\n",
        "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1')\n",
        "    plt.title(\"Decision Boundary\")\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Изменение потерь\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(losses, color='green')\n",
        "    plt.title(\"Loss Over Iterations\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Генерация синтетических данных\n",
        "X, y = make_blobs(n_samples=300, centers=2, cluster_std=1.5, random_state=42)\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)  # Нормализация\n",
        "\n",
        "# Запуск градиентного спуска\n",
        "weights, bias, losses = logistic_regression(X, y, lr=0.1, n_iters=1000)\n",
        "\n",
        "# Визуализация\n",
        "plot_decision_boundary(X, y, weights, bias)"
      ],
      "metadata": {
        "id": "fk0ZnLGUYK_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅5. Формулы метрик качества бинарной классификации (научное изложение)\n",
        "\n",
        "Для оценки эффективности моделей бинарной классификации применяются стандартные метрики, основанные на анализе четырёх исходов предсказаний:\n",
        "\n",
        "- **TP (True Positive)** — истинно положительные случаи,\n",
        "- **TN (True Negative)** — истинно отрицательные случаи,\n",
        "- **FP (False Positive)** — ложноположительные случаи,\n",
        "- **FN (False Negative)** — ложноотрицательные случаи.\n",
        "\n",
        "Ниже приведены формальные математические выражения наиболее распространённых метрик.\n",
        "\n",
        "\n",
        "\n",
        "#### 5.1. **Accuracy (Точность классификации)**  \n",
        "Accuracy определяется как доля правильно классифицированных образцов (положительных и отрицательных) среди общего числа образцов:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "Эта метрика предоставляет обобщённое представление о качестве модели, однако её информативность снижается в случае несбалансированных классов.\n",
        "\n",
        "\n",
        "\n",
        "#### 5.2. **Precision (Точность предсказания положительного класса)**  \n",
        "Precision характеризует долю действительно положительных объектов среди всех объектов, классифицированных моделью как положительные:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Метрика отражает надёжность модели при выявлении положительного класса: чем выше значение Precision, тем реже модель ошибается, предсказывая положительный класс.\n",
        "\n",
        "\n",
        "\n",
        "#### 5.3. **Recall (Полнота выявления положительного класса)**  \n",
        "Recall определяет долю верно распознанных положительных объектов среди всех реальных положительных примеров:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Recall описывает способность модели находить все положительные образцы. Высокое значение Recall указывает на низкий уровень пропусков положительных случаев.\n",
        "\n",
        "\n",
        "\n",
        "#### 5.4. **F1-score (F-мера)**  \n",
        "F1-score представляет собой гармоническое среднее между Precision и Recall, что позволяет учитывать оба аспекта одновременно:\n",
        "\n",
        "$$\n",
        "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "Данная метрика особенно актуальна в задачах с несбалансированными данными, где необходимо соблюдать компромисс между точностью и полнотой.\n",
        "\n",
        "\n",
        "\n",
        "### 📌 Сравнение метрик\n",
        "\n",
        "| Метрика     | Формула                                                                 | Интерпретация                                                |\n",
        "|-------------|--------------------------------------------------------------------------|---------------------------------------------------------------|\n",
        "| Accuracy    | $\\dfrac{TP + TN}{TP + TN + FP + FN}$                                    | Общая доля корректных предсказаний                            |\n",
        "| Precision   | $\\dfrac{TP}{TP + FP}$                                                   | Доля правильных положительных предсказаний                    |\n",
        "| Recall      | $\\dfrac{TP}{TP + FN}$                                                   | Доля найденных реальных положительных объектов                |\n",
        "| F1-score    | $2 \\cdot \\dfrac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Комбинированная мера, учитывающая Precision и Recall          |\n",
        "\n"
      ],
      "metadata": {
        "id": "DZpdV4YdYZIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прикладное применение логистической регрессии в задаче классификации спама: с использованием метрик качества и ROC-AUC\n",
        "\n",
        "\n",
        "\n",
        "В данном разделе  представлено прикладное применение метода **логистической регрессии** к задаче бинарной классификации — **определению принадлежности текстовых сообщений к категории \"спам\" или \"не спам\"**. Для демонстрации процесса обучения модели, оценки её эффективности и анализа предсказательных способностей используется небольшой искусственный датасет, состоящий из шести примеров. Каждый объект описывается тремя бинарными признаками, а целевая переменная принимает значения $ y \\in \\{0, 1\\} $.\n",
        "\n",
        "Кроме того, рассмотрены стандартные метрики качества бинарной классификации (Accuracy, Precision, Recall, F1-score), а также построена **ROC-кривая** и вычислено значение **AUC (Area Under the Curve)** как обобщённая мера способности модели различать положительные и отрицательные классы на всём диапазоне порогов принятия решений.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Описание датасета\n",
        "\n",
        "Рассматривается следующий набор данных:\n",
        "\n",
        "| № | x₁: наличие слова \"купить\" | x₂: восклицательный знак | x₃: длина > 20 символов | y: класс (0 - не спам, 1 - спам) |\n",
        "|---|----------------------------|---------------------------|--------------------------|----------------------------------|\n",
        "| 1 |             1              |             1             |            1             |                1                 |\n",
        "| 2 |             0              |             0             |            0             |                0                 |\n",
        "| 3 |             1              |             0             |            1             |                0                 |\n",
        "| 4 |             0              |             1             |            0             |                1                 |\n",
        "| 5 |             1              |             1             |            0             |                1                 |\n",
        "| 6 |             0              |             0             |            1             |                0                 |\n",
        "\n",
        "Обозначим:\n",
        "- $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)}, x_3^{(i)})^T$ — вектор признаков для $i$-го сообщения,\n",
        "- $y^{(i)} \\in \\{0, 1\\}$ — истинный класс сообщения,\n",
        "- $\\hat{y}^{(i)} \\in (0, 1)$ — вероятность принадлежности к положительному классу, вычисленная моделью.\n",
        "\n",
        "\n",
        "## 3. Формализация модели\n",
        "\n",
        "Логистическая регрессия представляет собой обобщённую линейную модель, в которой вероятность принадлежности объекта к положительному классу определяется через сигмоидную функцию:\n",
        "$$\n",
        "P(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b),\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{w} = (w_1, w_2, w_3)^T$ — вектор весовых коэффициентов,\n",
        "- $b$ — свободный член (bias),\n",
        "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ — сигмоидная функция.\n",
        "\n",
        "Для каждого объекта вычисляется линейная комбинация:\n",
        "$$\n",
        "z^{(i)} = w_1 x_1^{(i)} + w_2 x_2^{(i)} + w_3 x_3^{(i)} + b,\n",
        "$$\n",
        "и соответствующая вероятность:\n",
        "$$\n",
        "\\hat{y}^{(i)} = \\sigma(z^{(i)}).\n",
        "$$\n",
        "\n",
        "Начальные значения параметров модели заданы следующим образом:\n",
        "$$\n",
        "\\mathbf{w}_0 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\end{bmatrix}, \\quad b_0 = 0.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 4. Прямой проход и предсказания модели\n",
        "\n",
        "На основе начальных значений весов проведено вычисление вероятностей $\\hat{y}^{(i)}$ для всех объектов выборки. Результаты прямого прохода представлены ниже:\n",
        "\n",
        "| № | $y^{(i)}$ | $\\hat{y}^{(i)}$ |\n",
        "|---|-------------|------------------|\n",
        "| 1 |      1      |       0.5498     |\n",
        "| 2 |      0      |       0.5        |\n",
        "| 3 |      0      |       0.5744     |\n",
        "| 4 |      1      |       0.4750     |\n",
        "| 5 |      1      |       0.5        |\n",
        "| 6 |      0      |       0.5498     |\n",
        "\n",
        "После применения порога $t = 0.5$, получены бинарные предсказания:\n",
        "\n",
        "| № | $y^{(i)}$ | $\\hat{y}^{(i)}$ | $\\hat{y}_{\\text{bin}}^{(i)}$ |\n",
        "|---|-------------|------------------|-------------------------------|\n",
        "| 1 |      1      |       0.5498     |               1               |\n",
        "| 2 |      0      |       0.5        |               1               |\n",
        "| 3 |      0      |       0.5744     |               1               |\n",
        "| 4 |      1      |       0.4750     |               0               |\n",
        "| 5 |      1      |       0.5        |               1               |\n",
        "| 6 |      0      |       0.5498     |               1               |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 🧮 5. Вычисление функции потерь (бинарной кросс-энтропии)\n",
        "\n",
        "## Формула:\n",
        "Для бинарной классификации используется следующая функция потерь — **бинарная кросс-энтропия**:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}, b) = -\\sum_{i=1}^{n} \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right],\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $ y^{(i)} $ — истинное значение целевой переменной ($0$ или $1$),\n",
        "- $ \\hat{y}^{(i)} = \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) $ — предсказанная моделью вероятность принадлежности объекта к положительному классу,\n",
        "- $ n $ — количество примеров в выборке.\n",
        "\n",
        "## Исходные данные:\n",
        "Мы используем начальные значения весов:\n",
        "$$\n",
        "\\mathbf{w}_0 = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\end{bmatrix}, \\quad b = 0.\n",
        "$$\n",
        "\n",
        "Для каждого из 6 объектов вычислим:\n",
        "- $ z^{(i)} = w_1 x_1^{(i)} + w_2 x_2^{(i)} + w_3 x_3^{(i)} + b $,\n",
        "- $ \\hat{y}^{(i)} = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}} $,\n",
        "- $ L_i = - \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] $ — индивидуальная потеря для $ i $-го примера.\n",
        "\n",
        "### Результаты вычислений:\n",
        "\n",
        "| № | $x_1$ | $x_2$ | $x_3$ | $y$ | $z^{(i)}$ | $\\hat{y}^{(i)}$ | $L_i$ |\n",
        "|---|--------|--------|--------|-----|------------|------------------|---------|\n",
        "| 1 |   1    |   1    |   1    |  1  |   0.2      |     0.5498       |  0.597  |\n",
        "| 2 |   0    |   0    |   0    |  0  |   0.0      |     0.5          |  0.693  |\n",
        "| 3 |   1    |   0    |   1    |  0  |   0.3      |     0.5744       |  0.849  |\n",
        "| 4 |   0    |   1    |   0    |  1  |  -0.1      |     0.4750       |  0.744  |\n",
        "| 5 |   1    |   1    |   0    |  1  |   0.0      |     0.5          |  0.693  |\n",
        "| 6 |   0    |   0    |   1    |  0  |   0.2      |     0.5498       |  0.798  |\n",
        "\n",
        "### Общая потеря:\n",
        "$$\n",
        "J = L_1 + L_2 + L_3 + L_4 + L_5 + L_6 = 0.597 + 0.693 + 0.849 + 0.744 + 0.693 + 0.798 = 4.374.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "# 🔁 6. Градиентный спуск: обновление параметров модели\n",
        "\n",
        "Градиентный спуск позволяет уменьшить значение функции потерь за счёт последовательного изменения параметров модели в направлении отрицательного градиента.\n",
        "\n",
        "## Формулы для частных производных:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_j} = \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}, \\quad j = 1, 2, 3\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})\n",
        "$$\n",
        "\n",
        "Рассчитаем эти значения на основе таблицы выше.\n",
        "\n",
        "### Таблица значений $ \\hat{y}^{(i)} - y^{(i)} $:\n",
        "\n",
        "| № | $ \\hat{y}^{(i)} $ | $ y^{(i)} $ | $ \\hat{y}^{(i)} - y^{(i)} $ |\n",
        "|---|--------------------|---------------|-------------------------------|\n",
        "| 1 | 0.5498             | 1             | -0.4502                       |\n",
        "| 2 | 0.5                | 0             | 0.5                           |\n",
        "| 3 | 0.5744             | 0             | 0.5744                        |\n",
        "| 4 | 0.4750             | 1             | -0.5250                       |\n",
        "| 5 | 0.5                | 1             | -0.5                          |\n",
        "| 6 | 0.5498             | 0             | 0.5498                        |\n",
        "\n",
        "### Вычисляем градиенты:\n",
        "\n",
        "#### По $ w_1 $:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_1} = (-0.4502)(1) + (0.5)(0) + (0.5744)(1) + (-0.5250)(0) + (-0.5)(1) + (0.5498)(0) = -0.3758\n",
        "$$\n",
        "\n",
        "#### По $ w_2 $:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_2} = (-0.4502)(1) + (0.5)(0) + (0.5744)(0) + (-0.5250)(1) + (-0.5)(1) + (0.5498)(0) = -1.4752\n",
        "$$\n",
        "\n",
        "#### По $ w_3 $:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_3} = (-0.4502)(1) + (0.5)(0) + (0.5744)(1) + (-0.5250)(0) + (-0.5)(0) + (0.5498)(1) = 0.674\n",
        "$$\n",
        "\n",
        "#### По $ b $:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = -0.4502 + 0.5 + 0.5744 - 0.525 - 0.5 + 0.5498 = 0.149\n",
        "$$\n",
        "\n",
        "Итак, градиент:\n",
        "$$\n",
        "\\nabla J = \\begin{bmatrix} -0.3758 \\\\ -1.4752 \\\\ 0.674 \\\\ 0.149 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "## 📈 Обновление параметров\n",
        "\n",
        "Скорость обучения: $ \\alpha = 0.1 $\n",
        "\n",
        "Обновление весов:\n",
        "$$\n",
        "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\alpha \\cdot \\nabla_{\\mathbf{w}} J\n",
        "$$\n",
        "$$\n",
        "b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\nabla_b J\n",
        "$$\n",
        "\n",
        "### Начальные значения:\n",
        "$$\n",
        "\\mathbf{w}_{\\text{old}} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\end{bmatrix}, \\quad b_{\\text{old}} = 0\n",
        "$$\n",
        "\n",
        "### Обновление:\n",
        "$$\n",
        "w_1 := 0.1 - 0.1 \\cdot (-0.3758) = 0.1 + 0.0376 = 0.1376\n",
        "$$\n",
        "$$\n",
        "w_2 := -0.1 - 0.1 \\cdot (-1.4752) = -0.1 + 0.1475 = 0.0475\n",
        "$$\n",
        "$$\n",
        "w_3 := 0.2 - 0.1 \\cdot 0.674 = 0.2 - 0.0674 = 0.1326\n",
        "$$\n",
        "$$\n",
        "b := 0 - 0.1 \\cdot 0.149 = -0.0149\n",
        "$$\n",
        "\n",
        "### Обновлённые параметры:\n",
        "$$\n",
        "\\mathbf{w}_{\\text{new}} = \\begin{bmatrix} 0.1376 \\\\ 0.0475 \\\\ 0.1326 \\end{bmatrix}, \\quad b_{\\text{new}} = -0.0149\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 🧠 Анализ изменений\n",
        "\n",
        "- Вес признака $ x_2 $ увеличился наиболее сильно ($ w_2 $): это связано с тем, что он дал наибольший вклад в ошибку.\n",
        "- Вес признака $ x_3 $ уменьшился, так как его влияние оказалось положительным, но избыточным на данном этапе.\n",
        "- Свободный член $ b $ стал слегка отрицательным, что немного снижает порог активации модели.\n",
        "\n",
        "Эти изменения должны привести к улучшению качества модели при следующем проходе.\n",
        "\n",
        "\n",
        "# 7. Метрики качества классификации\n",
        "\n",
        "Для объективной оценки эффективности модели бинарной классификации на этапе обучения и тестирования применяются стандартные метрики, основанные на анализе четырёх возможных исходов предсказаний:\n",
        "\n",
        "- **TP (True Positive)** — истинно положительные случаи: модель верно предсказала положительный класс ($y=1$),\n",
        "- **TN (True Negative)** — истинно отрицательные случаи: модель верно предсказала отрицательный класс ($y=0$),\n",
        "- **FP (False Positive)** — ложноположительные случаи: модель ошибочно предсказала положительный класс,\n",
        "- **FN (False Negative)** — ложноотрицательные случаи: модель ошибочно предсказала отрицательный класс.\n",
        "\n",
        "На основе этих значений вычисляются следующие метрики качества классификации:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 7.1. Accuracy (Точность классификации)\n",
        "\n",
        "**Accuracy** — это доля правильно классифицированных объектов среди общего числа примеров выборки. Формально определяется как:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}.\n",
        "$$\n",
        "\n",
        "### Значение:\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{2 + 0}{2 + 0 + 3 + 1} = \\frac{2}{6} \\approx 0.333.\n",
        "$$\n",
        "\n",
        "### Анализ:\n",
        "Полученное значение accuracy свидетельствует о низком уровне общей точности модели. Это указывает на то, что модель допускает значительное количество ошибок при классификации объектов. В данном случае снижение значения связано с преобладанием ложноположительных результатов (FP), а также наличием одного ложноотрицательного результата (FN).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 7.2. Precision (Точность предсказания положительного класса)\n",
        "\n",
        "**Precision** характеризует долю действительно положительных объектов среди всех, которые модель классифицировала как положительные. Определяется формулой:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}.\n",
        "$$\n",
        "\n",
        "### Значение:\n",
        "$$\n",
        "\\text{Precision} = \\frac{2}{2 + 3} = \\frac{2}{5} = 0.400.\n",
        "$$\n",
        "\n",
        "### Анализ:\n",
        "Низкое значение precision говорит о том, что из всех сообщений, предсказанных как спам (положительный класс), только 40% действительно оказались спамом. Оставшиеся 60% составили ложные срабатывания. Это критично в задачах, где важно минимизировать ошибки первого типа (например, при фильтрации писем).\n",
        "\n",
        "\n",
        "\n",
        "## 7.3. Recall (Полнота выявления положительного класса)\n",
        "\n",
        "**Recall** отражает способность модели находить все реальные положительные образцы. Определяется как доля найденных положительных объектов относительно всех существующих:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}.\n",
        "$$\n",
        "\n",
        "### Значение:\n",
        "$$\n",
        "\\text{Recall} = \\frac{2}{2 + 1} = \\frac{2}{3} \\approx 0.667.\n",
        "$$\n",
        "\n",
        "### Анализ:\n",
        "Значение recall показывает, что модель смогла найти около двух третей всех реальных случаев спама. Это является удовлетворительным результатом, особенно если важна минимизация пропущенных случаев положительного класса (например, при медицинской диагностике или выявлении мошенничества).\n",
        "\n",
        "\n",
        "\n",
        "## 7.4. F1-score (F-мера)\n",
        "\n",
        "**F1-score** представляет собой гармоническое среднее между precision и recall и используется для комплексной оценки модели, особенно в условиях несбалансированности классов:\n",
        "\n",
        "$$\n",
        "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\n",
        "$$\n",
        "\n",
        "### Значение:\n",
        "$$\n",
        "F1 = 2 \\cdot \\frac{0.4 \\cdot 0.667}{0.4 + 0.667} = 2 \\cdot \\frac{0.2668}{1.067} \\approx 0.500.\n",
        "$$\n",
        "\n",
        "### Анализ:\n",
        "Полученное значение F1-score находится на среднем уровне и подтверждает, что модель пока не достигла высокого качества классификации. Для улучшения необходимо повысить как precision, так и recall, что может быть достигнуто за счёт дальнейшей оптимизации параметров, увеличения количества итераций обучения или изменения порога принятия решений.\n",
        "\n",
        "\n",
        "\n",
        "## 📋 Сводная таблица метрик качества\n",
        "\n",
        "| Метрика     | Формула                                                                 | Значение   |\n",
        "|-------------|--------------------------------------------------------------------------|------------|\n",
        "| Accuracy    | $\\dfrac{TP + TN}{TP + TN + FP + FN}$                                    | 0.333      |\n",
        "| Precision   | $\\dfrac{TP}{TP + FP}$                                                   | 0.400      |\n",
        "| Recall      | $\\dfrac{TP}{TP + FN}$                                                   | 0.667      |\n",
        "| F1-score    | $2 \\cdot \\dfrac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | 0.500      |\n",
        "\n",
        "\n",
        "\n",
        "## 8. ROC-кривая и AUC\n",
        "\n",
        "Для более полной оценки качества модели бинарной классификации, учитывающей поведение модели при различных порогах принятия решений, строится **ROC-кривая (Receiver Operating Characteristic Curve)**. Эта кривая позволяет визуализировать компромисс между чувствительностью (TPR) и специфичностью (1 - FPR) при изменении порога.\n",
        "\n",
        "\n",
        "\n",
        "## 8.1. Определение координат ROC-кривой\n",
        "\n",
        "Координаты ROC-кривой определяются следующими величинами:\n",
        "\n",
        "- **True Positive Rate (TPR)** — чувствительность, доля найденных положительных объектов:\n",
        "$$\n",
        "\\text{TPR} = \\frac{TP}{TP + FN}.\n",
        "$$\n",
        "\n",
        "- **False Positive Rate (FPR)** — доля ложноположительных среди отрицательных:\n",
        "$$\n",
        "\\text{FPR} = \\frac{FP}{FP + TN}.\n",
        "$$\n",
        "\n",
        "ROC-кривая строится по множеству пар $(\\text{FPR}, \\text{TPR})$, соответствующих различным значениям порога $t$, при котором объект классифицируется как положительный, если $\\hat{y}^{(i)} \\geq t$.\n",
        "\n",
        "\n",
        "## 8.2. Вычисление AUC (Area Under the Curve)\n",
        "\n",
        "**AUC (Area Under the Curve)** — это скалярная метрика, равная площади под ROC-кривой. Она принимает значения в интервале $[0, 1]$ и интерпретируется следующим образом:\n",
        "\n",
        "- $ \\text{AUC} = 1 $: идеальная модель, полностью разделяет классы,\n",
        "- $ \\text{AUC} = 0.5 $: модель не лучше случайного угадывания,\n",
        "- $ \\text{AUC} < 0.5 $: модель работает хуже случайной, её предсказания следует инвертировать.\n",
        "\n",
        "### Вычисленное значение:\n",
        "$$\n",
        "\\text{AUC} \\approx 0.667.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 8.3. Интерпретация AUC\n",
        "\n",
        "Полученное значение AUC = 0.667 указывает на то, что модель работает лучше случайного угадывания. Однако оно всё ещё достаточно невелико, что свидетельствует о необходимости дальнейших шагов оптимизации. Такое значение говорит о том, что модель обладает некоторой дискриминирующей способностью, но она пока недостаточна для надёжного разделения классов.\n",
        "\n",
        "Можно также дать статистическую интерпретацию AUC: она равна вероятности того, что модель правильно ранжирует пару объектов, один из которых из положительного класса, другой — из отрицательного:\n",
        "\n",
        "$$\n",
        "\\text{AUC} = P(\\hat{y}_i > \\hat{y}_j \\mid y_i = 1, y_j = 0).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 8.4. Графическое представление ROC-кривой\n",
        "\n",
        "Хотя графическое изображение ROC-кривой не представлено в данном тексте, его можно легко воспроизвести программно. При этом ROC-кривая будет состоять из нескольких точек, соответствующих каждому из шести объектов датасета, и будет начинаться в точке $(0, 0)$ и заканчиваться в точке $(1, 1)$. Площадь под этой кривой (AUC) составляет приблизительно 0.667, что подтверждает наличие слабой, но положительной дискриминационной способности модели.\n",
        "\n",
        "\n",
        "\n",
        "## 📌 Общий анализ качества модели\n",
        "\n",
        "| Метрика     | Значение | Анализ |\n",
        "|-------------|----------|--------|\n",
        "| Accuracy    | 0.333    | Низкий уровень общей точности классификации |\n",
        "| Precision   | 0.400    | Низкая надёжность предсказаний положительного класса |\n",
        "| Recall      | 0.667    | Умеренная способность находить реальные положительные объекты |\n",
        "| F1-score    | 0.500    | Средняя оценка качества, требует улучшения |\n",
        "| AUC         | 0.667    | Лучше случайного, но требует дополнительной оптимизации |\n",
        "\n",
        "\n",
        "На основе проведённого анализа можно сделать  следующие выводы:\n",
        "\n",
        "- Текущее состояние модели после первого шага градиентного спуска характеризуется низким уровнем accuracy и precision, что делает её недостаточно надёжной для практического применения.\n",
        "- Умеренное значение recall указывает на потенциальную возможность модели находить большую часть положительных случаев, однако это достигается за счёт увеличения числа ложных срабатываний.\n",
        "- Полученное значение F1-score подтверждает необходимость дальнейшей оптимизации модели, направленной на улучшение как precision, так и recall.\n",
        "- Значение AUC = 0.667 демонстрирует, что модель обладает некоторой способностью различать классы, но эта способность пока ограничена. Для повышения качества модели рекомендуется:\n",
        "  - выполнить несколько итераций градиентного спуска,\n",
        "  - скорректировать скорость обучения,\n",
        "  - рассмотреть возможность регуляризации,\n",
        "  - использовать более представительную обучающую выборку.\n",
        "\n",
        "Эти меры позволят значительно повысить качество классификации и сделать модель применимой к реальным задачам."
      ],
      "metadata": {
        "id": "i_2sOjVral7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Данные\n",
        "X = np.array([\n",
        "    [1, 1, 1],\n",
        "    [0, 0, 0],\n",
        "    [1, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [1, 1, 0],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "\n",
        "y = np.array([1, 0, 0, 1, 1, 0])\n",
        "\n",
        "# Инициализация параметров\n",
        "w = np.array([0.1, -0.1, 0.2])\n",
        "b = 0.0\n",
        "\n",
        "# Параметры обучения\n",
        "alpha = 0.1\n",
        "num_iterations = 100\n",
        "\n",
        "# Сигмоидная функция\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Предсказание вероятности\n",
        "def predict(X, w, b):\n",
        "    z = X @ w + b\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Функция потерь — бинарная кросс-энтропия\n",
        "def compute_loss(y, y_hat):\n",
        "    epsilon = 1e-15  # для предотвращения log(0)\n",
        "    return -np.mean(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
        "\n",
        "# Градиенты\n",
        "def compute_gradients(X, y, y_hat):\n",
        "    m = len(y)\n",
        "    dw = (1/m) * X.T @ (y_hat - y)\n",
        "    db = (1/m) * np.sum(y_hat - y)\n",
        "    return dw, db\n",
        "\n",
        "# Метрики\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    acc = np.mean(y_true == y_pred)\n",
        "    prec = np.sum((y_pred == 1) & (y_true == 1)) / (np.sum(y_pred == 1) + 1e-10)\n",
        "    rec = np.sum((y_pred == 1) & (y_true == 1)) / (np.sum(y_true == 1) + 1e-10)\n",
        "    f1 = 2 * (prec * rec) / (prec + rec + 1e-10)\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "# История обучения\n",
        "loss_history = []\n",
        "acc_history = []\n",
        "f1_history = []\n",
        "\n",
        "# Обучение\n",
        "for i in range(num_iterations):\n",
        "    y_hat = predict(X, w, b)\n",
        "    loss = compute_loss(y, y_hat)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    dw, db = compute_gradients(X, y, y_hat)\n",
        "    w -= alpha * dw\n",
        "    b -= alpha * db\n",
        "\n",
        "    y_pred = (y_hat >= 0.5).astype(int)\n",
        "    acc, prec, rec, f1 = compute_metrics(y, y_pred)\n",
        "    acc_history.append(acc)\n",
        "    f1_history.append(f1)\n",
        "\n",
        "# Последнее предсказание\n",
        "y_hat_final = predict(X, w, b)\n",
        "y_pred_final = (y_hat_final >= 0.5).astype(int)\n",
        "\n",
        "# Метрики\n",
        "acc, prec, rec, f1 = compute_metrics(y, y_pred_final)\n",
        "\n",
        "# Матрица ошибок\n",
        "cm = confusion_matrix(y, y_pred_final)\n",
        "labels = [\"Not Spam\", \"Spam\"]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"\\n=== Final Results ===\")\n",
        "print(\"True Labels:\", y)\n",
        "print(\"Predicted Labels:\", y_pred_final)\n",
        "print(\"Predicted Probabilities:\", y_hat_final)\n",
        "print(\"\\nMetrics:\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall: {rec:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Веса модели\n",
        "print(\"\\n=== Model Parameters ===\")\n",
        "print(f\"Weights: {w}\")\n",
        "print(f\"Bias: {b:.4f}\")\n",
        "\n",
        "# Графики\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# График потерь\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss_history, label=\"Loss\", color='blue')\n",
        "plt.title(\"Loss over Iterations\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Графики точности и F1\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(acc_history, label=\"Accuracy\", color='green')\n",
        "plt.plot(f1_history, label=\"F1 Score\", color='orange')\n",
        "plt.title(\"Metrics over Iterations\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Визуализация матрицы ошибок\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_8fQdQ4cYOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. ROC-кривая и AUC-ROC\n",
        "\n",
        "## 6.1. Введение в ROC-кривую\n",
        "\n",
        "**ROC-кривая (Receiver Operating Characteristic Curve)** — это график, который демонстрирует **качество бинарного классификатора** при различных порогах принятия решений. В отличие от метрик, таких как accuracy или F1-score, которые зависят от конкретного порога, ROC-кривая позволяет оценить **обобщённую способность модели различать два класса** на всём диапазоне возможных порогов.\n",
        "\n",
        "Кривая строится в координатах:\n",
        "- **Ось абсцисс**: $ \\text{FPR} = \\frac{FP}{FP + TN} $ — доля ложноположительных результатов (**False Positive Rate**),\n",
        "- **Ось ординат**: $ \\text{TPR} = \\frac{TP}{TP + FN} $ — доля истинноположительных результатов (**True Positive Rate**, совпадает с Recall), также называемый **чувствительностью (Sensitivity)**.\n",
        "\n",
        "ROC-кривая показывает, как изменяется соотношение между чувствительностью и долей ложных срабатываний при изменении порога принятия решения $ t $, начиная от 0 и заканчивая 1.\n",
        "\n",
        "## 6.2. Порог принятия решения\n",
        "\n",
        "В задаче бинарной классификации модель логистической регрессии выдает вероятность принадлежности объекта к положительному классу:  \n",
        "$$\n",
        "\\hat{p}^{(i)} = P(y=1 \\mid \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + b).\n",
        "$$\n",
        "\n",
        "Для получения предсказания класса обычно используется порог $t$:\n",
        "$$\n",
        "\\hat{y}^{(i)} =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } \\hat{p}^{(i)} \\geq t, \\\\\n",
        "0, & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "По умолчанию порог выбирается равным $t = 0.5$, но для несбалансированных данных или при необходимости увеличения полноты (Recall) или точности (Precision), порог может быть изменен.\n",
        "\n",
        "ROC-кривая строится по значениям TPR и FPR при **всех возможных значениях порога $t \\in [0, 1]$**.\n",
        "\n",
        "## 6.3. Построение ROC-кривой\n",
        "\n",
        "Алгоритм построения ROC-кривой:\n",
        "\n",
        "1. Получить прогнозы вероятностей $\\hat{p}^{(i)}$ для всех объектов из тестовой выборки.\n",
        "2. Отсортировать все пары $(\\hat{p}^{(i)}, y^{(i)})$ в порядке убывания $\\hat{p}^{(i)}$.\n",
        "3. Для каждого возможного порога $t$:\n",
        "   - Присвоить класс $1$ всем объектам, у которых $\\hat{p}^{(i)} \\geq t$,\n",
        "   - Вычислить TP, FP, TN, FN,\n",
        "   - Найти соответствующие значения TPR и FPR.\n",
        "4. Нанести точки $(\\text{FPR}, \\text{TPR})$ на график и соединить их линией.\n",
        "\n",
        "Таким образом, ROC-кривая отражает **все возможные компромиссы между чувствительностью и специфичностью (1 - FPR)** при разных значениях порога.\n",
        "\n",
        "## 6.4. Интерпретация ROC-кривой\n",
        "\n",
        "- **Идеальный классификатор** имеет ROC-кривую, которая проходит через верхний левый угол графика (TPR = 1, FPR = 0). Это означает, что классификатор идеально разделяет классы при любом пороге.\n",
        "- **Случайный классификатор** соответствует диагональной прямой из нижнего левого угла в верхний правый (то есть, TPR = FPR для любого порога). Эта линия называется **диагональю случайности (random guess line)**.\n",
        "- Если кривая находится **выше диагонали**, это указывает на то, что модель работает лучше случайного угадывания.\n",
        "- Если кривая находится **ниже диагонали**, это говорит о том, что модель работает хуже случайного, возможно, требуется инвертировать предсказания.\n",
        "\n",
        "## 6.5. AUC — Area Under the ROC Curve\n",
        "\n",
        "**AUC (Area Under the Curve)** — это скалярная метрика, характеризующая качество классификатора. Она равна площади под ROC-кривой и принимает значения в диапазоне $[0, 1]$.\n",
        "\n",
        "### Свойства AUC:\n",
        "\n",
        "- **AUC = 1**: идеальная модель, полностью отделяет положительные и отрицательные классы.\n",
        "- **AUC = 0.5**: модель не лучше случайного угадывания.\n",
        "- **AUC < 0.5**: модель работает хуже случайной, её можно \"обратить\" (менять класс на противоположный), чтобы получить AUC > 0.5.\n",
        "- **AUC > 0.5**: модель лучше случайной.\n",
        "\n",
        "### Интерпретация AUC:\n",
        "\n",
        "Можно дать статистическую интерпретацию AUC: она равна **вероятности того, что модель правильно ранжирует пару объектов**, один из которых из положительного класса, другой — из отрицательного.\n",
        "\n",
        "Формально:\n",
        "$$\n",
        "\\text{AUC} = P(\\hat{p}_i > \\hat{p}_j \\mid y_i = 1, y_j = 0),\n",
        "$$\n",
        "где $(\\mathbf{x}_i, y_i)$ — положительный пример, а $(\\mathbf{x}_j, y_j)$ — отрицательный.\n",
        "\n",
        "Это делает AUC особенно полезной метрикой в задачах, где важно **ранжирование объектов**, например, при рекомендации товаров, кредитном скоринге и т.д.\n",
        "\n",
        "## 6.6. Преимущества ROC-AUC\n",
        "\n",
        "- Не зависит от баланса классов (в отличие от Accuracy).\n",
        "- Учитывает не только конечный класс, но и уверенность модели в своём прогнозе (вероятность).\n",
        "- Инвариантна к выбору порога.\n",
        "- Позволяет сравнивать модели независимо от конкретного применения.\n",
        "\n",
        "## 6.7. Когда использовать ROC-AUC, а когда другие метрики?\n",
        "\n",
        "| Метрика | Когда использовать |\n",
        "|--------|---------------------|\n",
        "| **ROC-AUC** | Когда важна общая способность модели различать классы при разных порогах, особенно если классы несбалансированы. |\n",
        "| **Accuracy** | Когда классы сбалансированы и ошибки обоих типов одинаково важны. |\n",
        "| **F1-score** | Когда важны и Precision, и Recall, например, в задачах медицинской диагностики, где ложные отрицательные ответы критичны. |\n",
        "| **Precision-Recall Curve (AUC-PRC)** | Когда интересуют только положительные классы и количество ложных срабатываний, особенно при сильно несбалансированных данных. |\n",
        "\n",
        "## 6.8. Примеры ROC-кривых\n",
        "\n",
        "1. **Хорошая модель**: кривая резко поднимается вверх, затем плавно идет вправо → высокий AUC (> 0.9).\n",
        "2. **Средняя модель**: кривая проходит заметно выше диагонали, но не слишком высоко → AUC ≈ 0.7–0.8.\n",
        "3. **Плохая модель**: кривая близка к диагонали или даже ниже → AUC ≈ 0.5 или меньше.\n"
      ],
      "metadata": {
        "id": "QIlhsiuHdwXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, roc_auc_score, confusion_matrix)\n",
        "\n",
        "# 1. Создаем датасет\n",
        "data = {\n",
        "    'x1': [1, 0, 1, 0, 1, 0],\n",
        "    'x2': [1, 0, 0, 1, 1, 0],\n",
        "    'x3': [1, 0, 1, 0, 0, 1],\n",
        "    'y':  [1, 0, 0, 1, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['x1', 'x2', 'x3']]\n",
        "y = df['y']\n",
        "\n",
        "# 2. Обучаем модель логистической регрессии\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# 3. Получаем вероятности и предсказания\n",
        "y_proba = model.predict_proba(X)[:, 1]  # Вероятности класса 1\n",
        "y_pred = model.predict(X)               # Предсказанные классы\n",
        "\n",
        "# 4. Вычисляем метрики\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "precision = precision_score(y, y_pred)\n",
        "recall = recall_score(y, y_pred)\n",
        "f1 = f1_score(y, y_pred)\n",
        "auc_score = roc_auc_score(y, y_proba)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "print(f\"AUC-ROC: {auc_score:.2f}\")\n",
        "\n",
        "# 5. Выводим датасет с предсказаниями\n",
        "df['y_proba'] = y_proba\n",
        "df['y_pred'] = y_pred\n",
        "print(\"\\nДатасет с предсказаниями:\")\n",
        "print(df)\n",
        "\n",
        "# 6. Построение ROC-кривой\n",
        "fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve for Logistic Regression')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. Матрица ошибок\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aYDnbuB8dwpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Логистическая регрессия (много классная классификация)\n",
        "\n",
        "## 1. Введение\n",
        "\n",
        "**Логистическая регрессия** — это популярный метод машинного обучения, который может быть обобщён для решения задач **многоклассовой классификации**, где целевая переменная принимает $ K > 2 $ возможных значений:  \n",
        "$$\n",
        "y \\in \\{1, 2, \\dots, K\\}.\n",
        "$$\n",
        "\n",
        "Каждый объект описывается **вектором признаков** $\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T \\in \\mathbb{R}^n$, где $n$ — количество признаков.\n",
        "\n",
        "Цель моделирования состоит в том, чтобы построить функцию, позволяющую предсказывать вероятность принадлежности объекта к каждому из классов:\n",
        "$$\n",
        "P(y=k \\mid \\mathbf{x}), \\quad k = 1, \\dots, K.\n",
        "$$\n",
        "\n",
        "Для этой цели используется **логистическая регрессия с несколькими классами**, которая моделирует указанные вероятности с помощью **софтмакс-функции** (обобщение сигмоиды на многоклассовый случай).\n",
        "\n",
        "\n",
        "\n",
        "## 2. Модель многоклассовой логистической регрессии\n",
        "\n",
        "В отличие от бинарной классификации, где используется сигмоида, в случае нескольких классов модель определяется следующим образом:\n",
        "\n",
        "Для каждого класса $k \\in \\{1, \\dots, K\\}$ задан свой вектор весов $\\mathbf{w}_k \\in \\mathbb{R}^n$ и свободный член $b_k \\in \\mathbb{R}$. Тогда линейная комбинация признаков для класса $k$:\n",
        "$$\n",
        "z_k = \\mathbf{w}_k^T \\mathbf{x} + b_k.\n",
        "$$\n",
        "\n",
        "Софтмакс-функция преобразует эти значения в вероятности:\n",
        "$$\n",
        "P(y=k \\mid \\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}.\n",
        "$$\n",
        "\n",
        "Эта функция гарантирует, что выход представляет собой распределение вероятностей:  \n",
        "$$\n",
        "0 < P(y=k \\mid \\mathbf{x}) < 1, \\quad \\sum_{k=1}^{K} P(y=k \\mid \\mathbf{x}) = 1.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 3. Вероятностная интерпретация модели\n",
        "\n",
        "Распределение целевой переменной $y$ при фиксированном $\\mathbf{x}$ задаётся как **категориальное распределение** (обобщение распределения Бернулли на несколько классов):\n",
        "\n",
        "$$\n",
        "P(y=k \\mid \\mathbf{x}; \\mathbf{W}, \\mathbf{b}) = \\frac{e^{\\mathbf{w}_k^T \\mathbf{x} + b_k}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x} + b_j}},\n",
        "$$\n",
        "где:\n",
        "- $\\mathbf{W} = (\\mathbf{w}_1, \\dots, \\mathbf{w}_K)$ — матрица весов размерности $n \\times K$,\n",
        "- $\\mathbf{b} = (b_1, \\dots, b_K)$ — вектор свободных членов.\n",
        "\n",
        "Такое представление удобно при построении функции правдоподобия и последующей оценке параметров модели.\n",
        "\n",
        "\n",
        "## 4. Функция правдоподобия и логарифмический правдоподобие\n",
        "\n",
        "Пусть у нас имеется обучающая выборка из $ n $ независимых наблюдений:\n",
        "$$\n",
        "D = \\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^n,\n",
        "$$\n",
        "где $ \\mathbf{x}^{(i)} \\in \\mathbb{R}^d $ — вектор признаков объекта $ i $, а $ y^{(i)} \\in \\{1, 2, \\dots, K\\} $ — соответствующая метка класса.\n",
        "\n",
        "### Функция правдоподобия\n",
        "\n",
        "Функция правдоподобия всей выборки:\n",
        "$$\n",
        "L(\\mathbf{W}, \\mathbf{b}) = \\prod_{i=1}^{n} P(y^{(i)} \\mid \\mathbf{x}^{(i)}; \\mathbf{W}, \\mathbf{b}).\n",
        "$$\n",
        "\n",
        "Подставляя формулу софтмакса:\n",
        "$$\n",
        "L(\\mathbf{W}, \\mathbf{b}) = \\prod_{i=1}^{n} \\left[ \\frac{e^{\\mathbf{w}_{y^{(i)}}^T \\mathbf{x}^{(i)} + b_{y^{(i)}}}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x}^{(i)} + b_j}} \\right].\n",
        "$$\n",
        "\n",
        "### Логарифмическая функция правдоподобия\n",
        "\n",
        "Переходим к логарифму функции правдоподобия:\n",
        "$$\n",
        "\\ell(\\mathbf{W}, \\mathbf{b}) = \\log L(\\mathbf{W}, \\mathbf{b}) = \\sum_{i=1}^{n} \\log \\left[ \\frac{e^{\\mathbf{w}_{y^{(i)}}^T \\mathbf{x}^{(i)} + b_{y^{(i)}}}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x}^{(i)} + b_j}} \\right].\n",
        "$$\n",
        "\n",
        "Упрощаем:\n",
        "$$\n",
        "\\ell(\\mathbf{W}, \\mathbf{b}) = \\sum_{i=1}^{n} \\left[ \\mathbf{w}_{y^{(i)}}^T \\mathbf{x}^{(i)} + b_{y^{(i)}} - \\log \\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x}^{(i)} + b_j} \\right].\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 5. Функция потерь (кросс-энтропия)\n",
        "\n",
        "Для минимизации вместо максимизации правдоподобия вводится функция потерь:\n",
        "$$\n",
        "J(\\mathbf{W}, \\mathbf{b}) = -\\ell(\\mathbf{W}, \\mathbf{b}) = -\\sum_{i=1}^{n} \\left[ \\mathbf{w}_{y^{(i)}}^T \\mathbf{x}^{(i)} + b_{y^{(i)}} - \\log \\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x}^{(i)} + b_j} \\right].\n",
        "$$\n",
        "\n",
        "Эта функция называется **кросс-энтропийной функцией потерь** в случае многоклассовой классификации.\n",
        "\n",
        "\n",
        "\n",
        "## 6. Градиентный спуск в многоклассовой логистической регрессии\n",
        "\n",
        "Чтобы минимизировать функцию потерь, мы используем градиентный спуск. Для этого вычислим частные производные $ J $ по каждому из параметров модели.\n",
        "\n",
        "### Обозначения\n",
        "\n",
        "Пусть:\n",
        "- $ z_k^{(i)} = \\mathbf{w}_k^T \\mathbf{x}^{(i)} + b_k $,\n",
        "- $ p_k^{(i)} = P(y=k \\mid \\mathbf{x}^{(i)}) = \\frac{e^{z_k^{(i)}}}{\\sum_{j=1}^{K} e^{z_j^{(i)}}} $ — предсказанная моделью вероятность класса $k$.\n",
        "\n",
        "### Частные производные\n",
        "\n",
        "#### По $ \\mathbf{w}_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{w}_k} = \\sum_{i=1}^{n} \\left( p_k^{(i)} - \\mathbb{1}(y^{(i)} = k) \\right) \\cdot \\mathbf{x}^{(i)},\n",
        "$$\n",
        "где $ \\mathbb{1}(y^{(i)} = k) $ — индикаторная функция, равная 1, если $ y^{(i)} = k $, и 0 иначе.\n",
        "\n",
        "#### По $ b_k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_k} = \\sum_{i=1}^{n} \\left( p_k^{(i)} - \\mathbb{1}(y^{(i)} = k) \\right).\n",
        "$$\n",
        "\n",
        "### Градиент функции потерь\n",
        "\n",
        "Объединяя все компоненты, получаем выражение для градиента:\n",
        "$$\n",
        "\\nabla J(\\mathbf{W}, \\mathbf{b}) =\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle \\sum_{i=1}^{n} \\left( p_1^{(i)} - \\mathbb{1}(y^{(i)} = 1) \\right) \\cdot \\mathbf{x}^{(i)} \\\\\n",
        "\\vdots \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} \\left( p_K^{(i)} - \\mathbb{1}(y^{(i)} = K) \\right) \\cdot \\mathbf{x}^{(i)} \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} \\left( p_1^{(i)} - \\mathbb{1}(y^{(i)} = 1) \\right) \\\\\n",
        "\\vdots \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} \\left( p_K^{(i)} - \\mathbb{1}(y^{(i)} = K) \\right)\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 7. Алгоритм обновления параметров (градиентный спуск)\n",
        "\n",
        "На каждом шаге итерационного процесса параметры модели обновляются следующим образом:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}_k := \\mathbf{w}_k - \\alpha \\cdot \\frac{\\partial J}{\\partial \\mathbf{w}_k}, \\quad\n",
        "b_k := b_k - \\alpha \\cdot \\frac{\\partial J}{\\partial b_k},\n",
        "$$\n",
        "для всех $ k = 1, \\dots, K $,\n",
        "\n",
        "где $ \\alpha > 0 $ — **скорость обучения**.\n",
        "\n",
        "Процесс повторяется до достижения заданной точности или максимального числа итераций.\n",
        "\n",
        "\n",
        "\n",
        "## 8. Особенности многоклассовой логистической регрессии\n",
        "\n",
        "| Параметр | Бинарная классификация | Многоклассовая классификация |\n",
        "|---------|--------------------------|-------------------------------|\n",
        "| Выход модели | Сигмоида $ \\sigma(z) $ | Софтмакс $ \\text{Softmax}(z_1, ..., z_K) $ |\n",
        "| Функция потерь | Бинарная кросс-энтропия | Кросс-энтропия |\n",
        "| Количество параметров | $n+1$ | $K(n+1)$ |\n",
        "| Распределение целевой переменной | Бернулли | Категориальное |\n",
        "| Оптимизация | Выпуклая | Не всегда выпуклая (зависит от данных и регуляризации) |\n"
      ],
      "metadata": {
        "id": "QaCYwVtOhqGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eLM-R3WPJjqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9xx_x919Jk2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 9. Мультлебеловая классификация (Multi-label Classification)\n",
        "\n",
        "## 9.1. Постановка задачи\n",
        "\n",
        "В предыдущих разделах были рассмотрены две фундаментальные задачи классификации:\n",
        "- **Бинарная классификация** — объект принадлежит **одному из двух классов**.\n",
        "- **Многоклассовая классификация** — объект принадлежит **ровно одному из $K > 2$ классов** (взаимоисключающие категории).\n",
        "\n",
        "Однако на практике часто встречаются задачи, в которых **один объект может одновременно принадлежать нескольким классам**. Такие задачи называются **мультлебеловыми (multi-label)**.\n",
        "\n",
        "### 🔹 Формальная постановка\n",
        "\n",
        "Пусть дан объект, описываемый вектором признаков:\n",
        "$$\n",
        "\\mathbf{x} \\in \\mathbb{R}^n.\n",
        "$$\n",
        "\n",
        "Целевая переменная $\\mathbf{y}$ — это **вектор длины $K$**, где каждый элемент $y_k \\in \\{0, 1\\}$, $k = 1, \\dots, K$, указывает, принадлежит ли объект $k$-му классу:\n",
        "- $y_k = 1$ — объект принадлежит классу $k$,\n",
        "- $y_k = 0$ — объект **не принадлежит** классу $k$.\n",
        "\n",
        "В отличие от многоклассовой классификации, **не требуется**, чтобы сумма $\\sum_{k=1}^K y_k = 1$. Возможны случаи:\n",
        "- $\\sum y_k = 0$ — объект не принадлежит ни одному классу,\n",
        "- $\\sum y_k = 1$ — объект принадлежит ровно одному классу (вырожденный случай),\n",
        "- $\\sum y_k > 1$ — объект принадлежит **нескольким классам одновременно**.\n",
        "\n",
        "### 🔹 Примеры задач мультлебеловой классификации\n",
        "\n",
        "| Задача | Объект | Признаки | Метки (классы) |\n",
        "|--------|--------|----------|----------------|\n",
        "| Тегирование изображений | Фото | Пиксели, признаки CNN | $[1, 0, 1, 1]$ → \"собака\", \"улица\", \"солнце\" |\n",
        "| Классификация текста | Новостная статья | TF-IDF, эмбеддинги | $[1, 1, 0]$ → \"политика\", \"экономика\" |\n",
        "| Медицинская диагностика | Пациент | Анализы, анамнез | $[1, 0, 1]$ → \"диабет\", \"гипертония\" |\n",
        "| Классификация музыки | Аудиозапись | Спектрограммы | $[1, 1, 0, 1]$ → \"рок\", \"электроника\", \"русский язык\" |\n",
        "\n",
        "Таким образом, мультлебеловая классификация — это **обобщение бинарной классификации на случай нескольких независимых (или слабо зависимых) бинарных задач**.\n",
        "\n",
        "\n",
        "\n",
        "## 9.2. Модель мультлебеловой логистической регрессии\n",
        "\n",
        "### 🔹 Архитектура модели\n",
        "\n",
        "Для каждого класса $k = 1, \\dots, K$ определим:\n",
        "- Весовой вектор $\\mathbf{w}_k \\in \\mathbb{R}^n$,\n",
        "- Свободный член (bias) $b_k \\in \\mathbb{R}$.\n",
        "\n",
        "Вычислим **линейный прогноз** для каждого класса:\n",
        "$$\n",
        "z_k = \\mathbf{w}_k^T \\mathbf{x} + b_k.\n",
        "$$\n",
        "\n",
        "Затем применим **сигмоиду** к каждому $z_k$, чтобы получить **вероятность принадлежности к классу $k$**:\n",
        "$$\n",
        "p_k = \\sigma(z_k) = \\frac{1}{1 + e^{-z_k}}.\n",
        "$$\n",
        "\n",
        "Таким образом, модель выдает вектор вероятностей:\n",
        "$$\n",
        "\\mathbf{p}(\\mathbf{x}) = \\left( p_1, p_2, \\dots, p_K \\right) \\in (0, 1)^K.\n",
        "$$\n",
        "\n",
        "> ⚠️ Важно: в отличие от **софтмакса**, **сумма $\\sum_{k=1}^K p_k$ не обязана быть равна 1** — это ключевое отличие от многоклассовой классификации.\n",
        "\n",
        "### 🔹 Предсказание меток\n",
        "\n",
        "Чтобы получить **бинарные метки**, применяется **пороговая функция** (обычно $t = 0.5$):\n",
        "$$\n",
        "\\hat{y}_k =\n",
        "\\begin{cases}\n",
        "1, & \\text{если } p_k \\geq t, \\\\\n",
        "0, & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Таким образом, предсказанный вектор меток:\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\left( \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_K \\right) \\in \\{0, 1\\}^K.\n",
        "$$\n",
        "\n",
        "Порог $t$ может быть настроен в зависимости от задачи (например, для повышения полноты или точности).\n",
        "\n",
        "\n",
        "## 9.3. Вероятностная интерпретация\n",
        "\n",
        "В мультлебеловой классификации предполагается, что **для каждого класса $k$** целевая переменная $y_k$ подчиняется **независимому распределению Бернулли**:\n",
        "$$\n",
        "y_k \\mid \\mathbf{x} \\sim \\text{Bernoulli}(p_k), \\quad \\text{где } p_k = \\sigma(\\mathbf{w}_k^T \\mathbf{x} + b_k).\n",
        "$$\n",
        "\n",
        "Поскольку метки $y_k$ считаются **независимыми при заданных признаках** (упрощающее предположение), совместная вероятность вектора меток:\n",
        "$$\n",
        "P(\\mathbf{y} \\mid \\mathbf{x}; \\mathbf{W}, \\mathbf{b}) = \\prod_{k=1}^{K} P(y_k \\mid \\mathbf{x}; \\mathbf{w}_k, b_k) = \\prod_{k=1}^{K} \\left[ \\sigma(z_k) \\right]^{y_k} \\left[ 1 - \\sigma(z_k) \\right]^{1 - y_k}.\n",
        "$$\n",
        "\n",
        "Это выражение аналогично бинарной логистической регрессии, но **повторяется для каждого класса независимо**.\n",
        "\n",
        "\n",
        "\n",
        "## 9.4. Метод максимального правдоподобия (MLE)\n",
        "\n",
        "Пусть имеется обучающая выборка:\n",
        "$$\n",
        "D = \\left\\{ (\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}) \\right\\}_{i=1}^n,\n",
        "$$\n",
        "где $\\mathbf{y}^{(i)} \\in \\{0, 1\\}^K$ — вектор меток $i$-го объекта.\n",
        "\n",
        "### 🔹 Функция правдоподобия\n",
        "\n",
        "С учётом независимости объектов и независимости меток по классам:\n",
        "$$\n",
        "L(\\mathbf{W}, \\mathbf{b}) = \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\left[ \\sigma(z_k^{(i)}) \\right]^{y_k^{(i)}} \\left[ 1 - \\sigma(z_k^{(i)}) \\right]^{1 - y_k^{(i)}},\n",
        "$$\n",
        "где $z_k^{(i)} = \\mathbf{w}_k^T \\mathbf{x}^{(i)} + b_k$.\n",
        "\n",
        "### 🔹 Логарифмическая функция правдоподобия\n",
        "\n",
        "$$\n",
        "\\ell(\\mathbf{W}, \\mathbf{b}) = \\log L(\\mathbf{W}, \\mathbf{b}) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\left[ y_k^{(i)} \\log \\sigma(z_k^{(i)}) + (1 - y_k^{(i)}) \\log (1 - \\sigma(z_k^{(i)})) \\right].\n",
        "$$\n",
        "\n",
        "\n",
        "## 9.5. Функция потерь (бинарная кросс-энтропия по каждому классу)\n",
        "\n",
        "Минимизация отрицательного логарифма правдоподобия даёт **функцию потерь**:\n",
        "$$\n",
        "J(\\mathbf{W}, \\mathbf{b}) = -\\ell(\\mathbf{W}, \\mathbf{b}) = -\\sum_{i=1}^{n} \\sum_{k=1}^{K} \\left[ y_k^{(i)} \\log \\sigma(z_k^{(i)}) + (1 - y_k^{(i)}) \\log (1 - \\sigma(z_k^{(i)})) \\right].\n",
        "$$\n",
        "\n",
        "Эта функция называется **суммой бинарных кросс-энтропий** (sum of binary cross-entropies) и является стандартной функцией потерь в мультлебеловой классификации.\n",
        "\n",
        "> 💡 Интерпретация: мы решаем $K$ независимых бинарных задач классификации, и функция потерь — это сумма потерь по всем классам.\n",
        "\n",
        "\n",
        "## 9.6. Градиентный спуск в мультлебеловой логистической регрессии\n",
        "\n",
        "### 🔹 Вспомогательные формулы\n",
        "\n",
        "Как и ранее, используем:\n",
        "- $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
        "- $\\frac{\\partial z_k^{(i)}}{\\partial \\mathbf{w}_k} = \\mathbf{x}^{(i)}, \\quad \\frac{\\partial z_k^{(i)}}{\\partial b_k} = 1$\n",
        "\n",
        "### 🔹 Производная функции потерь по $z_k^{(i)}$\n",
        "\n",
        "Рассмотрим одно слагаемое:\n",
        "$$\n",
        "J_{ik} = - \\left[ y_k^{(i)} \\log \\sigma(z_k^{(i)}) + (1 - y_k^{(i)}) \\log (1 - \\sigma(z_k^{(i)})) \\right].\n",
        "$$\n",
        "\n",
        "Производная по $z_k^{(i)}$:\n",
        "$$\n",
        "\\frac{\\partial J_{ik}}{\\partial z_k^{(i)}} = \\sigma(z_k^{(i)}) - y_k^{(i)} = p_k^{(i)} - y_k^{(i)}.\n",
        "$$\n",
        "\n",
        "### 🔹 Частные производные по параметрам\n",
        "\n",
        "#### По $\\mathbf{w}_k$:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{w}_k} = \\sum_{i=1}^{n} \\frac{\\partial J_{ik}}{\\partial z_k^{(i)}} \\cdot \\frac{\\partial z_k^{(i)}}{\\partial \\mathbf{w}_k} = \\sum_{i=1}^{n} (p_k^{(i)} - y_k^{(i)}) \\cdot \\mathbf{x}^{(i)}.\n",
        "$$\n",
        "\n",
        "#### По $b_k$:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_k} = \\sum_{i=1}^{n} (p_k^{(i)} - y_k^{(i)}).\n",
        "$$\n",
        "\n",
        "### 🔹 Градиент функции потерь\n",
        "\n",
        "Градиент по всем параметрам — это объединение градиентов по каждому классу:\n",
        "$$\n",
        "\\nabla J(\\mathbf{W}, \\mathbf{b}) =\n",
        "\\begin{bmatrix}\n",
        "\\displaystyle \\sum_{i=1}^{n} (p_1^{(i)} - y_1^{(i)}) \\cdot \\mathbf{x}^{(i)} \\\\\n",
        "\\vdots \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} (p_K^{(i)} - y_K^{(i)}) \\cdot \\mathbf{x}^{(i)} \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} (p_1^{(i)} - y_1^{(i)}) \\\\\n",
        "\\vdots \\\\\n",
        "\\displaystyle \\sum_{i=1}^{n} (p_K^{(i)} - y_K^{(i)})\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 9.7. Алгоритм обновления параметров (градиентный спуск)\n",
        "\n",
        "На каждом шаге параметры обновляются следующим образом:\n",
        "$$\n",
        "\\mathbf{w}_k := \\mathbf{w}_k - \\alpha \\cdot \\sum_{i=1}^{n} (p_k^{(i)} - y_k^{(i)}) \\cdot \\mathbf{x}^{(i)},\n",
        "$$\n",
        "$$\n",
        "b_k := b_k - \\alpha \\cdot \\sum_{i=1}^{n} (p_k^{(i)} - y_k^{(i)}),\n",
        "$$\n",
        "для всех $k = 1, \\dots, K$.\n",
        "\n",
        "Процесс повторяется до сходимости или достижения максимального числа итераций.\n",
        "\n",
        "\n",
        "\n",
        "## 9.8. Особенности мультлебеловой классификации\n",
        "\n",
        "| Характеристика | Описание |\n",
        "|----------------|---------|\n",
        "| **Активация** | $K$ сигмоид (по одной на класс) |\n",
        "| **Функция потерь** | Сумма бинарных кросс-энтропий |\n",
        "| **Распределение** | $K$ независимых Бернулли |\n",
        "| **Сумма вероятностей** | $\\sum_{k=1}^K p_k \\neq 1$ |\n",
        "| **Предсказание** | Пороговая функция по каждому $p_k$ |\n",
        "| **Независимость меток** | Модель предполагает независимость, но на практике метки могут быть коррелированы (например, \"собака\" и \"парк\") |\n",
        "| **Количество параметров** | $K(n + 1)$ — аналогично многоклассовой, но интерпретация другая |\n",
        "\n",
        "\n",
        "\n",
        "## 9.9. Сравнение с другими типами классификации\n",
        "\n",
        "| Критерий | Бинарная | Многоклассовая | Мультлебеловая |\n",
        "|--------|--------|----------------|----------------|\n",
        "| Количество классов | 2 | $K > 2$ | $K \\geq 2$ |\n",
        "| Один объект — сколько классов? | 1 | 1 (взаимоисключающие) | $\\geq 0$ (может быть несколько) |\n",
        "| Выход модели | $\\sigma(z) \\in (0,1)$ | $\\text{Softmax}(z_1,\\dots,z_K)$ | $[\\sigma(z_1), \\dots, \\sigma(z_K)]$ |\n",
        "| Сумма выходов | — | = 1 | $\\neq 1$ |\n",
        "| Функция потерь | Бинарная кросс-энтропия | Кросс-энтропия | Сумма бинарных кросс-энтропий |\n",
        "| Распределение | Бернулли | Категориальное | $K$ независимых Бернулли |\n",
        "| Пример | Спам/не спам | Цифра 0–9 | Теги фото: кот, улица, солнце |\n",
        "| Предсказание | $\\hat{y} = [p \\geq 0.5]$ | $\\hat{y} = \\arg\\max_k p_k$ | $\\hat{y}_k = [p_k \\geq 0.5]$ |\n",
        "\n",
        "\n",
        "\n",
        "## 9.10. Метрики качества в мультлебеловой классификации\n",
        "\n",
        "Поскольку каждый класс — это отдельная бинарная задача, метрики можно считать **по каждому классу отдельно**, а затем усреднять.\n",
        "\n",
        "### 🔹 Основные метрики\n",
        "\n",
        "Для каждого класса $k$ можно вычислить:\n",
        "- **Accuracy**, **Precision**, **Recall**, **F1-score** — как в бинарной классификации.\n",
        "\n",
        "Затем применяются стратегии усреднения:\n",
        "- **Macro**: среднее по классам\n",
        "- **Micro**: усреднение по всем объектам (глобальные TP, FP и т.д.)\n",
        "- **Weighted**: с учётом доли объектов каждого класса\n",
        "\n",
        "### 🔹 Специфические метрики\n",
        "\n",
        "| Метрика | Формула / описание |\n",
        "|--------|--------------------|\n",
        "| **Hamming Loss** | Доля неправильно предсказанных меток: $\\frac{1}{nK} \\sum_{i=1}^n \\sum_{k=1}^K \\mathbf{1}(\\hat{y}_k^{(i)} \\neq y_k^{(i)})$ — чем меньше, тем лучше |\n",
        "| **Subset Accuracy** | Доля объектов, у которых **все метки предсказаны верно**: $\\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}(\\hat{\\mathbf{y}}^{(i)} = \\mathbf{y}^{(i)})$ — строгая метрика |\n",
        "| **F1-micro** | Глобальный F1, вычисленный по суммарным TP, FP, FN |\n",
        "| **Jaccard Similarity** | $\\frac{|\\hat{\\mathbf{y}} \\cap \\mathbf{y}|}{|\\hat{\\mathbf{y}} \\cup \\mathbf{y}|}$ — похожесть предсказанных и истинных меток |\n",
        "\n",
        "\n",
        "\n",
        "## 9.11. Реализация в `sklearn`\n",
        "\n",
        "В библиотеке `scikit-learn` мультлебеловые задачи поддерживаются через:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Способ 1: OneVsRestClassifier\n",
        "model = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Способ 2: MultiOutputClassifier (если метки независимы)\n",
        "model = MultiOutputClassifier(LogisticRegression())\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)  # список вероятностей по классам\n",
        "```\n",
        "\n",
        "> ⚠️ `OneVsRestClassifier` — наиболее распространённый выбор для мультлебеловой логистической регрессии.\n",
        "\n",
        "\n",
        "\n",
        "## 9.12. Выводы\n",
        "\n",
        "- **Мультлебеловая классификация** — это задача, в которой **один объект может принадлежать нескольким классам одновременно**.\n",
        "- Модель использует **$K$ сигмоид** и **сумму бинарных кросс-энтропий** как функцию потерь.\n",
        "- Предполагается **независимость меток**, хотя на практике они могут быть коррелированы.\n",
        "- Подход обобщает бинарную классификацию на многометочный случай.\n",
        "- Требует специальных метрик: Hamming Loss, Subset Accuracy, F1-macro/micro и др.\n",
        "- Реализуется в `sklearn` через `OneVsRestClassifier` или `MultiOutputClassifier`.\n"
      ],
      "metadata": {
        "id": "cumZ0QUmJk5K"
      }
    }
  ]
}