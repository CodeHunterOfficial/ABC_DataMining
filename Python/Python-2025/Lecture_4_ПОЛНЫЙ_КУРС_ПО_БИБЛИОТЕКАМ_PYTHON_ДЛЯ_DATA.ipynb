{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPj4PT9W/ul7scfMkEwuLtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/Python/Python-2025/Lecture_4_%D0%9F%D0%9E%D0%9B%D0%9D%D0%AB%D0%99_%D0%9A%D0%A3%D0%A0%D0%A1_%D0%9F%D0%9E_%D0%91%D0%98%D0%91%D0%9B%D0%98%D0%9E%D0%A2%D0%95%D0%9A%D0%90%D0%9C_PYTHON_%D0%94%D0%9B%D0%AF_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 1: NUMPY — ФУНДАМЕНТ НАУЧНЫХ ВЫЧИСЛЕНИЙ\n",
        "\n",
        "## РАЗДЕЛ I. Введение в ndarray и архитектуру NumPy\n",
        "\n",
        "### 1.1. Роль NumPy в экосистеме Python\n",
        "\n",
        "**NumPy** (*Numerical Python*) является краеугольным камнем современной экосистемы научных вычислений на языке Python. Эта библиотека де-факто стала стандартом для эффективных численных операций и служит основой для большинства инструментов в области анализа данных, машинного обучения и научной визуализации.\n",
        "\n",
        "Фундаментальное значение NumPy заключается в его способности преодолевать ограничения интерпретируемого языка Python. За счёт высокооптимизированных вычислительных ядер, написанных на C и Fortran, NumPy предоставляет пользователю простой и элегантный синтаксис для выполнения сложных математических операций, обеспечивая при этом производительность, сопоставимую с низкоуровневыми языками. Эта эффективность критически важна при работе с большими объёмами данных.\n",
        "\n",
        "Области применения NumPy чрезвычайно широки — от академических исследований до промышленного анализа. Например, NumPy сыграл ключевую роль в обработке данных коллаборации LIGO, что привело к подтверждению существования гравитационных волн. В машинном обучении NumPy лежит в основе реализаций таких библиотек, как XGBoost и LightGBM, а также является основой для визуализационных инструментов, включая Matplotlib, Seaborn и Plotly. Вместе с SciPy NumPy формирует обязательный набор инструментов для любого исследователя или разработчика, работающего с числовыми данными.\n",
        "\n",
        "> **Пример: простое сложение массивов — сравнение с Python-списками**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Стандартный список Python\n",
        "python_list = list(range(1000000))\n",
        "%timeit [x + 1 for x in python_list]  # медленно: интерпретируемый цикл\n",
        "\n",
        "# Массив NumPy\n",
        "numpy_array = np.arange(1000000)\n",
        "%timeit numpy_array + 1  # быстро: векторизованная операция\n",
        "```\n",
        "\n",
        "> *Пояснение:* В этом примере демонстрируется разница в производительности между векторизованной операцией над `ndarray` и циклом по обычному списку. Время выполнения векторизованной операции может быть в десятки или сотни раз меньше.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Основы структуры `ndarray`\n",
        "\n",
        "Центральным объектом NumPy является **`ndarray`** (*N-dimensional array*) — контейнер для хранения **гомогенных** данных (все элементы одного типа) в непрерывном или почти непрерывном блоке памяти. Это фундаментальное отличие от стандартных списков Python, которые являются гетерогенными и хранят ссылки на объекты, что значительно снижает эффективность при численных вычислениях.\n",
        "\n",
        "#### Гомогенность и `dtype` (тип данных)\n",
        "\n",
        "Ключевой характеристикой массива является его **тип данных** (`dtype`), который определяет, как элементы интерпретируются и хранятся в памяти. Например, `np.int32` обозначает 32-битное целое, а `np.float64` — 64-битное число с плавающей точкой.\n",
        "\n",
        "Явное управление `dtype` позволяет контролировать потребление памяти и избегать численных ошибок. Например, если сохранить значение `128` в массив типа `np.int8`, диапазон которого ограничен значениями от –128 до 127, результат будет неверным — произойдёт переполнение, и значение «обрежется» до –128. В научных и инженерных расчётах подобные ошибки недопустимы.\n",
        "\n",
        "При операциях между массивами разных типов NumPy автоматически применяет **правила продвижения типов** (*type promotion*), выбирая общий тип, способный вместить все исходные значения без потерь. Например, сложение массивов типов `np.uint32` и `np.int32` приведёт к массиву типа `np.int64`, который безопасно охватывает диапазон обоих исходных типов.\n",
        "\n",
        "> **Пример: контроль типа данных и последствия переполнения**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Опасный пример с переполнением\n",
        "arr_int8 = np.array([127], dtype=np.int8)\n",
        "print(arr_int8 + 1)  # Вывод: [-128] — переполнение!\n",
        "\n",
        "# Безопасный пример с автоматическим продвижением типа\n",
        "arr_uint32 = np.array([1000], dtype=np.uint32)\n",
        "arr_int32 = np.array([-500], dtype=np.int32)\n",
        "result = arr_uint32 + arr_int32\n",
        "print(result, result.dtype)  # Вывод: [500] dtype('int64')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Первый пример иллюстрирует, как переполнение может привести к некорректным результатам. Второй — как NumPy автоматически выбирает безопасный тип при смешанных операциях.\n",
        "\n",
        "#### Архитектурные преимущества\n",
        "\n",
        "Гомогенность и непрерывное хранение позволяют NumPy использовать **C- или Fortran-континуальный порядок** размещения данных в памяти. Это критически важно для эффективной передачи блоков данных в низкоуровневые библиотеки, такие как **BLAS** и **LAPACK**, которые реализуют высокооптимизированные линейные алгебраические операции. Благодаря этому достигается **экспоненциальный выигрыш в скорости** по сравнению с операциями над стандартными списками.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. Сравнение производительности: векторизация\n",
        "\n",
        "**Векторизация** — ключевой принцип высокой производительности в NumPy. Вместо того чтобы писать явные циклы `for` в Python, которые медленны из-за интерпретируемой природы языка, операции применяются сразу ко всему массиву через вызов оптимизированных функций на C или Fortran.\n",
        "\n",
        "На практике это означает, что **арифметические, логические и многие другие операции автоматически распространяются на все элементы массива**. Если же разработчик по неопытности оставляет цикл Python внутри критического участка кода, этот участок неизбежно становится **«узким местом»**, замедляя всю программу.\n",
        "\n",
        "> **Пример: векторизованная функция против цикла**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 10, 1000000)\n",
        "\n",
        "# Невекторизованный (медленный) подход\n",
        "def slow_sin(x):\n",
        "    return np.array([np.sin(val) for val in x])\n",
        "\n",
        "# Векторизованный (быстрый) подход\n",
        "def fast_sin(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "%timeit slow_sin(x)  # медленно\n",
        "%timeit fast_sin(x)  # быстро\n",
        "```\n",
        "\n",
        "> *Пояснение:* Векторизованный вызов `np.sin(x)` выполняется напрямую в C, без итераций в Python. Это делает его значительно быстрее даже для простых функций.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ II. Создание и инициализация массивов\n",
        "\n",
        "### 2.1. Создание массивов из последовательностей (`np.array`)\n",
        "\n",
        "Самый прямой способ создать массив — преобразовать стандартные Python-структуры, такие как списки или кортежи, с помощью функции `np.array(object, dtype=None)`.\n",
        "\n",
        "Уровень вложенности последовательности определяет размерность массива: одномерный список создаёт вектор, список списков — матрицу, и так далее. В научных задачах **рекомендуется явно указывать `dtype`**, особенно если требуется контролировать точность или потребление памяти.\n",
        "\n",
        "> **Пример: создание массивов разной размерности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 1D-массив\n",
        "vector = np.array([1, 2, 3])\n",
        "\n",
        "# 2D-массив\n",
        "matrix = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "\n",
        "# Явное указание типа\n",
        "int_array = np.array([1, 2, 3], dtype=np.int64)\n",
        "float_array = np.array([1, 2, 3], dtype=np.float32)\n",
        "\n",
        "print(\"vector:\", vector)\n",
        "print(\"matrix:\\n\", matrix)\n",
        "print(\"int_array dtype:\", int_array.dtype)\n",
        "print(\"float_array dtype:\", float_array.dtype)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Явное задание типа помогает избежать неожиданных преобразований и экономит память, например, при использовании `float32` вместо `float64`, если задача допускает снижение точности.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Создание массивов с фиксированными значениями\n",
        "\n",
        "Для инициализации вычислительных пространств или создания «заготовок» под результаты используются специализированные функции. `np.zeros(shape, dtype=float64)` создаёт массив, заполненный нулями, `np.ones(shape, dtype=float64)` — единицами, а `np.full(shape, fill_value, dtype=None)` — произвольным значением.\n",
        "\n",
        "Эти функции особенно полезны при настройке начальных условий в численных методах или выделении памяти под промежуточные результаты, когда известна итоговая форма данных, но сами значения ещё не вычислены.\n",
        "\n",
        "> **Пример: инициализация массивов**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "zeros_2d = np.zeros((3, 4))\n",
        "ones_1d = np.ones(5, dtype=np.int32)\n",
        "custom = np.full((2, 2), 7.5)\n",
        "\n",
        "print(\"zeros_2d:\\n\", zeros_2d)\n",
        "print(\"ones_1d:\", ones_1d)\n",
        "print(\"custom:\\n\", custom)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такие массивы часто используются в алгоритмах, где нужно «собирать» результаты постепенно — например, при построении матрицы корреляций или накоплении градиентов в итеративных методах оптимизации.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. Создание регулярных последовательностей: `arange` и `linspace`\n",
        "\n",
        "Для генерации одномерных числовых последовательностей NumPy предоставляет две основные функции.\n",
        "\n",
        "**`np.arange(start, stop, step)`** создаёт последовательность с фиксированным шагом и является аналогом встроенной функции `range`, но возвращает `ndarray`. Однако её **не рекомендуется использовать с дробным шагом** из-за накопления ошибок округления, присущих арифметике с плавающей точкой.\n",
        "\n",
        "**`np.linspace(start, stop, num)`** создаёт **ровно `num` точек**, равномерно распределённых между `start` и `stop`, включая обе границы. Эта функция предпочтительна при построении численных сеток, дискретизации функций и любых задачах, где важен точный контроль над количеством и расположением точек.\n",
        "\n",
        "> **Пример: сравнение `arange` и `linspace`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# arange: риск неточности при float-шаге\n",
        "arr1 = np.arange(0, 1, 0.1)\n",
        "print(\"arange:\", arr1)\n",
        "\n",
        "# linspace: гарантированное количество точек\n",
        "arr2 = np.linspace(0, 1, 10)\n",
        "print(\"linspace:\", arr2)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `linspace` более надёжен при работе с вещественными числами, особенно в задачах, требующих точного контроля над границами и количеством точек, таких как построение графиков или решение дифференциальных уравнений.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Воспроизводимая генерация случайных чисел (RNG)\n",
        "\n",
        "Начиная с версии **1.17**, NumPy использует современный API генерации псевдослучайных чисел через объекты **`Generator`**. Этот подход основан на более быстрых и статистически надёжных алгоритмах, таких как **PCG64**, по сравнению со старым классом `RandomState`.\n",
        "\n",
        "#### Управление воспроизводимостью\n",
        "\n",
        "Для воспроизводимости экспериментов и симуляций необходимо инициализировать генератор с фиксированным **зерном** (*seed*):\n",
        "\n",
        "```python\n",
        "rng = np.random.default_rng(seed=42)\n",
        "random_array = rng.random((2, 3))  # массив 2×3 из [0, 1)\n",
        "```\n",
        "\n",
        "#### Роль `SeedSequence`\n",
        "\n",
        "Внутри `Generator` использует **`SeedSequence`** — механизм, который «перемешивает» входное зерно в надёжное начальное состояние. Это позволяет избегать проблем с «плохими» зёрнами, создавать **независимые подпотоки случайности** через метод `.spawn()` и безопасно использовать генерацию в распределённых вычислениях.\n",
        "\n",
        "#### Векторизованные случайные операции\n",
        "\n",
        "`Generator` поддерживает не только базовые распределения, но и **векторизованные операции**, такие как `rng.permuted(x, axis=1)`, которая перемешивает срезы массива вдоль указанной оси, или `rng.shuffle(x)`, которая перемешивает массив *in-place*.\n",
        "\n",
        "> **Пример: генерация и перестановка**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "# Генерация случайных чисел\n",
        "data = rng.normal(loc=0.0, scale=1.0, size=(3, 4))\n",
        "print(\"Исходные данные:\\n\", data)\n",
        "\n",
        "# Перемешивание по строкам\n",
        "shuffled = rng.permuted(data, axis=1)\n",
        "print(\"После перемешивания по строкам:\\n\", shuffled)\n",
        "\n",
        "# In-place перемешивание (по умолчанию — по первой оси)\n",
        "copy_data = data.copy()\n",
        "rng.shuffle(copy_data)\n",
        "print(\"In-place shuffle (по строкам):\\n\", copy_data)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такие функции широко применяются в статистике — например, при бутстреппинге, кросс-валидации или генерации случайных разбиений данных для обучения моделей машинного обучения."
      ],
      "metadata": {
        "id": "x9mLyuvqebLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## РАЗДЕЛ III. Манипуляции формой и осями\n",
        "\n",
        "### 3.1. Форма и описание массива\n",
        "\n",
        "Каждый массив в NumPy характеризуется набором неизменяемых атрибутов, которые полностью описывают его структуру и содержимое. Ключевыми из них являются: `.shape` — кортеж, задающий количество элементов вдоль каждой оси (например, `(3, 4)` означает 3 строки и 4 столбца); `.ndim` — целое число, указывающее ранг массива (количество измерений); `.size` — общее число элементов, равное произведению всех компонент `.shape`; и `.dtype` — тип данных элементов, такой как `int64` или `float32`.\n",
        "\n",
        "Эти атрибуты доступны только для чтения и фиксированы для данного объекта `ndarray`. Любое изменение формы требует создания нового представления или копии данных, но не модификации исходного объекта.\n",
        "\n",
        "> **Пример: основные атрибуты массива**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12).reshape(3, 4)\n",
        "print(\"Массив a:\\n\", a)\n",
        "print(\"shape:\", a.shape)      # (3, 4)\n",
        "print(\"ndim:\", a.ndim)        # 2\n",
        "print(\"size:\", a.size)        # 12\n",
        "print(\"dtype:\", a.dtype)      # int64 (на большинстве систем)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти атрибуты являются первым шагом в диагностике и понимании структуры любого числового массива.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Изменение формы массива (`reshape` и `ravel`)\n",
        "\n",
        "NumPy позволяет эффективно **переинтерпретировать** данные в памяти без их физического перемещения, при условии, что общее количество элементов сохраняется. Это достигается за счёт изменения метаданных о форме и порядке размещения.\n",
        "\n",
        "Функция `np.reshape(a, newshape)` возвращает массив с новой формой. Один из размеров может быть задан как `-1`, что позволяет NumPy автоматически вычислить его на основе `a.size`. Параметр `order` управляет порядком: `'C'` (построчный, по умолчанию) или `'F'` (постолбцовый), что критично при работе с данными, поступающими из Fortran-кода или внешних библиотек.\n",
        "\n",
        "> **Пример: reshape с автоматическим размером**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12)               # shape: (12,)\n",
        "b = a.reshape(3, -1)            # shape: (3, 4)\n",
        "c = a.reshape(-1, 2, 2)         # shape: (3, 2, 2)\n",
        "\n",
        "print(\"Исходный массив:\", a)\n",
        "print(\"После reshape(3, -1):\\n\", b)\n",
        "print(\"После reshape(-1, 2, 2):\\n\", c)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `-1` устраняет необходимость ручного расчёта размеров и снижает вероятность ошибок.\n",
        "\n",
        "Для преобразования многомерного массива в одномерный существуют две функции. `np.ravel(a)` возвращает **представление (view)**, если это возможно, не копируя данные и обеспечивая высокую производительность. В отличие от него, `a.flatten()` всегда создаёт **новую копию** данных, что гарантирует независимость от исходного массива, но увеличивает потребление памяти и время выполнения.\n",
        "\n",
        "> **Пример: разница между ravel и flatten**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "flat_view = np.ravel(a)\n",
        "flat_copy = a.flatten()\n",
        "\n",
        "# Изменяем view — исходный массив тоже изменится\n",
        "flat_view[0] = 999\n",
        "print(\"После изменения view:\\n\", a)        # [[999, 2], [3, 4]]\n",
        "\n",
        "# Копия не влияет на оригинал\n",
        "flat_copy[0] = 0\n",
        "print(\"После изменения копии:\\n\", a)       # всё ещё [[999, 2], [3, 4]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* В вычислительно интенсивных задачах стоит отдавать предпочтение `ravel()`, чтобы избежать ненужного копирования памяти.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Управление осями (транспонирование и пермутация)\n",
        "\n",
        "Изменение порядка осей — частая операция при подготовке данных для матричных операций, нейросетевых архитектур или визуализации. Для 2D-массивов классическое матричное транспонирование выполняется через атрибут `.T`. Для массивов произвольного ранга используется функция `np.transpose(a)`, которая по умолчанию инвертирует порядок всех осей, но может принимать явный кортеж `axes`, задающий новую перестановку.\n",
        "\n",
        "Обе операции возвращают **представление**, если структура памяти позволяет, что делает их крайне эффективными.\n",
        "\n",
        "> **Пример: транспонирование**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.random.rand(2, 3, 4)  # shape: (2, 3, 4)\n",
        "b = a.T                      # shape: (4, 3, 2)\n",
        "c = np.transpose(a, axes=(2, 0, 1))  # явный порядок: (4, 2, 3)\n",
        "\n",
        "print(\"Исходная форма:\", a.shape)\n",
        "print(\"После .T:\", b.shape)\n",
        "print(\"После transpose(2,0,1):\", c.shape)\n",
        "```\n",
        "\n",
        "Для более гибкого перемещения отдельных осей применяется функция `np.moveaxis(a, source, destination)`. Она позволяет переместить одну или несколько осей в новые позиции, сохранив относительный порядок остальных. Это особенно полезно при работе с тензорами, например, при преобразовании формата изображений из `(batch, height, width, channels)` в `(batch, channels, height, width)` для совместимости с фреймворками глубокого обучения.\n",
        "\n",
        "> **Пример: moveaxis**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Тензор: (batch, height, width, channels) → хотим (batch, channels, height, width)\n",
        "x = np.random.rand(10, 64, 64, 3)\n",
        "x_moved = np.moveaxis(x, source=3, destination=1)  # перемещаем ось 3 на позицию 1\n",
        "print(\"Новая форма:\", x_moved.shape)  # (10, 3, 64, 64)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `moveaxis` делает код читаемее и безопаснее, чем ручное перечисление всех осей в `transpose`.\n",
        "\n",
        "Ещё один важный приём — добавление новой оси размером 1 с помощью `np.newaxis` (псевдоним для `None`). Это ключевой механизм для подготовки массивов к бродкастингу. Например, вектор формы `(n,)` можно превратить в столбец `(n, 1)` или строку `(1, n)`, что позволяет выполнять операции, иначе запрещённые из-за несовместимости форм.\n",
        "\n",
        "> **Пример: превращение вектора в столбец**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(4)           # shape: (4,)\n",
        "b = a[:, np.newaxis]       # shape: (4, 1)\n",
        "c = a[np.newaxis, :]       # shape: (1, 4)\n",
        "\n",
        "print(\"Исходный:\", a.shape)\n",
        "print(\"Столбец:\", b.shape)\n",
        "print(\"Строка:\", c.shape)\n",
        "\n",
        "# Теперь можно, например, вычесть вектор из каждой строки матрицы\n",
        "matrix = np.random.rand(4, 5)\n",
        "result = matrix - b  # broadcasting: (4,5) - (4,1) → (4,5)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без добавления оси такие операции были бы невозможны, что подчёркивает центральную роль `np.newaxis` в векторизованных вычислениях.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ IV. Доступ к элементам: индексация и маскирование\n",
        "\n",
        "NumPy предоставляет несколько мощных и взаимодополняющих механизмов доступа к данным, каждый из которых имеет свои особенности с точки зрения производительности, гибкости и поведения в памяти.\n",
        "\n",
        "### 4.1. Базовая индексация и срезы\n",
        "\n",
        "Базовая индексация включает доступ к отдельным элементам и создание срезов с использованием целых чисел и стандартного синтаксиса срезов (`start:stop:step`). Эта форма индексации всегда возвращает **представление (view)**, то есть новый объект массива, который разделяет память с исходным. Это делает операцию чрезвычайно быстрой, но требует осторожности: любое изменение среза напрямую влияет на исходный массив.\n",
        "\n",
        "> **Пример: срез как view**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "sub = a[0, :]  # первая строка — view\n",
        "sub[0] = 999\n",
        "print(\"Изменённый массив:\\n\", a)  # [[999, 2, 3], [4, 5, 6]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Если изоляция данных необходима, следует явно вызывать метод `.copy()`.\n",
        "\n",
        "### 4.2. Булева индексация (маскирование)\n",
        "\n",
        "Булева индексация использует массив логических значений (`True`/`False`) той же формы, что и исходный массив, для выбора элементов, где маска равна `True`. Этот механизм является основным инструментом для фильтрации данных по произвольному условию, замены значений или удаления пропущенных (NaN) или некорректных наблюдений.\n",
        "\n",
        "В отличие от базовой индексации, булево маскирование **всегда возвращает копию** данных в виде **одномерного массива**, независимо от того, к каким осям применяется маска.\n",
        "\n",
        "> **Пример: фильтрация и замена**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2, np.nan, 3])\n",
        "\n",
        "# Замена отрицательных чисел на 0\n",
        "x[x < 0] = 0\n",
        "print(\"После замены отрицательных:\", x)\n",
        "\n",
        "# Удаление NaN (возвращает копию!)\n",
        "clean_x = x[~np.isnan(x)]\n",
        "print(\"Без NaN:\", clean_x)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Булево маскирование является основой для условной обработки данных в векторизованном стиле.\n",
        "\n",
        "### 4.3. Продвинутая (fancy) целочисленная индексация\n",
        "\n",
        "Продвинутая индексация использует массивы или списки целых чисел для выбора произвольных, не обязательно смежных или упорядоченных элементов. Элементы могут повторяться, а их порядок в результате будет точно соответствовать порядку индексов.\n",
        "\n",
        "При передаче массивов индексов для нескольких осей они **согласуются (broadcastятся)** и комбинируются поэлементно. Например, если передать два массива длины 3 для строк и столбцов, будет выбрано ровно 3 элемента — на пересечении `(row[0], col[0])`, `(row[1], col[1])` и так далее.\n",
        "\n",
        "Этот тип индексации **всегда создаёт копию** данных и возвращает массив, форма которого определяется результатом broadcasting индексных массивов.\n",
        "\n",
        "> **Пример: выбор конкретных позиций**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12).reshape(3, 4)\n",
        "print(\"Массив a:\\n\", a)\n",
        "\n",
        "# Выбор элементов (0,1), (1,2), (2,3)\n",
        "rows = np.array([0, 1, 2])\n",
        "cols = np.array([1, 2, 3])\n",
        "selected = a[rows, cols]\n",
        "print(\"Выбранные элементы:\", selected)  # [1, 6, 11]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Результат представляет собой одномерный массив, даже если исходный массив был многомерным, что важно учитывать при проектировании алгоритмов.\n",
        "\n",
        "### 4.4. Комбинированная индексация: функция `np.ix_`\n",
        "\n",
        "Прямая передача двух отдельных массивов индексов для строк и столбцов не даёт полной подматрицы, а выбирает только диагональные пары. Для получения **всех комбинаций** индексов — то есть подматрицы на пересечении заданных строк и столбцов — используется функция `np.ix_`.\n",
        "\n",
        "Эта функция преобразует одномерные массивы индексов в совместимые формы: первый массив превращается в столбец `(n, 1)`, второй — в строку `(1, m)`. Это позволяет механизму бродкастинга создать полную двумерную сетку индексов.\n",
        "\n",
        "> **Пример: выбор подматрицы с помощью `np.ix_`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[ 0,  1,  2],\n",
        "              [ 3,  4,  5],\n",
        "              [ 6,  7,  8],\n",
        "              [ 9, 10, 11]])\n",
        "\n",
        "# Строки с чётной суммой\n",
        "rows_mask = (x.sum(axis=1) % 2 == 0)  # [True, False, True, False]\n",
        "# Столбцы 0 и 2\n",
        "cols = np.array([0, 2])\n",
        "\n",
        "# Правильный способ: создать полную подматрицу\n",
        "subset = x[np.ix_(rows_mask, cols)]\n",
        "print(\"Подматрица на пересечении:\\n\", subset)\n",
        "# Результат:\n",
        "# [[0  2]\n",
        "#  [6  8]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.ix_` является незаменимым инструментом для сложной фильтрации по нескольким измерениям и обеспечивает полный контроль над структурой результата.\n",
        "\n",
        "---\n",
        "\n",
        "> **Примечание:** Эта часть завершает вводный обзор ключевых возможностей `ndarray`. В следующем модуле будут рассмотрены **математические функции**, **агрегации**, **broadcasting** и **производительность** в NumPy."
      ],
      "metadata": {
        "id": "TMDG0VuWec0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## РАЗДЕЛ V. Векторизованные операции и бродкастинг\n",
        "\n",
        "### 5.1. Универсальные функции (UFuncs)\n",
        "\n",
        "**Универсальные функции** (*universal functions*, или **UFuncs**) составляют основу векторизованных вычислений в NumPy. Это функции, которые выполняют **поэлементные операции** над массивами с высокой скоростью, поскольку их внутренние циклы реализованы на C и не зависят от интерпретируемой природы Python. К этому классу относятся арифметические операции (`np.add`, `np.multiply`), элементарные математические функции (`np.sin`, `np.exp`, `np.log`) и логические операторы (`np.greater`, `np.equal`, `np.logical_and`). Все UFuncs автоматически применяют правила бродкастинга, что позволяет им корректно работать с массивами разной, но совместимой формы, обеспечивая при этом максимальную производительность и экономию памяти.\n",
        "\n",
        "> **Пример: UFunc в действии**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "# Поэлементное умножение через UFunc\n",
        "result = np.multiply(a, b)  # эквивалентно a * b\n",
        "print(result)  # [4 10 18]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Благодаря UFuncs операции над массивами выглядят как обычные арифметические выражения, но выполняются на уровне C — без циклов Python.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Теория бродкастинга (Broadcasting Theory)\n",
        "\n",
        "**Бродкастинг** — это мощный механизм, позволяющий NumPy выполнять арифметические операции над массивами **разной формы**, не копируя данные физически. Вместо дублирования памяти он логически «растягивает» меньший массив при доступе к элементам. Это критически важно для эффективного использования памяти и производительности, особенно при работе с большими тензорами.\n",
        "\n",
        "Совместимость форм определяется строгими правилами. Сравнение начинается с последней (самой правой) оси и движется влево. Для каждой пары осей допускаются два случая: либо их размеры равны, либо один из них равен единице. Если один массив имеет меньше осей, он виртуально дополняется осями размером 1 слева. Если ни одно из условий не выполняется для хотя бы одной пары осей, возникает исключение `ValueError: operands could not be broadcast together`.\n",
        "\n",
        "> **Пример: проверка совместимости**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Формы: (3, 4) и (4,) → совместимы: (3,4) vs (1,4) → (3,4)\n",
        "A = np.ones((3, 4))\n",
        "b = np.array([1, 2, 3, 4])\n",
        "C = A + b  # OK\n",
        "\n",
        "# Формы: (2, 3) и (3, 2) → НЕсовместимы: 3 ≠ 2 и ни одно ≠ 1\n",
        "try:\n",
        "    D = np.ones((2, 3)) + np.ones((3, 2))\n",
        "except ValueError as e:\n",
        "    print(\"Ошибка:\", e)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Бродкастинг — это не копирование, а **логическая «растяжка»** данных при доступе к памяти.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Практические примеры бродкастинга\n",
        "\n",
        "Наиболее простой случай — операция массива со скаляром. Скаляр «виртуально растягивается» до формы массива, и операция применяется поэлементно. Более сложный и часто встречающийся сценарий — добавление одномерного вектора к каждой строке двумерной матрицы. Если длина вектора совпадает с числом столбцов матрицы, он автоматически добавляется к **каждой строке** без необходимости явного цикла или копирования.\n",
        "\n",
        "Для создания **всех возможных комбинаций** между двумя векторами используется приём с `np.newaxis`. Преобразуя один вектор в столбец `(n, 1)`, а другой оставляя строкой `(m,)`, можно построить двумерную матрицу результатов `(n, m)`, что эквивалентно внешнему произведению, но применимо к любой бинарной операции.\n",
        "\n",
        "> **Пример: внешняя сумма**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([0, 10, 20])    # (3,)\n",
        "b = np.array([1, 2])         # (2,)\n",
        "\n",
        "# Превращаем a в столбец: (3, 1)\n",
        "outer_sum = a[:, np.newaxis] + b  # (3,1) + (2,) → (3,2)\n",
        "print(\"Внешняя сумма:\\n\", outer_sum)\n",
        "# [[ 1  2]\n",
        "#  [11 12]\n",
        "#  [21 22]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой приём часто используется для построения сеток значений, вычисления попарных расстояний или ядерных функций.\n",
        "\n",
        "Ключевое преимущество бродкастинга — **экономия памяти**. Например, при умножении изображения формы `(256, 256, 3)` на скаляр NumPy не создаёт копию объёмом в сотни мегабайт, а выполняет операцию на лету, что делает его незаменимым в задачах компьютерного зрения и обработки сигналов.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Условные операции: `np.where()`\n",
        "\n",
        "Функция `np.where(condition, x, y)` представляет собой **векторизованный аналог конструкции `if-else`**. Для каждого элемента результирующего массива выбирается значение из `x` или `y` в зависимости от соответствующего логического условия. Эта функция возвращает новый массив, что делает её поведение предсказуемым и безопасным по сравнению с in-place операциями.\n",
        "\n",
        "> **Пример: замена значений по условию**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "# Заменить отрицательные на 0, положительные — на 1, нули оставить\n",
        "result = np.where(x < 0, 0, np.where(x > 0, 1, x))\n",
        "print(result)  # [0 0 0 1 1]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.where` особенно полезен в сложных условиях, где требуется несколько уровней ветвления, и позволяет избежать цепочек булевых масок.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ VI. Математика и статистика массивов\n",
        "\n",
        "NumPy предоставляет богатый набор **векторизованных агрегационных функций**, которые могут применяться ко всему массиву или вдоль заданной оси. Эти функции реализованы на C и обеспечивают высокую производительность даже для больших наборов данных.\n",
        "\n",
        "### 6.1. Статистические агрегации\n",
        "\n",
        "К базовым статистическим функциям относятся `np.sum`, `np.mean`, `np.std`, `np.min` и `np.max`. Все они поддерживают параметр `axis`, который определяет, **вдоль какой оси «схлопывается»** массив. Например, при `axis=0` операция выполняется по строкам (результат — вектор по столбцам), а при `axis=1` — по столбцам (результат — вектор по строкам).\n",
        "\n",
        "> **Пример: агрегация по осям**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "\n",
        "print(\"Сумма по всему массиву:\", np.sum(A))        # 21\n",
        "print(\"Сумма по строкам (axis=1):\", np.sum(A, axis=1))  # [6, 15]\n",
        "print(\"Сумма по столбцам (axis=0):\", np.sum(A, axis=0)) # [5, 7, 9]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Параметр `axis` является ключевым для многомерного анализа и часто используется в предобработке данных для машинного обучения.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Кумулятивные операции: `np.cumsum`\n",
        "\n",
        "Функция `np.cumsum(a, axis=None)` возвращает массив с **накопленными суммами**. Важно понимать, что из-за особенностей машинной арифметики с плавающей точкой, последний элемент результата `np.cumsum(a)[-1]` **может не совпадать** с `np.sum(a)`. Причина в том, что `np.sum` использует **оптимизированные алгоритмы суммирования** (например, pairwise summation), которые минимизируют ошибку округления, тогда как `cumsum` выполняет строгую последовательную аккумуляцию, накапливая ошибку на каждом шаге.\n",
        "\n",
        "> **Пример: разница в точности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Массив с очень малыми и большими числами\n",
        "x = np.array([1e10, 1, -1e10, 1])\n",
        "\n",
        "total_sum = np.sum(x)\n",
        "cumsum_last = np.cumsum(x)[-1]\n",
        "\n",
        "print(\"np.sum(x):\", total_sum)           # 2.0\n",
        "print(\"cumsum(x)[-1]:\", cumsum_last)     # 0.0 — потеря точности!\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для итоговых сумм предпочтительнее `np.sum`; `cumsum` следует использовать только если требуется вся последовательность накопленных значений.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. Вычисление квантилей: `np.percentile` и `np.quantile`\n",
        "\n",
        "Функция `np.percentile(a, q, axis=None, method='linear')` вычисляет **перцентили** (квантили, умноженные на 100). Значение `q=50` соответствует медиане, `q=25` и `q=75` — первому и третьему квартилям. Параметр `method` позволяет выбрать алгоритм интерполяции между соседними точками данных. Доступны такие варианты, как `'linear'` (по умолчанию), `'lower'`, `'higher'`, `'midpoint'` и `'inverted_cdf'`, последний из которых соответствует классическому статистическому определению квантиля.\n",
        "\n",
        "> **Пример: медиана и квартили**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "median = np.percentile(data, 50)\n",
        "q1 = np.percentile(data, 25)\n",
        "q3 = np.percentile(data, 75)\n",
        "\n",
        "print(\"Медиана:\", median)  # 5.0\n",
        "print(\"Q1, Q3:\", q1, q3)   # 3.0, 7.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Выбор метода должен соответствовать статистической методологии исследования — особенно при сравнении результатов с другими пакетами, такими как R или pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.4. Управление агрегацией по осям: `axis` и `keepdims`\n",
        "\n",
        "При выполнении агрегации по оси соответствующее измерение **удаляется** из результата. Это может вызвать трудности при последующих операциях, например, при вычитании среднего значения из исходного массива, поскольку формы перестают быть совместимыми. Решение — использование параметра `keepdims=True`, который **сохраняет оси размером 1** в результирующем массиве, делая его совместимым с исходным для бродкастинга.\n",
        "\n",
        "> **Пример: стандартизация данных**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Исходные данные: 10 наблюдений, 5 признаков\n",
        "X = np.random.rand(10, 5)\n",
        "\n",
        "# Без keepdims: среднее — вектор (5,)\n",
        "mean_bad = np.mean(X, axis=0)\n",
        "# X - mean_bad → работает благодаря бродкастингу, но не всегда очевидно\n",
        "\n",
        "# С keepdims: среднее — матрица (1, 5)\n",
        "mean_good = np.mean(X, axis=0, keepdims=True)\n",
        "std_good = np.std(X, axis=0, keepdims=True)\n",
        "\n",
        "# Стандартизация: каждая строка центрируется и масштабируется\n",
        "X_standardized = (X - mean_good) / std_good\n",
        "\n",
        "print(\"Форма X:\", X.shape)               # (10, 5)\n",
        "print(\"Форма mean_good:\", mean_good.shape)  # (1, 5)\n",
        "print(\"Форма X_standardized:\", X_standardized.shape)  # (10, 5)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `keepdims=True` делает код **более явным, устойчивым к ошибкам** и совместимым с последующими бродкастинг-операциями. Это особенно важно в машинном обучении, где центрирование и масштабирование — стандартные этапы предобработки.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:** Векторизованные операции, UFuncs и бродкастинг — это три кита, на которых стоит эффективная работа с данными в NumPy. Понимание этих механизмов позволяет писать код, который не только короче и читабельнее, но и **на порядки быстрее** и **экономичнее по памяти**, чем эквивалент, написанный с использованием циклов Python."
      ],
      "metadata": {
        "id": "aHuYl7A9ehWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## РАЗДЕЛ VII. Линейная алгебра (подмодуль `numpy.linalg`)\n",
        "\n",
        "Подмодуль **`numpy.linalg`** предоставляет доступ к **высокооптимизированным реализациям** стандартных операций линейной алгебры, основанным на промышленных библиотеках **BLAS** и **LAPACK**. Эти функции гарантируют как скорость, так и численную надёжность при работе с матрицами.\n",
        "\n",
        "### 7.1. Матричные произведения\n",
        "\n",
        "NumPy предлагает два основных способа выполнения матричного умножения, различающихся семантикой и областью применения. Функция **`np.dot(A, B)`** является универсальной: для одномерных массивов она возвращает скалярное произведение, для двумерных — выполняет классическое матричное умножение, а для массивов более высокого ранга — суммирует по последней оси первого аргумента и предпоследней оси второго.\n",
        "\n",
        "В отличие от неё, оператор **`A @ B`** или функция **`np.matmul(A, B)`** предназначены строго для матричного умножения. Они не поддерживают скалярное произведение одномерных векторов (выбрасывая исключение `ValueError`), но корректно обрабатывают **«стеки» матриц**, например, тензоры форм `(N, M, K)` и `(N, K, L)`, результатом умножения которых будет тензор `(N, M, L)`. Этот подход строже следует правилам линейной алгебры и делает намерения кода более явными.\n",
        "\n",
        "> **Пример: сравнение `dot` и `matmul`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Скалярное произведение\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "print(\"np.dot(a, b):\", np.dot(a, b))  # 32\n",
        "\n",
        "try:\n",
        "    print(\"a @ b:\", a @ b)  # Ошибка: 1D @ 1D не поддерживается\n",
        "except ValueError as e:\n",
        "    print(\"Ошибка @ с 1D:\", e)\n",
        "\n",
        "# Матричное умножение\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "print(\"A @ B:\\n\", A @ B)\n",
        "\n",
        "# Стеки матриц\n",
        "C = np.random.rand(2, 3, 4)  # 2 матрицы 3×4\n",
        "D = np.random.rand(2, 4, 5)  # 2 матрицы 4×5\n",
        "E = C @ D  # результат: (2, 3, 5)\n",
        "print(\"Форма результата стека:\", E.shape)\n",
        "```\n",
        "\n",
        "> *Пояснение:* В современном коде **предпочтителен оператор `@`**, так как он делает намерения разработчика более явными и безопасными.\n",
        "\n",
        "### 7.2. Решение систем линейных уравнений (СЛУ)\n",
        "\n",
        "Для решения системы вида **`A x = B`**, где `A` — квадратная матрица, используется функция `np.linalg.solve(A, B)`. Она требует, чтобы матрица `A` была **квадратной** и **несингулярной** (полного ранга). Если система **переопределена** (уравнений больше, чем неизвестных) или **недоопределена**, следует использовать **метод наименьших квадратов** через функцию `np.linalg.lstsq(A, B, rcond=None)`.\n",
        "\n",
        "> **Проверка решения:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[3, 1], [1, 2]])\n",
        "B = np.array([9, 8])\n",
        "x = np.linalg.solve(A, B)\n",
        "\n",
        "print(\"Решение x:\", x)\n",
        "print(\"Проверка A @ x ≈ B:\", np.allclose(A @ x, B))  # True\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.allclose` учитывает погрешности машинной арифметики и корректно сравнивает результаты с плавающей точкой.\n",
        "\n",
        "### 7.3. Обратная матрица и численная стабильность\n",
        "\n",
        "Функция `np.linalg.inv(A)` вычисляет обратную матрицу **`A⁻¹`**. Однако **прямое обращение — плохая практика** в большинстве приложений. Если матрица **плохо обусловлена** (близка к сингулярной), результат будет **числово неточным**. Число обусловленности, вычисляемое как `np.linalg.cond(A) = σ_max / σ_min`, количественно оценивает эту чувствительность: чем больше значение, тем выше риск ошибки.\n",
        "\n",
        "Рекомендуется вместо выражения `x = inv(A) @ B` всегда использовать **`x = solve(A, B)`** — это не только быстрее, но и значительно стабильнее с точки зрения численной математики.\n",
        "\n",
        "> **Пример: сравнение точности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Плохо обусловленная матрица Гильберта\n",
        "A = np.array([[1, 1/2], [1/2, 1/3]], dtype=np.float64)\n",
        "B = np.array([1, 1])\n",
        "\n",
        "# Плохой способ\n",
        "x_bad = np.linalg.inv(A) @ B\n",
        "\n",
        "# Хороший способ\n",
        "x_good = np.linalg.solve(A, B)\n",
        "\n",
        "true_x = np.array([4, -6])  # точное решение\n",
        "print(\"Ошибка через inv:\", np.linalg.norm(x_bad - true_x))\n",
        "print(\"Ошибка через solve:\", np.linalg.norm(x_good - true_x))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Даже на малых матрицах разница может быть значимой. В реальных задачах (машинное обучение, физика) предпочтение `solve` критично.\n",
        "\n",
        "### 7.4. Сингулярное разложение (SVD)\n",
        "\n",
        "**Сингулярное разложение (SVD)** — одна из самых мощных техник линейной алгебры. Любая матрица **`A`** может быть разложена как:\n",
        "\\[\n",
        "A = U \\cdot S \\cdot V^H\n",
        "\\]\n",
        "где `U` — левые сингулярные векторы, `S` — диагональная матрица сингулярных значений (в NumPy — вектор `s`), а `V^H` — сопряжённо-транспонированные правые сингулярные векторы.\n",
        "\n",
        "Функция `U, s, Vh = np.linalg.svd(A, full_matrices=False)` возвращает **усечённое SVD**, что экономит память и используется в задачах **PCA**, сжатия данных и регуляризации. Все функции `linalg`, включая `svd`, `solve` и `inv`, поддерживают работу с **N-мерными массивами**, применяя операции к последним двум осям — что идеально для обработки батчей в машинном обучении.\n",
        "\n",
        "> **Пример: реконструкция и PCA-подобное сжатие**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.random.rand(5, 4)\n",
        "U, s, Vh = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "# Реконструкция\n",
        "A_rec = U @ np.diag(s) @ Vh\n",
        "print(\"Ошибка реконструкции:\", np.linalg.norm(A - A_rec))  # ~1e-15\n",
        "\n",
        "# Сжатие: оставить только 2 главных компоненты\n",
        "k = 2\n",
        "A_approx = U[:, :k] @ np.diag(s[:k]) @ Vh[:k, :]\n",
        "print(\"Сжатая форма:\", A_approx.shape)  # (5, 4), но ранг ≈ 2\n",
        "```\n",
        "\n",
        "> *Пояснение:* SVD лежит в основе **метода главных компонент (PCA)**, рекомендательных систем и решения некорректных СЛУ.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ VIII. Производительность, продвинутые методы и практическое применение\n",
        "\n",
        "### 8.1. Продвинутая векторизация: конвенция Эйнштейна (`np.einsum`)\n",
        "\n",
        "Для сложных тензорных операций, которые неудобно выражать через `@` или `dot`, NumPy предоставляет **`np.einsum`** — реализацию **конвенции суммирования Эйнштейна**. Синтаксис функции задаётся строкой вида `'индексы_входов->индексы_выхода'`, что делает код читаемым и близким к математической записи.\n",
        "\n",
        "Например, матричное умножение записывается как `'ij,jk->ik'`, скалярное произведение — как `'i,i->'`, а извлечение диагонали — как `'ii->i'`. Преимущества `np.einsum` многообразны: высокая читаемость, гибкость в выражении сложных свёрток и перестановок, а также возможность автоматической оптимизации порядка операций через параметр `optimize=True`, что особенно важно для больших тензоров. Эта функция лежит в основе тензорных операций в современных фреймворках, таких как TensorFlow и PyTorch.\n",
        "\n",
        "> **Пример: ускорение через оптимизацию**\n",
        "\n",
        "```python\n",
        "X = np.random.rand(100, 50, 20)\n",
        "Y = np.random.rand(50, 20, 80)\n",
        "\n",
        "# Без оптимизации — медленно\n",
        "%timeit np.einsum('ijk,jkl->il', X, Y)\n",
        "\n",
        "# С оптимизацией — может быть в разы быстрее\n",
        "%timeit np.einsum('ijk,jkl->il', X, Y, optimize=True)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.einsum` — ключевой инструмент в библиотеках вроде **TensorFlow**, **PyTorch** (через `torch.einsum`) и **JAX**.\n",
        "\n",
        "### 8.2. Производительность: бенчмаркинг и профилирование\n",
        "\n",
        "Векторизованный код на NumPy **на порядки быстрее** циклов Python. Однако «островки» не-векторизованного кода легко становятся **узкими местами**. Для поиска и устранения таких проблем рекомендуется использовать `%timeit` для измерения времени выполнения отдельных участков и профилировщики, такие как `cProfile` или `line_profiler`. Основной стратегией оптимизации остаётся минимизация явных циклов `for` в пользу UFuncs, `np.where` и `np.einsum`.\n",
        "\n",
        "> **Пример: векторизация vs цикл**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(1000000)\n",
        "\n",
        "# Медленно\n",
        "def slow_log(x):\n",
        "    return np.array([np.log(val) if val > 0 else 0 for val in x])\n",
        "\n",
        "# Быстро\n",
        "def fast_log(x):\n",
        "    out = np.zeros_like(x)\n",
        "    mask = x > 0\n",
        "    out[mask] = np.log(x[mask])\n",
        "    return out\n",
        "\n",
        "%timeit slow_log(x)  # ~100 ms\n",
        "%timeit fast_log(x)  # ~1 ms\n",
        "```\n",
        "\n",
        "> *Пояснение:* Разница в 100 раз — типична для перехода от Python-циклов к векторизации.\n",
        "\n",
        "### 8.3. Практика 1: обработка изображений\n",
        "\n",
        "Изображение — это **3D-массив** формы `(высота, ширина, каналы)`. NumPy позволяет выполнять базовые операции **без сторонних библиотек**. Например, нормализацию к диапазону `[0, 1]`, геометрические преобразования (поворот, отражение) и центрирование по каналам можно реализовать с помощью стандартных функций и бродкастинга.\n",
        "\n",
        "> **Пример: нормализация и геометрические преобразования**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Имитация цветного изображения 100×100×3\n",
        "img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "# Нормализация к [0, 1]\n",
        "img_norm = img.astype(np.float32) / 255.0\n",
        "\n",
        "# Поворот на 90° по часовой стрелке\n",
        "img_rot = np.rot90(img, k=-1)\n",
        "\n",
        "# Отражение по горизонтали\n",
        "img_flip = np.fliplr(img)\n",
        "\n",
        "# Удаление среднего по каналам\n",
        "mean_per_channel = img_norm.mean(axis=(0, 1), keepdims=True)\n",
        "img_centered = img_norm - mean_per_channel\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для сложных операций (размытие, градиенты, морфология) используется **`scipy.ndimage`**, но **NumPy — основа** всех этих преобразований.\n",
        "\n",
        "### 8.4. Практика 2: фильтрация сигналов и симуляции\n",
        "\n",
        "Временные ряды и физические модели представляют собой **одно- или двумерные массивы**, которые идеально подходят для NumPy. Например, скользящее среднее можно эффективно вычислить через кумулятивную сумму, избегая циклов. В симуляциях динамических систем все обновления состояний (позиций, скоростей) должны выполняться векторизованно, что обеспечивает высокую производительность и читаемость кода.\n",
        "\n",
        "> **Пример: скользящее среднее через кумулятивную сумму**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def moving_average(x, window):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (cumsum[window:] - cumsum[:-window]) / window\n",
        "\n",
        "signal = np.random.randn(1000)\n",
        "smoothed = moving_average(signal, window=50)\n",
        "```\n",
        "\n",
        "> **Симуляции:**  \n",
        "> Используйте `Generator` для воспроизводимости, а все обновления — через векторизованные операции:\n",
        "\n",
        "```python\n",
        "rng = np.random.default_rng(42)\n",
        "positions = rng.normal(size=(100, 2))  # 100 частиц в 2D\n",
        "velocities = np.zeros_like(positions)\n",
        "\n",
        "for _ in range(1000):\n",
        "    forces = -positions  # упрощённая сила (пружина)\n",
        "    velocities += forces * 0.01\n",
        "    positions += velocities * 0.01\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой подход лежит в основе **численного интегрирования**, **машинного обучения с подкреплением**, **моделирования погоды** и др.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**NumPy — это не просто библиотека, а философия эффективных научных вычислений.** Его архитектура строится на трёх китах:\n",
        "\n",
        "1. **`ndarray`** — гомогенный, непрерывный контейнер, позволяющий использовать оптимизированные ядра C/Fortran.\n",
        "2. **Векторизация и бродкастинг** — механизм, устраняющий циклы Python и экономящий память.\n",
        "3. **Численно устойчивые алгоритмы** в `numpy.linalg` и `numpy.random` — основа для надёжных расчётов.\n",
        "\n",
        "Однако эффективность требует **осознанного подхода**:\n",
        "- Предпочитайте **`view` над `copy`** (`ravel` вместо `flatten`).\n",
        "- Используйте **`keepdims=True`** при агрегации для корректного бродкастинга.\n",
        "- Избегайте **явного обращения матриц** — выбирайте `solve`.\n",
        "- Применяйте **`np.newaxis`** для подготовки к бродкастингу.\n",
        "- Используйте **`np.einsum`** для сложных тензорных операций.\n",
        "\n",
        "С освоением NumPy вы получаете **единый, мощный и стандартизированный язык** для работы с данными — язык, на котором говорят **SciPy**, **pandas**, **scikit-learn**, **Matplotlib**, и даже **PyTorch** и **TensorFlow** (через совместимость с `ndarray`).\n",
        "\n",
        "> Таким образом, овладение NumPy — это не первый шаг в data science, а **фундамент всего здания современных вычислений на Python**."
      ],
      "metadata": {
        "id": "xtlNUalxhZcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 2: Библиотека Pandas — Комплексный анализ и обработка табличных данных\n",
        "\n",
        "Библиотека **Pandas** является краеугольным камнем современной экосистемы обработки данных на языке Python. Она предоставляет высокопроизводительные, удобные в использовании структуры данных и инструменты для анализа, делая Python мощным инструментом для работы с табличными данными — на уровне таких систем, как **R** или **SQL**.\n",
        "\n",
        "В рамках этого модуля рассматриваются ключевые концепции и практики работы с Pandas, необходимые для построения надёжных **ETL-конвейеров** (Extract, Transform, Load): от загрузки и создания структур данных до очистки, трансформации и сохранения результатов.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Фундаментальные структуры данных Pandas\n",
        "\n",
        "Pandas строится на трёх основных структурах: **`Series`**, **`DataFrame`** и **`Index`**. Понимание их взаимосвязи и внутренних механизмов — особенно **принципа выравнивания данных (Data Alignment)** — критически важно для эффективной и предсказуемой работы с библиотекой.\n",
        "\n",
        "### I.1. Обзор: `Series`, `DataFrame`, `Index`\n",
        "\n",
        "#### **`Series` — одномерный индексированный массив**\n",
        "\n",
        "Объект `Series` представляет собой одномерный массив с **явно заданными метками** (индексом). Его можно рассматривать как **индексированный аналог NumPy-массива** или как **высокопроизводительный словарь с фиксированным порядком ключей**.\n",
        "\n",
        "> **Пример: создание Series**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ser = pd.Series([10, 20, 30], index=['a', 'b', 'c'], name='values')\n",
        "print(\"Series:\\n\", ser)\n",
        "# Вывод:\n",
        "# a    10\n",
        "# b    20\n",
        "# c    30\n",
        "# Name: values, dtype: int64\n",
        "```\n",
        "\n",
        "> *Пояснение:* В отличие от обычного словаря, `Series` поддерживает векторизованные операции и интеграцию с `DataFrame`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`DataFrame` — двумерная таблица с метками**\n",
        "\n",
        "`DataFrame` — основная структура данных в Pandas. Это двумерная, изменяемая таблица с **метками строк (индекс)** и **метками столбцов (колонки)**. Концептуально `DataFrame` можно представить как **словарь из `Series`**, где каждый столбец — это отдельный `Series`, а все они разделяют общий индекс.\n",
        "\n",
        "> **Пример: `DataFrame` из `Series`**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ser = pd.Series([0, 1, 2], index=['a', 'b', 'c'], name='ser_data')\n",
        "df_from_ser = pd.DataFrame(ser)\n",
        "\n",
        "print(\"DataFrame из Series:\\n\", df_from_ser)\n",
        "# Вывод:\n",
        "#    ser_data\n",
        "# a         0\n",
        "# b         1\n",
        "# c         2\n",
        "```\n",
        "\n",
        "> *Пояснение:* Имя `Series` автоматически становится именем столбца. Индекс сохраняется.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Принцип выравнивания данных (Data Alignment)**\n",
        "\n",
        "Ключевое отличие Pandas от NumPy — **автоматическое выравнивание по меткам** при выполнении операций. Арифметические и логические операции выполняются **по совпадающим строкам и столбцам**, а отсутствующие метки заполняются `NaN`.\n",
        "\n",
        "Это гарантирует, что вы всегда работаете с **соответствующими элементами**, независимо от порядка или полноты данных.\n",
        "\n",
        "> **Пример: выравнивание при сложении**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
        "s2 = pd.Series([10, 20], index=['b', 'c'])\n",
        "\n",
        "result = s1 + s2\n",
        "print(\"Результат сложения с выравниванием:\\n\", result)\n",
        "# Вывод:\n",
        "# a    NaN\n",
        "# b    12.0\n",
        "# c    23.0\n",
        "# dtype: float64\n",
        "```\n",
        "\n",
        "> *Пояснение:* Элемент `'a'` отсутствует в `s2`, поэтому результат — `NaN`. Это предотвращает ошибки, характерные для «слепых» операций над массивами без меток.\n",
        "\n",
        "---\n",
        "\n",
        "### I.2. Атрибуты объектов: `shape`, `dtypes`, `index`, `columns`\n",
        "\n",
        "Эти атрибуты предоставляют метаданные, необходимые для анализа структуры и качества данных.\n",
        "\n",
        "- **`.shape`** — кортеж вида `(число строк, число столбцов)`, совпадает с соглашением NumPy.\n",
        "- **`.dtypes`** — `Series`, показывающий тип данных каждого столбца. Тип `object` часто указывает на строки или смешанные типы — это сигнал к проверке данных.\n",
        "- **`.index`** — метки строк (`Index`-объект).\n",
        "- **`.columns`** — метки столбцов (`Index`-объект).\n",
        "\n",
        "> **Пример: доступ к атрибутам**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'ID': [101, 102, 103, 104, 105],\n",
        "    'Value': [10.5, 20.1, 30.7, 40.2, 50.8],\n",
        "    'Category': ['A', 'B', 'A', 'C', 'B']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data, index=['r1', 'r2', 'r3', 'r4', 'r5'])\n",
        "\n",
        "print(\"Форма (shape):\", df.shape)  # (5, 3)\n",
        "print(\"\\nТипы данных (dtypes):\\n\", df.dtypes)\n",
        "# ID           int64\n",
        "# Value      float64\n",
        "# Category    object\n",
        "\n",
        "print(\"\\nИндекс строк:\", df.index)\n",
        "# Index(['r1', 'r2', 'r3', 'r4', 'r5'], dtype='object')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Анализ `.dtypes` помогает выявить неоптимальное хранение (например, числа как `object`) и спланировать преобразования.\n",
        "\n",
        "---\n",
        "\n",
        "### I.3. Создание объектов из различных источников\n",
        "\n",
        "Pandas поддерживает гибкое создание структур из Python-объектов.\n",
        "\n",
        "#### 1. Из словаря списков\n",
        "\n",
        "Наиболее распространённый способ: ключи → имена столбцов, значения → данные.\n",
        "\n",
        "```python\n",
        "dict_data = {\n",
        "    'City': ['Moscow', 'Kazan', 'Saint Petersburg'],\n",
        "    'Population': [12600000, 1270000, 5400000]\n",
        "}\n",
        "df_dict = pd.DataFrame(dict_data)\n",
        "print(\"Из словаря списков:\\n\", df_dict)\n",
        "```\n",
        "\n",
        "#### 2. Из словаря `Series` (демонстрация выравнивания)\n",
        "\n",
        "```python\n",
        "data_series = {\n",
        "    'Col_A': pd.Series([10, 20], index=['x', 'y']),\n",
        "    'Col_B': pd.Series([100, 200, 300], index=['y', 'z', 'x'])\n",
        "}\n",
        "df_aligned = pd.DataFrame(data_series)\n",
        "print(\"Выравнивание Series:\\n\", df_aligned)\n",
        "# Вывод:\n",
        "#    Col_A  Col_B\n",
        "# x   10.0  300.0\n",
        "# y   20.0  100.0\n",
        "# z    NaN  200.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Pandas автоматически объединил все уникальные метки (`x`, `y`, `z`) и вставил `NaN` там, где данных нет.\n",
        "\n",
        "#### 3. Из NumPy-массива\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "numpy_array = np.array([[1, 2, 3],\n",
        "                        [4, 5, 6],\n",
        "                        [7, 8, 9]])\n",
        "\n",
        "df_numpy = pd.DataFrame(\n",
        "    numpy_array,\n",
        "    index=['r1', 'r2', 'r3'],\n",
        "    columns=['c1', 'c2', 'c3']\n",
        ")\n",
        "print(\"Из NumPy-массива:\\n\", df_numpy)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без явного указания `index` и `columns` будут использованы целочисленные метки по умолчанию.\n",
        "\n",
        "---\n",
        "\n",
        "## II. Операции ввода-вывода (I/O) и оптимизация загрузки\n",
        "\n",
        "Эффективность анализа данных начинается с **быстрой и корректной загрузки**. Pandas предлагает мощные инструменты для работы с CSV, Excel, JSON, Parquet и другими форматами.\n",
        "\n",
        "### II.1. Загрузка данных: `pd.read_csv()`\n",
        "\n",
        "Функция `pd.read_csv()` — основной инструмент для чтения табличных данных. Она поддерживает локальные файлы, URL (`http`, `s3`, `gs`), а также потоки (`StringIO`).\n",
        "\n",
        "#### Ключевые параметры для оптимизации:\n",
        "\n",
        "| Параметр | Назначение | Зачем это важно |\n",
        "|--------|-----------|----------------|\n",
        "| `dtype` | Явное указание типов данных | Предотвращает ошибки угадывания (`object` вместо `int`), экономит память и ускоряет операции |\n",
        "| `usecols` | Загрузка только нужных столбцов | Снижает потребление памяти и время парсинга в разы |\n",
        "| `index_col` | Назначение столбца(ов) как индекса | Упрощает последующий анализ и фильтрацию |\n",
        "| `parse_dates` + `date_format` | Парсинг дат с явным форматом | Ускоряет обработку временных меток и избегает неоднозначности |\n",
        "\n",
        "> **Пример: оптимизированная загрузка**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Предположим, файл содержит: Timestamp (строка), Cost (целое), Description (текст), Region (категория)\n",
        "file_path = 'data/large_data.csv'\n",
        "\n",
        "data_types = {\n",
        "    'Cost': 'int32',          # меньше памяти, чем int64\n",
        "    'Region': 'category'      # идеально для строк с небольшим числом уникальных значений\n",
        "}\n",
        "\n",
        "df_optimized = pd.read_csv(\n",
        "    file_path,\n",
        "    usecols=['Timestamp', 'Cost', 'Region'],  # только нужное\n",
        "    dtype=data_types,\n",
        "    parse_dates=['Timestamp'],                # преобразуем в datetime\n",
        "    index_col='Timestamp'                     # делаем индексом\n",
        ")\n",
        "\n",
        "print(\"Типы после загрузки:\\n\", df_optimized.dtypes)\n",
        "# Timestamp    datetime64[ns]\n",
        "# Cost                  int32\n",
        "# Region             category\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `'category'` для `Region` может сократить потребление памяти в 10–100 раз по сравнению с `'object'`.\n",
        "\n",
        "---\n",
        "\n",
        "### II.2. Сохранение данных: `DataFrame.to_csv()`\n",
        "\n",
        "Эта функция завершает этап **Load** в ETL-процессе.\n",
        "\n",
        "#### Ключевые параметры:\n",
        "\n",
        "- **`index=False`** — не сохранять индекс (если он просто `0,1,2,...`).\n",
        "- **`compression='gzip'`** — сжимать «на лету» (поддержка `infer` по расширению: `.csv.gz`).\n",
        "- **`date_format='%Y-%m-%d'`** — контролировать формат дат при экспорте.\n",
        "\n",
        "> **Пример: сохранение с оптимизацией**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Создаём данные с датами\n",
        "dates = pd.date_range('2023-01-01', periods=5)\n",
        "df_result = pd.DataFrame({'Sales': [100, 150, 200, 180, 220]}, index=dates)\n",
        "df_result.index.name = 'Date'\n",
        "\n",
        "# Сохраняем в сжатый CSV\n",
        "df_result.to_csv(\n",
        "    'results/sales_summary.csv.gz',\n",
        "    index=True,               # сохраняем даты как индекс\n",
        "    date_format='%Y-%m-%d',\n",
        "    compression='gzip'\n",
        ")\n",
        "\n",
        "print(\"Данные сохранены в сжатый файл: results/sales_summary.csv.gz\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Сжатие особенно важно при работе с большими объёмами данных — файлы могут быть в 3–10 раз меньше.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas превращает неструктурированные и полуобработанные данные в **аналитически пригодные структуры**, обеспечивая надёжность через выравнивание, гибкость через метки и производительность через интеграцию с NumPy. Освоение базовых структур и оптимизированного I/O — первый шаг к построению промышленных конвейеров обработки данных.\n"
      ],
      "metadata": {
        "id": "-Vtjc_dPiDTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## III. Индексирование и селекция данных\n",
        "\n",
        "Эффективная выборка и модификация данных — основа любого преобразования в Pandas. Библиотека предоставляет **специализированные аксессоры**, оптимизированные под разные сценарии: от массовой фильтрации до сверхбыстрого доступа к отдельным ячейкам.\n",
        "\n",
        "### III.1. Выборка по меткам и позициям: `loc` и `iloc`\n",
        "\n",
        "#### `loc` — индексирование по **меткам**\n",
        "\n",
        "`df.loc[rows, cols]` выбирает данные **исключительно по именам строк и столбцов**.  \n",
        "\n",
        "**Важная особенность:** при использовании срезов (`:`) с метками **конечная метка включается** в результат (инклюзивное срезание).\n",
        "\n",
        "> **Пример: `loc` с инклюзивным срезом**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    np.arange(12).reshape(3, 4),\n",
        "    index=['r_a', 'r_b', 'r_c'],\n",
        "    columns=['c1', 'c2', 'c3', 'c4']\n",
        ")\n",
        "\n",
        "# Выбираем строки от 'r_a' до 'r_c' (включительно) и столбцы от 'c2' до 'c4' (включительно)\n",
        "result_loc = df.loc['r_a':'r_c', 'c2':'c4']\n",
        "print(\"Селекция через .loc (инклюзивно):\\n\", result_loc)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "     c2  c3  c4\n",
        "r_a   1   2   3\n",
        "r_b   5   6   7\n",
        "r_c   9  10  11\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такое поведение интуитивно для аналитиков: «от A до C» обычно включает и C.\n",
        "\n",
        "---\n",
        "\n",
        "#### `iloc` — индексирование по **целочисленным позициям**\n",
        "\n",
        "`df.iloc[rows, cols]` работает **только с целыми индексами** (0, 1, 2, ...), как в NumPy.  \n",
        "\n",
        "**Семантика срезов стандартная:** конечный индекс **не включается** (эксклюзивное срезание).\n",
        "\n",
        "> **Пример: `iloc` с эксклюзивным срезом**\n",
        "\n",
        "```python\n",
        "# Те же данные\n",
        "result_iloc = df.iloc[0:2, 1:3]  # строки 0–1, столбцы 1–2\n",
        "print(\"\\nСелекция через .iloc (эксклюзивно):\\n\", result_iloc)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "     c2  c3\n",
        "r_a   1   2\n",
        "r_b   5   6\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это важно помнить при переходе от меток к позициям — границы ведут себя по-разному.\n",
        "\n",
        "---\n",
        "\n",
        "### III.2. Оптимизированный скалярный доступ: `at` и `iat`\n",
        "\n",
        "Для **чтения или записи одного значения** используйте `at` (по метке) и `iat` (по позиции). Они **значительно быстрее**, чем `loc`/`iloc`, так как не создают промежуточных объектов.\n",
        "\n",
        "> **Пример: высокоскоростной доступ к ячейкам**\n",
        "\n",
        "```python\n",
        "# Изменяем значение по метке\n",
        "df.at['r_b', 'c3'] = 999\n",
        "value_at = df.at['r_b', 'c3']\n",
        "\n",
        "# Изменяем значение по позиции: строка 2 ('r_c'), столбец 3 ('c4')\n",
        "df.iat[2, 3] = 1000\n",
        "value_iat = df.iat[2, 3]\n",
        "\n",
        "print(f\"\\nЗначение в [r_b, c3] после .at: {value_at}\")\n",
        "print(f\"Значение в [r_c, c4] после .iat: {value_iat}\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти методы критичны в редких случаях, когда без итерации не обойтись (например, в сложной логике с зависимыми условиями). Однако **векторизация всегда предпочтительнее**.\n",
        "\n",
        "---\n",
        "\n",
        "### III.3. Булево индексирование и логические операторы\n",
        "\n",
        "Фильтрация по условиям выполняется через **булевы маски**. При объединении условий **обязательно используйте побитовые операторы**:\n",
        "\n",
        "- `&` вместо `and`\n",
        "- `|` вместо `or`\n",
        "- `~` вместо `not`\n",
        "\n",
        "> **Пример: сложная фильтрация**\n",
        "\n",
        "```python\n",
        "data_bool = {\n",
        "    'Age': [25, 35, 45, 65],\n",
        "    'Salary': [35000, 75000, 85000, 95000]\n",
        "}\n",
        "df_cond = pd.DataFrame(data_bool)\n",
        "\n",
        "# Условие: Возраст > 30 ИЛИ (Зарплата < 80000 И возраст ≤ 60)\n",
        "mask = (df_cond['Age'] > 30) | ((df_cond['Salary'] < 80000) & ~(df_cond['Age'] > 60))\n",
        "\n",
        "filtered = df_cond.loc[mask]\n",
        "print(\"\\nФильтрация через булеву маску:\\n\", filtered)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Age  Salary\n",
        "1   35   75000\n",
        "2   45   85000\n",
        "```\n",
        "\n",
        "> *Пояснение:* Скобки **обязательны** из-за приоритета операторов: `&` имеет более высокий приоритет, чем `|`.\n",
        "\n",
        "---\n",
        "\n",
        "### III.4. Высокопроизводительный запрос: метод `query()`\n",
        "\n",
        "Метод `df.query('условие')` позволяет писать фильтры в виде **строковых выражений**, используя синтаксис, близкий к SQL.\n",
        "\n",
        "#### Преимущества:\n",
        "- Использует библиотеку **NumExpr**, которая вычисляет выражения в C — без участия интерпретатора Python.\n",
        "- **Значительно быстрее** при работе с большими DataFrame (обычно > 50 000 строк).\n",
        "- Поддерживает **внешние переменные** через `@`.\n",
        "\n",
        "#### Синтаксис:\n",
        "- Столбцы с пробелами: `` `Column Name` ``\n",
        "- Внешние переменные: `@var_name`\n",
        "\n",
        "> **Пример: использование `query()`**\n",
        "\n",
        "```python\n",
        "target_age = 30\n",
        "target_salary = 90000\n",
        "\n",
        "# Фильтрация с внешними переменными\n",
        "result_query = df_cond.query('Age > @target_age and Salary < @target_salary')\n",
        "print(\"\\nФильтрация через .query():\\n\", result_query)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Age  Salary\n",
        "1   35   75000\n",
        "2   45   85000\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для небольших данных `query()` может быть **медленнее** из-за накладных расходов на парсинг строки. Используйте его осознанно.\n",
        "\n",
        "---\n",
        "\n",
        "#### Сравнительная таблица методов доступа\n",
        "\n",
        "| Метод      | Основа          | Возвращаемое значение     | Скорость (скаляр) | Основное применение |\n",
        "|-----------|------------------|----------------------------|-------------------|----------------------|\n",
        "| `.loc`    | Метка            | Series / DataFrame / Scalar| Умеренная         | Фильтрация по именам, диапазоны (включая конец) |\n",
        "| `.iloc`   | Позиция          | Series / DataFrame / Scalar| Умеренная         | Селекция по индексу (исключая конец) |\n",
        "| `.at`     | Метка            | **Скаляр**                 | **Высокая**       | Быстрое чтение/запись одной ячейки |\n",
        "| `.iat`    | Позиция          | **Скаляр**                 | **Высокая**       | То же по позиции |\n",
        "| `.query()`| Строка-выражение | DataFrame                  | Высокая (на больших данных) | Читаемые, сложные фильтры |\n",
        "\n",
        "---\n",
        "\n",
        "## IV. Методы очистки и подготовки данных\n",
        "\n",
        "Работа с пропущенными значениями — обязательный этап ETL. В Pandas отсутствующие данные обозначаются как **`np.nan`** (или `pd.NA` для новых nullable-типов). Стратегии обработки делятся на три категории.\n",
        "\n",
        "### IV.1. Теоретические основы обработки пропусков\n",
        "\n",
        "1. **Удаление (`dropna`)** — простой, но радикальный метод. Оправдан при малой доле пропусков.\n",
        "2. **Вменение (`fillna`)** — замена на константу, среднее, медиану. Сохраняет объём данных.\n",
        "3. **Интерполяция (`interpolate`)** — оценка пропусков на основе соседей. Идеальна для временных рядов.\n",
        "\n",
        "> **Выбор стратегии зависит от:**\n",
        "> - природы данных,\n",
        "> - доли пропусков,\n",
        "> - наличия структуры (например, временной упорядоченности).\n",
        "\n",
        "---\n",
        "\n",
        "### IV.2. Идентификация и удаление пропусков\n",
        "\n",
        "- **`isna()` / `notna()`** — возвращают булеву маску.\n",
        "- **`dropna()`** — удаляет строки/столбцы с пропусками.\n",
        "\n",
        "Параметры:\n",
        "- `axis=0` — удалять строки (по умолчанию), `axis=1` — столбцы.\n",
        "- `how='any'` — удалить при **любом** `NaN`; `how='all'` — только если **все** значения `NaN`.\n",
        "\n",
        "> **Пример: работа с пропусками**\n",
        "\n",
        "```python\n",
        "df_na = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [5, np.nan, np.nan, 8],\n",
        "    'C': [np.nan, np.nan, np.nan, np.nan]\n",
        "})\n",
        "\n",
        "print(\"Пропуски по столбцам:\\n\", df_na.isna().sum())\n",
        "# A: 1, B: 2, C: 4\n",
        "\n",
        "# Удалить строки с хотя бы одним NaN\n",
        "print(\"\\nПосле dropna(axis=0, how='any'):\\n\", df_na.dropna())\n",
        "\n",
        "# Удалить столбцы с хотя бы одним NaN\n",
        "print(\"\\nПосле dropna(axis=1, how='any'):\\n\", df_na.dropna(axis=1))\n",
        "# Пустой DataFrame — все столбцы содержат NaN\n",
        "```\n",
        "\n",
        "> *Пояснение:* Столбец `C` полностью пуст — его часто удаляют на этапе предварительного анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### IV.3. Заполнение пропусков: `fillna()`\n",
        "\n",
        "#### 1. Скалярные значения и статистика\n",
        "\n",
        "```python\n",
        "# Восстановим исходный df_na\n",
        "df_na = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [5, np.nan, np.nan, 8]\n",
        "})\n",
        "\n",
        "# Замена средним\n",
        "df_na['A'] = df_na['A'].fillna(df_na['A'].mean())\n",
        "\n",
        "# Замена константой\n",
        "df_na = df_na.fillna(0)\n",
        "print(\"После fillna:\\n\", df_na)\n",
        "```\n",
        "\n",
        "#### 2. Пропагация значений (`ffill`, `bfill`)\n",
        "\n",
        "- `method='ffill'` — заполнить предыдущим значением (forward fill).\n",
        "- `method='bfill'` — заполнить следующим значением (backward fill).\n",
        "- `limit` — ограничить количество заполняемых подряд пропусков.\n",
        "\n",
        "> **Пример: пропагация**\n",
        "\n",
        "```python\n",
        "ser = pd.Series([1.0, np.nan, np.nan, 5.0, np.nan, 7.0])\n",
        "\n",
        "# Ffill с лимитом — заполнит только один пропуск\n",
        "ffill_limited = ser.fillna(method='ffill', limit=1)\n",
        "print(\"Ffill (limit=1):\\n\", ffill_limited)\n",
        "# [1.0, 1.0, nan, 5.0, 5.0, 7.0]\n",
        "\n",
        "# Bfill — заполняет в обратном направлении\n",
        "bfill_full = ser.fillna(method='bfill')\n",
        "print(\"\\nBfill:\\n\", bfill_full)\n",
        "# [1.0, 5.0, 5.0, 5.0, 7.0, 7.0]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `limit` предотвращает «размазывание» значения на слишком большой промежуток.\n",
        "\n",
        "---\n",
        "\n",
        "### IV.4. Интерполяция данных: `interpolate()`\n",
        "\n",
        "Интерполяция **оценивает пропущенные значения**, основываясь на соседях. По умолчанию — **линейная**.\n",
        "\n",
        "> **Пример: линейная интерполяция**\n",
        "\n",
        "```python\n",
        "ser_interp = pd.Series([0, 10, np.nan, np.nan, 40, 50])\n",
        "result = ser_interp.interpolate(method='linear')\n",
        "print(\"Линейная интерполяция:\\n\", result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "0     0.0\n",
        "1    10.0\n",
        "2    20.0\n",
        "3    30.0\n",
        "4    40.0\n",
        "5    50.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Особенно полезно для **временных рядов**, **геоданных**, **датчиков**, где данные упорядочены и гладки.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas предоставляет **полный арсенал инструментов** для точечного и массового доступа к данным, а также для гибкой обработки пропусков. Осознанное использование `loc`/`iloc`, `at`/`iat`, `query`, `fillna` и `interpolate` позволяет строить **надёжные, читаемые и производительные** конвейеры очистки и трансформации данных.\n"
      ],
      "metadata": {
        "id": "UPm9PAamjFW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## V. Векторизация, применение пользовательских функций и цепочки вызовов\n",
        "\n",
        "Основа **высокопроизводительного кода в Pandas** — **векторизация**: делегирование вычислений оптимизированным ядрам C/NumPy вместо медленных циклов Python. Однако при необходимости применения пользовательской логики важно выбирать правильный инструмент — от простого `map` до декларативного `assign` и оптимизированного `eval`.\n",
        "\n",
        "---\n",
        "\n",
        "### V.1. Векторизация и применение функций: `map`, `apply`, `applymap`\n",
        "\n",
        "Эти методы различаются по гибкости, производительности и области применения.\n",
        "\n",
        "> **Важно:** все они **медленнее чисто векторизованных операций** (`+`, `np.log`, `str.upper` и др.). Используйте их только тогда, когда векторизация невозможна.\n",
        "\n",
        "#### V.1.1. `Series.map()`\n",
        "\n",
        "Применяется **только к `Series`**, работает поэлементно.  \n",
        "**Идеален для:**\n",
        "- замены значений по словарю (например, кодирование категорий),\n",
        "- простых преобразований через лямбда-функции.\n",
        "\n",
        "> **Ограничение:** не поддерживает передачу дополнительных аргументов.\n",
        "\n",
        "#### V.1.2. `Series.apply()`\n",
        "\n",
        "Более гибкий, чем `map`.  \n",
        "**Позволяет:**\n",
        "- передавать `args` и `kwargs`,\n",
        "- возвращать сложные объекты (например, `Series` → `DataFrame`).\n",
        "\n",
        "#### V.1.3. `DataFrame.apply()`\n",
        "\n",
        "Применяет функцию **к строкам (`axis=1`) или столбцам (`axis=0`)**.\n",
        "\n",
        "- **`axis=0`** (по умолчанию): функция получает каждый **столбец** как `Series` → полезно для статистики.\n",
        "- **`axis=1`**: функция получает каждую **строку** как `Series` → полезно для вычисления признаков из нескольких столбцов.\n",
        "\n",
        "> **Пример: `map` и `apply(axis=1)`**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_func = pd.DataFrame({\n",
        "    'Status_Code': [1, 2, 3, 1],\n",
        "    'Height': [170, 165, 180, 175],   # в см\n",
        "    'Weight': [70, 60, 90, 80]        # в кг\n",
        "})\n",
        "\n",
        "# 1. Замена кодов статусов на метки\n",
        "status_map = {1: 'Active', 2: 'Inactive', 3: 'Pending'}\n",
        "df_func['Status'] = df_func['Status_Code'].map(status_map)\n",
        "\n",
        "# 2. Расчёт BMI построчно\n",
        "def calculate_bmi(row):\n",
        "    height_m = row['Height'] / 100.0\n",
        "    return row['Weight'] / (height_m ** 2)\n",
        "\n",
        "df_func['BMI'] = df_func.apply(calculate_bmi, axis=1)\n",
        "\n",
        "print(\"DataFrame после map() и apply(axis=1):\\n\", df_func)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Status_Code  Height  Weight    Status        BMI\n",
        "0            1     170      70    Active  24.221474\n",
        "1            2     165      60  Inactive  22.038567\n",
        "2            3     180      90   Pending  27.777778\n",
        "3            1     175      80    Active  26.122449\n",
        "```\n",
        "\n",
        "> *Пояснение:* `map` использован для **кодирования**, `apply(axis=1)` — для **расчёта составного признака**. Оба метода создают **новые столбцы**, не изменяя исходные данные.\n",
        "\n",
        "---\n",
        "\n",
        "### V.2. Создание новых признаков: `assign()` и `eval()`\n",
        "\n",
        "#### V.2.1. `DataFrame.assign()` — декларативное создание столбцов\n",
        "\n",
        "Метод `assign()` возвращает **новый DataFrame** с добавленными столбцами. Его главное преимущество — **интеграция в цепочки вызовов** (method chaining), что улучшает читаемость и функциональность кода.\n",
        "\n",
        "```python\n",
        "df_new = df_func.assign(\n",
        "    BMI_rounded = lambda x: x['BMI'].round(1),\n",
        "    Is_Overweight = lambda x: x['BMI'] > 25\n",
        ")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `lambda` позволяет ссылаться на **уже существующие столбцы** в том же вызове `assign`.\n",
        "\n",
        "#### V.2.2. `pandas.eval()` — высокоскоростные вычисления через строку\n",
        "\n",
        "Функция `pd.eval()` использует движок **NumExpr** для выполнения арифметических и логических выражений **в C-слое**, минимизируя overhead Python.\n",
        "\n",
        "> **Когда использовать?**  \n",
        "> Только для **очень больших DataFrame** (обычно > 100 000 строк). Для малых данных накладные расходы на парсинг строки перевешивают выгоду.\n",
        "\n",
        "> **Пример: `eval` с множественными выражениями**\n",
        "\n",
        "```python\n",
        "N = 500_000\n",
        "df_large = pd.DataFrame(\n",
        "    np.random.randint(1, 100, size=(N, 3)),\n",
        "    columns=['A', 'B', 'C']\n",
        ")\n",
        "\n",
        "# Два новых признака за один вызов\n",
        "df_large.eval(\n",
        "    \"D = (A + B) / C; E = A * B - C\",\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "print(\"После eval (первые 3 строки):\\n\", df_large.head(3))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Разделение выражений точкой с запятой позволяет выполнить **несколько операций без промежуточных копий** — это ключ к максимальной производительности.\n",
        "\n",
        "---\n",
        "\n",
        "### V.3. Построение конвейеров: `pipe()`\n",
        "\n",
        "Метод `pipe()` позволяет строить **читаемые, последовательные конвейеры**, где результат предыдущей функции передаётся как первый аргумент в следующую.\n",
        "\n",
        "> **Преимущества:**\n",
        "> - Избегает вложенности: `f(g(h(df)))` → `df.pipe(h).pipe(g).pipe(f)`\n",
        "> - Поддерживает любые пользовательские функции\n",
        "> - Упрощает отладку и тестирование\n",
        "\n",
        "> **Пример: ETL-конвейер через `pipe()`**\n",
        "\n",
        "```python\n",
        "def filter_high_value(df, threshold):\n",
        "    return df[df['Sales'] > threshold]\n",
        "\n",
        "def add_bonus(df, rate):\n",
        "    return df.assign(Bonus=df['Sales'] * rate)\n",
        "\n",
        "data_pipe = pd.DataFrame({\n",
        "    'Agent': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'Sales': [800, 1200, 950, 1500],\n",
        "    'Region': ['North', 'South', 'North', 'South']\n",
        "})\n",
        "\n",
        "df_transformed = (\n",
        "    data_pipe\n",
        "    .pipe(filter_high_value, threshold=1000)\n",
        "    .pipe(add_bonus, rate=0.1)\n",
        "    .sort_values('Sales', ascending=False)\n",
        ")\n",
        "\n",
        "print(\"Результат конвейера через .pipe():\\n\", df_transformed)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "    Agent  Sales Region   Bonus\n",
        "3   Diana   1500  South   150.0\n",
        "1     Bob   1200  South   120.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* `pipe()` — это **синтаксический сахар**, не дающий прироста скорости. Производительность зависит от **векторизации внутри функций**.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Группировка, агрегация и реструктуризация\n",
        "\n",
        "Эти инструменты позволяют выполнять **сводный анализ**, обогащать данные и приводить их к нужному формату для моделирования или визуализации.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.1. Принцип Split-Apply-Combine: объект `GroupBy`\n",
        "\n",
        "Pandas реализует классическую парадигму:\n",
        "\n",
        "1. **Split** — разбиение на группы по одному или нескольким ключам.\n",
        "2. **Apply** — применение функции к каждой группе (агрегация, трансформация, фильтрация).\n",
        "3. **Combine** — сбор результатов в единый объект.\n",
        "\n",
        "> **Создание группы:** `df.groupby('column')` или `df.groupby(['col1', 'col2'])`\n",
        "\n",
        "---\n",
        "\n",
        "### VI.2. Агрегация данных: `agg()`\n",
        "\n",
        "Метод `agg()` (или `aggregate()`) вычисляет сводную статистику **по группам** и **возвращает результат меньшей размерности**.\n",
        "\n",
        "> **Пример: множественная агрегация по столбцам**\n",
        "\n",
        "```python\n",
        "df_group = pd.DataFrame({\n",
        "    'Department': ['HR', 'IT', 'IT', 'Sales', 'HR', 'IT'],\n",
        "    'Salary': [45000, 70000, 80000, 75000, 50000, 70000],\n",
        "    'Experience': [2, 5, 10, 6, 3, 7]\n",
        "})\n",
        "\n",
        "summary = df_group.groupby('Department').agg({\n",
        "    'Salary': ['mean', 'median'],\n",
        "    'Experience': ['sum', 'max']\n",
        "})\n",
        "\n",
        "print(\"Множественная агрегация:\\n\", summary)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "           Salary            Experience      \n",
        "             mean   median        sum max\n",
        "Department                                 \n",
        "HR        47500.0   47500.0          5   3\n",
        "IT        73333.3   70000.0         22  10\n",
        "Sales     75000.0   75000.0          6   6\n",
        "```\n",
        "\n",
        "> *Пояснение:* Результат имеет **MultiIndex в столбцах**, что позволяет точно идентифицировать каждую агрегацию.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.3. Трансформация данных: `transform()`\n",
        "\n",
        "`transform()` применяет функцию к группе, но **возвращает объект той же формы и индекса**, что и исходный DataFrame.\n",
        "\n",
        "> **Сценарий:** добавление групповой статистики к каждой строке **без слияния**.\n",
        "\n",
        "> **Пример: нормализация внутри групп**\n",
        "\n",
        "```python\n",
        "# Z-оценка зарплаты внутри отдела\n",
        "df_group['Salary_Z'] = df_group.groupby('Department')['Salary'].transform(\n",
        "    lambda x: (x - x.mean()) / x.std(ddof=1)\n",
        ")\n",
        "\n",
        "print(\"Трансформация (Z-оценка):\\n\", df_group[['Department', 'Salary', 'Salary_Z']])\n",
        "```\n",
        "\n",
        "> *Пояснение:* `transform` — это «невидимая сила» ETL: он обогащает данные, сохраняя их структуру.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.4. Фильтрация групп: `filter()`\n",
        "\n",
        "Метод `filter()` удаляет **целые группы**, если они не удовлетворяют условию.\n",
        "\n",
        "> **Условие:** функция должна возвращать **одно булево значение** на группу.\n",
        "\n",
        "> **Пример: оставить только отделы с >2 сотрудниками**\n",
        "\n",
        "```python\n",
        "df_filtered = df_group.groupby('Department').filter(lambda x: len(x) > 2)\n",
        "print(\"После фильтрации групп:\\n\", df_filtered['Department'].unique())  # ['HR' 'IT']\n",
        "```\n",
        "\n",
        "> *Пояснение:* Отдел `Sales` (1 сотрудник) исключён.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.5. Объединение таблиц\n",
        "\n",
        "#### VI.5.1. `pd.merge()` — реляционные соединения\n",
        "\n",
        "Аналог **SQL JOIN**. Ключевые параметры:\n",
        "- `on` — столбец-ключ,\n",
        "- `how` — тип соединения (`inner`, `left`, `right`, `outer`).\n",
        "\n",
        "> **Пример: LEFT и INNER JOIN**\n",
        "\n",
        "```python\n",
        "df_left = pd.DataFrame({\n",
        "    'Key': ['K0', 'K1', 'K2', 'K3'],\n",
        "    'A': ['A0', 'A1', 'A2', 'A3']\n",
        "})\n",
        "\n",
        "df_right = pd.DataFrame({\n",
        "    'Key': ['K1', 'K3', 'K4', 'K5'],\n",
        "    'B': ['B1', 'B3', 'B4', 'B5']\n",
        "})\n",
        "\n",
        "left_join = pd.merge(df_left, df_right, on='Key', how='left')\n",
        "inner_join = pd.merge(df_left, df_right, on='Key', how='inner')\n",
        "\n",
        "print(\"LEFT JOIN:\\n\", left_join)\n",
        "print(\"\\nINNER JOIN:\\n\", inner_join)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `left` сохраняет все строки из левой таблицы, `inner` — только совпадающие.\n",
        "\n",
        "#### VI.5.2. `pd.concat()` — конкатенация\n",
        "\n",
        "«Склеивает» объекты вдоль оси:\n",
        "- `axis=0` — вертикально (добавление строк),\n",
        "- `axis=1` — горизонтально (добавление столбцов).\n",
        "\n",
        "> **Параметр `join`:**\n",
        "> - `'outer'` (по умолчанию) — объединение индексов,\n",
        "> - `'inner'` — пересечение.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.6. Изменение формы данных\n",
        "\n",
        "#### Wide vs. Long Format\n",
        "\n",
        "- **Wide**: одна строка = одно наблюдение, переменные — отдельные столбцы.\n",
        "- **Long**: одна строка = одно измерение, переменные и значения — в двух столбцах.\n",
        "\n",
        "> **Long предпочтителен** для `groupby`, `seaborn`, `plotly`.\n",
        "\n",
        "#### `melt()` — Wide → Long\n",
        "\n",
        "```python\n",
        "df_wide = pd.DataFrame({\n",
        "    'ID': [1, 2],\n",
        "    'Score_Math': [90, 85],\n",
        "    'Score_Science': [88, 92]\n",
        "})\n",
        "\n",
        "df_long = df_wide.melt(\n",
        "    id_vars='ID',\n",
        "    var_name='Subject',\n",
        "    value_name='Score'\n",
        ")\n",
        "print(\"Wide → Long:\\n\", df_long)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   ID        Subject  Score\n",
        "0   1     Score_Math     90\n",
        "1   2     Score_Math     85\n",
        "2   1  Score_Science     88\n",
        "3   2  Score_Science     92\n",
        "```\n",
        "\n",
        "#### `pivot_table()` — Long → Wide\n",
        "\n",
        "```python\n",
        "df_pivot = df_long.pivot_table(\n",
        "    index='ID',\n",
        "    columns='Subject',\n",
        "    values='Score',\n",
        "    aggfunc='mean'  # обработка дубликатов\n",
        ")\n",
        "print(\"Long → Wide:\\n\", df_pivot)\n",
        "```\n",
        "\n",
        "#### `stack()` / `unstack()` — работа с MultiIndex\n",
        "\n",
        "- `unstack()`: переносит уровень индекса в столбцы.\n",
        "- `stack()`: обратная операция.\n",
        "\n",
        "> **Пример: unstack**\n",
        "\n",
        "```python\n",
        "index = pd.MultiIndex.from_tuples(\n",
        "    [('X', 'a'), ('X', 'b'), ('Y', 'a'), ('Y', 'b')],\n",
        "    names=['Level1', 'Level2']\n",
        ")\n",
        "df_multi = pd.DataFrame({'Data': [1, 2, 3, 4]}, index=index)\n",
        "\n",
        "df_unstacked = df_multi.unstack('Level2')\n",
        "print(\"Unstacked:\\n\", df_unstacked)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas предоставляет мощный и гибкий инструментарий для **анализа, трансформации и реструктуризации** данных. Освоение `groupby`, `merge`, `melt`, `assign` и `pipe` позволяет строить **промышленные ETL-конвейеры**, сочетающие читаемость, производительность и надёжность.\n"
      ],
      "metadata": {
        "id": "Ui7lqQuLj8UL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## VII. Работа с временными рядами и категориальными данными\n",
        "\n",
        "Работа с **временными рядами** и оптимизация памяти через **категориальные типы** — ключевые навыки для анализа больших и структурированных данных. Pandas предоставляет специализированные инструменты, превращающие эти задачи из «проблем» в «возможности».\n",
        "\n",
        "---\n",
        "\n",
        "### VII.1. Обработка временных данных: `pd.to_datetime()`\n",
        "\n",
        "Функция `pd.to_datetime()` преобразует строки, целые числа (Unix timestamp) или другие представления в объекты типа **`datetime64[ns]`** — основу всей временной аналитики в Pandas.\n",
        "\n",
        "> **Критически важно:** при работе с большими файлами всегда указывайте **`format`** явно. Это отключает медленный механизм «угадывания» формата и ускоряет парсинг в **10–100 раз**.\n",
        "\n",
        "> **Пример: безопасная конвертация дат**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "date_strings = ['2023/10/25', '2023/10/26', '2023/10/27']\n",
        "ser_dates = pd.Series(date_strings)\n",
        "\n",
        "# Явное указание формата — быстро и надёжно\n",
        "dates_dti = pd.to_datetime(ser_dates, format='%Y/%m/%d')\n",
        "print(\"Конвертация в DatetimeIndex:\\n\", dates_dti)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без `format` Pandas пытается проанализировать каждую строку — это недопустимо при загрузке миллионов записей.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.2. Передискретизация (Resampling): `DataFrame.resample()`\n",
        "\n",
        "Метод `resample()` изменяет **частоту временного ряда**, требуя **`DatetimeIndex`** в качестве индекса.\n",
        "\n",
        "- **Downsampling** (понижение частоты): `D → M` → требует **агрегации** (`sum`, `mean`).\n",
        "- **Upsampling** (повышение частоты): `M → D` → требует **заполнения** (`fillna`, `interpolate`).\n",
        "\n",
        "> **Пример: ежедневные данные → еженедельная сумма**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Ежедневный временной ряд\n",
        "daily_index = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n",
        "ts_daily = pd.Series(np.random.randint(10, 50, size=10), index=daily_index)\n",
        "\n",
        "# Downsample: сумма за неделю\n",
        "weekly_sum = ts_daily.resample('W').sum()\n",
        "print(\"\\nЕженедельная сумма:\\n\", weekly_sum)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `resample` возвращает объект `Resampler`, к которому можно применять любые агрегационные функции.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.3. Оконные функции: `DataFrame.rolling()`\n",
        "\n",
        "Метод `rolling(window=N)` создаёт **скользящее окно** фиксированного размера для вычисления локальных статистик.\n",
        "\n",
        "> **Пример: 3-дневное скользящее среднее**\n",
        "\n",
        "```python\n",
        "values = [10, 20, 30, 40, 50]\n",
        "df_ts = pd.DataFrame({'Value': values})\n",
        "\n",
        "# Скользящее среднее (первые 2 значения — NaN)\n",
        "df_ts['Rolling_Mean'] = df_ts['Value'].rolling(window=3).mean()\n",
        "print(\"\\nСкользящее среднее:\\n\", df_ts)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Value  Rolling_Mean\n",
        "0     10           NaN\n",
        "1     20           NaN\n",
        "2     30          20.0\n",
        "3     40          30.0\n",
        "4     50          40.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Скользящие окна — основа технического анализа, сглаживания шума и выявления трендов.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.4. Оптимизация памяти: тип данных `category`\n",
        "\n",
        "Столбцы с **низкой кардинальностью** (мало уникальных значений) — идеальные кандидаты на конвертацию в `category`.\n",
        "\n",
        "> **Как это работает?**  \n",
        "> Вместо хранения строк «HR», «IT», «HR»... Pandas хранит:\n",
        "> - словарь: `{0: 'HR', 1: 'IT'}`,\n",
        "> - массив целых: `[0, 1, 0, ...]`.\n",
        "\n",
        "> **Эффект:** сокращение памяти **в 10–20 раз**.\n",
        "\n",
        "> **Пример: сравнение потребления памяти**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Создаём 100 000 строк с 3 категориями\n",
        "np.random.seed(42)\n",
        "df_mem = pd.DataFrame({\n",
        "    'Category': np.random.choice(['HR', 'IT', 'Sales'], size=100_000),\n",
        "    'Values': np.random.randint(1, 100, size=100_000)\n",
        "})\n",
        "\n",
        "print(\"Память до (object):\")\n",
        "print(df_mem.memory_usage(deep=True) // 1024)  # в КБ\n",
        "\n",
        "# Конвертация в category\n",
        "df_mem['Category'] = df_mem['Category'].astype('category')\n",
        "\n",
        "print(\"\\nПамять после (category):\")\n",
        "print(df_mem.memory_usage(deep=True) // 1024)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это один из самых простых и эффективных способов **масштабировать Pandas** на большие данные.\n",
        "\n",
        "---\n",
        "\n",
        "## VIII. Комплексная аналитика и оптимизация производительности\n",
        "\n",
        "Профессиональный анализ требует **интеграции всех инструментов** в единый, производительный и читаемый конвейер.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.1. Пример ETL-конвейера на основе Pandas\n",
        "\n",
        "> **Сценарий:** анализ журнала продаж — от загрузки до агрегации.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. EXTRACTION ---\n",
        "df_raw = pd.DataFrame({\n",
        "    'Order_ID': [101, 102, 103, 104, 105],\n",
        "    'Date_Str': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n",
        "    'Amount': [100.5, np.nan, 250.0, 150.0, 300.5],\n",
        "    'Client_Type': ['New', 'Old', 'New', 'Old', 'New']\n",
        "})\n",
        "\n",
        "# Преобразуем даты\n",
        "df_raw['Date'] = pd.to_datetime(df_raw['Date_Str'], format='%Y-%m-%d')\n",
        "df_raw.drop(columns=['Date_Str'], inplace=True)\n",
        "\n",
        "# --- 2. TRANSFORMATION ---\n",
        "# 2.1. Заполнение пропусков\n",
        "df_raw['Amount'].fillna(df_raw['Amount'].median(), inplace=True)\n",
        "\n",
        "# 2.2. Извлечение признаков из даты\n",
        "df_raw['DayOfWeek'] = df_raw['Date'].dt.dayofweek  # 0=Пн, 6=Вс\n",
        "\n",
        "# 2.3. Агрегация: продажи по дням\n",
        "daily_sales = df_raw.groupby('Date').agg(Total_Amount=('Amount', 'sum'))\n",
        "\n",
        "# 2.4. Обогащение: добавляем дневную сумму к каждой строке\n",
        "df_raw['Daily_Total'] = df_raw.groupby('Date')['Amount'].transform('sum')\n",
        "\n",
        "# --- 3. LOADING ---\n",
        "df_raw.to_csv('results/sales_etl_output.csv', index=False)\n",
        "print(\"ETL завершён. Первые 3 строки:\\n\", df_raw.head(3))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Конвейер демонстрирует **полный цикл**: загрузка → очистка → feature engineering → агрегация → сохранение.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.2. Продвинутая разработка признаков (Feature Engineering)\n",
        "\n",
        "#### 1. Извлечение из дат\n",
        "```python\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5\n",
        "```\n",
        "\n",
        "#### 2. Бинаризация (Binning)\n",
        "```python\n",
        "bins = [0, 100, 500, 1000, np.inf]\n",
        "labels = ['Low', 'Medium', 'High', 'Very High']\n",
        "df['Amount_Category'] = pd.cut(df['Amount'], bins=bins, labels=labels)\n",
        "```\n",
        "\n",
        "#### 3. One-Hot Encoding\n",
        "```python\n",
        "df_encoded = pd.get_dummies(df, columns=['Client_Type'], prefix='Type')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти методы критичны для **подготовки данных к машинному обучению**.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.3. Ускорение пользовательских функций: Numba\n",
        "\n",
        "Когда векторизация невозможна, **Numba** компилирует Python-код в машинные инструкции.\n",
        "\n",
        "> **Пример: ускорение поэлементной функции**\n",
        "\n",
        "```python\n",
        "import numba\n",
        "import numpy as np\n",
        "\n",
        "@numba.vectorize\n",
        "def custom_transform(x):\n",
        "    return np.sqrt(x) if x > 0 else 0.0\n",
        "\n",
        "# Применяем к NumPy-массиву\n",
        "series = pd.Series(np.random.rand(1_000_000) * 1000)\n",
        "result = custom_transform(series.to_numpy())  # возвращает ndarray\n",
        "```\n",
        "\n",
        "> *Пояснение:* Numba работает **только с NumPy-массивами**, не с Pandas-объектами. Передавайте `.to_numpy()`.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.4. Параллельные вычисления: Dask и Swifter\n",
        "\n",
        "- **Dask**: разбивает DataFrame на **partitions**, распределяет вычисления по ядрам/кластеру. API похож на Pandas, но с отложенным выполнением.\n",
        "- **Swifter**: автоматически выбирает между **векторизацией** и **параллелизацией**:\n",
        "  ```python\n",
        "  df['new_col'] = df['col'].swifter.apply(complex_function)\n",
        "  ```\n",
        "\n",
        "> *Пояснение:* Используйте Dask/Swifter, когда данные **не помещаются в память** или когда `apply` слишком медлен.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Pandas — это методология, а не просто библиотека.** Её мощь раскрывается через **стратегический выбор инструментов**:\n",
        "\n",
        "| Уровень оптимизации             | Инструменты                                  | Цель                              |\n",
        "|-------------------------------|---------------------------------------------|-----------------------------------|\n",
        "| **I/O**                       | `dtype`, `usecols`, `parse_dates`           | Быстрая и безопасная загрузка     |\n",
        "| **Память**                    | `category`, `pd.Int64`                      | Снижение потребления RAM          |\n",
        "| **Массовые вычисления**       | `eval()`, `query()`, векторизованные UFuncs | Высокая скорость на больших данных|\n",
        "| **Пользовательская логика**   | `Numba`, `Cython`                           | Ускорение нетривиальных функций   |\n",
        "| **Архитектура кода**          | `pipe()`, `assign()`, `loc`                 | Читаемость, модульность, надёжность |\n",
        "\n",
        "Таким образом, Pandas предоставляет **полную экосистему** для построения промышленных конвейеров обработки данных — от первичной очистки до подготовки данных для машинного обучения. Освоение его принципов позволяет не просто «работать с таблицами», а **строить масштабируемые, воспроизводимые и производительные аналитические системы**.\n",
        "\n"
      ],
      "metadata": {
        "id": "2VkX1-FPkkv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# МОДУЛЬ 3: Библиотека Polars — Архитектура высокопроизводительной обработки данных\n",
        "\n",
        "**Polars** — это не просто ещё одна библиотека для работы с табличными данными, а **архитектурная эволюция** аналитических вычислений в Python. В отличие от традиционных инструментов, Polars переносит вычислительную нагрузку за пределы интерпретатора Python, опираясь на современные стандарты памяти и системное программирование. Это позволяет ему достигать **порядков прироста в скорости** и **линейного масштабирования** на многоядерных системах, особенно при работе с большими объёмами данных.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Архитектурные принципы Polars: Фундамент производительности\n",
        "\n",
        "Производительность Polars не является результатом «улучшенной обёртки» вокруг Pandas, а обусловлена **глубинной перестройкой стека**: от языка реализации до модели хранения данных.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1. Двуединый языковой стек: Rust и Apache Arrow\n",
        "\n",
        "#### 1.1.1. Ядро на Rust: скорость без компромиссов\n",
        "\n",
        "Polars полностью написан на **Rust** — языке системного программирования, сочетающем:\n",
        "\n",
        "- **Безопасность памяти** без сборщика мусора,\n",
        "- **Высокую производительность** за счёт компиляции в нативный код,\n",
        "- **Нативную многопоточность** без блокировок.\n",
        "\n",
        "Ключевое преимущество: **обход Global Interpreter Lock (GIL)** Python. В то время как Pandas-операции часто выполняются в одном потоке, ядро Polars **автоматически распределяет работу по всем ядрам CPU**, используя пулы потоков и SIMD-векторизацию. Это обеспечивает **линейное ускорение** при увеличении числа ядер — особенно при агрегациях, фильтрации и трансформациях.\n",
        "\n",
        "#### 1.1.2. Apache Arrow: колоночная память как стандарт\n",
        "\n",
        "Внутреннее представление данных в Polars строится на **Apache Arrow** — отраслевом стандарте для **колоночного хранения данных в оперативной памяти**.\n",
        "\n",
        "> **Почему это важно?**  \n",
        "> Большинство аналитических операций (например, `groupby`, `filter`, `sum`) работают **со столбцами**, а не со строками. Колоночный формат:\n",
        "> - минимизирует промахи кэша CPU,\n",
        "> - позволяет загружать в процессор только нужные данные,\n",
        "> - исключает накладные расходы на упаковку/распаковку объектов.\n",
        "\n",
        "В отличие от Pandas (где `object`-столбцы хранят ссылки на Python-объекты), Arrow хранит **непрерывные блоки однотипных данных**, что делает доступ к ним чрезвычайно быстрым.\n",
        "\n",
        "> **Zero-Copy Interoperability**  \n",
        "> Поскольку Polars строго следует спецификации Arrow, он может **обмениваться данными без копирования** с другими Arrow-совместимыми системами (Apache Spark, DuckDB, PyArrow, Vaex), просто передавая указатели на буферы памяти.\n",
        "\n",
        "> **Важно:** Polars использует **собственную реализацию буферов и вычислений на Rust**, а не обёртку вокруг PyArrow. Это даёт полный контроль над оптимизациями и избегает внешних зависимостей.\n",
        "\n",
        "---\n",
        "\n",
        "### Сравнение архитектур: Polars vs Pandas\n",
        "\n",
        "| Критерий                     | Polars                                      | Pandas (стандартный стек)                |\n",
        "|-----------------------------|---------------------------------------------|------------------------------------------|\n",
        "| **Язык ядра**               | Rust (компилируемый, безопасный)            | Python / C (NumPy)                       |\n",
        "| **Модель памяти**           | Apache Arrow (колоночная, непрерывная)      | NumPy (часто строковая или блочная)      |\n",
        "| **Параллелизм**             | Нативная многопоточность + SIMD             | Преимущественно однопоточный (GIL)       |\n",
        "| **Модель выполнения**       | Eager **и** Lazy (с оптимизатором запросов) | Только Eager                             |\n",
        "| **Типизация**               | Строгая (тип выводится из выражений)        | Гибкая (неявные приведения, `object`)    |\n",
        "| **Стратегия копирования**   | Минимизация (Zero-Copy при совместимости)   | Частые копии (особенно при `copy()` и `fillna`) |\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Параллелизм и распределение нагрузки\n",
        "\n",
        "#### 1.2.1. «Embarrassingly Parallel» по дизайну\n",
        "\n",
        "Polars изначально спроектирован как **лёгкораспараллеливаемая система** (*Embarrassingly Parallel*). Это означает:\n",
        "\n",
        "- Рабочая нагрузка **автоматически делится** между всеми доступными ядрами.\n",
        "- Независимые выражения (например, два новых столбца в `select`) вычисляются **параллельно**.\n",
        "- При `group_by().agg()` каждая группа может обрабатываться **отдельным потоком**.\n",
        "\n",
        "> **Пример:**  \n",
        "> Запрос вида  \n",
        "> ```python\n",
        "> df.select([\n",
        ">     pl.col(\"A\").mean(),\n",
        ">     pl.col(\"B\").std()\n",
        "> ])\n",
        "> ```  \n",
        "> будет выполнен в **двух потоках одновременно**, без участия пользователя.\n",
        "\n",
        "#### 1.2.2. Совместимость с Python multiprocessing\n",
        "\n",
        "Внутренняя многопоточность на Rust накладывает **ограничения на использование `multiprocessing` в Python**:\n",
        "\n",
        "- В Unix-системах метод `fork` (по умолчанию) **копирует состояние всех потоков Rust**, что может привести к **нестабильности**.\n",
        "- **Рекомендация:** при использовании `multiprocessing` всегда устанавливайте контекст `spawn` или `forkserver`:\n",
        "  ```python\n",
        "  import multiprocessing as mp\n",
        "  if __name__ == \"__main__\":\n",
        "      mp.set_start_method(\"spawn\")  # или \"forkserver\"\n",
        "      # ... запуск процессов\n",
        "  ```\n",
        "\n",
        "> *Пояснение:* Это не недостаток, а признак того, что Polars — **независимый вычислительный движок**, а не «тонкая обёртка».\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Модели выполнения запросов: Eager vs Lazy\n",
        "\n",
        "Polars поддерживает два режима работы: **Eager** (немедленное выполнение) и **Lazy** (отложенное с оптимизацией). **Lazy API — предпочтительный подход** для производительного анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1. Концепции Eager и Lazy\n",
        "\n",
        "#### 2.1.1. Eager API — как в Pandas\n",
        "\n",
        "Каждая операция выполняется **сразу**, результат возвращается как `DataFrame`.\n",
        "\n",
        "```python\n",
        "import polars as pl\n",
        "\n",
        "# Eager: данные загружаются немедленно\n",
        "df = pl.read_csv(\"data.csv\")\n",
        "filtered = df.filter(pl.col(\"value\") > 100)\n",
        "```\n",
        "\n",
        "> **Плюсы:** простота, интерактивность.  \n",
        "> **Минусы:** нет глобальной оптимизации, возможны избыточные вычисления.\n",
        "\n",
        "#### 2.1.2. Lazy API — сила оптимизатора\n",
        "\n",
        "Операции строят **логический план запроса** (`LogicalPlan`), но **не выполняются** до вызова `.collect()`.\n",
        "\n",
        "```python\n",
        "# Lazy: создаётся план, данные не загружаются\n",
        "lf = pl.scan_csv(\"large_data.csv\")\n",
        "result = (\n",
        "    lf\n",
        "    .filter(pl.col(\"value\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"price\").mean())\n",
        "    .collect()  # ← запуск выполнения\n",
        ")\n",
        "```\n",
        "\n",
        "> **Преимущество:** движок видит **весь запрос целиком** и может его **оптимизировать глобально**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Оптимизатор запросов Polars\n",
        "\n",
        "Оптимизатор преобразует `LogicalPlan` в **физический план выполнения**, применяя мощные правила:\n",
        "\n",
        "#### Predicate Pushdown (проталкивание фильтров)\n",
        "\n",
        "Фильтры применяются **на этапе чтения данных**, а не после загрузки.  \n",
        "**Результат:** чтение **только релевантных строк** из файла → меньше I/O, меньше памяти.\n",
        "\n",
        "> **До оптимизации:**  \n",
        "> `read → group_by → filter`  \n",
        "> **После:**  \n",
        "> `read + filter → group_by`\n",
        "\n",
        "#### Projection Pushdown (проталкивание проекций)\n",
        "\n",
        "Загружаются **только нужные столбцы**.  \n",
        "Если в запросе используются 3 из 50 столбцов — читаются только эти 3.\n",
        "\n",
        "#### Другие ключевые оптимизации\n",
        "\n",
        "- **Join Ordering** — перестановка соединений для минимизации промежуточных размеров и предотвращения OOM.\n",
        "- **Common Subplan Elimination** — кэширование повторяющихся подзапросов.\n",
        "- **Expression Simplification** — свёртка констант, упрощение логики.\n",
        "- **Type Coercion** — приведение типов к минимально достаточным (например, `Int32` вместо `Int64`).\n",
        "\n",
        "> **Практическое значение:**  \n",
        "> Пользователю **не нужно** вручную оптимизировать порядок операций (например, «фильтровать до сортировки»). Polars делает это **автоматически**.\n",
        "\n",
        "---\n",
        "\n",
        "### Ключевые оптимизации Polars Query Optimizer\n",
        "\n",
        "| Оптимизация                  | Принцип работы                                      | Влияние                              |\n",
        "|-----------------------------|-----------------------------------------------------|--------------------------------------|\n",
        "| **Predicate Pushdown**      | Фильтрация на уровне источника данных               | ↓ I/O, ↓ RAM, ↑ скорость             |\n",
        "| **Projection Pushdown**     | Загрузка только используемых столбцов               | ↓ Потребление памяти                 |\n",
        "| **Join Ordering**           | Выбор порядка JOIN для минимизации промежуточных данных | ↓ Риск OOM, ↑ стабильность         |\n",
        "| **Common Subplan Elimination** | Повторное использование вычисленных подвыражений  | ↓ Избыточные вычисления              |\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Polars переосмысливает обработку данных, заменяя «интерпретируемую» модель Pandas на **компилируемый, колоночный, многопоточный движок**. Его архитектура — ответ на вызовы современной аналитики: **скорость**, **масштабируемость** и **эффективность памяти**. Освоение Lazy API и понимание оптимизаций — ключ к раскрытию всего потенциала библиотеки.\n",
        "\n"
      ],
      "metadata": {
        "id": "jG65vH4wlAg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Система выражений (DSL) Polars\n",
        "\n",
        "Сердце Polars — это **декларативный предметно-ориентированный язык (DSL)** на основе объектов типа `pl.Expr`. Выражения не просто трансформируют данные — они описывают **логический план вычислений**, который движок Polars анализирует, оптимизирует и выполняет параллельно в Rust.\n",
        "\n",
        "### 3.1. Выражения как строительные блоки\n",
        "\n",
        "Каждое выражение:\n",
        "- принимает **столбец (`Series`)** на входе,\n",
        "- возвращает **новый столбец** на выходе,\n",
        "- является **компонуемым**: можно строить цепочки без промежуточных копий.\n",
        "\n",
        "> **Пример: цепочка трансформаций**\n",
        "\n",
        "```python\n",
        "import polars as pl\n",
        "\n",
        "expr = (\n",
        "    pl.col(\"Revenue\")\n",
        "    .mul(100)                  # Умножить на 100\n",
        "    .log()                     # Натуральный логарифм\n",
        "    .alias(\"Scaled_Log_Revenue\")  # Переименовать\n",
        ")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эта конструкция **не вычисляется сразу**. Она становится частью логического плана, который будет оптимизирован и выполнен позже.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Сложные агрегации в `group_by().agg()`\n",
        "\n",
        "Выражения позволяют встраивать **фильтрацию, сортировку и даже подзапросы** непосредственно в агрегацию — без пользовательских функций (UDF).\n",
        "\n",
        "> **Пример: условная агрегация по группе**\n",
        "\n",
        "```python\n",
        "df = pl.DataFrame({\n",
        "    \"ID\": [1, 1, 2, 2],\n",
        "    \"Val\": [10, 15, 18, 20],\n",
        "    \"Flag\": [5, 12, 10, 18]\n",
        "})\n",
        "\n",
        "result = df.group_by(\"ID\").agg(\n",
        "    pl.col(\"Val\")\n",
        "    .filter(pl.col(\"Flag\") > pl.mean(\"Flag\"))  # Только где Flag > среднего в группе\n",
        "    .max()\n",
        "    .alias(\"Conditional_Max\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (2, 2)\n",
        "┌─────┬─────────────────┐\n",
        "│ ID  ┆ Conditional_Max │\n",
        "│ --- ┆ ---             │\n",
        "│ i64 ┆ i64             │\n",
        "╞═════╪═════════════════╡\n",
        "│ 1   ┆ 15              │\n",
        "│ 2   ┆ 20              │\n",
        "└─────┴─────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* В первой группе среднее `Flag = 8.5`. Только вторая строка (`Flag=12`) удовлетворяет условию, поэтому максимум `Val = 15`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Условная логика: `pl.when().then().otherwise()`\n",
        "\n",
        "Это **SIMD-оптимизированная**, безветвёвая реализация условий — аналог `CASE WHEN` в SQL или `np.where` в NumPy, но **значительно быстрее**.\n",
        "\n",
        "> **Пример: условное среднее**\n",
        "\n",
        "```python\n",
        "df_cond = pl.DataFrame({\n",
        "    \"age\": [25, 35, 45, 55],\n",
        "    \"height\": [170, 175, 180, 165]\n",
        "})\n",
        "\n",
        "cutoff = 30\n",
        "result = df_cond.select(\n",
        "    pl.when(pl.col(\"age\") < cutoff)\n",
        "    .then(pl.lit(1.0))\n",
        "    .otherwise(pl.col(\"height\"))\n",
        "    .mean()\n",
        "    .alias(\"Whenthen_Mean\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (1, 1)\n",
        "┌────────────────┐\n",
        "│ Whenthen_Mean  │\n",
        "│ ---            │\n",
        "│ f64            │\n",
        "╞════════════════╡\n",
        "│ 173.333333     │  # (1 + 175 + 180 + 165) / 4\n",
        "└────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Благодаря **branchless-коду** и **SIMD**, эта операция выполняется на порядки быстрее, чем аналог с циклами или медленными UDF.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Продвинутые методы обработки данных\n",
        "\n",
        "### 4.1. Оконные функции: `.over()`\n",
        "\n",
        "Метод `over()` реализует **оконные функции**, аналогичные `OVER (PARTITION BY ...)` в SQL. Он позволяет вычислять агрегаты **внутри групп**, не сворачивая DataFrame.\n",
        "\n",
        "> **Пример: скользящее среднее по региону**\n",
        "\n",
        "```python\n",
        "data = pl.DataFrame({\n",
        "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"South\"],\n",
        "    \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
        "    \"sales\": [100, 150, 200, 250, 300]\n",
        "}).with_columns(\n",
        "    pl.col(\"date\").str.to_date(\"%Y-%m-%d\")  # Преобразуем в Date\n",
        ").sort([\"region\", \"date\"])\n",
        "\n",
        "# Добавляем среднее по региону ко всем строкам\n",
        "result = data.with_columns(\n",
        "    pl.col(\"sales\").mean().over(\"region\").alias(\"Avg_Sales_By_Region\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (5, 4)\n",
        "┌────────┬────────────┬───────┬─────────────────────────┐\n",
        "│ region ┆ date       ┆ sales ┆ Avg_Sales_By_Region     │\n",
        "│ ---    ┆ ---        ┆ ---   ┆ ---                     │\n",
        "│ str    ┆ date       ┆ i64   ┆ f64                     │\n",
        "╞════════╪════════════╪═══════╪═════════════════════════╡\n",
        "│ North  ┆ 2023-01-01 ┆ 100   ┆ 125.0                   │\n",
        "│ North  ┆ 2023-01-02 ┆ 150   ┆ 125.0                   │\n",
        "│ South  ┆ 2023-01-01 ┆ 200   ┆ 250.0                   │\n",
        "│ South  ┆ 2023-01-02 ┆ 250   ┆ 250.0                   │\n",
        "│ South  ┆ 2023-01-03 ┆ 300   ┆ 250.0                   │\n",
        "└────────┴────────────┴───────┴─────────────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Каждая строка «видит» среднее своей группы, но сохраняет свою позицию.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Обработка временных рядов: Asof Join\n",
        "\n",
        "`join_asof` — **специализированное соединение для временных рядов**, где точные совпадения времени не требуются.\n",
        "\n",
        "> **Параметры:**\n",
        "> - `strategy`: `'backward'` (по умолчанию), `'forward'`, `'nearest'`,\n",
        "> - `tolerance`: максимальное допустимое отклонение (например, `'1h'`, `'5min'`).\n",
        "\n",
        "> **Пример: присоединение курсов к транзакциям**\n",
        "\n",
        "```python\n",
        "df_transactions = pl.DataFrame({\n",
        "    \"tx_time\": [\n",
        "        pl.datetime(2023, 1, 1, 10, 0),\n",
        "        pl.datetime(2023, 1, 1, 10, 30)\n",
        "    ],\n",
        "    \"amount\": [1000, 1500]\n",
        "})\n",
        "\n",
        "df_fx_rates = pl.DataFrame({\n",
        "    \"fx_time\": [\n",
        "        pl.datetime(2023, 1, 1, 9, 50),   # Ближайший до 10:00\n",
        "        pl.datetime(2023, 1, 1, 10, 15)   # Ближайший до 10:30\n",
        "    ],\n",
        "    \"rate\": [1.1, 1.2]\n",
        "})\n",
        "\n",
        "result = df_transactions.join_asof(\n",
        "    df_fx_rates,\n",
        "    left_on=\"tx_time\",\n",
        "    right_on=\"fx_time\",\n",
        "    strategy=\"backward\",\n",
        "    tolerance=\"1h\"\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (2, 3)\n",
        "┌─────────────────────┬────────┬──────┐\n",
        "│ tx_time             ┆ amount ┆ rate │\n",
        "│ ---                 ┆ ---    ┆ ---  │\n",
        "│ datetime[μs]        ┆ i64    ┆ f64  │\n",
        "╞═════════════════════╪════════╪══════╡\n",
        "│ 2023-01-01 10:00:00 ┆ 1000   ┆ 1.1  │\n",
        "│ 2023-01-01 10:30:00 ┆ 1500   ┆ 1.2  │\n",
        "└─────────────────────┴────────┴──────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это стандартный паттерн в финтехе, IoT и лог-аналитике.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Производительность, память и интеграция\n",
        "\n",
        "### 5.1. Оптимизация типов данных\n",
        "\n",
        "Полный контроль над типами — ключ к эффективному использованию памяти.\n",
        "\n",
        "| Исходный тип               | Рекомендуемый тип Polars      | Эффект                          |\n",
        "|---------------------------|-------------------------------|----------------------------------|\n",
        "| Строки с низкой кардинальностью | `pl.Categorical` или `pl.Enum` | ↓ память в 10–100×, ↑ скорость сравнения |\n",
        "| `float64`                 | `pl.Float32`                  | ↓ память в 2×                   |\n",
        "| `int64` (малый диапазон)  | `pl.Int32`, `pl.Int16`        | ↓ память, ↑ кэш-эффективность   |\n",
        "\n",
        "> **Пример: загрузка с оптимизацией типов**\n",
        "\n",
        "```python\n",
        "df = pl.read_csv(\n",
        "    \"large_file.csv\",\n",
        "    dtypes={\n",
        "        \"user_id\": pl.Int32,\n",
        "        \"category\": pl.Categorical,\n",
        "        \"price\": pl.Float32\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Streaming API: обработка данных «вне памяти»\n",
        "\n",
        "Для датасетов **больше RAM** используйте **Streaming API**:\n",
        "\n",
        "```python\n",
        "# LazyFrame\n",
        "lf = pl.scan_csv(\"huge_file.csv\")\n",
        "\n",
        "# Обработка потоками\n",
        "result = (\n",
        "    lf\n",
        "    .filter(pl.col(\"value\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"price\").mean())\n",
        "    .collect(streaming=True)  # ← ключевой параметр\n",
        ")\n",
        "```\n",
        "\n",
        "> **Как это работает?**  \n",
        "> Polars разбивает запрос на этапы и обрабатывает данные **пакетами**, никогда не загружая всё в память. Если операция не поддерживает streaming (например, глобальная сортировка), Polars автоматически откатывается к in-memory режиму.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Интеграция с ML: `to_numpy()` и Zero-Copy\n",
        "\n",
        "Для передачи данных в `scikit-learn` или `PyTorch`:\n",
        "\n",
        "```python\n",
        "X = df.select(pl.col([\"feature1\", \"feature2\"])).to_numpy()\n",
        "```\n",
        "\n",
        "> **Zero-Copy достигается, если:**\n",
        "> - все столбцы — одного числового типа (`Float32` или `Int64`),\n",
        "> - нет пропущенных значений (`null`),\n",
        "> - данные хранятся в одном блоке (chunk),\n",
        "> - порядок памяти — колоночный (Fortran-style, по умолчанию в Polars).\n",
        "\n",
        "Если условия не выполнены, Polars **автоматически выполнит копирование и приведение типов** — но это будет чётко и безопасно.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Сравнение производительности: Polars vs Pandas\n",
        "\n",
        "Независимые бенчмарки (включая [**db-benchmark**](https://h2oai.github.io/db-benchmark/)) демонстрируют:\n",
        "\n",
        "| Операция                | Преимущество Polars       | Типичный прирост скорости |\n",
        "|------------------------|----------------------------|----------------------------|\n",
        "| Чтение + фильтрация    | Predicate Pushdown + параллелизм | **3–4×**                  |\n",
        "| `group_by().agg()`     | Параллельная агрегация по группам | **4–5×**                  |\n",
        "| `join`                 | Join Ordering + Arrow      | **до 14×**                |\n",
        "| Условная логика        | SIMD + branchless          | **2–10×** (в зависимости от сложности) |\n",
        "\n",
        "> **Пояснение:** Выигрыш особенно заметен **на данных > 1 млн строк**, где накладные расходы Pandas (GIL, копирование, отсутствие глобальной оптимизации) становятся критичными.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Polars — это архитектурный прорыв** в обработке табличных данных. Он не пытается «ускорить Pandas», а предлагает **новую парадигму**:\n",
        "\n",
        "- **Вычисления** — в компилируемом, многопоточном ядре на **Rust**,\n",
        "- **Память** — в эффективном колоночном формате **Apache Arrow**,\n",
        "- **Оптимизация** — через **Lazy API** и **логический план запроса**,\n",
        "- **Выразительность** — через **DSL на основе выражений**.\n",
        "\n",
        "Эти принципы позволяют Polars:\n",
        "- обрабатывать **миллионы и миллиарды строк** на одном компьютере,\n",
        "- выполнять **сложные аналитические запросы** без написания UDF,\n",
        "- масштабироваться **линейно с числом ядер**,\n",
        "- интегрироваться с **экосистемой Arrow** без копирования данных.\n",
        "\n",
        "Таким образом, Polars не просто альтернатива Pandas — это **следующее поколение фреймворка для аналитики данных**, объединяющее скорость системного программирования, выразительность DSL и удобство Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "TY2jtxOpmstR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 4. Dask: Архитектура, методология и масштабирование вычислений в экосистеме Python\n",
        "\n",
        "## Введение: Масштабирование PyData и архитектурный вызов\n",
        "\n",
        "Экосистема Scientific Python — с её столпами **NumPy** и **Pandas** — давно стала де-факто стандартом для анализа данных и научных вычислений. Эти библиотеки достигают высокой производительности за счёт **векторизованных операций**, реализованных на C/C++, и оптимизированных под работу **в оперативной памяти (in-memory)**.\n",
        "\n",
        "Однако на практике данные часто:\n",
        "- **превышают объём RAM** (out-of-core),\n",
        "- требуют **интенсивных CPU-вычислений**, которые не могут быть ускорены одним ядром.\n",
        "\n",
        "В этих сценариях традиционный стек PyData сталкивается с фундаментальными ограничениями — в первую очередь, из-за **Global Interpreter Lock (GIL)** в CPython, который блокирует истинный параллелизм на уровне потоков.\n",
        "\n",
        "**Dask** был создан как гибкая платформа параллельных вычислений, которая **расширяет**, а не заменяет, экосистему PyData. Он состоит из двух компонентов:\n",
        "\n",
        "1. **Низкоуровневого планировщика задач**, управляющего исполнением графа вычислений,\n",
        "2. **Высокоуровневых коллекций** (`Dask Array`, `Dask DataFrame`, `Dask Bag`), которые имитируют интерфейсы NumPy, Pandas и итераторов Python.\n",
        "\n",
        "Ключевое преимущество Dask — **масштабируемость вниз и вверх**:\n",
        "- **Вниз**: запуск на ноутбуке для обработки 100 ГБ данных с диска,\n",
        "- **Вверх**: распределённый кластер с тысячами ядер для обработки петабайтов.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Фундаментальные принципы архитектуры Dask\n",
        "\n",
        "### 1.1. Ленивые вычисления (Lazy Evaluation)\n",
        "\n",
        "Все операции в Dask **ленивы**: вызов метода не выполняет расчёты, а лишь **строит граф задач**. Фактическое выполнение запускается только при вызове терминальных методов:\n",
        "\n",
        "- `.compute()` — возвращает итоговый результат в памяти (например, как `pandas.DataFrame`),\n",
        "- `.persist()` — сохраняет промежуточные результаты в распределённой памяти (полезно для интерактивных сессий).\n",
        "\n",
        "Этот подход позволяет Dask **анализировать весь план вычислений целиком** и применять оптимизации: удалять избыточные операции, минимизировать передачу данных, выбирать порядок выполнения.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Графы задач (Task Graphs)\n",
        "\n",
        "Все алгоритмы в Dask кодируются как **ориентированные ациклические графы (DAG)**:\n",
        "\n",
        "- **Узлы** — функции или операции,\n",
        "- **Рёбра** — зависимости между результатами.\n",
        "\n",
        "Этот граф служит **универсальным промежуточным представлением (IR)** для всех коллекций.\n",
        "\n",
        "> **Пример: построение графа с `dask.delayed`**\n",
        "\n",
        "```python\n",
        "import dask\n",
        "\n",
        "@dask.delayed\n",
        "def calculate_mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "# Создаём отложенные объекты\n",
        "data1 = [1, 2, 3, 4]\n",
        "data2 = [10, 20, 30]\n",
        "\n",
        "mean1 = calculate_mean(data1)\n",
        "mean2 = calculate_mean(data2)\n",
        "\n",
        "# Складываем результаты\n",
        "final_sum = dask.delayed(lambda x, y: x + y)(mean1, mean2)\n",
        "\n",
        "# Вычисление запускается здесь\n",
        "result = final_sum.compute()\n",
        "print(\"Результат:\", result)  # 20.5\n",
        "```\n",
        "\n",
        "> *Пояснение:* Каждый вызов отложенной функции добавляет узел в граф. Dask выполняет их **параллельно**, если зависимости позволяют.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. Динамический планировщик (Scheduler)\n",
        "\n",
        "Планировщик — «мозг» Dask. Он:\n",
        "- распределяет задачи по исполнителям (workers),\n",
        "- управляет зависимостями,\n",
        "- оптимизирует **локальность данных** (стремится выполнять задачи там, где уже находятся входные данные).\n",
        "\n",
        "#### Типы планировщиков:\n",
        "\n",
        "| Тип | Описание | Использование |\n",
        "|-----|----------|---------------|\n",
        "| **Single-machine** | Локальный пул потоков/процессов | Быстрый старт, небольшие данные |\n",
        "| **Distributed** | Полноценный кластер (даже на одной машине через `LocalCluster`) | Масштабирование, дашборд, отказоустойчивость |\n",
        "\n",
        "> **Дашборд (Dashboard)** — одно из главных преимуществ распределённого планировщика: визуализация графа, загрузки CPU, объёма памяти, сетевого трафика в реальном времени.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.4. Накладные расходы и гранулярность задач\n",
        "\n",
        "Dask спроектирован с минимальными накладными расходами (~1 мс на задачу), но **это не бесплатно**. Если задача выполняется < 100 мс, накладные расходы на планирование и передачу данных могут **перевесить выгоду от параллелизма**.\n",
        "\n",
        "> **Рекомендация:**  \n",
        "> Размер чанка (chunk/partition) должен быть таким, чтобы **время выполнения одной задачи ≥ 100 мс**.\n",
        "\n",
        "Неправильный выбор гранулярности — частая ошибка новичков, приводящая к **деградации производительности** по сравнению с Pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### Сравнение высокоуровневых коллекций Dask\n",
        "\n",
        "| Коллекция | Основа | Принцип | Использование |\n",
        "|----------|--------|--------|----------------|\n",
        "| **Dask Array** | `numpy.ndarray` | Блочная (chunked) структура | Научные расчёты, многомерные данные, линейная алгебра |\n",
        "| **Dask DataFrame** | `pandas.DataFrame` | Разбиение по строкам (partitioning) | ETL, агрегация, обработка больших таблиц |\n",
        "| **Dask Bag** | Python-итераторы | Параллельные коллекции объектов | Логи, JSON, неструктурированные данные |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dask DataFrame: параллельная обработка табличных данных\n",
        "\n",
        "### 2.1. Архитектура и секционирование\n",
        "\n",
        "`Dask DataFrame` — это **коллекция обычных `pandas.DataFrame`**, разбитых по строкам. Каждый «кусок» (partition) обрабатывается независимо.\n",
        "\n",
        "- **Преимущество**: может обрабатывать **100 ГБ на ноутбуке** или **100 ТБ на кластере**.\n",
        "- **Ограничение**: операции, требующие **перетасовки (shuffle)**, становятся медленными из-за межпроцессной коммуникации.\n",
        "\n",
        "> **Совет:** если вы часто фильтруете или группируете по определённому столбцу, **разбивайте данные по этому столбцу заранее** (например, при сохранении в Parquet).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Совместимость с Pandas и ленивое исполнение\n",
        "\n",
        "API `Dask DataFrame` **почти идентичен Pandas**:\n",
        "\n",
        "```python\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Ленивая загрузка (данные не читаются!)\n",
        "df = dd.read_parquet(\"data/*.parquet\")\n",
        "\n",
        "# Ленивые трансформации\n",
        "filtered = df[df.value > 0]\n",
        "aggregated = filtered.groupby(\"category\").value.mean()\n",
        "\n",
        "# Фактическое вычисление\n",
        "result = aggregated.compute()  # → pandas.Series\n",
        "```\n",
        "\n",
        "> **Важно:** `compute()` возвращает **обычный Pandas-объект**. Если результат не помещается в память — используйте `.to_parquet()` или другие методы сохранения без материализации.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. Производительность: что работает быстро, а что — нет\n",
        "\n",
        "- ✅ **Быстро**: `groupby().sum()`, `groupby().mean()` — **декомпозируемые агрегации** (MapReduce).\n",
        "- ❌ **Медленно**: `groupby().apply(custom_func)` — требует **shuffle**, так как все строки одной группы должны быть на одном worker'е.\n",
        "\n",
        "> **Пример: избегайте `apply`, если можно**\n",
        "\n",
        "```python\n",
        "# ПЛОХО: медленно из-за shuffle\n",
        "df.groupby(\"user\").apply(lambda x: fit_model(x))\n",
        "\n",
        "# ХОРОШО: используйте встроенные агрегаты или перепишите логику через map_partitions\n",
        "```\n",
        "\n",
        "> *Пояснение:* Dask не может оптимизировать произвольные функции. Старайтесь оставаться в рамках векторизованных операций.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Dask Array: масштабирование многомерных массивов\n",
        "\n",
        "### 3.1. Архитектура и чанкинг\n",
        "\n",
        "`Dask Array` — это блочная структура поверх `numpy.ndarray`. Данные разбиваются на **чанки** — небольшие NumPy-массивы, которые помещаются в память одного worker'а.\n",
        "\n",
        "> **Как выбрать размер чанка?**\n",
        "> - Слишком мал → много накладных расходов,\n",
        "> - Слишком велик → не помещается в память,\n",
        "> - **Идея**: 10–100 МБ на чанк, время выполнения ≥ 100 мс.\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "\n",
        "# Создаём массив 10 000×10 000, разбитый на чанки 1000×1000\n",
        "x = da.random.random((10_000, 10_000), chunks=(1000, 1000))\n",
        "y = x + x.T  # Ленивые операции\n",
        "result = y.sum().compute()  # Фактическое вычисление\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Совместимость с NumPy\n",
        "\n",
        "Dask Array поддерживает **большую часть API NumPy**:\n",
        "- Универсальные функции (`sin`, `log`, `+`, `*`) — применяются к каждому чанку,\n",
        "- Редукции (`sum`, `mean`) — координируются между чанками,\n",
        "- Линейная алгебра (`dot`, `svd`) — реализована с учётом распределённости.\n",
        "\n",
        "> **Важно:** не все функции NumPy доступны. Если операция требует глобального контекста (например, `np.argsort`), Dask либо выдаст ошибку, либо предложит альтернативу.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Пользовательские функции: `map_blocks`\n",
        "\n",
        "Для внедрения собственной логики используется `map_blocks`:\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "import numpy as np\n",
        "\n",
        "x = da.arange(1000, chunks=100)\n",
        "\n",
        "def block_max(block):\n",
        "    return np.array([block.max()])  # 100 элементов → 1\n",
        "\n",
        "# Указываем форму выходного чанка\n",
        "result = x.map_blocks(block_max, chunks=(1,), dtype=x.dtype)\n",
        "\n",
        "print(result.compute())  # [99, 199, 299, ..., 999]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `map_blocks` — точка расширения для высокопроизводительных кернелов (Numba, Cython), которые можно масштабировать на весь массив.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Dask Bag: параллелизм для неструктурированных данных\n",
        "\n",
        "### 4.1. Архитектура и использование\n",
        "\n",
        "`Dask Bag` — коллекция **произвольных Python-объектов** (словари, строки, JSON). Используется на ранних этапах ETL:\n",
        "\n",
        "```python\n",
        "import dask.bag as db\n",
        "import json\n",
        "\n",
        "# Чтение и параллельная обработка логов\n",
        "bag = db.read_text(\"logs/*.json.gz\").map(json.loads)\n",
        "\n",
        "# Пайплайн: фильтрация → извлечение → агрегация\n",
        "top_jobs = (\n",
        "    bag\n",
        "    .filter(lambda r: r.get(\"age\", 0) > 30)\n",
        "    .map(lambda r: r[\"job\"])\n",
        "    .frequencies()\n",
        "    .topk(5, key=lambda x: x[1])\n",
        "    .compute()\n",
        ")\n",
        "```\n",
        "\n",
        "> **Преимущество:** гибкость для «грязных» данных без схемы.\n",
        "\n",
        "> **Недостаток:** высокие накладные расходы на **сериализацию** (каждый объект pickle'ится при передаче между процессами).\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Методология: Bag — только на входе\n",
        "\n",
        "**Рекомендация:** используйте `Dask Bag` **только для первоначальной очистки и парсинга**. Как только данные становятся структурированными — **конвертируйте в `Dask DataFrame`**:\n",
        "\n",
        "```python\n",
        "# После парсинга JSON\n",
        "df = bag.to_dataframe()  # или bag.to_delayed() → обработка → dd.from_delayed()\n",
        "```\n",
        "\n",
        "Так вы получите преимущества **типизированной памяти**, **векторизации** и **оптимизированного планировщика**.\n",
        "\n"
      ],
      "metadata": {
        "id": "XExFt3xhmv9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Запуск и управление вычислениями: планирование и диагностика\n",
        "\n",
        "### 5.1. Модель «Клиент–Планировщик–Работник»\n",
        "\n",
        "Распределённый планировщик Dask (`Dask Distributed`) использует классическую трёхзвенную архитектуру, применимую как на локальной машине, так и в кластере:\n",
        "\n",
        "1. **Клиент (Client)** — интерфейс пользователя. Отправляет граф задач планировщику и получает результаты.\n",
        "2. **Планировщик (Scheduler)** — центральный узел. Управляет зависимостями, распределяет задачи, отслеживает состояние работников и локальность данных.\n",
        "3. **Работник (Worker)** — исполнитель. Выполняет задачи, хранит промежуточные результаты в памяти и может обмениваться данными с другими работниками напрямую (по указанию планировщика), минимизируя задержки.\n",
        "\n",
        "Эта архитектура обеспечивает **масштабируемость**, **отказоустойчивость** и **гибкое управление ресурсами**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Локальный запуск с `LocalCluster`\n",
        "\n",
        "Даже при работе на одном компьютере рекомендуется использовать **распределённый режим** через `LocalCluster` — он предоставляет доступ к **асинхронному API** и, что особенно важно, к **диагностическому дашборду**.\n",
        "\n",
        "> **Пример: инициализация локального кластера**\n",
        "\n",
        "```python\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "# Запускает планировщик и несколько worker-процессов\n",
        "cluster = LocalCluster(\n",
        "    n_workers=4,        # количество процессов\n",
        "    threads_per_worker=2,\n",
        "    memory_limit=\"2GB\"  # лимит памяти на worker\n",
        ")\n",
        "\n",
        "# Подключаем клиент\n",
        "client = Client(cluster)\n",
        "\n",
        "# URL дашборда выводится автоматически (например, http://127.0.0.1:8787)\n",
        "print(\"Дашборд:\", client.dashboard_link)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `LocalCluster` использует **процессы** (а не потоки), чтобы обойти GIL и обеспечить истинный параллелизм даже на одном ядре.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Интерактивная диагностика: Dask Dashboard\n",
        "\n",
        "Дашборд — **главный инструмент профилирования** в Dask. Он построен на Bokeh и предоставляет в реальном времени:\n",
        "\n",
        "#### **Task Stream**\n",
        "- Каждый прямоугольник — задача на одном потоке.\n",
        "- **Цвета** — тип операции (`read-parquet`, `groupby-sum` и т.д.).\n",
        "- **Красные полосы** — передача данных между работниками (**shuffle**). Много красного = проблема с чанкингом или избыточной коммуникацией.\n",
        "- **Белые промежутки** — простой потока = несбалансированная нагрузка или блокировки.\n",
        "\n",
        "#### **Memory Usage**\n",
        "- **Синий** — безопасный уровень памяти,\n",
        "- **Оранжевый** — данные начинают сбрасываться на диск (spilling),\n",
        "- **Красный** — worker приостановлен из-за нехватки памяти.\n",
        "\n",
        "> **Методология:**  \n",
        "> Эффективный разработчик Dask **не просто пишет код**, а **анализирует Task Stream** после каждого запуска, корректируя:\n",
        "> - размер чанков,\n",
        "> - структуру графа,\n",
        "> - выбор операций (избегая `apply` и `shuffle`).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Интеграция с машинным обучением: Dask-ML\n",
        "\n",
        "Библиотека **`dask-ml`** расширяет экосистему Scikit-learn для работы с **out-of-core** и **распределёнными** данными.\n",
        "\n",
        "### 6.1. Параллельная предобработка\n",
        "\n",
        "Модули `dask_ml.preprocessing` предоставляют трансформеры, совместимые с `sklearn`:\n",
        "- `StandardScaler`, `MinMaxScaler`,\n",
        "- `OneHotEncoder` с поддержкой `CategoricalDtype`.\n",
        "\n",
        "Все они работают **лениво** и **параллельно** на `Dask DataFrame` и `Dask Array`.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Масштабирование обучения: мета-оценщики\n",
        "\n",
        "#### **`ParallelPostFit`**\n",
        "Оборачивает обученную модель и позволяет **параллельно применять** `predict`/`transform` к большим данным.\n",
        "\n",
        "#### **`Incremental`**\n",
        "Для моделей, поддерживающих `partial_fit` (например, `SGDClassifier`), обучает **блок за блоком**, не загружая всё в память.\n",
        "\n",
        "> **Пример: обучение на 1 млрд записей**\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from dask_ml.wrappers import Incremental\n",
        "\n",
        "# Большие данные (out-of-core)\n",
        "X = da.random.normal(size=(1_000_000_000, 10), chunks=(100_000, 10))\n",
        "y = (X.sum(axis=1) > 0).astype(int)\n",
        "\n",
        "# Обучение по блокам\n",
        "model = Incremental(SGDClassifier(random_state=42))\n",
        "model.fit(X, y)  # каждый чанк → partial_fit\n",
        "\n",
        "# Параллельный прогноз\n",
        "predictions = model.predict(X).compute()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой подход делает возможным обучение на данных, которые **никогда не помещаются в RAM**.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. Поиск гиперпараметров: инкрементальные методы\n",
        "\n",
        "- **`IncrementalSearchCV`** и **`HyperbandSearchCV`** — аналоги `GridSearchCV`, но с **ранней остановкой**.\n",
        "- Модели, показывающие плохую сходимость, **отбрасываются досрочно**, что экономит ресурсы.\n",
        "\n",
        "> **Ограничение:** требует поддержки `partial_fit` от модели.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Комплексные практические кейсы\n",
        "\n",
        "### 7.1. Методология out-of-core ETL/ELT\n",
        "\n",
        "Оптимальный пайплайн в Dask включает:\n",
        "\n",
        "1. **Параллельная загрузка**: `dd.read_parquet(\"s3://bucket/data*.parquet\")`.\n",
        "2. **Ленивая трансформация**: фильтрация, очистка, feature engineering.\n",
        "3. **Персистенция**: если за шагом следует несколько дорогих операций, вызовите `.persist()`, чтобы **закешировать промежуточный результат** в распределённой памяти.\n",
        "4. **Сохранение**: `.to_parquet()`, `.to_csv()` или запись в БД — **без `.compute()`**, чтобы избежать материализации.\n",
        "\n",
        "> **Пример:**\n",
        "\n",
        "```python\n",
        "df = dd.read_parquet(\"raw_data/\")\n",
        "clean = df[df.value.notnull()].assign(...)\n",
        "clean = clean.persist()  # ← кешируем\n",
        "\n",
        "# Несколько независимых агрегаций\n",
        "agg1 = clean.groupby(\"cat\").value.mean()\n",
        "agg2 = clean.groupby(\"cat\").value.std()\n",
        "\n",
        "# Сохраняем без полной загрузки в память\n",
        "agg1.to_parquet(\"results/mean.parquet\")\n",
        "agg2.to_parquet(\"results/std.parquet\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7.2. Методологическая карта перехода к Dask\n",
        "\n",
        "**Не используйте Dask «на всякий случай»**. Переход оправдан **только при соблюдении критериев**:\n",
        "\n",
        "| Критерий | Dask **НЕ рекомендован** | Dask **рекомендован** | Обоснование |\n",
        "|--------|--------------------------|------------------------|-------------|\n",
        "| **Размер данных** | Помещаются в RAM | Превышают RAM (out-of-core) | Накладные расходы не окупаются |\n",
        "| **Длительность вычисления** | < 1 секунды | > 1–2 секунд | Минимальная задача ≥ 100 мс |\n",
        "| **Диагностика** | Не требуется | Нужен контроль памяти и производительности | Дашборд — ключ к оптимизации |\n",
        "\n",
        "> **Важно:** на малых данных Dask может быть **в 10–100 раз медленнее** Pandas из-за инициализации графа и планировщика.\n",
        "\n",
        "---\n",
        "\n",
        "### 7.3. Кейс: гибридный пайплайн временного ряда\n",
        "\n",
        "**Задача:** обработать 50 ГБ метеоданных → фильтрация → FFT → агрегация.\n",
        "\n",
        "**Архитектура:**\n",
        "\n",
        "1. **Табличная фильтрация (Dask DataFrame)**  \n",
        "   ```python\n",
        "   df = dd.read_parquet(\"weather/\")\n",
        "   filtered = df[(df.temp > 0) & (df.time >= \"2020\")]\n",
        "   ```\n",
        "\n",
        "2. **Переход к численным данным (Dask Array)**  \n",
        "   ```python\n",
        "   signal = filtered.temp.values  # → dask.array\n",
        "   signal = signal.rechunk(chunks=(\"auto\",))  # оптимизация чанков под FFT\n",
        "   ```\n",
        "\n",
        "3. **Численный анализ (Dask Array)**  \n",
        "   ```python\n",
        "   fft_result = da.fft.fft(signal)\n",
        "   power = da.abs(fft_result) ** 2\n",
        "   ```\n",
        "\n",
        "4. **Финальная агрегация (обратно в DataFrame)**  \n",
        "   ```python\n",
        "   result_df = dd.from_dask_array(power, columns=[\"power\"])\n",
        "   daily_stats = result_df.groupby(result_df.index // 86400).mean()\n",
        "   daily_stats.to_parquet(\"fft_stats/\")\n",
        "   ```\n",
        "\n",
        "> **Ключевой принцип:**  \n",
        "> Данные **мигрируют между коллекциями** в зависимости от задачи:\n",
        "> - `DataFrame` — для структурированной фильтрации,\n",
        "> - `Array` — для HPC-операций.\n",
        ">\n",
        "> Минимизируйте переходы и **выравнивайте чанки**, чтобы избежать дорогостоящего `rechunk`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Dask — это зрелая, гибкая и диагностически прозрачная платформа** для масштабирования аналитики данных в экосистеме Python. Его сила — не в «автомагическом» ускорении, а в **осознанном управлении вычислениями**:\n",
        "\n",
        "- через **ленивые графы**,\n",
        "- через **блочную память**,\n",
        "- через **интерактивную диагностику**.\n",
        "\n",
        "Использование Dask требует **методологической дисциплины**: понимания накладных расходов, гранулярности задач, архитектуры данных и инструментов профилирования.\n",
        "\n",
        "При правильном применении Dask позволяет:\n",
        "- обрабатывать **петабайты данных** на кластере,\n",
        "- выполнять **численные расчёты на миллиардах точек**,\n",
        "- обучать **ML-модели на out-of-core данных**,\n",
        "- и всё это — **в знакомом синтаксисе PyData**.\n",
        "\n",
        "Таким образом, Dask завершает эволюцию от **in-memory аналитики** (Pandas) к **масштабируемой, распределённой и диагностируемой** вычислительной платформе для современных задач данных.\n",
        "\n",
        "\n",
        "\n",
        "✅ **Цикл полностью завершён.**  \n",
        "Вы прошли путь от основ (NumPy) → аналитики (Pandas) → высокой производительности (Polars) → распределённых вычислений (Dask).\n"
      ],
      "metadata": {
        "id": "Q9UOCsmzoXq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 5: Apache Spark и PySpark — Архитектура и практика распределённой обработки Big Data\n",
        "\n",
        "## Введение\n",
        "\n",
        "В эпоху Big Data обработка объёмов информации, превышающих возможности единичной вычислительной машины, стала неотъемлемой частью научных исследований, промышленного анализа и разработки интеллектуальных систем. Apache Spark представляет собой одну из наиболее зрелых и широко применяемых платформ для решения подобных задач. В отличие от библиотек, ориентированных на in-memory вычисления (таких как Pandas или Polars), Spark изначально спроектирован как **распределённый вычислительный движок**, способный масштабироваться от локального режима до кластеров, охватывающих тысячи узлов.\n",
        "\n",
        "Архитектурная целостность Spark обеспечивается не только его способностью распараллеливать вычисления, но и глубокой интеграцией механизмов отказоустойчивости, оптимизации запросов и эффективного управления памятью. Понимание этих механизмов — от жизненного цикла приложения до работы оптимизатора Catalyst — является необходимым условием для построения производительных и надёжных систем обработки больших данных.\n",
        "\n",
        "Настоящий модуль посвящён систематическому изложению архитектурных основ Spark, эволюции его программных интерфейсов и принципов функционирования его вычислительного ядра. Особое внимание уделено специфике взаимодействия PySpark с JVM-базой Spark, что критически важно для разработчиков, использующих Python в качестве основного языка аналитики. Все теоретические положения сопровождаются практическими примерами, демонстрирующими их применение в реальных сценариях.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Архитектурные основы распределённой модели Spark\n",
        "\n",
        "### 1.1. Диспетчер, Исполнители и Рабочие Узлы: Формальные определения\n",
        "\n",
        "Распределённое приложение Spark функционирует на основе трёх взаимосвязанных компонентов: Диспетчера программы (Driver Program), Исполнителей (Executors) и Рабочих Узлов (Worker Nodes).\n",
        "\n",
        "**Диспетчер программы** является центральным управляющим элементом любого Spark-приложения. Он инициализируется при создании объекта `SparkSession` и может размещаться либо на клиентской машине (в режиме client), либо на одном из узлов кластера (в режиме cluster). Основные функции Диспетчера включают: преобразование последовательности пользовательских преобразований и действий в направленный ациклический граф (DAG), который служит логическим планом вычислений; взаимодействие с кластерным менеджером для запроса и выделения вычислительных ресурсов в виде Исполнителей; мониторинг статуса выполнения задач на Исполнителях и обеспечение отказоустойчивости; сбор и агрегация окончательных результатов вычислений.\n",
        "\n",
        "**Исполнители** представляют собой рабочие процессы, запускаемые на Рабочих Узлах кластера. Каждый Исполнитель получает в своё распоряжение выделенный объём оперативной памяти и набор процессорных ядер (Cores), которые служат минимальными единицами параллельного исполнения. Исполнители отвечают за непосредственное выполнение задач, назначенных им Диспетчером, над партициями данных.\n",
        "\n",
        "**Рабочие Узлы** — это физические или виртуальные машины, составляющие вычислительный кластер. На каждом Рабочем Узле может быть запущено один или несколько Исполнителей.\n",
        "\n",
        "> **Пример: Инициализация SparkSession и базовое распределённое вычисление**\n",
        "\n",
        "В следующем примере демонстрируется создание Spark-приложения и выполнение простой операции подсчёта. Даже в локальном режиме (`master=\"local[*]\"`) Spark создаёт Диспетчер и один Исполнитель (внутри того же процесса), что позволяет изучать его архитектуру на одной машине.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Инициализация Диспетчера (Driver)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Architecture Example\") \\\n",
        "    .master(\"local[*]\") \\  # Локальный режим со всеми доступными ядрами\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание RDD из диапазона чисел. Данные автоматически разбиваются на партиции.\n",
        "numbers = spark.sparkContext.parallelize(range(1, 1000001), numSlices=4)\n",
        "\n",
        "# Действие (Action): запускает DAG и возвращает результат Диспетчеру\n",
        "total = numbers.reduce(lambda a, b: a + b)\n",
        "print(f\"Сумма чисел от 1 до 1000000: {total}\")\n",
        "\n",
        "# Завершение работы приложения\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Вызов `parallelize` создаёт RDD с 4 партициями. Метод `reduce` является действием (Action), которое инициирует вычисление. Диспетчер разбивает задачу на 4 подзадачи, которые выполняются параллельно на Исполнителе (в локальном режиме — в том же процессе). Итоговый результат агрегируется и возвращается в Диспетчер.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.5. Влияние замыканий (Closures) на состояние Driver\n",
        "\n",
        "При разработке распределённых приложений на PySpark крайне важно понимать механизм передачи кода и данных от Диспетчера к Исполнителям.\n",
        "\n",
        "> **Пример: Демонстрация неизменности переменной Driver из Исполнителя**\n",
        "\n",
        "Следующий код иллюстрирует классическую ошибку, связанную с непониманием замыканий в распределённой среде. Разработчик пытается инкрементировать переменную `counter` из функции, выполняемой на Исполнителе. Однако результат оказывается неожиданным.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Closure Example\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Глобальная переменная в процессе Диспетчера\n",
        "counter = 0\n",
        "\n",
        "def increment_counter(value):\n",
        "    global counter\n",
        "    counter += 1  # Эта операция изменяет ЛОКАЛЬНУЮ копию переменной на Исполнителе\n",
        "    return value\n",
        "\n",
        "# Создаём RDD и применяем функцию\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.map(increment_counter).collect()  # Запускаем действие\n",
        "\n",
        "print(f\"Значение counter в Диспетчере: {counter}\")  # Вывод: 0\n",
        "\n",
        "# Правильный способ: использование аккумулятора\n",
        "acc_counter = sc.accumulator(0)\n",
        "\n",
        "def increment_accumulator(value):\n",
        "    global acc_counter\n",
        "    acc_counter.add(1)  # Аккумулятор гарантирует агрегацию в Диспетчере\n",
        "    return value\n",
        "\n",
        "rdd.map(increment_accumulator).collect()\n",
        "print(f\"Значение аккумулятора: {acc_counter.value}\")  # Вывод: 5\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* В первом случае переменная `counter` внутри `increment_counter` является независимой копией на каждом Исполнителе. Изменения не отражаются в Диспетчере. Во втором случае используется специальный объект `Accumulator`, который предназначен для безопасной агрегации информации из Исполнителей в Диспетчер.\n",
        "\n",
        "---\n",
        "\n",
        "## II. Эволюция Abstraction API: От RDD к структурированной обработке\n",
        "\n",
        "### 2.2. DataFrame API (PySpark)\n",
        "\n",
        "В современной практике, где подавляющее большинство данных имеет табличную структуру, DataFrame API является стандартом де-факто.\n",
        "\n",
        "> **Пример: Чтение данных, трансформации и анализ с помощью DataFrame**\n",
        "\n",
        "В этом примере показано, как с помощью DataFrame API можно выполнить типичный ETL-пайплайн: загрузку данных из CSV-файла, фильтрацию, агрегацию и анализ плана выполнения.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame API Example\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 1. Чтение данных в DataFrame\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .csv(\"sales_data.csv\")\n",
        "\n",
        "# Показать схему данных\n",
        "df.printSchema()\n",
        "# root\n",
        "#  |-- product: string (nullable = true)\n",
        "#  |-- category: string (nullable = true)\n",
        "#  |-- price: double (nullable = true)\n",
        "#  |-- quantity: integer (nullable = true)\n",
        "\n",
        "# 2. Применение преобразований (ленивые операции)\n",
        "filtered_df = df.filter(col(\"price\") > 10.0)\n",
        "aggregated_df = filtered_df.groupBy(\"category\").agg(avg(\"price\").alias(\"avg_price\"))\n",
        "\n",
        "# 3. Анализ физического плана выполнения\n",
        "print(\"Физический план:\")\n",
        "aggregated_df.explain(\"formatted\")\n",
        "\n",
        "# 4. Выполнение действия и сбор результата\n",
        "result = aggregated_df.collect()\n",
        "for row in result:\n",
        "    print(f\"Категория: {row['category']}, Средняя цена: {row['avg_price']:.2f}\")\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Все операции до вызова `collect()` являются преобразованиями (Transformations) и лишь строят логический план. Метод `explain(\"formatted\")` выводит читаемый физический план, в котором можно увидеть использование оптимизаций Catalyst, таких как `Filter` перед `Scan` (Predicate Pushdown).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Взаимодействие PySpark и JVM: Сериализационный барьер и Apache Arrow\n",
        "\n",
        "Для устранения сериализационного барьера в Spark была интегрирована библиотека **Apache Arrow**.\n",
        "\n",
        "> **Пример: Сравнение производительности UDF с и без Arrow**\n",
        "\n",
        "Следующий пример демонстрирует, как включить Arrow и создать векторизованную UDF, которая работает с целыми столбцами данных за один вызов, а не построчно.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Arrow UDF Example\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание тестового DataFrame\n",
        "df = spark.range(0, 1000000).toDF(\"id\")\n",
        "df = df.withColumn(\"value\", col(\"id\") * 2.0)\n",
        "\n",
        "# Скалярная UDF (медленная, без Arrow)\n",
        "def slow_udf(x):\n",
        "    return x * 1.1\n",
        "\n",
        "spark.udf.register(\"slow_udf\", slow_udf, DoubleType())\n",
        "start = time.time()\n",
        "df.selectExpr(\"slow_udf(value) as new_value\").collect()\n",
        "slow_time = time.time() - start\n",
        "\n",
        "# Векторизованная UDF с Arrow (быстрая)\n",
        "@pandas_udf(DoubleType())\n",
        "def fast_udf(v: pd.Series) -> pd.Series:\n",
        "    return v * 1.1\n",
        "\n",
        "start = time.time()\n",
        "df.select(fast_udf(col(\"value\")).alias(\"new_value\")).collect()\n",
        "fast_time = time.time() - start\n",
        "\n",
        "print(f\"Скалярная UDF: {slow_time:.2f} секунд\")\n",
        "print(f\"Векторизованная (Arrow) UDF: {fast_time:.2f} секунд\")\n",
        "print(f\"Ускорение: {slow_time / fast_time:.2f}x\")\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Векторизованная UDF, отмеченная декоратором `@pandas_udf`, получает и возвращает целые `pandas.Series`. Благодаря Arrow, передача данных между JVM и Python происходит без сериализации, что приводит к значительному ускорению.\n",
        "\n",
        "---\n",
        "\n",
        "## III. Движок исполнения Spark: Catalyst и Tungsten (Глубокий анализ)\n",
        "\n",
        "### 3.2. Физический план: Выбор стратегий и стоимостное моделирование\n",
        "\n",
        "Умение анализировать физический план является ключевым навыком для оптимизации запросов.\n",
        "\n",
        "> **Пример: Анализ плана выполнения операции Join**\n",
        "\n",
        "В этом примере создаются два DataFrame и выполняется операция соединения. Анализ плана позволяет понять, какой алгоритм Join был выбран оптимизатором.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Join Plan Analysis\").getOrCreate()\n",
        "\n",
        "# Создание двух небольших DataFrame\n",
        "df1 = spark.createDataFrame([(i, f\"user_{i}\") for i in range(1, 101)], [\"id\", \"name\"])\n",
        "df2 = spark.createDataFrame([(i, f\"category_{i % 5}\") for i in range(1, 101)], [\"id\", \"category\"])\n",
        "\n",
        "# Выполнение Join\n",
        "joined_df = df1.join(df2, on=\"id\")\n",
        "\n",
        "# Вывод расширенного плана\n",
        "print(\"Расширенный план выполнения:\")\n",
        "joined_df.explain(\"extended\")\n",
        "\n",
        "# В реальных сценариях для больших таблиц Spark может выбрать\n",
        "# BroadcastHashJoin или SortMergeJoin в зависимости от статистики.\n",
        "```\n",
        "\n",
        "> *Пояснение:* В выводе `explain` можно увидеть физический оператор, например, `BroadcastHashJoin`. Это означает, что Catalyst определил, что одна из таблиц достаточно мала, чтобы быть переданной (broadcasted) всем Исполнителям, что избегает дорогостоящей операции shuffle.\n",
        "\n"
      ],
      "metadata": {
        "id": "C-opDJc48ICU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IV. Практика PySpark: Код, оптимизация и анализ плана выполнения\n",
        "\n",
        "В производственных задачах PySpark используется для создания надёжных и масштабируемых ETL-пайплайнов, что требует не только знания синтаксиса DataFrame, но и глубокого понимания того, как операции высокого уровня трансформируются в низкоуровневые распределённые события. Настоящий раздел посвящён переходу от теории к практике: мы рассмотрим канонические примеры, проанализируем их физические планы и продемонстрируем методы оптимизации, применяемые в реальных инженерных задачах.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1. Идиоматичное использование DataFrame API: Пример ETL с оконными функциями\n",
        "\n",
        "Оконные функции (Window Functions) являются мощным инструментом для выполнения сложных аналитических операций, таких как ранжирование, кумулятивные суммы и скользящие средние, без необходимости выполнять дорогостоящую глобальную агрегацию.\n",
        "\n",
        "> **Пример: Ранжирование сотрудников по зарплате внутри отдела**\n",
        "\n",
        "Рассмотрим типичную задачу из корпоративной аналитики: необходимо для каждого отдела проранжировать сотрудников по убыванию заработной платы. В реляционных базах данных эта задача решается с помощью аналитических функций `RANK() OVER (PARTITION BY ... ORDER BY ...)`. В PySpark аналогичный результат достигается с помощью модуля `pyspark.sql.window`.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Инициализация сессии\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Window Function Example\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание тестового набора данных\n",
        "data = [\n",
        "    (1, \"Alice\", 10, 5000),\n",
        "    (2, \"Bob\", 10, 6000),\n",
        "    (3, \"Charlie\", 20, 7000),\n",
        "    (4, \"David\", 10, 5000),\n",
        "    (5, \"Eve\", 20, 8000)\n",
        "]\n",
        "columns = [\"empno\", \"ename\", \"deptno\", \"sal\"]\n",
        "emp_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Определение оконной спецификации:\n",
        "# — партиционирование по отделу (deptno),\n",
        "# — сортировка внутри партиции по зарплате (убывание)\n",
        "window_spec = Window.partitionBy(\"deptno\").orderBy(F.col(\"sal\").desc())\n",
        "\n",
        "# Применение оконной функции ранжирования\n",
        "result_df = emp_df.withColumn(\"rank\", F.rank().over(window_spec))\n",
        "\n",
        "print(\"Результат ранжирования сотрудников:\")\n",
        "result_df.show()\n",
        "\n",
        "# Анализ физического плана выполнения\n",
        "print(\"\\nФизический план выполнения:\")\n",
        "result_df.explain(\"formatted\")\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "+-----+-------+------+----+----+\n",
        "|empno|  ename|deptno| sal|rank|\n",
        "+-----+-------+------+----+----+\n",
        "|    2|    Bob|    10|6000|   1|\n",
        "|    1|  Alice|    10|5000|   2|\n",
        "|    4|  David|    10|5000|   2|\n",
        "|    5|    Eve|    20|8000|   1|\n",
        "|    3|Charlie|    20|7000|   2|\n",
        "+-----+-------+------+----+----+\n",
        "```\n",
        "\n",
        "> *Пояснение:* В физическом плане, который выводится командой `explain(\"formatted\")`, можно наблюдать следующую последовательность:\n",
        "> 1. **ShuffleExchange** по столбцу `deptno` — все строки с одинаковым `deptno` перераспределяются на один исполнитель.\n",
        "> 2. **Sort** внутри каждой партиции по `sal DESC`.\n",
        "> 3. **Window** — применение функции `rank()`.\n",
        ">\n",
        "> Несмотря на лаконичность кода, операция требует **shuffle**, что делает её потенциально дорогой при большом объёме данных. Это подчёркивает важный принцип: даже высокоуровневые API скрывают низкоуровневые распределённые операции, которые необходимо учитывать при проектировании пайплайнов.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Кейс 1: Устранение дорогостоящих операций Shuffle\n",
        "\n",
        "Shuffle — это операция перераспределения данных по ключу, которая неизбежна при `JOIN`, `GROUP BY` и оконных функциях с партиционированием. Её стоимость обусловлена сериализацией, передачей данных по сети и десериализацией.\n",
        "\n",
        "> **Оптимизация: Broadcast Join для малых таблиц**\n",
        "\n",
        "Наиболее эффективный способ избежать shuffle — использовать **Broadcast Hash Join**, когда одна из таблиц мала (например, справочник регионов). Spark транслирует малую таблицу в память каждого исполнителя, что позволяет выполнять соединение локально.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Broadcast Join Example\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"104857600\") \\  # 100 МБ\n",
        "    .getOrCreate()\n",
        "\n",
        "# Большая таблица фактов (например, транзакции)\n",
        "large_df = spark.range(0, 1000000).toDF(\"id\").withColumn(\"region_id\", F.col(\"id\") % 10)\n",
        "\n",
        "# Малая справочная таблица (регионы)\n",
        "small_df = spark.createDataFrame(\n",
        "    [(i, f\"Region_{i}\") for i in range(10)],\n",
        "    [\"region_id\", \"region_name\"]\n",
        ")\n",
        "\n",
        "# Принудительный broadcast для гарантии\n",
        "result = large_df.join(\n",
        "    F.broadcast(small_df),\n",
        "    on=\"region_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "print(\"Результат соединения (первые 5 строк):\")\n",
        "result.show(5)\n",
        "\n",
        "# Анализ плана: в выводе будет BroadcastExchange и BroadcastHashJoin\n",
        "print(\"\\nФизический план (фрагмент):\")\n",
        "result.explain(\"simple\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* В выводе `explain` появится строка вида `*(2) BroadcastHashJoin`, что подтверждает использование broadcast-стратегии. Это означает, что **shuffle для большой таблицы не происходит**, и вся операция выполняется за счёт локальных вычислений на каждом исполнителе. В производственной среде рекомендуется **явно указывать `broadcast()`**, даже если автоматическая оптимизация включена, чтобы избежать неожиданного переключения на shuffle-join при изменении размера данных.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3. Кейс 2: Методы борьбы с Data Skew (перекосом данных)\n",
        "\n",
        "**Data Skew** возникает, когда распределение ключей крайне неравномерно — например, 90% транзакций относятся к одному клиенту. Это приводит к тому, что одна партиция обрабатывается значительно дольше остальных, что замедляет весь этап.\n",
        "\n",
        "> **Техника салтинга (Salting) для агрегации**\n",
        "\n",
        "Салтинг — это метод искусственного разбиения «горячего» ключа на несколько подключей с помощью случайного суффикса («соли»). Это позволяет распределить нагрузку по нескольким партициям.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Salting Example\").getOrCreate()\n",
        "\n",
        "# Создание данных с перекосом: 90% записей имеют key=1\n",
        "skewed_data = (\n",
        "    [(1, 1.0)] * 900000 +  # \"горячий\" ключ\n",
        "    [(i, 1.0) for i in range(2, 10001)]  # остальные ключи\n",
        ")\n",
        "skewed_df = spark.createDataFrame(skewed_data, [\"skewed_key\", \"value\"])\n",
        "\n",
        "# Параметр: количество \"бакетов соли\"\n",
        "N = 10\n",
        "\n",
        "# Шаг 1: Добавление соли и частичная агрегация\n",
        "salted_df = (\n",
        "    skewed_df\n",
        "    .withColumn(\"salt\", (F.rand() * N).cast(IntegerType()))\n",
        "    .groupBy(\"skewed_key\", \"salt\")\n",
        "    .agg(F.sum(\"value\").alias(\"partial_sum\"))\n",
        ")\n",
        "\n",
        "# Шаг 2: Финальная агрегация (без соли)\n",
        "final_df = (\n",
        "    salted_df\n",
        "    .groupBy(\"skewed_key\")\n",
        "    .agg(F.sum(\"partial_sum\").alias(\"total_value\"))\n",
        ")\n",
        "\n",
        "print(\"Результат агрегации после салтинга:\")\n",
        "final_df.show(5)\n",
        "\n",
        "# Анализ плана: два этапа агрегации вместо одного\n",
        "print(\"\\nФизический план:\")\n",
        "final_df.explain(\"simple\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без салтинга вся работа по ключу `1` выполнялась бы на одном исполнителе, что привело бы к «застреванию» задачи. Салтинг разбивает её на 10 партиций, что ускоряет выполнение в 5–8 раз в реальных сценариях. В физическом плане видны **два этапа агрегации**: `HashAggregate` → `Exchange` → `HashAggregate`, что подтверждает корректность двойного подхода.\n",
        "\n",
        "---\n",
        "\n",
        "## V. Управление ресурсами и производственный контекст\n",
        "\n",
        "Оптимальная работа Spark в production зависит не только от кода, но и от настройки ресурсов, выбора форматов хранения и использования современных архитектур.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1. Настройка ресурсов кластера для production\n",
        "\n",
        "Неправильная конфигурация ресурсов — частая причина низкой производительности. Основной принцип: **избегать больших JVM-процессов**.\n",
        "\n",
        "> **Пример: Запуск через `spark-submit` с оптимальными параметрами**\n",
        "\n",
        "```bash\n",
        "spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --num-executors 20 \\\n",
        "  --executor-cores 4 \\\n",
        "  --executor-memory 12g \\\n",
        "  --driver-memory 4g \\\n",
        "  --conf spark.sql.adaptive.enabled=true \\\n",
        "  --conf spark.sql.autoBroadcastJoinThreshold=104857600 \\\n",
        "  --conf spark.shuffle.service.enabled=true \\\n",
        "  your_etl_script.py\n",
        "```\n",
        "\n",
        "> *Пояснение:*  \n",
        "> - `--executor-cores 4` — обеспечивает баланс между параллелизмом и GC-паузами.  \n",
        "> - `--executor-memory 12g` — достаточно для большинства задач без спиллинга на диск.  \n",
        "> - Включение `spark.shuffle.service.enabled` — обязательно для dynamic allocation и отказоустойчивости.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Современные форматы хранения: от Parquet к Delta Lake\n",
        "\n",
        "> **Пример: Работа с Delta Lake**\n",
        "\n",
        "```python\n",
        "from delta import DeltaTable\n",
        "\n",
        "# Запись в Delta-таблицу\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/sales_delta\")\n",
        "\n",
        "# Создание DeltaTable для транзакционных операций\n",
        "delta_table = DeltaTable.forPath(spark, \"/data/sales_delta\")\n",
        "\n",
        "# Операция MERGE (upsert)\n",
        "new_data = spark.createDataFrame([(101, 1500.0)], [\"id\", \"amount\"])\n",
        "delta_table.alias(\"t\").merge(\n",
        "    new_data.alias(\"s\"),\n",
        "    \"t.id = s.id\"\n",
        ").whenMatchedUpdate(set={\"amount\": \"s.amount\"}) \\\n",
        " .whenNotMatchedInsert(values={\"id\": \"s.id\", \"amount\": \"s.amount\"}) \\\n",
        " .execute()\n",
        "\n",
        "# Time Travel: чтение предыдущей версии\n",
        "historical_df = spark.read.format(\"delta\") \\\n",
        "    .option(\"versionAsOf\", 0) \\\n",
        "    .load(\"/data/sales_delta\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Delta Lake добавляет **ACID-транзакции**, **MERGE**, **Time Travel** и **оптимизированную статистику** поверх Parquet. Это делает его стандартом для современных Lakehouse-архитектур.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Мониторинг и тюнинг производительности (Production Ready)\n",
        "\n",
        "### 6.2. Диагностика через Spark UI\n",
        "\n",
        "> **Как читать метрики:**\n",
        "> - **Skew**: если 99-й перцентиль времени выполнения задач в 10–100 раз больше медианы — есть перекос.\n",
        "> - **Spill to Disk**: наличие значений в колонке `Spill (Memory)` или `Spill (Disk)` означает нехватку памяти.\n",
        "> - **GC Time**: если время GC превышает 10–15% от общего времени выполнения — уменьшайте `--executor-cores`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Мастерство работы с PySpark заключается в способности **предсказывать распределённое поведение** на основе высокоуровневого кода. Это требует:\n",
        "- понимания жизненного цикла приложения (Driver, Executors, DAG),\n",
        "- умения анализировать физический план (`explain`),\n",
        "- знания методов оптимизации (Broadcast Join, Salting),\n",
        "- владения современными инструментами (Delta Lake, Arrow-UDF),\n",
        "- системного подхода к мониторингу (Spark UI, метрики).\n",
        "\n"
      ],
      "metadata": {
        "id": "dDMl-R0kBh-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 6: Веб-скрейпинг и парсинг данных — от статических страниц до распределённых динамических приложений\n",
        "\n",
        "### Введение: Фундамент сбора данных в сети\n",
        "\n",
        "Веб-скрейпинг (Web Scraping) представляет собой методический процесс автоматизированного извлечения больших объёмов неструктурированных или полуструктурированных данных с веб-сайтов. В контексте современной обработки данных этот процесс является неотъемлемой частью фазы Extract в общем цикле ETL (Extract, Transform, Load). В отличие от использования официального программного интерфейса (API), скрейпинг требует активного анализа и парсинга сырого HTML-кода и DOM-структуры, поскольку целевой ресурс не предоставляет гарантированно стабильного и структурированного формата данных. Эта особенность делает скрейпинг одновременно гибким и уязвимым инструментом, требующим глубокого понимания как веб-технологий, так и этико-правовых рамок.\n",
        "\n",
        "### 1.1. Определение Веб-скрейпинга и его Место в ETL-процессах\n",
        "\n",
        "Профессиональный скрейпинг всегда рассматривается как высоконагруженный ETL-процесс. Фаза Extract заключается в непосредственном сборе данных, который может осуществляться посредством прямых HTTP-запросов или эмуляции поведения веб-браузера. Фаза Transform включает очистку, валидацию, нормализацию и дедупликацию извлечённых данных. Фаза Load завершает цикл — данные сохраняются в целевое хранилище: реляционную или документную базу данных, файловую систему, облачное хранилище или потоковую платформу. Архитектура промышленных фреймворков, таких как Scrapy, напрямую отражает этот цикл: задачи сбора данных делегируются «паукам» (Spiders), а обработка и сохранение — конвейерам элементов (Item Pipelines).\n",
        "\n",
        "### 1.2. Этические и Правовые Границы\n",
        "\n",
        "Прежде чем приступать к сбору данных, необходимо провести тщательный юридический и этический аудит. Правовое поле веб-скрейпинга неоднозначно и сильно зависит от юрисдикции, характера собираемых данных и условий использования целевого сайта. Условия предоставления услуг (Terms of Service, ToS) имеют приоритетное значение: если ToS явно запрещают автоматизированный сбор данных, выполнение скрейпинга может повлечь юридическую ответственность и техническую блокировку IP-адресов. Регламент GDPR (General Data Protection Regulation) строго регулирует сбор и обработку идентифицируемых персональных данных (PII). Сбор таких данных без явного согласия пользователя представляет собой высокий юридический риск, даже если информация публично доступна. Промышленные системы сбора данных обязаны включать процедуры обработки запросов на удаление персональной информации. Файл `robots.txt` является де-факто стандартом для коммуникации между веб-мастерами и автоматизированными агентами. Профессиональные скрейперы должны неукоснительно соблюдать директивы `Disallow` и уважать параметр `Crawl-delay`. «Дружественный» скрейпинг означает, что процесс должен быть незаметным, не нарушать нормальное функционирование целевого сервера и не создавать чрезмерную нагрузку на его ресурсы.\n",
        "\n",
        "### 1.3. Архитектура Современных Веб-приложений\n",
        "\n",
        "Сложность инструментария, необходимого для сбора данных, напрямую определяется архитектурой целевого сайта. Статический HTML — самый простой случай: весь контент (текст, ссылки, таблицы) полностью содержится в исходном коде, полученном в ответ на HTTP-запрос. Для такого сайта достаточно базовых HTTP-клиентов и HTML-парсеров. Server-Side Rendering (SSR) представляет промежуточную сложность: основной контент генерируется на сервере, но отдельные элементы (рейтинги, комментарии, рекомендации) могут подгружаться асинхронно через AJAX. В таких случаях полнота данных может потребовать анализа сетевых запросов или частичного рендеринга. Наибольшую сложность представляют приложения с Client-Side Rendering (CSR) и архитектурой Single Page Application (SPA). Такие сайты отдают минимальный HTML-каркас и набор JavaScript-скриптов, а вся визуализация и формирование DOM-дерева происходят на стороне клиента. Для извлечения данных с подобных ресурсов требуется полная эмуляция браузерного окружения с выполнением JavaScript — для этого используются такие инструменты, как Playwright, Puppeteer или Selenium.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 1: Основы парсинга статического контента (BeautifulSoup4 + Requests)\n",
        "\n",
        "Для работы со статическими или SSR-страницами основным инструментарием в экосистеме Python являются библиотека `requests` для выполнения HTTP-запросов и `BeautifulSoup4` (BS4) для парсинга HTML-документов. Эта связка обеспечивает простоту, читаемость и достаточную гибкость для большинства задач начального и среднего уровня.\n",
        "\n",
        "### 2.1. Теория Клиент-Серверного Взаимодействия\n",
        "\n",
        "Протокол HTTP (Hypertext Transfer Protocol) лежит в основе взаимодействия между клиентом и сервером. Будучи протоколом без сохранения состояния (stateless), HTTP не запоминает контекст предыдущих запросов. Для поддержания сессий (авторизации, корзины, навигации) используются заголовки (Headers) и куки (Cookies). Библиотека `requests` позволяет эффективно управлять этим состоянием через объект `requests.Session()`. Сессия автоматически сохраняет полученные куки и прикрепляет их к последующим запросам, что позволяет имитировать поведение реального браузера и обеспечивает устойчивость к перенаправлениям, CSRF-токенам и другим механизмам защиты.\n",
        "\n",
        "### 2.2. Устойчивые HTTP-запросы с requests\n",
        "\n",
        "При скрейпинге крайне важна маскировка и устойчивость. Многие сайты анализируют HTTP-заголовки и блокируют запросы с подозрительными сигнатурами. Наличие реалистичного заголовка `User-Agent`, имитирующего популярный браузер (например, Chrome или Firefox), а также заголовка `Referer`, указывающего на предыдущую страницу, значительно снижает вероятность обнаружения и блокировки.\n",
        "\n",
        "Не менее важна обработка ошибок. В промышленном скрейпинге неудача при получении ответа — например, HTTP-код 429 «Too Many Requests» или 5xx «Server Error» — не должна приводить к немедленному повторному запросу. Такое поведение усугубляет нагрузку на сервер и гарантирует блокировку. Профессиональным решением является использование стратегии **экспоненциального замедления** (Exponential Backoff). Эта методика предусматривает постепенное увеличение задержки между повторными попытками: вместо фиксированной паузы задержка растёт с каждой неудачной попыткой, например, по формуле $D = \\text{backoff\\_factor} \\cdot (2^{(R - 1)})$, где $D$ — задержка, а $R$ — номер попытки. Такой подход демонстрирует «дружественное» поведение, даёт серверу время на восстановление и повышает общую надёжность скрипта.\n",
        "\n",
        "Пример реализации устойчивой сессии с поддержкой экспоненциального замедления:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def create_resilient_session(max_retries=5, backoff_factor=1):\n",
        "    \"\"\"Создаёт сессию requests с логикой повторных попыток и экспоненциальным замедлением.\"\"\"\n",
        "    retry_strategy = Retry(\n",
        "        total=max_retries,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        backoff_factor=backoff_factor,\n",
        "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session = requests.Session()\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    return session\n",
        "\n",
        "# Пример использования\n",
        "session = create_resilient_session()\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "try:\n",
        "    response = session.get('https://example.com/data', headers=headers, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    print(\"Успешно получен статус:\", response.status_code)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Критическая ошибка после всех попыток: {e}\")\n",
        "```\n",
        "\n",
        "Этот код создаёт HTTP-сессию, которая автоматически повторяет запросы при временных ошибках, постепенно увеличивая паузу между попытками. Такая практика является стандартом для production-скриптов.\n",
        "\n",
        "### 2.3. Построение DOM-Дерева и Навигация (BeautifulSoup)\n",
        "\n",
        "После получения HTML-контента необходимо преобразовать его из плоского текста в иерархическую структуру — дерево объектов (DOM-дерево). Библиотека BeautifulSoup4 (BS4) является наиболее популярным инструментом для этой задачи благодаря своей толерантности к невалидному HTML и интуитивно понятному API. BS4 позволяет легко находить элементы по тегам, атрибутам, текстовому содержимому и CSS-селекторам.\n",
        "\n",
        "Однако важно учитывать производительность. BS4 сама по себе является обёрткой над внешними парсерами. Наиболее эффективный выбор — использовать `lxml` в качестве бэкенда. Библиотека `lxml` основана на высокоскоростных C-библиотеках `libxml2` и `libxslt`, что делает её значительно быстрее встроенного `html.parser`, особенно при обработке больших объёмов данных. Кроме того, `lxml` поддерживает мощный язык запросов XPath, который позволяет точно навигировать по сложным и глубоко вложенным структурам. Сама BS4 не поддерживает XPath напрямую, но при использовании `lxml` как парсера можно комбинировать подходы или перейти к более продвинутым библиотекам, таким как `parsel` (используется в Scrapy).\n",
        "\n",
        "Пример парсинга с использованием BS4 и `lxml`:\n",
        "\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://example-news-site.com\"\n",
        "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0...\"})\n",
        "soup = BeautifulSoup(response.content, 'lxml')  # Используем lxml для скорости\n",
        "\n",
        "# Извлечение заголовков статей с помощью CSS-селектора\n",
        "headlines = [h.get_text(strip=True) for h in soup.select('h2.article-title')]\n",
        "print(\"Найдено заголовков:\", len(headlines))\n",
        "```\n",
        "\n",
        "В этом примере используется CSS-селектор `h2.article-title` для точного извлечения заголовков, а `lxml` обеспечивает быструю обработку даже при большом объёме HTML.\n",
        "\n",
        "### 2.4. Практика: Парсинг и Обход Пагинации\n",
        "\n",
        "Обход пагинации — одна из самых распространённых задач при сборе данных. Сайты реализуют пагинацию по-разному: через параметры URL (например, `?page=2`), смещение (`?offset=20`), или динамическую подгрузку по клику на кнопку «Далее». В случае статической пагинации процесс сводится к циклическому формированию URL и извлечению данных с каждой страницы.\n",
        "\n",
        "Ключевые шаги: сначала анализируется структура URL или HTML-кнопки перехода, затем реализуется цикл, который последовательно запрашивает каждую страницу. Важно соблюдать «дружественные» практики: добавлять задержки между запросами, использовать устойчивую сессию и обрабатывать возможные ошибки.\n",
        "\n",
        "Пример обхода пагинации:\n",
        "\n",
        "```python\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def scrape_paginated_site(base_url, total_pages):\n",
        "    all_data = []\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\"User-Agent\": \"Mozilla/5.0...\"})\n",
        "    \n",
        "    for page_num in range(1, total_pages + 1):\n",
        "        url = f\"{base_url}?page={page_num}\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            \n",
        "            # Извлечение элементов с помощью CSS-селектора\n",
        "            items = soup.select('div.product-item h3')\n",
        "            for item in items:\n",
        "                title = item.get_text(strip=True)\n",
        "                all_data.append(title)\n",
        "                \n",
        "            print(f\"Обработана страница {page_num}\")\n",
        "            time.sleep(1.5)  # Уважительная задержка\n",
        "            \n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Ошибка на странице {page_num}: {e}\")\n",
        "            break\n",
        "            \n",
        "    return all_data\n",
        "\n",
        "# Запуск сбора\n",
        "data = scrape_paginated_site(\"https://example-store.com/products\", total_pages=10)\n",
        "print(f\"Всего собрано элементов: {len(data)}\")\n",
        "```\n",
        "\n",
        "Этот код демонстрирует полный цикл: инициализация сессии, постраничный запрос, извлечение данных и уважительная задержка. Такой подход легко масштабируется и адаптируется под различные схемы пагинации.\n",
        "\n",
        "### 2.5. Резюме Части 1\n",
        "\n",
        "Инструментарий `requests` и `BeautifulSoup4` (предпочтительно с парсером `lxml`) идеально подходит для быстрого прототипирования, сбора данных с небольших статических или SSR-сайтов, а также для первичного анализа структуры веб-ресурсов. Его преимущества — простота, читаемость кода и низкий порог входа. Однако у этого подхода есть чёткие границы применимости: он является синхронным, не масштабируется для высоконагруженных задач и совершенно неспособен обрабатывать контент, генерируемый JavaScript. При переходе к промышленным объёмам, динамическим SPA или требованию высокой пропускной способности необходимо переходить к асинхронным фреймворкам, таким как Scrapy, или к решениям с полной эмуляцией браузера.\n"
      ],
      "metadata": {
        "id": "sNG32N6rFAhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Часть 2: Промышленный фреймворк для скрейпинга (Scrapy)\n",
        "\n",
        "Scrapy — это мощный, полнофункциональный фреймворк для веб-скрейпинга, написанный на Python. Он представляет собой промышленный стандарт для высоконагруженного сбора данных с сайтов, не требующих выполнения JavaScript. Благодаря своей архитектуре, Scrapy обеспечивает не только высокую производительность, но и чёткую структуру для всего ETL-цикла — от извлечения до загрузки.\n",
        "\n",
        "### 3.1. Архитектура Scrapy: Асинхронное Ядро\n",
        "\n",
        "Ключевое архитектурное преимущество Scrapy заключается в использовании асинхронного фреймворка Twisted в качестве основы. Этот подход позволяет эффективно управлять тысячами одновременных сетевых операций в одном потоке, избегая блокировок и достигая высокой скорости обхода. В отличие от синхронных решений (например, `requests`), Scrapy не ждёт завершения каждого запроса, а продолжает обрабатывать другие задачи, что делает его особенно эффективным при работе с большим числом URL.\n",
        "\n",
        "Архитектура Scrapy состоит из нескольких взаимосвязанных компонентов. Ядро (Scrapy Engine) выступает главным контроллером, координирующим обмен данными между всеми частями системы. Планировщик (Scheduler) отвечает за управление очередью запросов: он получает их от пауков, упорядочивает по приоритету и гарантирует, что одна и та же страница не будет запрошена дважды благодаря встроенному механизму дедупликации. Менеджер загрузок (Downloader) выполняет асинхронные HTTP-запросы и возвращает HTML-ответы. Пауки (Spiders) содержат логику обхода сайта и извлечения данных. Item Pipelines отвечают за обработку и сохранение структурированных данных, а Downloader Middleware позволяет вмешиваться в процесс обработки запросов и ответов на низком уровне — например, для ротации прокси или заголовков.\n",
        "\n",
        "### 3.2. Structuring ETL: Items, Pipelines и Middleware\n",
        "\n",
        "В Scrapy данные структурируются с помощью классов `scrapy.Item`. Эти контейнеры похожи на словари, но имеют строго определённые поля (`Field`), что обеспечивает типобезопасность и предсказуемость на всех этапах обработки. Например, можно явно указать, что каждый товар должен иметь `title`, `price`, `url` и `sku`.\n",
        "\n",
        "Пример определения структуры данных:\n",
        "\n",
        "```python\n",
        "# items.py\n",
        "import scrapy\n",
        "\n",
        "class ProductItem(scrapy.Item):\n",
        "    title = scrapy.Field()\n",
        "    price = scrapy.Field()\n",
        "    url = scrapy.Field()\n",
        "    sku = scrapy.Field()  # Артикул для дедупликации\n",
        "```\n",
        "\n",
        "Item Pipelines — это последовательность классов, через которые проходят все извлечённые элементы. Каждый этап конвейера выполняет определённую задачу, соответствующую фазам Transform и Load ETL-цикла. На первом этапе данные могут быть очищены от HTML-тегов, преобразованы в числовые типы или проверены на наличие обязательных полей. Невалидные элементы можно отбрасывать с помощью исключения `DropItem`. На следующем этапе реализуется дедупликация — например, по уникальному `sku` или `url`, чтобы избежать дублирования в хранилище. Наконец, данные сохраняются в выбранный формат: JSON Lines, CSV, или напрямую в базу данных с использованием SQLAlchemy или другого ORM.\n",
        "\n",
        "Downloader Middleware — это мощный механизм для настройки поведения запросов. Он позволяет реализовать ротацию заголовков `User-Agent`, автоматическую смену прокси-серверов и гибкую логику повторных попыток. Scrapy включает встроенный `RetryMiddleware`, который автоматически повторяет запросы при временных ошибках (например, HTTP 500, 503, 504). Количество попыток и коды ошибок настраиваются через параметры `RETRY_TIMES` и `RETRY_HTTP_CODES`. Неудачные запросы возвращаются в очередь с пониженным приоритетом, что обеспечивает устойчивость к временным сбоям сети или сервера.\n",
        "\n",
        "### 3.3. Создание Паука (Spider) и Обход\n",
        "\n",
        "Паук (Spider) — это сердце любого Scrapy-проекта. Он определяет, с каких URL начинать обход, как извлекать данные и как переходить по ссылкам. Scrapy использует библиотеку `parsel` (основанную на `lxml`) для навигации по HTML с помощью CSS-селекторов и XPath.\n",
        "\n",
        "Пример паука для каталога товаров:\n",
        "\n",
        "```python\n",
        "# spiders/product_spider.py\n",
        "import scrapy\n",
        "from myproject.items import ProductItem\n",
        "\n",
        "class ProductSpider(scrapy.Spider):\n",
        "    name = \"product_spider\"\n",
        "    start_urls = ['https://example-store.com/catalog']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Извлечение карточек товаров\n",
        "        for card in response.css('div.product-card'):\n",
        "            item = ProductItem()\n",
        "            item['title'] = card.css('h2.title::text').get()\n",
        "            item['price'] = card.css('span.price::text').re_first(r'(\\d+)')\n",
        "            item['url'] = response.urljoin(card.css('a::attr(href)').get())\n",
        "            yield item\n",
        "\n",
        "        # Переход на следующую страницу пагинации\n",
        "        next_page = response.css('a.pagination-next::attr(href)').get()\n",
        "        if next_page:\n",
        "            yield response.follow(next_page, self.parse)\n",
        "```\n",
        "\n",
        "Этот код демонстрирует три ключевых паттерна Scrapy: извлечение данных с текущей страницы, генерация структурированного элемента и рекурсивный обход по ссылкам. Все запросы обрабатываются асинхронно, а дубликаты URL автоматически фильтруются планировщиком.\n",
        "\n",
        "### 3.4. Масштабирование и Распределённый Скрейпинг\n",
        "\n",
        "При сборе данных в промышленных масштабах (миллионы и миллиарды страниц) одного сервера недостаточно. Для горизонтального масштабирования используется расширение **Scrapy-Redis**. Оно заменяет локальный планировщик Scrapy на распределённый, использующий Redis в качестве централизованного хранилища состояния.\n",
        "\n",
        "В такой архитектуре все рабочие узлы (воркеры) подключаются к одному экземпляру Redis. Запросы, сгенерированные любым пауком, помещаются в общую очередь, откуда их может взять любой свободный узел. Глобальный механизм дедупликации на основе отпечатков (fingerprints) хранится в Redis, что гарантирует, что одна и та же страница не будет обработана дважды, даже если генерация запроса происходила на разных машинах. При сбое одного из воркеров его задачи автоматически перераспределяются, что обеспечивает отказоустойчивость.\n",
        "\n",
        "Для обхода сайтов с динамическим контентом Scrapy можно интегрировать с инструментами рендеринга JavaScript. Например, **Scrapy-Playwright** или **Scrapy-Splash** работают как специальные Downloader Middleware: они перехватывают запросы, требующие выполнения JavaScript, отправляют их во внешний браузерный движок, а затем возвращают полностью отрендеренный HTML в паук для обычного парсинга. Это позволяет сохранить преимущества асинхронной архитектуры Scrapy даже при работе с SPA.\n",
        "\n",
        "Для команд, не желающих заниматься инфраструктурой, существуют облачные платформы, такие как **Zyte** (ранее Scrapy Cloud). Они предоставляют управляемую среду для деплоя, мониторинга и автоматического масштабирования пауков, а также включают встроенные инструменты для обхода антибот-систем, ротации прокси и решения CAPTCHA.\n",
        "\n",
        "### 3.5. Резюме Части 2\n",
        "\n",
        "Scrapy — это зрелый, масштабируемый фреймворк, идеально подходящий для крупномасштабного сбора данных со статических и SSR-сайтов. Его архитектура обеспечивает высокую производительность, а модульная структура — чёткое разделение ответственности между этапами ETL. Однако он не предназначен для сайтов, где основной контент полностью генерируется JavaScript на клиенте. В таких случаях требуется полная эмуляция браузера, что выводит нас за пределы возможностей классического Scrapy.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 3: Автоматизация браузера для сложного JavaScript (Selenium)\n",
        "\n",
        "Когда сайт построен по архитектуре Single Page Application (SPA), традиционный HTTP-скрейпинг теряет смысл: сервер возвращает только пустой HTML-каркас и JavaScript-файлы, а весь контент формируется в браузере. Для извлечения данных с таких ресурсов требуется эмуляция поведения реального пользователя — именно эту задачу решает **Selenium WebDriver**.\n",
        "\n",
        "### 4.1. Теория: Принципы работы WebDriver\n",
        "\n",
        "Selenium использует протокол W3C WebDriver для взаимодействия между кодом на Python и физическим браузером (например, Chrome или Firefox). Архитектура состоит из трёх уровней: скрипт на Python → драйвер браузера (например, `chromedriver`) → сам браузер. Все команды передаются через HTTP-запросы, что делает архитектуру универсальной, но вносит задержки. Важное отличие от статического парсинга: Selenium не просто получает HTML — он запускает полноценный браузер, выполняет весь JavaScript, загружает ресурсы и рендерит DOM, как это сделал бы человек.\n",
        "\n",
        "### 4.2. Практика: Настройка и Управление\n",
        "\n",
        "Работа с Selenium требует установки соответствующего драйвера для выбранного браузера. Для упрощения управления рекомендуется использовать менеджеры драйверов, такие как `webdriver-manager`, которые автоматически скачивают нужную версию.\n",
        "\n",
        "Одной из главных сложностей при работе с динамическим контентом является **синхронизация**. Selenium не может автоматически определить, когда JavaScript завершил рендеринг или когда AJAX-запрос вернул данные. Попытка взаимодействовать с элементом до его появления приведёт к исключению. Для решения этой проблемы используются **явные ожидания** (Explicit Waits) через класс `WebDriverWait`.\n",
        "\n",
        "Пример безопасного ожидания динамического элемента:\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example.com/dynamic-content\")\n",
        "\n",
        "try:\n",
        "    # Ожидание появления элемента с ID 'result' в течение 10 секунд\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.ID, \"result\"))\n",
        "    )\n",
        "    print(\"Данные загружены:\", element.text)\n",
        "except Exception as e:\n",
        "    print(\"Элемент не появился вовремя:\", e)\n",
        "finally:\n",
        "    driver.quit()\n",
        "```\n",
        "\n",
        "Этот подход делает скрипты надёжными: вместо фиксированных задержек (`time.sleep()`) код ждёт именно нужного состояния страницы.\n",
        "\n",
        "### 4.3. Реализация Сложных Сценариев\n",
        "\n",
        "Selenium позволяет эмулировать действия пользователя: кликать по кнопкам, вводить текст в формы, прокручивать страницу и даже загружать файлы. Это критически важно для скрейпинга сайтов с ленивой загрузкой контента или многошаговыми формами.\n",
        "\n",
        "Например, для загрузки скрытых товаров в интернет-магазине можно прокручивать страницу вниз до тех пор, пока не перестанут появляться новые элементы:\n",
        "\n",
        "```python\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "```\n",
        "\n",
        "Для отладки сложных сценариев полезно сохранять скриншоты или HTML-код страницы в момент ошибки — это помогает понять, на каком этапе скрипт отклонился от ожидаемого поведения.\n",
        "\n",
        "\n",
        "### 4.4. Обход CAPTCHA\n",
        "\n",
        "Столкновение с системами CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) — одна из наиболее частых и сложных проблем при автоматизации веб-скрейпинга. Современные реализации, такие как **reCAPTCHA v2/v3** от Google или **hCaptcha**, интегрированы в форму отправки данных и активируются при подозрении на автоматизированное поведение. Хотя полностью обойти CAPTCHA без внешней помощи невозможно, её можно интегрировать в автоматизированный workflow с использованием специализированных сервисов распознавания.\n",
        "\n",
        "Общий сценарий обхода CAPTCHA состоит из нескольких этапов:  \n",
        "— сначала скрипт должен обнаружить наличие CAPTCHA на странице;  \n",
        "— затем извлечь её идентификаторы и параметры (в первую очередь `sitekey`);  \n",
        "— передать эти данные в сторонний сервис решения (например, 2Captcha, Anti-Captcha или CapMonster);  \n",
        "— дождаться получения токена-ответа;  \n",
        "— ввести этот токен в скрытое поле формы;  \n",
        "— и только после этого выполнить отправку.\n",
        "\n",
        "Рассмотрим каждый шаг на примере обхода **reCAPTCHA v2** с использованием Selenium и сервиса **2Captcha**.\n",
        "\n",
        "#### Шаг 1: Обнаружение CAPTCHA\n",
        "\n",
        "Скрипт должен уметь определять, появилась ли CAPTCHA. Для reCAPTCHA это делается через поиск iframe с определённым идентификатором или проверку наличия скрытого поля с атрибутом `data-sitekey`.\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example-form-with-recaptcha.com\")\n",
        "\n",
        "# Ожидание появления reCAPTCHA на странице\n",
        "try:\n",
        "    captcha_iframe = WebDriverWait(driver, 5).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[src*='recaptcha']\"))\n",
        "    )\n",
        "    print(\"Обнаружена reCAPTCHA v2.\")\n",
        "    has_captcha = True\n",
        "except:\n",
        "    has_captcha = False\n",
        "    print(\"CAPTCHA не обнаружена.\")\n",
        "```\n",
        "\n",
        "#### Шаг 2: Извлечение параметров\n",
        "\n",
        "Если CAPTCHA обнаружена, необходимо извлечь её ключ — `sitekey`. Он обычно содержится в атрибуте `data-sitekey` у самого элемента `<div class=\"g-recaptcha\">` или в URL iframe.\n",
        "\n",
        "```python\n",
        "if has_captcha:\n",
        "    # Извлечение sitekey из DOM\n",
        "    recaptcha_div = driver.find_element(By.CSS_SELECTOR, \"div.g-recaptcha\")\n",
        "    sitekey = recaptcha_div.get_attribute(\"data-sitekey\")\n",
        "    page_url = driver.current_url\n",
        "    print(f\"Извлечён sitekey: {sitekey}\")\n",
        "```\n",
        "\n",
        "#### Шаг 3: Отправка задачи на решение\n",
        "\n",
        "Сервисы вроде 2Captcha предоставляют REST API для решения CAPTCHA. Для reCAPTCHA v2 требуется отправить POST-запрос с вашим API-ключом, `sitekey` и URL страницы.\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import time\n",
        "\n",
        "API_KEY = \"ваш_ключ_2captcha\"\n",
        "CAPTCHA_METHOD = \"userrecaptcha\"\n",
        "\n",
        "# Отправка задачи на решение\n",
        "task_resp = requests.post(\"http://2captcha.com/in.php\", data={\n",
        "    'key': API_KEY,\n",
        "    'method': CAPTCHA_METHOD,\n",
        "    'googlekey': sitekey,\n",
        "    'pageurl': page_url,\n",
        "    'json': 1\n",
        "})\n",
        "\n",
        "task_data = task_resp.json()\n",
        "if task_data.get(\"status\") == 1:\n",
        "    captcha_id = task_data[\"request\"]\n",
        "    print(f\"Задача отправлена, ID: {captcha_id}\")\n",
        "else:\n",
        "    raise Exception(f\"Ошибка создания задачи: {task_data.get('request')}\")\n",
        "\n",
        "# Ожидание результата (обычно 10–30 секунд)\n",
        "while True:\n",
        "    time.sleep(5)\n",
        "    result_resp = requests.get(\n",
        "        f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "    )\n",
        "    result_data = result_resp.json()\n",
        "    if result_data.get(\"status\") == 1:\n",
        "        captcha_token = result_data[\"request\"]\n",
        "        print(\"Токен CAPTCHA получен.\")\n",
        "        break\n",
        "    elif result_data[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "        continue\n",
        "    else:\n",
        "        raise Exception(f\"Ошибка получения результата: {result_data['request']}\")\n",
        "```\n",
        "\n",
        "#### Шаг 4: Вставка токена и отправка формы\n",
        "\n",
        "reCAPTCHA v2 ожидает, что токен будет помещён в скрытое поле формы с именем `g-recaptcha-response`. Иногда это поле изначально отсутствует и создаётся динамически — в таком случае его нужно вставить в DOM вручную.\n",
        "\n",
        "```python\n",
        "# Найти или создать скрытое поле для токена\n",
        "try:\n",
        "    token_field = driver.find_element(By.NAME, \"g-recaptcha-response\")\n",
        "except:\n",
        "    # Если поле не существует, создаём его\n",
        "    driver.execute_script(\"\"\"\n",
        "        var response = document.createElement('textarea');\n",
        "        response.name = 'g-recaptcha-response';\n",
        "        response.style.display = 'none';\n",
        "        document.querySelector('form').appendChild(response);\n",
        "    \"\"\")\n",
        "    token_field = driver.find_element(By.NAME, \"g-recaptcha-response\")\n",
        "\n",
        "# Вставить токен\n",
        "driver.execute_script(\"arguments[0].value = arguments[1];\", token_field, captcha_token)\n",
        "\n",
        "# Отправить форму\n",
        "submit_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n",
        "submit_button.click()\n",
        "\n",
        "print(\"Форма отправлена с решённой CAPTCHA.\")\n",
        "```\n",
        "\n",
        "#### Альтернатива: Ручной ввод\n",
        "\n",
        "В исследовательских или низкочастотных сценариях можно приостановить выполнение и запросить у оператора ввод решения вручную. Это особенно полезно при отладке или при работе с дорогостоящими CAPTCHA.\n",
        "\n",
        "```python\n",
        "if has_captcha:\n",
        "    input(\"CAPTCHA обнаружена. Пройдите проверку вручную и нажмите Enter...\")\n",
        "    # После ввода оператором скрипт продолжает выполнение\n",
        "```\n",
        "\n",
        "Такой подход не масштабируем, но исключительно надёжен и не требует финансовых затрат.\n",
        "\n",
        "#### Важные нюансы\n",
        "\n",
        "Стоимость решения одной reCAPTCHA v2 через 2Captcha составляет около \\$0.5–\\$1 за 1000 решений, что делает такой подход экономически оправданным только при высокой ценности данных. Для reCAPTCHA v3, которая не требует визуального взаимодействия, сервисы имитируют поведенческий профиль и возвращают оценку `score` в виде токена. Кроме того, некоторые сайты используют «невидимую» CAPTCHA, которая срабатывает фоново — в таких случаях необходимо эмулировать поведение пользователя (движения мыши, задержки) до отправки формы, иначе токен может быть отклонён.\n",
        "\n",
        "Таким образом, обход CAPTCHA — это не обход в прямом смысле, а **делегирование** задачи распознавания человеку или машинному сервису с последующей интеграцией ответа в автоматизированный процесс. Это требует тщательной обработки ошибок, управления временем ожидания и соблюдения этических и юридических норм при использовании внешних сервисов.\n",
        "\n",
        "\n",
        "### 4.5. Резюме Части 3\n",
        "\n",
        "Selenium — мощный инструмент для работы с динамическими, JavaScript-интенсивными сайтами. Он обеспечивает полную эмуляцию поведения пользователя, что делает его незаменимым для сложных сценариев. Однако его архитектура на основе HTTP-коммуникации с внешним браузером приводит к высокой ресурсоёмкости, низкой скорости и потенциальной нестабильности. Эти недостатки делают Selenium менее подходящим для высокоскоростного, массового скрейпинга, где предпочтение отдаётся более лёгким и управляемым решениям, таким как Playwright или Puppeteer в headless-режиме.\n"
      ],
      "metadata": {
        "id": "_m23d5ZYGPXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Часть 4: Современный подход к браузерной автоматизации (Playwright)\n",
        "\n",
        "Playwright — это современный фреймворк для автоматизации браузеров, разработанный Microsoft и быстро ставший промышленным стандартом для скрейпинга динамических веб-приложений. В отличие от устаревших решений, Playwright устраняет ключевые архитектурные недостатки Selenium и обеспечивает высокую производительность, надёжность и удобство разработки.\n",
        "\n",
        "### 5.1. Теория: Архитектурные Преимущества\n",
        "\n",
        "Фундаментальное отличие Playwright заключается в способе взаимодействия с браузерными движками. В то время как Selenium опирается на HTTP-протокол и промежуточные драйверы (например, `chromedriver`), Playwright устанавливает прямое, постоянное соединение с ядром браузера через WebSocket. Такой подход обеспечивает нативный контроль над Chromium, Firefox и WebKit, минимизируя задержки и накладные расходы на сериализацию запросов. Это не просто архитектурное улучшение — это кардинальное повышение эффективности.\n",
        "\n",
        "Одной из самых значимых инноваций Playwright является механизм **автоматического ожидания** (Auto-Wait). Перед выполнением любого действия — клика, ввода текста, извлечения содержимого — Playwright автоматически проверяет, что целевой элемент присутствует в DOM, видим, стабилен и готов к взаимодействию. Это устраняет необходимость вручную настраивать сложные условия ожидания, как это требуется в Selenium, где разработчик должен явно указывать, на что именно стоит ждать (`visibility_of_element_located`, `element_to_be_clickable` и т.д.). В результате код становится короче, чище и устойчивее к колебаниям времени загрузки страницы.\n",
        "\n",
        "### 5.2. Практика Playwright: Скорость и Эффективность\n",
        "\n",
        "Playwright предоставляет унифицированный и лаконичный API как для синхронного, так и для асинхронного использования. Он поддерживает headless-режим по умолчанию, что делает его идеальным для автоматизированных задач сбора данных.\n",
        "\n",
        "Пример базового скрипта для извлечения динамического контента:\n",
        "\n",
        "```python\n",
        "from playwright.sync_api import sync_playwright\n",
        "\n",
        "def scrape_dynamic_data(url):\n",
        "    with sync_playwright() as p:\n",
        "        browser = p.chromium.launch(headless=True)\n",
        "        page = browser.new_page()\n",
        "        page.goto(url)\n",
        "        \n",
        "        # Автоматическое ожидание появления элемента\n",
        "        page.wait_for_selector(\".dynamic-content\", state=\"visible\")\n",
        "        \n",
        "        # Взаимодействие с элементом\n",
        "        page.click(\"button#load-more\")\n",
        "        \n",
        "        # Извлечение текста из всех элементов с классом .result-item\n",
        "        data = page.locator(\".result-item\").all_text_contents()\n",
        "        \n",
        "        browser.close()\n",
        "        return data\n",
        "```\n",
        "\n",
        "Этот код демонстрирует ключевые преимущества Playwright: отсутствие явных ожиданий, интуитивный синтаксис (`page.click`, `page.locator`) и встроенную поддержку headless-режима. Playwright также позволяет легко эмулировать различные условия — например, мобильные устройства или конкретные геолокации — через создание контекстов с заданными параметрами. Это особенно полезно при сборе данных, которые варьируются в зависимости от региона или типа устройства.\n",
        "\n",
        "### 5.3. Перехват и Модификация Сетевых Запросов (Оптимизация)\n",
        "\n",
        "Одной из самых мощных функций Playwright является возможность перехвата и модификации всего сетевого трафика. Это открывает путь к радикальной оптимизации производительности при работе с SPA-приложениями.\n",
        "\n",
        "При загрузке типичного современного сайта до 80% времени и ресурсов уходит на получение ненужных для скрейпинга ресурсов: изображений, шрифтов, рекламных скриптов, аналитики и метрик. Playwright позволяет блокировать такие запросы на лету, что значительно сокращает время загрузки, потребление памяти и сетевой трафик.\n",
        "\n",
        "Пример блокировки ненужных ресурсов:\n",
        "\n",
        "```python\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def run_scraper_optimized(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch()\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Блокировка изображений и шрифтов\n",
        "        await page.route(\n",
        "            \"**/*.{png,jpg,jpeg,gif,webp,woff,woff2,ttf,eot}\",\n",
        "            lambda route: route.abort()\n",
        "        )\n",
        "\n",
        "        await page.goto(url)\n",
        "        content = await page.locator(\"div.main-content\").inner_text()\n",
        "        await browser.close()\n",
        "        return content\n",
        "```\n",
        "\n",
        "Кроме блокировки, перехват запросов позволяет получать данные напрямую из AJAX-вызовов. Например, если SPA загружает данные через `fetch()` в формате JSON, можно перехватить этот ответ и извлечь структурированные данные, минуя рендеринг DOM и парсинг HTML. Это не только быстрее, но и надёжнее, поскольку структура JSON-ответа обычно стабильнее, чем разметка страницы.\n",
        "\n",
        "### 5.4. Организация Параллельного Выполнения\n",
        "\n",
        "Playwright эффективно использует системные ресурсы, позволяя создавать множество изолированных браузерных контекстов в рамках одного процесса. Каждый контекст представляет собой независимую сессию с собственными куками, локальным хранилищем и настройками. Это обеспечивает высокую степень параллелизации без необходимости запускать отдельный экземпляр браузера на каждый запрос, как это часто делают в Selenium. В результате пропускная способность скрейпера возрастает на порядки.\n",
        "\n",
        "### 5.5. Резюме Части 4\n",
        "\n",
        "Playwright представляет собой современное, архитектурно превосходящее решение для скрейпинга динамических сайтов. Его нативное взаимодействие с браузерными движками, автоматические ожидания, возможность перехвата и фильтрации сетевых запросов, а также поддержка эффективной параллелизации делают его наиболее производительным и надёжным инструментом для работы со сложными SPA. В промышленной практике Playwright постепенно вытесняет Selenium как основной выбор для задач, требующих выполнения JavaScript.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 5: Продвинутые техники обхода ограничений\n",
        "\n",
        "Современные веб-сайты активно защищаются от автоматизированного сбора данных, используя многоуровневые системы обнаружения ботов. Это требует от промышленных скрейперов применения адаптивных, многослойных стратегий защиты и обхода.\n",
        "\n",
        "### 6.1. Теория: Эволюция Систем Защиты\n",
        "\n",
        "Современные решения, такие как Cloudflare, Akamai или PerimeterX, используют комплексные эвристики для идентификации нечеловеческого трафика. Основные методы включают ограничение скорости запросов с одного IP-адреса (Rate Limiting), что приводит к ответам с кодом 429 «Too Many Requests». Более изощрённые системы применяют **фингерпринтинг** — создание уникального «отпечатка» браузера на основе сотен параметров: версии движка, списка плагинов, характеристик Canvas и WebGL, поведения при рендеринге и даже временных задержек в JavaScript. Отдельное внимание уделяется обнаружению признаков автоматизации: во многих браузерных движках, управляемых через WebDriver, устанавливается скрытое свойство `window.webdriver`, которое легко детектируется. Наконец, системы могут анализировать поведение пользователя: неестественно быстрый скроллинг, отсутствие движений мыши, идеально точные клики — всё это может быть признаком бота.\n",
        "\n",
        "### 6.2. Построение Надёжной Системы Ротации\n",
        "\n",
        "Для успешного обхода защит требуется динамическая смена идентификационных данных. Ключевой компонент — это ротация HTTP-заголовков, в первую очередь `User-Agent`. Использование библиотек вроде `fake-useragent` позволяет генерировать реалистичные, постоянно обновляемые строки, имитирующие популярные браузеры и устройства. Ещё важнее — управление IP-адресами. Датацентровые прокси, хотя и дешевы, легко блокируются, так как их IP-адреса принадлежат известным диапазонам центров обработки данных. В то же время резидентские прокси, использующие IP-адреса реальных домашних или мобильных пользователей, значительно эффективнее обходят современные системы защиты. В Scrapy ротация прокси и заголовков реализуется через Downloader Middleware, а в Playwright — через параметры при создании нового контекста (`browser.new_context(proxy=...)`).\n",
        "\n",
        "### 6.3. Маскировка Автоматизации (Stealth Techniques)\n",
        "\n",
        "Даже при использовании резидентских прокси и реалистичных заголовков автоматизированный браузер может выдать себя через специфические JavaScript-свойства. Для решения этой проблемы применяются **stealth-техники** — инъекция скриптов, которые модифицируют или удаляют признаки WebDriver до загрузки целевой страницы. Например, можно переопределить `navigator.webdriver`, подменить WebGL-рендерер, скрыть автоматическое разрешение экрана и многое другое. В экосистеме Playwright существуют специализированные библиотеки, такие как `playwright-stealth`, которые автоматически применяют десятки проверенных модификаций, делая автоматизированный браузер практически неотличимым от обычного.\n",
        "\n",
        "### 6.4. Многоуровневый Алгоритм Реагирования на Блокировки\n",
        "\n",
        "Для обеспечения промышленной устойчивости необходимо внедрить иерархическую систему реагирования на ошибки. На первом уровне — временные сбои (HTTP 429, 5xx) — применяется стратегия экспоненциального замедления и повторная попытка. Если повторный запрос также завершается ошибкой, система переходит ко второму уровню: выполняется ротация заголовков и, при необходимости, смена cookies. На третьем уровне, при устойчивых блокировках (HTTP 403 Forbidden), запрос перенаправляется через новый IP-адрес из пула резидентских прокси. Наконец, если сайт выдаёт CAPTCHA, запрос автоматически передаётся в сторонний сервис решения (например, 2Captcha), и полученный токен вводится в форму через тот же Playwright. Такой многоступенчатый подход позволяет поддерживать высокий уровень успешности даже при работе с наиболее защищёнными ресурсами.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 6: Инструменты и правовые аспекты промышленного скрейпинга\n",
        "\n",
        "### 7.1. Сравнительный Анализ Производительности Парсеров\n",
        "\n",
        "Выбор парсера напрямую влияет на производительность промышленного скрейпинга. Библиотека `lxml`, основанная на высокоскоростных C-библиотеках `libxml2` и `libxslt`, является безусловным лидером по скорости парсинга HTML и XML. Она поддерживает мощный язык XPath, что делает её незаменимой для навигации по сложным структурам. `Parsel` — это обёртка над `lxml`, используемая в Scrapy для унификации селекторов; она сохраняет всю производительность `lxml`, добавляя поддержку CSS-селекторов. В отличие от этого, `BeautifulSoup4`, хотя и превосходит конкурентов в устойчивости к невалидному HTML, значительно уступает в скорости и рекомендуется только для прототипирования или небольших задач, где важна простота кода, а не пропускная способность.\n",
        "\n",
        "### 7.2. Алгоритм Принятия Решения: Scraping vs Official API\n",
        "\n",
        "Стратегический выбор между использованием официального API и разработкой собственного скрейпера должен основываться на комплексной оценке. Официальный API всегда предпочтителен: он предоставляет структурированные, стабильные данные, минимизирует юридические риски и не требует постоянного сопровождения из-за изменений в DOM-структуре. Скрейпинг оправдан только в ситуациях, когда API отсутствует, непомерно дорог, накладывает жёсткие ограничения на объём или частоту запросов, либо не предоставляет доступ к историческим данным, необходимым для анализа. Таким образом, скрейпинг — это вынужденная мера, а не предпочтительный путь.\n",
        "\n",
        "### 7.3. Юридические Прецеденты и Практика Соблюдения\n",
        "\n",
        "Хотя судебная практика в некоторых юрисдикциях (например, в США по делу *hiQ Labs v. LinkedIn*) подтверждает право на сбор публично доступных данных, это не отменяет обязательств перед условиями предоставления услуг (ToS) и требованиями регуляторов. Соблюдение файла `robots.txt` остаётся минимальным условием «дружественного» скрейпинга. При работе с любыми данными, которые могут быть связаны с физическим лицом — даже если они публичны, как профили в соцсетях — необходимо учитывать требования GDPR. Это включает разработку внутренних процедур на случай получения запроса на удаление персональной информации («право на забвение»).\n",
        "\n",
        "### 7.4. Обзор Управляемых Инструментов\n",
        "\n",
        "Сложность современных систем защиты привела к росту популярности управляемых решений. **Zyte API** (ранее Scrapy Cloud) предлагает не только платформу для деплоя и мониторинга пауков, но и функции обхода блокировок как услугу: автоматическая ротация прокси, решение CAPTCHA, защита от фингерпринтинга — всё это скрыто за простым HTTP-интерфейсом. Это позволяет разработчикам сосредоточиться исключительно на логике извлечения данных. Для менее требовательных задач может подойти **requests-html** — лёгкая библиотека, сочетающая `requests` и `lxml` с ограниченной возможностью рендеринга JavaScript через headless-браузер. Она полезна для случаев, где требуется минимальное выполнение скриптов без перехода к полной архитектуре Playwright или Scrapy.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение: Скрейпинг как Архитектурный Вызов\n",
        "\n",
        "Эффективный и надёжный веб-скрейпинг в Python — это, прежде всего, архитектурная задача. Выбор инструментария должен определяться двумя ключевыми факторами: природой целевого сайта и требуемым масштабом операции.\n",
        "\n",
        "Если сайт состоит из статического или серверно-рендеренного контента и объём данных невелик, оптимальным выбором будет связка `requests` и `BeautifulSoup4` с парсером `lxml`. Для промышленного сбора с таких ресурсов следует использовать **Scrapy** — его асинхронная архитектура, система Item Pipelines и поддержка распределённого выполнения через **Scrapy-Redis** обеспечивают надёжность и масштабируемость. В случае с динамическими SPA-приложениями, где контент формируется на клиенте, требуется полная эмуляция браузера, и здесь **Playwright** становится предпочтительным решением благодаря своей скорости, автоматическим ожиданиям и мощным инструментам оптимизации.\n",
        "\n",
        "Будущее веб-скрейпинга лежит в области высокопроизводительной браузерной автоматизации и интеграции с управляемыми API, которые берут на себя всю сложность обхода постоянно эволюционирующих систем защиты. Инженер данных должен не просто уметь писать парсеры, но и понимать, как устроены современные веб-приложения, как работают системы антибот-защиты и как проектировать устойчивые, этичные и масштабируемые архитектуры сбора данных.\n",
        "\n"
      ],
      "metadata": {
        "id": "L9NKBmWQG3Rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 7: Библиотека Matplotlib — основы построения научной визуализации\n",
        "\n",
        "### Раздел 1: Архитектура Matplotlib и Приоритет OO-Стиля для Научной Визуализации\n",
        "\n",
        "Matplotlib является фундаментальной библиотекой Python для создания статических, анимированных и интерактивных визуализаций данных. Для исследователей, стремящихся к высокой степени контроля, воспроизводимости и стандартизации своих графиков для публикации, необходимо глубокое понимание внутренней архитектуры библиотеки. Это понимание позволяет перейти от быстрого прототипирования к созданию изображений публикационного качества.\n",
        "\n",
        "#### 1.1. Фундаментальная Объектная Иерархия (The Anatomy of a Plot)\n",
        "\n",
        "Архитектура Matplotlib строится на чёткой иерархии объектов, что критически важно для эффективного использования объектно-ориентированного (OO) API. В основе этой иерархии лежат три ключевых компонента: **Figure**, **Axes** и **Artist**.\n",
        "\n",
        "**Figure** представляет собой самый верхний контейнер — «холст», на котором размещаются все элементы визуализации. Он управляет дочерними объектами Axes, а также глобальными элементами, такими как общий заголовок (`fig.suptitle`), легенда на уровне всей фигуры или цветовая шкала (`fig.colorbar`). Типичный способ создания фигуры — вызов `fig = plt.figure()` для пустого холста или использование функции `plt.subplots()`, которая одновременно создаёт Figure и один или несколько объектов Axes.\n",
        "\n",
        "**Axes** — это не отдельная ось, а именно система координат, в которой отображаются данные. Именно на уровне Axes выполняется подавляющее большинство операций: построение линий, гистограмм, облаков точек, добавление подписей и легенд. Один Figure может содержать несколько Axes (например, в случае многопанельных графиков), и каждый из них полностью независим: имеет собственные оси, данные и оформление.\n",
        "\n",
        "**Artist** — это самая общая концепция в архитектуре Matplotlib. Любой видимый элемент на графике — линия, текст, изображение, метка, тик, сам Axes или даже Figure — является объектом Artist. OO-стиль работы с Matplotlib состоит в том, чтобы вызывать методы этих Artist-объектов для точной настройки их внешнего вида.\n",
        "\n",
        "#### 1.2. Сравнение Двух Интерфейсов: Pyplot vs. Object-Oriented (OO) API\n",
        "\n",
        "Matplotlib предоставляет два основных способа взаимодействия с графиками: **Pyplot API** и **Object-Oriented API**.\n",
        "\n",
        "**Pyplot API** (модуль `matplotlib.pyplot`, обычно импортируемый как `plt`) работает через механизм неявного состояния. Функции вроде `plt.plot()`, `plt.title()` или `plt.xlabel()` автоматически создают и управляют текущими объектами Figure и Axes «за кулисами». Пользователь не ссылается на эти объекты напрямую. Такой подход удобен для быстрого интерактивного анализа в Jupyter Notebook или при создании простых графиков в несколько строк кода.\n",
        "\n",
        "**Object-Oriented API** требует явного создания объектов Figure и Axes, например, через `fig, ax = plt.subplots()`. Все последующие действия — построение данных, настройка подписей, добавление легенд — выполняются через вызов методов этих объектов: `ax.plot()`, `ax.set_title()`, `fig.colorbar()`. Этот подход не полагается на глобальное состояние и предоставляет полный контроль над каждым элементом визуализации.\n",
        "\n",
        "Важно понимать, что Pyplot API на самом деле является обёрткой над OO-интерфейсом. Например, вызов `plt.plot(x, y)` эквивалентен последовательности `ax = plt.gca(); ax.plot(x, y)`. Аналогично, `plt.title()` преобразуется в `plt.gca().set_title()`. Таким образом, OO-стиль — это не альтернатива, а основа, на которой построен весь Matplotlib.\n",
        "\n",
        "#### 1.3. Ключевой Вывод для Научной Визуализации: Приоритет OO-Стиля\n",
        "\n",
        "Для создания сложных многопанельных графиков, написания функций, предназначенных для повторного использования в рамках крупного проекта, и обеспечения максимальной воспроизводимости в научных публикациях настоятельно рекомендуется использовать OO-стиль. Явное управление объектами `fig` и `ax` устраняет зависимость от внутреннего «текущего состояния» Matplotlib, которое может вести себя непредсказуемо при создании множества фигур в цикле или в асинхронной среде. OO-стиль обеспечивает чёткость, модульность и предсказуемость кода — качества, без которых невозможно строить надёжные научные pipeline’ы.\n",
        "\n",
        "#### 1.4. Практика: Использование `plt.subplots()` как Вход в OO-Мир\n",
        "\n",
        "Наиболее идиоматическим способом начать работу в OO-стиле является функция `plt.subplots()`. Для одиночного графика она возвращает кортеж `(fig, ax)`, где `fig` — объект Figure, а `ax` — единственный объект Axes. Для многопанельного макета вызов `fig, axs = plt.subplots(nrows=2, ncols=3)` создаёт фигуру и двумерный массив `axs` из шести объектов Axes, каждый из которых можно настраивать независимо.\n",
        "\n",
        "Пример простого графика в OO-стиле:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.sin(x)\n",
        "\n",
        "# Создание фигуры и осей в OO-стиле\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "# Построение данных\n",
        "ax.plot(x, y, color='steelblue', linewidth=2, label='sin(x)')\n",
        "\n",
        "# Настройка элементов графика\n",
        "ax.set_xlabel('Время (с)', fontsize=12)\n",
        "ax.set_ylabel('Амплитуда', fontsize=12)\n",
        "ax.set_title('Гармоническое колебание', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Отображение\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код демонстрирует ключевые принципы OO-стиля: явное создание `fig` и `ax`, вызов методов на `ax` для построения и настройки, и полный контроль над каждым аспектом визуализации.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 2: Базовые Инструменты Научной Визуализации в OO-Стиле\n",
        "\n",
        "Научная визуализация требует инструментов, способных точно представлять зависимости, распределения данных и сопутствующую им неопределённость. Все эти построения в OO-стиле осуществляются через методы, вызываемые на объекте Axes.\n",
        "\n",
        "#### 2.1. Отображение Зависимостей: Линейные и Точечные Графики\n",
        "\n",
        "Линейные и точечные графики — основа для демонстрации взаимосвязей между переменными. Метод `ax.plot()` используется для отображения функциональных зависимостей, временных рядов или любых упорядоченных данных. Он позволяет настраивать цвет (`color`), стиль линии (`linestyle`), ширину (`linewidth`) и маркеры (`marker`), что делает его гибким инструментом для отображения нескольких наборов данных на одном графике.\n",
        "\n",
        "Метод `ax.scatter()` предназначен для визуализации парных распределений. Он особенно полезен при анализе корреляций, выявлении кластеров и обнаружении выбросов. При работе с большими объёмами данных ключевым параметром становится `alpha` — прозрачность точек. Низкое значение `alpha` (например, 0.3) позволяет визуально выделить области с высокой плотностью точек, тогда как перекрывающиеся точки в стандартном режиме (`alpha=1`) создают «тёмные пятна», искажающие восприятие.\n",
        "\n",
        "Пример сравнения линейного и точечного графиков:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Синтетические данные с шумом\n",
        "x = np.linspace(0, 4, 100)\n",
        "y_true = np.exp(-x) * np.cos(2 * np.pi * x)\n",
        "y_obs = y_true + 0.1 * np.random.randn(len(x))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Линейный график\n",
        "ax1.plot(x, y_true, 'k--', label='Истинная модель')\n",
        "ax1.plot(x, y_obs, 'o', color='crimson', markersize=3, alpha=0.6, label='Наблюдения')\n",
        "ax1.set_title('Линейный + точечный (модель vs данные)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Чистый точечный график с прозрачностью\n",
        "ax2.scatter(x, y_obs, c=y_obs, cmap='viridis', alpha=0.6, edgecolors='none')\n",
        "ax2.set_title('Точечный график с прозрачностью')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот пример иллюстрирует, как `alpha` и цветовая кодировка помогают раскрыть структуру данных, которая была бы скрыта при стандартном отображении.\n",
        "\n",
        "#### 2.2. Визуализация Распределений и Статистики\n",
        "\n",
        "Для анализа формы распределения Matplotlib предоставляет несколько взаимодополняющих инструментов. Метод `ax.hist()` строит гистограмму — базовый способ визуализации частотного распределения. Важно выбирать адекватное количество бинов (`bins`), так как слишком мелкое или грубое разбиение может исказить представление о данных.\n",
        "\n",
        "Метод `ax.boxplot()` создаёт «ящик с усами» — компактное изображение, отражающее пять ключевых статистик: минимум, первый квартиль (Q1), медиану, третий квартиль (Q3) и максимум (с исключением выбросов). Box plot идеален для сравнения распределений между группами, но скрывает детали формы распределения.\n",
        "\n",
        "Более информативной альтернативой является **скрипичный график** (`ax.violinplot()`), который отображает ядерную оценку плотности распределения (KDE). Он сохраняет все преимущества box plot, но дополнительно показывает, является ли распределение унимодальным, бимодальным или скошенным. Для научных публикаций, где форма распределения имеет значение (например, при проверке нормальности остатков), скрипичный график часто предпочтительнее.\n",
        "\n",
        "Пример сравнения box plot и violin plot:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация синтетических выборок\n",
        "np.random.seed(42)\n",
        "data_A = np.random.normal(0, 1, 200)\n",
        "data_B = np.concatenate([np.random.normal(-2, 0.8, 100), np.random.normal(2, 0.8, 100)])  # бимодальное\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Box plots\n",
        "ax1.boxplot([data_A, data_B], labels=['Выборка A', 'Выборка B'])\n",
        "ax1.set_title('Box Plot')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Violin plots\n",
        "ax2.violinplot([data_A, data_B], showmedians=True)\n",
        "ax2.set_xticks([1, 2])\n",
        "ax2.set_xticklabels(['Выборка A', 'Выборка B'])\n",
        "ax2.set_title('Violin Plot')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "На этом примере видно, что box plot для выборки B выглядит как стандартный симметричный «ящик», тогда как violin plot явно демонстрирует наличие двух пиков — информацию, критически важную для интерпретации.\n",
        "\n",
        "#### 2.3. Добавление Элементов Ошибки (Error Bars)\n",
        "\n",
        "Научная визуализация неполна без количественной оценки неопределённости. Метод `ax.errorbar()` позволяет отображать погрешности — будь то стандартное отклонение, стандартная ошибка среднего или доверительный интервал. Это обязательный элемент для публикаций в рецензируемых журналах.\n",
        "\n",
        "Элементы ошибок могут быть симметричными (`yerr=0.1`) или асимметричными (`yerr=[[низ, низ], [верх, верх]]`). Кроме того, можно одновременно отображать ошибки по X и по Y (`xerr`, `yerr`), а также настраивать их внешний вид: цвет, ширину штрихов (`capsize`), стиль линий.\n",
        "\n",
        "Пример с элементами ошибок:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array([2.1, 3.9, 6.0, 8.2])\n",
        "yerr = np.array([0.2, 0.3, 0.25, 0.4])  # стандартная ошибка\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.errorbar(x, y, yerr=yerr, fmt='o', color='darkgreen', ecolor='lightgray',\n",
        "            elinewidth=2, capsize=5, markersize=6, label='Измерения ± SE')\n",
        "ax.set_xlabel('Независимая переменная')\n",
        "ax.set_ylabel('Зависимая переменная')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь `fmt='o'` задаёт стиль маркеров, `ecolor` — цвет полос ошибок, а `capsize` добавляет «шапочки» на концы, что улучшает читаемость. Такой график не только передаёт данные, но и честно демонстрирует степень уверенности в них.\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ15P4cSGLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Раздел 3: Тонкая Настройка Axes для Публикационного Качества (OO Customization)\n",
        "\n",
        "Достижение публикационного качества в научной визуализации требует не просто отображения данных, а точной и осознанной настройки каждого визуального элемента. Объектно-ориентированный (OO) стиль Matplotlib предоставляет явные, предсказуемые методы-сеттеры, которые позволяют полностью контролировать заголовки, подписи осей, тики, лимиты и декоративные элементы.\n",
        "\n",
        "#### 3.1. Управление Заголовками и Подписями Осей\n",
        "\n",
        "Для обеспечения ясности и профессионального вида графика необходимо чётко разделять заголовки подграфиков и общие заголовки всей фигуры. Метод `ax.set_title(\"Название графика\")` устанавливает заголовок конкретного объекта Axes, что особенно важно при работе с многопанельными композициями. Аналогично, `ax.set_xlabel(\"Ось X\")` и `ax.set_ylabel(\"Ось Y\")` задают метки осей с полным контролем над их текстом, шрифтом и положением. Если фигура содержит несколько подграфиков, а требуется передать общую тему исследования, используется метод `fig.suptitle(\"Общее название\")`, который размещает заголовок над всеми Axes и не привязан к какому-либо отдельному подграфику.\n",
        "\n",
        "Пример настройки заголовков в многопанельном графике:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "x = np.linspace(0, 5, 100)\n",
        "ax1.plot(x, np.sin(x), label='sin(x)')\n",
        "ax2.plot(x, np.cos(x), label='cos(x)', color='tab:orange')\n",
        "\n",
        "# Заголовки подграфиков\n",
        "ax1.set_title('Синусоида')\n",
        "ax2.set_title('Косинусоида')\n",
        "\n",
        "# Подписи осей\n",
        "ax1.set_xlabel('Угол (рад)')\n",
        "ax1.set_ylabel('Амплитуда')\n",
        "ax2.set_xlabel('Угол (рад)')\n",
        "\n",
        "# Общий заголовок фигуры\n",
        "fig.suptitle('Тригонометрические функции', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код демонстрирует чёткое разделение ответственности: каждый подграфик управляет своими локальными подписями, а фигура — общей темой.\n",
        "\n",
        "#### 3.2. Детальная Работа с Тиками и Лимитами Осей\n",
        "\n",
        "Автоматическая разметка осей, предлагаемая Matplotlib по умолчанию, часто не соответствует требованиям научной публикации. Исследователь должен иметь возможность точно определять, какие значения отображаются на осях и как они подписаны. Методы `ax.set_xlim()` и `ax.set_ylim()` позволяют ограничить отображаемый диапазон данных, исключая выбросы или нерелевантные области и фокусируя внимание читателя на ключевой зоне.\n",
        "\n",
        "Ещё более важна настройка тиков. Методы `ax.set_xticks()` и `ax.set_yticks()` принимают два аргумента: позиции тиков и, опционально, их текстовые метки. Это особенно полезно при работе с физическими величинами, денежными единицами или научной нотацией. Например, можно заменить числовые значения на подписи вида «\\$1.0 M» или «1.2 × 10⁴», что значительно улучшает читаемость.\n",
        "\n",
        "Пример ручной настройки тиков с форматированными метками:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Данные в миллионных единицах\n",
        "years = [2020, 2021, 2022, 2023]\n",
        "revenue = [1.2, 1.8, 2.5, 3.1]  # миллионы долларов\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.plot(years, revenue, marker='o')\n",
        "\n",
        "# Установка тиков по годам и форматированных меток по доходу\n",
        "ax.set_xticks(years)\n",
        "ax.set_yticks([1, 2, 3])\n",
        "ax.set_yticklabels(['\\$1.0 M', '\\$2.0 M', '\\$3.0 M'])\n",
        "\n",
        "ax.set_xlabel('Год')\n",
        "ax.set_ylabel('Выручка')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой подход гарантирует, что оси не только технически точны, но и интерпретируемы без дополнительных пояснений.\n",
        "\n",
        "#### 3.3. Легенды, Сетки и Стилизация\n",
        "\n",
        "Легенда (`ax.legend()`) играет ключевую роль при визуализации нескольких наборов данных на одном графике. В сложных композициях её расположение должно быть тщательно продумано, чтобы избежать перекрытия с данными. Это достигается с помощью параметра `loc` (например, `'upper right'`) или более точного управления через `bbox_to_anchor`, который позволяет размещать легенду в произвольной точке относительно Axes.\n",
        "\n",
        "Сетка (`ax.grid(True)`) облегчает точное считывание значений, особенно на графиках с плотным расположением точек. Однако её стиль должен быть ненавязчивым: рекомендуется использовать прерывистые линии (`linestyle='--'`) и пониженную прозрачность (`alpha=0.5`), чтобы сетка служила фоном, а не отвлекала от данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 4: Создание Сложных Многопанельных Макетов с GridSpec\n",
        "\n",
        "Для научных отчётов часто требуются несимметричные, иерархические композиции, где подграфики имеют разный размер и расположение. Стандартный подход `plt.subplots(nrows, ncols)` ограничен равномерными сетками. Для создания произвольной геометрии используется класс `GridSpec`.\n",
        "\n",
        "#### 4.1. Введение в GridSpec\n",
        "\n",
        "`GridSpec` определяет логическую сетку внутри объекта Figure, а затем позволяет объединять ячейки этой сетки с помощью синтаксиса срезов Python. Это превращает проектирование макета в декларативный процесс, управляемый индексами.\n",
        "\n",
        "Инициализация выполняется как `gs = GridSpec(nrows, ncols, figure=fig)`. После этого объекты Axes создаются вызовом `fig.add_subplot(gs[срез])`. Например, `gs[0, :]` охватывает всю первую строку, а `gs[1:, -1]` — последний столбец, начиная со второй строки. Такой подход особенно мощен при построении составных графиков, где, например, гистограмма распределения по X должна быть размещена под основным точечным графиком, а гистограмма по Y — справа от него.\n",
        "\n",
        "Пример асимметричного макета:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(0)\n",
        "x = np.random.randn(1000)\n",
        "y = 1.2 * x + np.random.randn(1000) * 0.5\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8), layout=\"constrained\")\n",
        "gs = gridspec.GridSpec(3, 3, figure=fig)\n",
        "\n",
        "# Основной scatter plot (занимает 2x2 в левом верхнем углу)\n",
        "ax_main = fig.add_subplot(gs[:2, :2])\n",
        "ax_main.scatter(x, y, alpha=0.6)\n",
        "ax_main.set_xlabel('X')\n",
        "ax_main.set_ylabel('Y')\n",
        "\n",
        "# Гистограмма по X (внизу)\n",
        "ax_hist_x = fig.add_subplot(gs[2, :2], sharex=ax_main)\n",
        "ax_hist_x.hist(x, bins=30, color='steelblue')\n",
        "ax_hist_x.set_ylabel('Частота')\n",
        "\n",
        "# Гистограмма по Y (справа)\n",
        "ax_hist_y = fig.add_subplot(gs[:2, 2], sharey=ax_main)\n",
        "ax_hist_y.hist(y, bins=30, orientation='horizontal', color='crimson')\n",
        "ax_hist_y.set_xlabel('Частота')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код создаёт классический «scatter plot с маргинальными гистограммами», где все подграфики точно выровнены, а их размеры определяются логикой анализа, а не техническими ограничениями.\n",
        "\n",
        "#### 4.2. Вложенные GridSpec (Nested GridSpec)\n",
        "\n",
        "Для ещё более сложных композиций, где разные области фигуры требуют независимых внутренних сеток, используется вложенная структура GridSpec. Сначала создаётся основной `GridSpec`, затем для выбранной области вызывается метод `subgridspec()`, который определяет дочернюю сетку. Это позволяет, например, разделить фигуру на две колонки, а в каждой — создать свою независимую композицию из нескольких графиков.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 5: Обеспечение Публикационного Качества: Constrained Layout и Сохранение\n",
        "\n",
        "Создание сложного макета часто сопровождается проблемой наложения или обрезания подписей, заголовков и легенд. Решение этой проблемы — использование современного механизма автоматической компоновки.\n",
        "\n",
        "#### 5.1. Современное Решение: Constrained Layout\n",
        "\n",
        "Исторически для этой цели использовалась функция `plt.tight_layout()`, но она имеет ограничения при работе со сложными элементами, такими как цветовые шкалы или многоуровневые легенды. Современный и рекомендуемый подход — **Constrained Layout**. Он активируется при создании фигуры через параметр `layout=\"constrained\"` и использует внутренний решатель для расчёта необходимого пространства под все элементы графика. Constrained Layout полностью совместим с `GridSpec` и вложенными макетами, обеспечивая гармоничную компоновку даже в самых сложных сценариях.\n",
        "\n",
        "#### 5.2. Интеграция Цветовых Шкал (Colorbars)\n",
        "\n",
        "Цветовые шкалы — частый источник проблем в макетировании. При вызове `fig.colorbar(im, ax=ax)` в сочетании с Constrained Layout система автоматически уменьшает размер указанных Axes и выделяет место для шкалы, предотвращая перекрытия. Это особенно важно при сравнении нескольких тепловых карт с общей цветовой шкалой — задача, типичная для научных публикаций в физике, биологии и геоинформатике.\n",
        "\n",
        "#### 5.3. Сохранение Графиков для Публикации: `fig.savefig()`\n",
        "\n",
        "Финальный шаг — экспорт графика в формате, пригодном для публикации. Метод `fig.savefig()` предоставляет ключевые параметры для контроля качества:\n",
        "\n",
        "- **Разрешение (dpi):** Для печати требуется не менее 300 DPI. По умолчанию Matplotlib использует 100 DPI, что недостаточно для журналов.\n",
        "- **Формат файла:** Векторные форматы (PDF, SVG) предпочтительны для академических публикаций, так как они масштабируются без потерь. Растровые форматы (PNG, JPG) используются для веба.\n",
        "- **Обрезка (bbox_inches='tight'):** Этот параметр автоматически удаляет избыточные белые поля, гарантируя, что сохранённый файл содержит только необходимые элементы.\n",
        "\n",
        "Пример экспорта:\n",
        "\n",
        "```python\n",
        "fig.savefig(\n",
        "    'scientific_plot.pdf',\n",
        "    format='pdf',\n",
        "    dpi=300,\n",
        "    bbox_inches='tight',\n",
        "    transparent=False\n",
        ")\n",
        "```\n",
        "\n",
        "Такой файл будет соответствовать требованиям большинства научных журналов и сохранит все детали визуализации при любом масштабе.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Matplotlib предоставляет строгую, иерархическую модель для создания научной визуализации публикационного качества. Ключ к успеху — системный подход, основанный на трёх принципах.\n",
        "\n",
        "Во-первых, **контроль через ОО-стиль**: начинайте с `plt.subplots()` и используйте явные методы `ax.set_*()` для настройки каждого элемента. Это исключает зависимость от глобального состояния и обеспечивает воспроизводимость.\n",
        "\n",
        "Во-вторых, **сложное макетирование через GridSpec**: при необходимости асимметричных или иерархических композиций переходите от простых сеток к срезам `GridSpec`, что позволяет программно определять пространственные отношения между графиками.\n",
        "\n",
        "В-третьих, **гарантия качества через Constrained Layout и правильный экспорт**: активируйте `layout=\"constrained\"` при создании фигуры, чтобы автоматически избежать наложений, и сохраняйте результат в векторном формате с `dpi=300` и `bbox_inches='tight'`.\n",
        "\n",
        "Освоение этих принципов превращает Matplotlib из инструмента быстрого прототипирования в мощную платформу для создания визуализаций, соответствующих самым строгим стандартам научной и технической коммуникации."
      ],
      "metadata": {
        "id": "-SrdkFwqJTse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6: Продвинутые возможности Matplotlib — анимация, 3D-визуализация, аннотации и интеграция\n",
        "\n",
        "### Раздел 1: Анимация данных для демонстрации динамики\n",
        "\n",
        "Статическая визуализация не всегда способна передать эволюцию процесса во времени. Matplotlib предоставляет мощные инструменты для создания анимаций, которые позволяют наглядно демонстрировать динамические системы, сходимость алгоритмов или изменение распределений. Основой анимации служит класс `FuncAnimation` из модуля `matplotlib.animation`.\n",
        "\n",
        "В отличие от построения серии отдельных кадров, `FuncAnimation` оптимизирует рендеринг, обновляя только те части графика, которые изменились. Это достигается за счёт механизма **blitting**, который сохраняет фон и перерисовывает только движущиеся элементы.\n",
        "\n",
        "Пример анимации гармонического осциллятора:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.set_xlim(0, 4 * np.pi)\n",
        "ax.set_ylim(-1.2, 1.2)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "x = np.linspace(0, 4 * np.pi, 200)\n",
        "line, = ax.plot([], [], 'b-', lw=2)\n",
        "point, = ax.plot([], [], 'ro', markersize=8)\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    point.set_data([], [])\n",
        "    return line, point\n",
        "\n",
        "def animate(frame):\n",
        "    t = x[:frame]\n",
        "    y = np.sin(t)\n",
        "    line.set_data(t, y)\n",
        "    if frame > 0:\n",
        "        point.set_data(t[-1], y[-1])\n",
        "    return line, point\n",
        "\n",
        "anim = FuncAnimation(\n",
        "    fig, animate, init_func=init, frames=len(x),\n",
        "    interval=30, blit=True, repeat=False\n",
        ")\n",
        "\n",
        "# Для сохранения: anim.save('oscillation.mp4', fps=30)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код иллюстрирует ключевые компоненты анимации: функцию инициализации (`init`), которая задаёт начальное состояние, и функцию обновления (`animate`), вызываемую для каждого кадра. Анимации особенно ценны в образовательных материалах и при представлении результатов моделирования, где важна временная последовательность событий.\n",
        "\n",
        "### Раздел 2: Трёхмерная визуализация научных данных\n",
        "\n",
        "Для анализа многомерных зависимостей или пространственных структур Matplotlib поддерживает 3D-графику через модуль `mpl_toolkits.mplot3d`. Объект `Axes3D` расширяет стандартный `Axes`, добавляя методы для построения поверхностей, облаков точек и контурных сечений в трёхмерном пространстве.\n",
        "\n",
        "Ключевыми методами являются `plot_surface` для отображения гладких функций двух переменных, `scatter` для визуализации трёхмерных наборов данных и `contour`/`contourf` для построения изолиний на плоскостях. Важно помнить, что 3D-графики в Matplotlib остаются статическими в формате PDF или PNG; интерактивное вращение возможно только в интерактивных средах (Jupyter, Qt).\n",
        "\n",
        "Пример построения поверхности:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Генерация сетки\n",
        "x = np.linspace(-5, 5, 50)\n",
        "y = np.linspace(-5, 5, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "# Построение поверхности с цветовой картой\n",
        "surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.9, edgecolor='none')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=10, label='Амплитуда')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Трёхмерная визуализация требует особой осторожности: перегруженные графики сложно интерпретировать на печати. Рекомендуется использовать прозрачность (`alpha`), упрощённую геометрию и вспомогательные проекции (например, контуры на дне графика).\n",
        "\n",
        "### Раздел 3: Аннотации и произвольные графические примитивы\n",
        "\n",
        "Научная визуализация часто требует выделения ключевых точек, добавления пояснительных стрелок или визуального подчёркивания определённых областей. Matplotlib предоставляет богатый набор инструментов для таких задач.\n",
        "\n",
        "Метод `ax.annotate()` позволяет размещать текст с указателем, направленным на конкретную координату. Это особенно полезно для подписи экстремумов, точек пересечения или выбросов. Для выделения областей используются методы вроде `ax.axhspan()` (горизонтальная полоса), `ax.axvline()` (вертикальная линия) или `ax.fill_between()` (заливка между кривыми).\n",
        "\n",
        "Пример аннотации экстремума:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = x * np.exp(-x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.plot(x, y, 'b-', lw=2)\n",
        "\n",
        "# Найдём максимум\n",
        "x_max = 1.0\n",
        "y_max = x_max * np.exp(-x_max)\n",
        "\n",
        "# Аннотация с изогнутой стрелкой\n",
        "ax.annotate(\n",
        "    f'Максимум\\n({x_max:.1f}, {y_max:.2f})',\n",
        "    xy=(x_max, y_max),\n",
        "    xytext=(4, 0.3),\n",
        "    arrowprops=dict(\n",
        "        arrowstyle='->',\n",
        "        connectionstyle='arc3,rad=0.3',\n",
        "        color='red'\n",
        "    ),\n",
        "    fontsize=11,\n",
        "    ha='center'\n",
        ")\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('f(x) = x·e⁻ˣ')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Дополнительно, через модуль `matplotlib.patches` можно добавлять геометрические фигуры: круги, прямоугольники, эллипсы и многоугольники. Это позволяет строить схематические диаграммы, выделять зоны неопределённости или создавать кастомные визуальные элементы.\n",
        "\n",
        "### Раздел 4: Интеграция с экосистемой Python и кастомизация стиля\n",
        "\n",
        "Хотя Matplotlib предоставляет низкоуровневый контроль, на практике часто используется совместно с библиотеками высокого уровня. Например, метод `.plot()` объектов pandas DataFrame и Series является обёрткой над `ax.plot()`, автоматически использующей индексы как X-координаты и имена столбцов как метки. Это значительно ускоряет анализ временных рядов и табличных данных.\n",
        "\n",
        "Для более сложной статистической визуализации (парные графики, регрессионные полосы, тепловые карты корреляций) исследователи часто прибегают к библиотеке **seaborn**, которая построена поверх Matplotlib и использует те же объекты Figure и Axes. Это означает, что любой график seaborn можно донастроить с помощью ОО-методов Matplotlib, сочетая удобство высокоуровневого API с гибкостью низкоуровневого.\n",
        "\n",
        "Кроме того, Matplotlib поддерживает систему стилей, позволяющую глобально изменять внешний вид всех графиков. Стиль можно активировать через `plt.style.use('seaborn-v0_8')` или загрузить из пользовательского файла `.mplstyle`. Это обеспечивает единообразие визуализации в рамках одного проекта или публикации.\n",
        "\n",
        "### Раздел 5: Поддержка математической нотации и работа с изображениями\n",
        "\n",
        "Для научных публикаций критически важна корректная отрисовка математических формул. Matplotlib встроенно поддерживает подмножество LaTeX через механизм **mathtext**. Достаточно заключить выражение в символы `$...$`, и библиотека отобразит его в соответствии с типографскими правилами математики.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```python\n",
        "ax.set_xlabel(r'Время $t$ (с)')\n",
        "ax.set_ylabel(r'Амплитуда $\\psi(t) = A e^{-\\gamma t} \\sin(\\omega t + \\phi)$')\n",
        "```\n",
        "\n",
        "Для отображения растровых данных (изображений, тепловых карт, спектрограмм) используются методы `ax.imshow()` и `ax.pcolormesh()`. Первый интерпретирует массив как изображение с пиксельной семантикой, второй — как дискретизированную функцию двух переменных. Оба метода поддерживают произвольные цветовые карты (`cmap`), нормализацию значений и добавление цветовых шкал, что делает их незаменимыми в обработке сигналов, компьютерном зрении и физическом моделировании.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение модуля**  \n",
        "> Matplotlib — это не только инструмент для построения статических линейных графиков, но и полноценная платформа для научной визуализации любого уровня сложности. От анимации динамических процессов и трёхмерного моделирования до точной типографики и интеграции с экосистемой Python — библиотека предоставляет исследователю исчерпывающий набор возможностей. Освоение этих продвинутых функций позволяет переходить от простого отображения данных к созданию выразительных, информативных и публикационно-готовых научных иллюстраций."
      ],
      "metadata": {
        "id": "CWbEhsjmJR9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Модуль 8: Статистическая Визуализация Высокого Уровня в Python (Seaborn)\n",
        "\n",
        "### 1. Методологические Основы Seaborn и Принцип «Опрятных Данных»\n",
        "\n",
        "#### 1.1. Роль Seaborn в конвейере EDA (Exploratory Data Analysis)\n",
        "\n",
        "Seaborn представляет собой высокоуровневую библиотеку для статистической визуализации, построенную поверх Matplotlib и тесно интегрированную с экосистемой pandas. В отличие от низкоуровневого Matplotlib, где требуется явное управление осями, метками и элементами графика, Seaborn предоставляет декларативный, ориентированный на данные API. Аналитик задаёт семантические отношения между переменными — например, указывает, что столбец `'region'` должен определять цвет (`hue`), а `'time'` — ось X, — и библиотека автоматически выполняет необходимую агрегацию, трассировку и отрисовку.\n",
        "\n",
        "Этот подход кардинально ускоряет процесс эксплораторного анализа данных (EDA). Вместо того чтобы вручную группировать данные, вычислять средние или строить отдельные кривые для каждой категории, исследователь формулирует гипотезу на языке переменных, и Seaborn мгновенно предоставляет визуальное подтверждение или опровержение. Таким образом, Seaborn заполняет критический пробел между сырыми табличными данными и статистическим пониманием, превращая визуализацию в инструмент прямого познания структуры данных.\n",
        "\n",
        "#### 1.2. Философия Tidy Data: Преимущества длинного формата данных (Long-Form Data)\n",
        "\n",
        "Эффективное использование Seaborn требует, чтобы данные соответствовали принципам «опрятных данных» (Tidy Data), предложенным Хадли Уикэмом. В этом формате каждая переменная занимает отдельный столбец, каждое наблюдение — отдельную строку, а каждое значение — одну ячейку. Такой подход противопоставляется широкому формату (wide-form), где, например, продажи по регионам могут быть разбросаны по разным столбцам (`sales_EU`, `sales_US`, `sales_APAC`).\n",
        "\n",
        "Длинный формат не является просто эстетическим предпочтением — он является архитектурной необходимостью для Seaborn. Ключевые компоненты, такие как `FacetGrid`, полагаются на возможность семантического сопоставления имени переменной с графическим атрибутом. Если уровни категории хранятся в одном столбце (например, `'region'` со значениями `'EU'`, `'US'`, `'APAC'`), система может автоматически создать фасеты по этим уровням. В широком формате такая информация теряется: библиотека не «знает», что три столбца относятся к одной и той же переменной.\n",
        "\n",
        "Преобразование данных в длинний формат легко выполняется с помощью метода `pandas.melt()`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Исходные данные в широком формате\n",
        "wide_df = pd.DataFrame({\n",
        "    'product': ['A', 'B'],\n",
        "    'Q1': [100, 150],\n",
        "    'Q2': [120, 160],\n",
        "    'Q3': [110, 155]\n",
        "})\n",
        "\n",
        "# Преобразование в длинний формат\n",
        "long_df = wide_df.melt(\n",
        "    id_vars='product',\n",
        "    value_vars=['Q1', 'Q2', 'Q3'],\n",
        "    var_name='quarter',\n",
        "    value_name='sales'\n",
        ")\n",
        "\n",
        "print(long_df)\n",
        "```\n",
        "\n",
        "Результат — таблица, где каждая строка представляет одно наблюдение (продажи продукта в квартале), что делает её идеальной для передачи в любую функцию Seaborn.\n",
        "\n",
        "#### 1.3. Эстетические основы: Управление стилями и выбор палитр\n",
        "\n",
        "Seaborn не только статистически, но и визуально улучшает стандартный вывод Matplotlib. Вызов `sns.set_theme()` активирует одну из встроенных тем (`'darkgrid'`, `'whitegrid'`, `'ticks'`), которая настраивает фон, сетку, шрифты и отступы, обеспечивая профессиональный вид «из коробки».\n",
        "\n",
        "Особое внимание в Seaborn уделяется **цветовым палитрам**, поскольку цвет является мощным, но и极易 вводящим в заблуждение каналом передачи информации. Библиотека предоставляет палитры, спроектированные с учётом перцептивной равномерности — то есть равные шаги в данных соответствуют равным шагам в восприятии.\n",
        "\n",
        "Различают три методологических типа палитр:\n",
        "\n",
        "- **Категориальные палитры** (например, `husl`, `Set1`) используют максимально различимые цвета для дискретных групп без внутреннего порядка.\n",
        "- **Последовательные палитры** (например, `Blues`, `viridis`) отображают монотонный градиент от низких к высоким значениям.\n",
        "- **Дивергентные палитры** (например, `vlag`, `coolwarm`) критически важны для данных с центральной точкой (обычно нулём или средним). Они используют контрастные цвета (синий/красный) для обозначения отклонений в противоположных направлениях.\n",
        "\n",
        "Некорректный выбор палитры может искажать интерпретацию. Например, при визуализации матрицы корреляций использование последовательной палитры (`Blues`) скроет знак коэффициентов: отрицательная корреляция будет выглядеть как «менее интенсивная», а не как противоположная по смыслу.\n",
        "\n",
        "Пример корректного выбора палитры для heatmap:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Загрузка примера данных\n",
        "flights = sns.load_dataset(\"flights\")\n",
        "flights_wide = flights.pivot(\"month\", \"year\", \"passengers\")\n",
        "\n",
        "# Использование последовательной палитры для положительных данных\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(flights_wide, cmap=\"YlGnBu\", annot=False, cbar_kws={'label': 'Пассажиры'})\n",
        "plt.title('Пассажиропоток по месяцам и годам')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь `YlGnBu` — последовательная палитра, уместная для неотрицательных данных. Для корреляций следовало бы выбрать `vlag`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Архитектура Сетки: Метод Малых Мультиплов (Small Multiples)\n",
        "\n",
        "#### 2.1. Концептуальное значение малых мультиплов\n",
        "\n",
        "Метод «малых мультиплов» (small multiples) — один из самых мощных приёмов в визуальном анализе многомерных данных. Он предполагает создание серии графиков одинакового типа, где каждый график отображает условный срез данных (например, по региону, году или категории). Поскольку визуальная кодировка остаётся постоянной, зритель может легко сравнивать формы распределений, тренды или отношения между переменными в разных условиях.\n",
        "\n",
        "#### 2.2. Класс FacetGrid и его измерения\n",
        "\n",
        "В Seaborn за реализацию малых мультиплов отвечает класс `FacetGrid`. Он создаёт сетку из объектов Axes, где структура определяется категориальными переменными. Основные измерения:\n",
        "\n",
        "- **`row` и `col`** задают физическое размещение подграфиков в двумерной сетке. Каждый уникальный уровень переменной порождает отдельную строку или столбец.\n",
        "- **`hue`** добавляет третье измерение через цвет: разные категории отображаются разными цветами на одном и том же подграфике.\n",
        "\n",
        "Таким образом, `FacetGrid` позволяет одновременно анализировать до четырёх переменных: две непрерывные (X и Y), одна для фасетирования по сетке и одна для цветового кодирования.\n",
        "\n",
        "#### 2.3. Взаимодействие с высокоуровневыми функциями\n",
        "\n",
        "Seaborn предоставляет два типа функций: **уровня фигуры** (Figure-Level) и **уровня осей** (Axes-Level).\n",
        "\n",
        "Функции уровня фигуры — `relplot()`, `displot()`, `catplot()`, `lmplot()` — автоматически создают `FacetGrid` и применяют к нему соответствующую функцию уровня осей (`scatterplot`, `histplot`, `boxplot`, `regplot`). Они идеальны для быстрого многомерного анализа.\n",
        "\n",
        "Функции уровня осей работают с уже существующим объектом Axes, что даёт полный контроль, но требует ручного управления макетом.\n",
        "\n",
        "Пример использования `relplot` для анализа по категориям:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Анализ зависимости чаевых от счёта, разбитый по полу и курению\n",
        "g = sns.relplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"sex\", col=\"smoker\", row=\"time\",\n",
        "    height=4, aspect=1\n",
        ")\n",
        "g.set_axis_labels(\"Счёт ($)\", \"Чаевые ($)\")\n",
        "g.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код создаёт сетку 2×2 графиков за одну строку, демонстрируя силу Figure-Level API. Если бы мы использовали `scatterplot`, пришлось бы вручную создавать сетку через `plt.subplots()` и писать цикл для заполнения.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Визуализация и Статистическая Интерпретация Одномерных Распределений\n",
        "\n",
        "Seaborn предлагает три взаимодополняющих метода визуализации одномерных распределений, каждый со своими статистическими достоинствами.\n",
        "\n",
        "#### 3.1. Гистограммы (`histplot`)\n",
        "\n",
        "Гистограмма разбивает диапазон значений на интервалы (бины) и отображает частоту или плотность наблюдений в каждом бине. Главный недостаток — зависимость от выбора количества и ширины бинов. Seaborn смягчает эту проблему, позволяя добавить к гистограмме кривую оценки плотности ядра (`kde=True`), что даёт более плавное представление формы распределения.\n",
        "\n",
        "#### 3.2. Оценка плотности ядра (`kdeplot`)\n",
        "\n",
        "KDE-график строит гладкую оценку функции плотности вероятности, суммируя ядра (обычно гауссовы) вокруг каждой точки данных. Ключевой параметр — **полоса пропускания** (bandwidth): слишком широкая скрывает мультимодальность, слишком узкая — усиливает шум. В Seaborn она настраивается через параметр `bw_adjust`.\n",
        "\n",
        "#### 3.3. Эмпирическая кумулятивная функция распределения (`ecdfplot`)\n",
        "\n",
        "ECDF-график показывает долю наблюдений, не превышающих заданное значение. Его главное преимущество — **отсутствие настраиваемых параметров**. Каждая точка данных отображается напрямую, что делает ECDF объективным инструментом для сравнения распределений. Хотя форма менее интуитивна, чем KDE, она свободна от субъективного сглаживания.\n",
        "\n",
        "Пример сравнения трёх методов:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "body_mass = penguins[\"body_mass_g\"].dropna()\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Гистограмма с KDE\n",
        "sns.histplot(body_mass, kde=True, ax=axs[0])\n",
        "axs[0].set_title('Гистограмма + KDE')\n",
        "\n",
        "# Чистый KDE\n",
        "sns.kdeplot(body_mass, ax=axs[1])\n",
        "axs[1].set_title('KDE')\n",
        "\n",
        "# ECDF\n",
        "sns.ecdfplot(body_mass, ax=axs[2])\n",
        "axs[2].set_title('ECDF')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "В академическом анализе, где важна воспроизводимость и объективность, ECDF следует рассматривать как основу, а гистограммы и KDE — как вспомогательные инструменты для интуитивного понимания.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Анализ Групп и Категориальных Переменных\n",
        "\n",
        "#### 4.1. Сводные распределения: `boxplot` и `violinplot`\n",
        "\n",
        "Boxplot предоставляет компактное резюме распределения: медиана, квартили, whiskers и выбросы. Он устойчив к выбросам и идеален для быстрого сравнения локации и разброса.\n",
        "\n",
        "Violinplot дополняет эту информацию, отображая полную форму распределения через KDE. Он может выявить бимодальность или асимметрию, которые boxplot скрывает.\n",
        "\n",
        "#### 4.2. Точечные представления: `stripplot` и `swarmplot`\n",
        "\n",
        "Swarmplot отображает каждую точку данных, избегая наложения за счёт небольшого смещения вдоль категориальной оси. Это позволяет видеть не только центральную тенденцию, но и плотность, количество и точное расположение наблюдений.\n",
        "\n",
        "#### 4.3. Комбинирование графиков для комплексного анализа\n",
        "\n",
        "Наиболее информативный подход — комбинировать методы. Например, наложить `swarmplot` на `violinplot`:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.violinplot(data=penguins, x=\"species\", y=\"body_mass_g\", inner=None, color=\".8\")\n",
        "sns.swarmplot(data=penguins, x=\"species\", y=\"body_mass_g\", size=4)\n",
        "plt.title('Масса пингвинов по видам')\n",
        "plt.ylabel('Масса (г)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь серый violinplot показывает форму распределения, а точки — фактические наблюдения. Это даёт полную картину: и статистическую, и эмпирическую.\n",
        "\n",
        "Кроме того, важно **упорядочивать категории осмысленно**. Если категории не имеют естественного порядка, их можно сортировать, например, по медиане:\n",
        "\n",
        "```python\n",
        "order = penguins.groupby(\"species\")[\"body_mass_g\"].median().sort_values().index\n",
        "sns.boxplot(data=penguins, x=\"species\", y=\"body_mass_g\", order=order)\n",
        "```\n",
        "\n",
        "Такой подход накладывает на визуализацию статистически обоснованную структуру, что улучшает интерпретацию.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение главы**  \n",
        "> Seaborn — это не просто библиотека для «красивых графиков», а инструмент для **статистического мышления через визуализацию**. Его архитектура, основанная на принципах tidy data, малых мультиплов и перцептивно обоснованных палитр, направляет исследователя к методологически корректному анализу. Освоение различий между Figure-Level и Axes-Level функциями, понимание сильных и слабых сторон каждого типа графика распределения, а также умение комбинировать визуальные методы позволяют превратить EDA из рутинной проверки в процесс глубокого познания данных. В руках внимательного аналитика Seaborn становится мостом между сырыми числами и научным выводом."
      ],
      "metadata": {
        "id": "-tDirS3oQCsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 5. Визуализация Отношений и Регрессионный Анализ\n",
        "\n",
        "### 5.1. Реляционные графики (`relplot`)\n",
        "\n",
        "Функция `relplot()` является ключевым инструментом Seaborn для визуализации взаимосвязей между переменными. Как функция уровня фигуры, она автоматически управляет сеткой подграфиков и поддерживает два основных типа графиков: точечные (`kind=\"scatter\"`) и линейные (`kind=\"line\"`). Главное преимущество `relplot` — его мощная семантическая кодировка, позволяющая одновременно отображать до пяти переменных: две позиционные (X и Y), а также цвет (`hue`), форму маркера (`style`), размер (`size`) и условные фасеты (`row`/`col`). Это превращает простой scatter plot в многомерный аналитический инструмент.\n",
        "\n",
        "Пример визуализации сложного отношения в данных о чаевых:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Отображение связи между счётом и чаевыми с учётом пола, курения и времени\n",
        "sns.relplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"sex\", style=\"smoker\", size=\"size\",\n",
        "    sizes=(40, 200),  # диапазон размеров маркеров\n",
        "    alpha=0.7,\n",
        "    height=5, aspect=1.2\n",
        ")\n",
        "plt.title('Многомерный анализ чаевых')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот график позволяет одновременно оценить, как размер счёта влияет на сумму чаевых, и как это соотношение изменяется в зависимости от пола клиента, привычки к курению и количества людей в группе. Такая визуализация служит отправной точкой для формулировки гипотез.\n",
        "\n",
        "### 5.2. Построение регрессионных моделей (`regplot` и `lmplot`)\n",
        "\n",
        "Для визуального представления линейной зависимости Seaborn предоставляет две функции. `regplot()` — это функция уровня осей, которая строит точечный график с наложенной линией регрессии и доверительным интервалом. `lmplot()` — это её обёртка уровня фигуры, интегрированная с `FacetGrid`, что позволяет строить отдельные регрессионные модели для каждого уровня категориальной переменной.\n",
        "\n",
        "Важно подчеркнуть, что Seaborn не предназначен для формального статистического вывода — для оценки коэффициентов, p-значений или критериев качества модели следует использовать библиотеки вроде `statsmodels` или `scikit-learn`. Регрессионные графики в Seaborn служат **визуальным руководством**: они помогают оценить силу, направление и линейность связи, а также выявить потенциальные выбросы или нелинейные паттерны.\n",
        "\n",
        "Пример сравнения регрессий по категориям:\n",
        "\n",
        "```python\n",
        "# Отдельная регрессия для курящих и некурящих\n",
        "sns.lmplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"smoker\",\n",
        "    height=5, aspect=1.2\n",
        ")\n",
        "plt.title('Регрессия чаевых по группам курильщиков')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой график сразу показывает, различается ли наклон регрессионной линии между группами — важный признак взаимодействия переменных.\n",
        "\n",
        "### 5.3. Диагностика модели: Применение `residplot()`\n",
        "\n",
        "Визуализация самой регрессии недостаточна — необходимо проверить адекватность модели. Для этого используется `residplot()`, который отображает остатки (разницу между наблюдаемыми и предсказанными значениями) в зависимости от предиктора.\n",
        "\n",
        "Для корректной линейной модели остатки должны быть **случайно рассеяны** вокруг горизонтальной линии `y = 0`. Любая структура — например, парабола, волна или «веер» (расширяющаяся дисперсия) — указывает на нарушение допущений: нелинейность, гетероскедастичность или пропущенную переменную.\n",
        "\n",
        "Seaborn позволяет углубить диагностику, подгоняя нелинейные модели. Например, параметр `order=2` строит квадратичную регрессию, а `lowess=True` добавляет сглаживающую кривую без параметрических допущений:\n",
        "\n",
        "```python\n",
        "# Диагностика линейной модели\n",
        "sns.residplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    lowess=True,  # непараметрическая линия тренда\n",
        "    scatter_kws={'alpha': 0.6}\n",
        ")\n",
        "plt.title('Остатки регрессионной модели')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Если кривая LOWESS явно отклоняется от нуля, это сигнал: линейная модель неадекватна, и следует рассмотреть нелинейные преобразования или добавление полиномиальных признаков.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Методология Отображения Статистической Неопределенности (`errorbar`)\n",
        "\n",
        "Начиная с версии 0.12, Seaborn ввёл унифицированный параметр `errorbar`, который строго разделяет два фундаментально разных типа неопределённости: **неопределённость оценки** и **разброс данных**.\n",
        "\n",
        "### 6.1. Два типа интервалов ошибки\n",
        "\n",
        "**Неопределённость оценки** отражает, насколько точно выборочная статистика (например, среднее) оценивает параметр генеральной совокупности. Этот интервал (доверительный интервал, CI, или стандартная ошибка, SE) **уменьшается с ростом размера выборки**.\n",
        "\n",
        "**Разброс данных** (стандартное отклонение, SD, или процентильный интервал, PI) описывает изменчивость самих наблюдений вокруг центра. Он **не зависит от размера выборки** и отражает дисперсию популяции.\n",
        "\n",
        "Смешивание этих понятий — серьёзная методологическая ошибка. Например, отображение SD вместо CI в графике средних по группам создаёт ложное впечатление, что различия между группами статистически значимы, даже если они нет.\n",
        "\n",
        "### 6.2. Методы построения доверительных интервалов\n",
        "\n",
        "Seaborn поддерживает два подхода к оценке неопределённости:\n",
        "\n",
        "- **Параметрический**: предполагает нормальность данных и использует аналитические формулы (например, `errorbar=(\"se\", 1)` для одной стандартной ошибки).\n",
        "- **Непараметрический (бутстрап)**: многократно ресэмплирует данные с замещением, строит эмпирическое распределение статистики и определяет интервал по процентилям (например, `errorbar=(\"ci\", 95)` для 95% доверительного интервала).\n",
        "\n",
        "Бутстрап особенно ценен при нарушении нормальности, наличии выбросов или сложных статистик, где аналитическая оценка дисперсии затруднена.\n",
        "\n",
        "Сравнение методов на практике:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# График средних с разными типами ошибок\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=(\"ci\", 95), ax=axes[0])\n",
        "axes[0].set_title('95% CI (бутстрап)')\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=\"se\", ax=axes[1])\n",
        "axes[1].set_title('Стандартная ошибка')\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=\"sd\", ax=axes[2])\n",
        "axes[2].set_title('Стандартное отклонение')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Первый график отвечает на вопрос: «В каком диапазоне, вероятно, лежит истинное среднее для всей популяции?». Третий — на вопрос: «Насколько сильно варьируются чаевые в группе?».\n",
        "\n",
        "### 6.3. Контроль прозрачности и извлечение данных\n",
        "\n",
        "При построении линейных графиков с доверительными интервалами (`lineplot`, `regplot`) неопределённость отображается в виде заштрихованной области. Её прозрачность регулируется параметром `err_kws={'alpha': 0.3}`. Это особенно важно при наложении нескольких линий, чтобы избежать визуального перегруза.\n",
        "\n",
        "Хотя Seaborn не предоставляет прямого API для извлечения численных значений границ CI, их можно получить из объекта Axes Matplotlib, что позволяет проводить дальнейший анализ или настраивать отображение.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Матричный Анализ: Корреляции и Иерархическая Кластеризация\n",
        "\n",
        "### 7.1. Тепловые карты (`heatmap`)\n",
        "\n",
        "Функция `heatmap()` предназначена для визуализации двумерных матриц, чаще всего — корреляционных. Использование **дивергентной палитры** (например, `vlag` или `coolwarm`) критически важно: она интуитивно разделяет положительные (тёплые цвета) и отрицательные (холодные) корреляции, а нейтральный центр (обычно белый или серый) обозначает отсутствие связи.\n",
        "\n",
        "Для научной публикации рекомендуется включать численные значения через `annot=True` и управлять их форматом через `fmt`:\n",
        "\n",
        "```python\n",
        "# Визуализация корреляционной матрицы\n",
        "numeric_vars = tips.select_dtypes(include=\"number\")\n",
        "corr_matrix = numeric_vars.corr()\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"vlag\",\n",
        "    center=0,\n",
        "    square=True,\n",
        "    cbar_kws={\"label\": \"Коэффициент корреляции Пирсона\"}\n",
        ")\n",
        "plt.title('Матрица корреляций')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой график позволяет мгновенно оценить силу и направление всех попарных линейных связей.\n",
        "\n",
        "### 7.2. Иерархическая кластеризация (`clustermap`)\n",
        "\n",
        "Функция `clustermap()` расширяет `heatmap`, добавляя **иерархическую кластеризацию** строк и столбцов. Алгоритм переупорядочивает элементы матрицы на основе их сходства (например, `1 - |ρ|` для корреляций), а вдоль осей отображает дендрограммы, показывающие иерархию объединения кластеров.\n",
        "\n",
        "`clustermap` — инструмент для **генерации гипотез**, а не для окончательного вывода. Выявленные кластеры требуют подтверждения формальными методами. Однако визуальная группировка сильно коррелирующих признаков или схожих наблюдений чрезвычайно полезна на этапе EDA.\n",
        "\n",
        "Важно: в отличие от большинства функций Seaborn, `heatmap` и `clustermap` работают с **широким форматом** (матрицей), хотя `clustermap` поддерживает tidy data через параметр `pivot_kws`.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```python\n",
        "# Кластеризация признаков по корреляции\n",
        "sns.clustermap(\n",
        "    corr_matrix,\n",
        "    cmap=\"vlag\",\n",
        "    center=0,\n",
        "    metric=\"correlation\",  # расстояние = 1 - |корреляция|\n",
        "    method=\"average\",      # метод связывания\n",
        "    figsize=(8, 8)\n",
        ")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот график выявляет, например, что `'total_bill'` и `'tip'` образуют один кластер, а `'size'` — другой, что может навести на мысль о латентных факторах (например, «размер группы» vs «щедрость»).\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Комплексное Применение Seaborn: Практические Кейсы EDA\n",
        "\n",
        "### 8.1. Пошаговый рабочий процесс EDA (набор данных `tips`)\n",
        "\n",
        "Эффективный EDA следует структурированному итеративному процессу.\n",
        "\n",
        "**Шаг 1: Унивариантный анализ.**  \n",
        "Изучение распределения ключевых переменных. Например, `sns.histplot(tips[\"total_bill\"], kde=True)` показывает, что распределение счёта скошено вправо, что может потребовать логарифмического преобразования перед регрессионным анализом.\n",
        "\n",
        "**Шаг 2: Создание производных метрик и сравнение групп.**  \n",
        "Аналитик вводит новую переменную — процент чаевых: `tips[\"tip_pct\"] = tips[\"tip\"] / tips[\"total_bill\"]`. Затем сравнивает его по категориям:  \n",
        "```python\n",
        "sns.boxplot(data=tips, x=\"day\", y=\"tip_pct\", hue=\"smoker\")\n",
        "```  \n",
        "Boxplot выявляет, что в выходные дни курящие оставляют меньший процент чаевых, чем некурящие.\n",
        "\n",
        "**Шаг 3: Многомерный анализ отношений.**  \n",
        "Семантический scatterplot позволяет одновременно учесть несколько факторов:  \n",
        "```python\n",
        "sns.scatterplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip_pct\",\n",
        "    hue=\"time\", style=\"sex\", size=\"size\"\n",
        ")\n",
        "```  \n",
        "График может показать, что в ужины (dinner) связь между размером счёта и процентом чаевых слабее, чем в обеды.\n",
        "\n",
        "**Шаг 4: Регрессия и корреляция.**  \n",
        "Построение `regplot` и матрицы корреляций завершает обзор, подтверждая или опровергая наблюдаемые тенденции количественно.\n",
        "\n",
        "### 8.2. Заключительные рекомендации по эффективному дизайну\n",
        "\n",
        "Эффективная статистическая визуализация требует методологической дисциплины:\n",
        "\n",
        "- **Чётко указывайте, что означают полосы ошибок** — CI, SE или SD. В подписи к графику пишите: «Планки ошибок: 95% доверительный интервал (бутстрап)».\n",
        "- **Используйте перцептивно корректные палитры**: дивергентные для корреляций, последовательные для положительных величин.\n",
        "- **Избегайте перегрузки**: не используйте одновременно `hue`, `style`, `size`, `row` и `col`, если это не критично для гипотезы.\n",
        "- **Помните о Matplotlib**: для тонкой настройки (аннотации, кастомные тики, LaTeX-формулы) используйте методы `ax.set_*()` после построения графика Seaborn.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Выводы и Методологическое Заключение\n",
        "\n",
        "Seaborn — это не просто инструмент для «красивых картинок», а **методологическая платформа для статистического мышления через визуализацию**. Его архитектура, основанная на принципах tidy data и малых мультиплов, направляет исследователя к структурированному, воспроизводимому анализу.\n",
        "\n",
        "Процесс EDA в Seaborn итеративен: гистограмма генерирует вопрос о распределении, scatterplot — о связи, а `residplot` — о валидности модели. Каждый график — не конечный результат, а **диалог с данными**.\n",
        "\n",
        "Особое внимание библиотека уделяет **статистической честности**. Разделение неопределённости оценки и разброса данных, использование робастных методов вроде бутстрапа, визуальная диагностика моделей — всё это защищает от поспешных выводов.\n",
        "\n",
        "Таким образом, Seaborn служит критически важным мостом между сырыми данными и обоснованным научным выводом. Его декларативный API позволяет исследователю сосредоточиться на содержании анализа, а не на технических деталях отрисовки, делая статистическую визуализацию неотъемлемой частью мышления аналитика."
      ],
      "metadata": {
        "id": "x6VZzGRlSqvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 9. Интерактивная Визуализация и Веб-Дашборды в Python: Методическая Лекция по Plotly и Dash\n",
        "\n",
        "### Раздел I. Архитектурные Основы Plotly: От Данных к Интерактивному Графику\n",
        "\n",
        "#### 1.1. Фигура Plotly как Фундаментальная Структура Данных\n",
        "\n",
        "Центральным элементом визуализации в библиотеке Plotly является объект `plotly.graph_objects.Figure`. Этот объект представляет собой декларативный контейнер, полностью описывающий график: его данные, внешний вид и интерактивные возможности. На самом низком уровне фигура Plotly структурирована как словарь Python, который последовательно транслируется в JSON-схему, интерпретируемую фронтенд-библиотекой **Plotly.js**.\n",
        "\n",
        "Такая архитектура превращает Plotly Python API в генератор строго формализованной спецификации. Любое изменение, внесённое через методы Python, неизбежно отражается в конкретных ключах и значениях этой схемы. Это обеспечивает высокую предсказуемость и воспроизводимость при кастомизации. Для разработчика, стремящегося к точному контролю, понимание иерархии этой структуры — не опция, а необходимость.\n",
        "\n",
        "Объект `Figure` состоит из трёх компонентов:\n",
        "\n",
        "- **`data`** — список следов (`traces`), каждый из которых описывает один набор данных и способ его отображения (точки, столбцы, поверхность и т.д.);\n",
        "- **`layout`** — объект, содержащий все стилистические настройки, не зависящие от данных: заголовки, оси, отступы, легенды, параметры 3D-сцены;\n",
        "- **`frames`** — список кадров, используемых для анимаций, где каждый кадр определяет новое состояние `data` и/или `layout`.\n",
        "\n",
        "#### 1.2. Объектная Модель Plotly: Trace и Layout\n",
        "\n",
        "Для построения и модификации фигур используются два класса объектов: **Trace** и **Layout**.\n",
        "\n",
        "Каждый **след** (`go.Scatter`, `go.Bar`, `go.Surface` и др.) соответствует определённому типу визуализации и инкапсулирует массивы данных (например, `x`, `y`, `z`) и параметры отображения (например, `mode='lines+markers'`). Plotly поддерживает сотни типов следов, включая геопространственные (`Choroplethmapbox`), 3D (`Surface`) и специализированные (`Sankey`, `Sunburst`). После создания фигуру можно динамически изменять: метод `Figure.add_traces()` добавляет новые следы, а `Figure.update_traces()` — массово обновляет свойства существующих (например, меняет цвет всех линий).\n",
        "\n",
        "Объект **`Layout`** управляет глобальным видом графика. Его свойства настраиваются через `Figure.update_layout()`. Для создания многопанельных композиций используется функция `make_subplots()`, которая генерирует предварительно настроенную фигуру с сеткой подграфиков. При добавлении следов в такую фигуру явно указывается их позиция (`row=1, col=2`).\n",
        "\n",
        "Важный методологический нюанс: при построении линейных графиков (например, временных рядов) **данные должны быть отсортированы по оси X**. Если этого не сделать, Plotly соединит точки в порядке их следования в массиве, что может привести к визуально искажённой, «путающейся» линии, не отражающей истинный тренд. Это не ошибка библиотеки, а следствие некорректной подготовки данных.\n",
        "\n",
        "#### 1.3. Сравнение Plotly Express (px) и Graph Objects (go)\n",
        "\n",
        "В Plotly существует два уровня API: высокоуровневый **Plotly Express** (`px`) и низкоуровневый **Graph Objects** (`go`).\n",
        "\n",
        "**Plotly Express** — это декларативный интерфейс, оптимизированный для быстрого создания типовых статистических графиков. Он принимает pandas DataFrame и автоматически назначает цвета, легенды, оси и даже анимации. Например, `px.scatter(df, x=\"income\", y=\"spending\", color=\"region\")` за одну строку строит многоцветный scatter plot. Это идеальный инструмент для разведочного анализа (EDA) и прототипирования.\n",
        "\n",
        "**Graph Objects** предоставляет полный контроль над каждым элементом фигуры. Он требует больше кода, но позволяет создавать сложные, нетиповые композиции: например, 3D-поверхность с наложенными контурами и точками, или интерактивную карту с несколькими слоями. При работе с `go` разработчик явно создаёт каждый след и настраивает каждый параметр макета.\n",
        "\n",
        "Выбор между `px` и `go` — это выбор между скоростью и гибкостью. Часто используется гибридный подход: график создаётся через `px`, а затем детально донастраивается через `fig.update_layout()` и `fig.update_traces()`.\n",
        "\n",
        "Пример гибридного подхода:\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Быстрое создание через px\n",
        "fig = px.line(\n",
        "    data_frame=df_sorted,\n",
        "    x=\"date\", y=\"value\",\n",
        "    color=\"category\",\n",
        "    title=\"Динамика показателей по категориям\"\n",
        ")\n",
        "\n",
        "# Детальная настройка через go-методы\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Дата\",\n",
        "    yaxis_title=\"Значение\",\n",
        "    legend_title=\"Категория\",\n",
        "    font=dict(family=\"Arial\", size=12)\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "#### 1.4. Экспорт и Сохранение Интерактивных и Статических Изображений\n",
        "\n",
        "Plotly предлагает два ключевых способа экспорта:\n",
        "\n",
        "- **Интерактивный HTML**: метод `fig.write_html(\"plot.html\")` сохраняет график в автономный HTML-файл, содержащий весь необходимый JavaScript (Plotly.js). Такой файл можно открыть в любом браузере, делиться им по email или встраивать в веб-страницы. Вся интерактивность (зум, панорамирование, тултипы) сохраняется.\n",
        "- **Статическое изображение**: метод `fig.write_image(\"plot.png\")` (или `.svg`, `.pdf`) генерирует растровое или векторное изображение для печати или вставки в презентации. Для этого требуется библиотека **Kaleido**, которая обеспечивает высококачественный рендеринг без зависимости от браузера.\n",
        "\n",
        "Эта гибкость делает Plotly универсальным решением: от интерактивных дашбордов до публикаций в научных журналах.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел II. Продвинутые Техники Визуализации с Plotly\n",
        "\n",
        "#### 2.1. Трёхмерная Визуализация: Scatter3D и Surface Plots\n",
        "\n",
        "Plotly предоставляет мощные инструменты для работы с трёхмерными данными, включая полный контроль над камерой, освещением и проекциями.\n",
        "\n",
        "**Scatter3D** (`go.Scatter3d`) отображает точки в пространстве, определяемом координатами X, Y, Z. Каждый маркер может быть окрашен, изменён по размеру или форме в зависимости от дополнительных переменных, что позволяет визуализировать до **пяти измерений** одновременно. Это особенно полезно при анализе многомерных наборов данных, таких как Iris или результатов моделирования.\n",
        "\n",
        "**Surface Plots** (`go.Surface`) предназначены для отображения функций вида Z = f(X, Y). Входные данные должны быть представлены в виде двумерных массивов, где каждый элемент `Z[i,j]` соответствует высоте над точкой `(X[i], Y[j])`. Plotly позволяет настраивать **контуры**: отображать их на самой поверхности или проецировать на плоскости XZ и YZ, что значительно улучшает восприятие формы.\n",
        "\n",
        "Полный контроль над 3D-сценой осуществляется через `layout.scene`. Ключевые параметры:\n",
        "\n",
        "- `camera.eye` — позиция камеры (вектор);\n",
        "- `aspectmode=\"manual\"` + `aspectratio` — соотношение масштабов по осям (важно для избежания искажений);\n",
        "- `xaxis.nticks` — количество тиков на оси.\n",
        "\n",
        "Пример 3D-поверхности с контурами:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Генерация сетки\n",
        "x = np.linspace(-5, 5, 50)\n",
        "y = np.linspace(-5, 5, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "fig = go.Figure(data=go.Surface(\n",
        "    x=X, y=Y, z=Z,\n",
        "    contours={\n",
        "        \"z\": {\"show\": True, \"start\": -1, \"end\": 1, \"size\": 0.1},\n",
        "        \"x\": {\"show\": True, \"start\": -5, \"end\": 5, \"size\": 1},\n",
        "        \"y\": {\"show\": True, \"start\": -5, \"end\": 5, \"size\": 1}\n",
        "    }\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        aspectmode=\"manual\",\n",
        "        aspectratio=dict(x=1, y=1, z=0.5)\n",
        "    ),\n",
        "    title=\"3D-поверхность с контурами\"\n",
        ")\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "Такой подход позволяет не просто отобразить данные, но и подчеркнуть их структуру — например, сделать вертикальные колебания более заметными за счёт сжатия оси Z.\n",
        "\n",
        "#### 2.2. Геопространственный Анализ: Choropleth, Scattermapbox и Стили Карт\n",
        "\n",
        "Plotly поддерживает два подхода к картографии:\n",
        "\n",
        "- **Контурные карты** (`layout.geo`) — работают без интернета, но имеют ограниченную детализацию;\n",
        "- **Плиточные карты** (`Mapbox` / `Maplibre`) — используют внешние тайловые сервисы для отображения улиц, зданий и рельефа.\n",
        "\n",
        "Начиная с версии 5.24, Plotly рекомендует использовать **Maplibre-based следы** (`go.Scattermap`, `go.Choroplethmap`), которые не требуют токена и могут работать с открытыми тайловыми сервисами (например, Stadia Maps). В отличие от них, следы на основе **Mapbox** требуют регистрации и токена доступа, что создаёт зависимости при развёртывании в корпоративных средах.\n",
        "\n",
        "Одной из самых мощных возможностей является **композитная картография** — наложение нескольких слоёв. Например, можно отобразить:\n",
        "\n",
        "1. **Choroplethmapbox** — для раскраски регионов (например, областей по среднему доходу), используя GeoJSON-файл с границами;\n",
        "2. **Scattermapbox** — для отображения точек (например, местоположений торговых точек), наложенных поверх регионов.\n",
        "\n",
        "Особая сложность возникает при создании **легенды для размеров маркеров** в пузырьковых картах. Plotly не генерирует автоматическую легенду размеров, поэтому её приходится строить вручную: для каждого уникального размера создаётся отдельный «невидимый» след с соответствующим именем и размером, который отображается только в легенде.\n",
        "\n",
        "#### 2.3. Динамическая Визуализация: Анимации\n",
        "\n",
        "Plotly Express упрощает создание анимаций с помощью параметров `animation_frame` и `animation_group`. Первый определяет переменную, по которой строятся кадры (например, год), второй — объекты, которые следует отслеживать (например, страна).\n",
        "\n",
        "Ключевое методологическое требование: **фиксировать диапазоны осей**. Если этого не сделать, масштаб будет автоматически подстраиваться под данные каждого кадра, что разрушит визуальную непрерывность и сделает сравнение во времени некорректным. Например, в «гонке по странам» (race bar chart) ось значений должна охватывать диапазон от **глобального минимума до глобального максимума** во всём временном интервале.\n",
        "\n",
        "Пример анимированного scatter plot:\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"gdp_per_capita\",\n",
        "    y=\"life_expectancy\",\n",
        "    size=\"population\",\n",
        "    color=\"continent\",\n",
        "    hover_name=\"country\",\n",
        "    animation_frame=\"year\",\n",
        "    animation_group=\"country\",\n",
        "    range_x=[0, 80000],\n",
        "    range_y=[20, 100],\n",
        "    title=\"Изменение здоровья и богатства стран с течением времени\"\n",
        ")\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "Здесь фиксированные `range_x` и `range_y` гарантируют, что движение точек отражает **реальные изменения**, а не артефакты масштабирования.\n",
        "\n"
      ],
      "metadata": {
        "id": "82Vvu-kdTRnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Раздел III. Основы Dash: Построение Реактивного Веб-Приложения\n",
        "\n",
        "### 3.1. Введение в Dash: Философия «No JavaScript Required»\n",
        "\n",
        "Dash — это декларативный и реактивный фреймворк с открытым исходным кодом, предназначенный для создания аналитических веб-приложений и дашбордов исключительно на языке Python. Под капотом он использует Flask в качестве бэкенда и комбинирует Plotly.js с React.js на фронтенде, автоматически преобразуя Python-объекты в HTML, CSS и JavaScript. Ключевое преимущество Dash для специалистов по данным — возможность строить полнофункциональные интерактивные веб-интерфейсы без написания ни строчки JavaScript, что значительно снижает порог входа в веб-разработку.\n",
        "\n",
        "### 3.2. Архитектура Приложения Dash: Инициализация и Макет\n",
        "\n",
        "Создание любого приложения Dash начинается с инициализации объекта:\n",
        "\n",
        "```python\n",
        "from dash import Dash, html\n",
        "app = Dash(__name__)\n",
        "```\n",
        "\n",
        "Этот объект инкапсулирует всю логику приложения. Макет интерфейса определяется через свойство `app.layout`, которое представляет собой декларативное дерево компонентов — фактически, описание DOM-структуры будущей веб-страницы. Макет, как правило, статичен при первом рендере, но может динамически изменяться через колбэки: например, при нажатии кнопки в контейнер `html.Div` может быть добавлен новый график или фильтр. Такой подход позволяет избежать прямой модификации `app.layout` и сохраняет предсказуемость реактивной системы.\n",
        "\n",
        "Приложение запускается вызовом:\n",
        "\n",
        "```python\n",
        "app.run_server(debug=True, port=8050)\n",
        "```\n",
        "\n",
        "В производственной среде, однако, встроенный сервер Flask заменяется на WSGI-совместимый сервер, такой как Gunicorn, для обеспечения стабильности и масштабируемости.\n",
        "\n",
        "### 3.3. HTML Компоненты (html) и Компоненты Dash Core (dcc)\n",
        "\n",
        "Интерфейс Dash строится из двух типов компонентов.\n",
        "\n",
        "Компоненты из модуля `dash.html` соответствуют стандартным HTML-тегам: `html.Div`, `html.H1`, `html.P` и т.д. Они используются для создания структуры страницы, заголовков, абзацев и контейнеров.\n",
        "\n",
        "Компоненты из модуля `dash.dcc` (Dash Core Components) предоставляют интерактивные элементы управления, характерные для аналитических дашбордов:\n",
        "\n",
        "- `dcc.Graph` — контейнер для встраивания фигур Plotly с полной поддержкой интерактивности (зум, выделение, тултипы);\n",
        "- `dcc.Dropdown`, `dcc.Slider`, `dcc.RadioItems` — элементы для выбора параметров и фильтрации;\n",
        "- `dcc.Tabs` — для организации многосекционного интерфейса;\n",
        "- `dcc.Location` и `dcc.Link` — для навигации в многостраничных приложениях.\n",
        "\n",
        "Важно помнить: все свойства компонентов должны быть JSON-сериализуемыми (строки, числа, списки, словари), так как они передаются между Python-бэкендом и React-фронтендом через JSON-мост.\n",
        "\n",
        "### 3.4. Стилевое Оформление и UI/UX Основы\n",
        "\n",
        "Стилизация в Dash осуществляется двумя способами:\n",
        "\n",
        "- через аргумент `className`, который связывает компонент с классами из внешней CSS-таблицы (например, `style.css`);\n",
        "- через аргумент `style`, принимающий словарь для inline-стилей.\n",
        "\n",
        "При проектировании аналитических дашбордов следует придерживаться принципов UI/UX:\n",
        "\n",
        "- **Информационная иерархия**: ключевые метрики (KPI) должны быть видны сразу, без прокрутки («above the fold»). Пользователь должен понимать суть дашборда за 5 секунд.\n",
        "- **Контекстуализация**: каждая метрика должна сопровождаться сравнением (например, «+12% к прошлому месяцу»).\n",
        "- **Адаптивность**: макет должен корректно отображаться на мобильных устройствах, с учётом сенсорного ввода.\n",
        "- **Визуальная чистота**: избыток элементов повышает когнитивную нагрузку. Белое пространство и минимализм улучшают читаемость.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел IV. Механизм Реактивности Dash: Колбэки и Управление Потоком\n",
        "\n",
        "Реактивность — сердце Dash. Она реализуется через систему **колбэков**, управляемых декоратором `@app.callback`.\n",
        "\n",
        "### 4.1. Жизненный Цикл Колбэка\n",
        "\n",
        "Колбэк — это обычная функция Python, связывающая входные и выходные свойства компонентов. Когда свойство, указанное как `Input`, изменяется (например, пользователь выбирает значение в выпадающем списке), Dash запускает функцию, передаёт ей текущие значения всех `Input` и `State`, и использует возвращаемые значения для обновления компонентов, указанных в `Output`.\n",
        "\n",
        "Dash строит **граф зависимостей** между компонентами. Если один колбэк обновляет `Output`, который является `Input` для другого колбэка, система гарантирует, что второй колбэк запустится только после того, как первый завершит обновление. Это предотвращает использование устаревших или несогласованных данных.\n",
        "\n",
        "### 4.2. Центральная Концепция: Роль Input, Output и State\n",
        "\n",
        "Понимание различий между `Input`, `Output` и `State` — ключ к стабильности приложения.\n",
        "\n",
        "- **`Output`** — свойство компонента, которое будет обновлено в результате работы колбэка. Оно не вызывает запуск функции.\n",
        "- **`Input`** — свойство, изменение которого **триггерит** выполнение колбэка.\n",
        "- **`State`** — свойство, значение которого **считывается** в момент запуска колбэка, но его изменение **не вызывает** запуск.\n",
        "\n",
        "Разделение `Input` и `State` критически важно для предотвращения циклических зависимостей. Например, если колбэк обновляет `dcc.Store`, а другой колбэк читает его как `State`, цикла не возникает. Если бы `dcc.Store` был `Input`, любое обновление вызывало бы бесконечный цикл.\n",
        "\n",
        "### 4.3. Продвинутые Паттерны Колбэков и Оптимизация Запуска\n",
        "\n",
        "Колбэк может иметь **несколько `Output`**, возвращая кортеж значений. Если обновление определённого `Output` не требуется, можно вернуть специальное значение `dash.no_update`, что предотвращает ненужную передачу данных в браузер.\n",
        "\n",
        "По умолчанию все колбэки запускаются при инициализации приложения. Для повышения производительности и избежания ошибок с динамически создаваемыми компонентами рекомендуется использовать параметр `prevent_initial_call=True`:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('graph', 'figure'),\n",
        "    Input('dropdown', 'value'),\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def update_graph(selected_value):\n",
        "    return create_figure(selected_value)\n",
        "```\n",
        "\n",
        "Это особенно важно для колбэков, выполняющих тяжёлые вычисления или запросы к базе данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел V. Продвинутые Паттерны Dash: Состояние и Производительность\n",
        "\n",
        "### 5.1. Управление Состоянием на Стороне Клиента: Компонент `dcc.Store`\n",
        "\n",
        "Для эффективного управления данными между колбэками используется невидимый компонент `dcc.Store`. Он хранит данные в браузере и позволяет избежать повторных вычислений.\n",
        "\n",
        "Свойство `storage_type` определяет место хранения:\n",
        "\n",
        "- `'memory'` — данные сбрасываются при перезагрузке;\n",
        "- `'session'` — сохраняются до закрытия вкладки;\n",
        "- `'local'` — сохраняются между сессиями.\n",
        "\n",
        "Выбор типа хранения влияет на UX: например, `'session'` позволяет сохранить выбранные фильтры при обновлении страницы. Однако важно помнить об ограничениях: безопасно хранить до 2 МБ данных. Это указывает на то, что Dash не предназначен для передачи больших массивов через браузер — агрегация должна происходить на сервере.\n",
        "\n",
        "### 5.2. Паттерны Использования `dcc.Store`\n",
        "\n",
        "Три ключевых сценария:\n",
        "\n",
        "1. **Кэширование**: результаты дорогих вычислений сохраняются в `Store` и используются другими колбэками.\n",
        "2. **Инициализация**: для запуска колбэка при загрузке страницы используется `modified_timestamp` как `Input`, а данные — как `State`.\n",
        "3. **Разрыв циклов**: один колбэк записывает в `Store` (`Output`), другой читает (`State`), предотвращая петлю.\n",
        "\n",
        "### 5.3. Взаимосвязь Графиков (Linked Brushing)\n",
        "\n",
        "Plotly поддерживает события взаимодействия пользователя, которые можно использовать в Dash для создания связанных визуализаций. Например, выделение точек на scatter plot может фильтровать карту или временной ряд.\n",
        "\n",
        "Это достигается через специальные свойства `dcc.Graph`:\n",
        "\n",
        "- `clickData` — данные по клику;\n",
        "- `selectedData` — точки, выделенные инструментами Lasso или Box Select;\n",
        "- `hoverData` — данные под курсором;\n",
        "- `relayoutData` — изменения масштаба или позиции.\n",
        "\n",
        "Пример linked brushing:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('map', 'figure'),\n",
        "    Input('scatter', 'selectedData')\n",
        ")\n",
        "def update_map(selected_data):\n",
        "    if selected_data is None:\n",
        "        filtered_df = df\n",
        "    else:\n",
        "        indices = [p['pointIndex'] for p in selected_data['points']]\n",
        "        filtered_df = df.iloc[indices]\n",
        "    return px.scatter_mapbox(filtered_df, ...)\n",
        "```\n",
        "\n",
        "Такой подход превращает дашборд из набора изолированных графиков в единый интерактивный аналитический инструмент.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел VI. Архитектура Масштабируемых Дашбордов и Производственное Развертывание\n",
        "\n",
        "### 6.1. Организация Многостраничных Приложений (Multi-Page Apps)\n",
        "\n",
        "Сложные дашборды часто требуют разделения на страницы. В Dash это достигается через комбинацию `dcc.Location` и `dcc.Link`.\n",
        "\n",
        "- `dcc.Location` отражает текущий путь в адресной строке (`pathname`);\n",
        "- `dcc.Link` создаёт переходы без перезагрузки страницы.\n",
        "\n",
        "Колбэк использует `pathname` как `Input` и возвращает соответствующий макет:\n",
        "\n",
        "```python\n",
        "@app.callback(Output('page-content', 'children'), Input('url', 'pathname'))\n",
        "def display_page(pathname):\n",
        "    if pathname == '/analytics':\n",
        "        return analytics_layout\n",
        "    elif pathname == '/reporting':\n",
        "        return reporting_layout\n",
        "    return home_layout\n",
        "```\n",
        "\n",
        "Такой подход реализует архитектуру Single Page Application (SPA) на чистом Python.\n",
        "\n",
        "### 6.2. Создание Дашбордов в Реальном Времени с `dcc.Interval`\n",
        "\n",
        "Для мониторинговых приложений используется компонент `dcc.Interval`, который с заданной периодичностью увеличивает свойство `n_intervals`. Это свойство служит `Input` для колбэка, обновляющего данные.\n",
        "\n",
        "Чтобы дать пользователю контроль над частотой обновления, можно связать `dcc.Interval` с `dcc.Slider`:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('interval-component', 'interval'),\n",
        "    Input('frequency-slider', 'value')\n",
        ")\n",
        "def update_interval(seconds):\n",
        "    return seconds * 1000  # перевод в миллисекунды\n",
        "```\n",
        "\n",
        "Это позволяет балансировать между актуальностью данных и нагрузкой на систему.\n",
        "\n",
        "### 6.3. Развертывание Приложений Dash (Gunicorn и Heroku)\n",
        "\n",
        "В производственной среде встроенный сервер Flask заменяется на WSGI-сервер, такой как **Gunicorn**. Для развёртывания на Heroku требуется:\n",
        "\n",
        "- файл `requirements.txt` со списком зависимостей;\n",
        "- файл `Procfile` со строкой:  \n",
        "  `web: gunicorn app:server`\n",
        "\n",
        "Здесь `app` — имя Python-файла (например, `app.py`), а `server` — переменная `app.server`, которую Dash предоставляет как внутренний Flask-сервер.\n",
        "\n",
        "Развертывание осуществляется через Git:\n",
        "\n",
        "```bash\n",
        "git push heroku main\n",
        "```\n",
        "\n",
        "Этот процесс подчёркивает, что, несмотря на простоту для пользователя, Dash — полноценный веб-фреймворк, требующий стандартных DevOps-практик.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Plotly и Dash образуют мощный, сквозной стек для создания профессиональных аналитических приложений на Python. Plotly обеспечивает глубокую, методологически обоснованную визуализацию — от 3D-поверхностей с контролем ракурса до многослойных геокарт, где регионы и точки анализируются совместно. Dash превращает эти визуализации в реактивные веб-приложения, где каждое действие пользователя мгновенно отражается на всех связанных элементах.\n",
        "\n",
        "Архитектура Dash, основанная на декларативном макете и строгом разделении `Input`/`State`/`Output`, обеспечивает стабильность и предсказуемость даже в сложных, многокомпонентных дашбордах. Использование `dcc.Store` для управления состоянием, `prevent_initial_call` для оптимизации и `dcc.Interval` для потоковых данных — это не просто технические приёмы, а методологические практики, обеспечивающие производительность и удобство.\n",
        "\n",
        "Наконец, возможность развёртывания через стандартные веб-инструменты (Gunicorn, Heroku) подтверждает зрелость экосистемы: специалист по данным может не только создать, но и доставить до пользователя полноценное веб-приложение, не выходя из привычной среды Python. В совокупности, Plotly и Dash демократизируют создание интерактивной аналитики, делая её доступной для всех, кто владеет языком данных."
      ],
      "metadata": {
        "id": "wcH89FfOUyS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 11: SymPy — Символьные вычисления и аналитическая математика\n",
        "\n",
        "### Раздел 1. Фундаментальные Основы Символьных Вычислений в SymPy\n",
        "\n",
        "#### 1.1. Философия SymPy: Символьные Объекты и Строгая Точность\n",
        "\n",
        "SymPy представляет собой полнофункциональную систему компьютерной алгебры (Computer Algebra System, CAS), написанную на языке Python. Её фундаментальное отличие от численных библиотек, таких как NumPy или SciPy, заключается в том, что она оперирует не приближёнными значениями, а абстрактными математическими символами и выражениями, сохраняя их точную алгебраическую форму на протяжении всех манипуляций.\n",
        "\n",
        "Основным строительным блоком в SymPy является символическая переменная, создаваемая с помощью класса `Symbol`. Для удобства работы в интерактивных средах, таких как Jupyter Notebook, рекомендуется вызывать функцию `init_printing()`, которая обеспечивает форматированный вывод математических выражений с использованием LaTeX/MathJax, делая их визуально идентичными записям в научных публикациях.\n",
        "\n",
        "SymPy обеспечивает строгую математическую точность. Все числовые объекты в SymPy наследуются от класса `Number` и его подклассов, включая `Integer` и `Rational`. Это означает, что рациональные числа, например, $2/3$, сохраняются в виде точной дроби, а не в виде приближения с плавающей точкой, что полностью исключает ошибки округления в аналитических вычислениях. Символьные константы, такие как $\\pi$, $e$ (представляется как `E`), мнимая единица $\\mathbf{i}$ (`I`) и бесконечность (`oo`), также обрабатываются символически.\n",
        "\n",
        "Способность сохранять числа в виде точных рациональных дробей или символьных констант является краеугольным камнем философии SymPy. При аналитическом выводе даже минимальное округление может скрыть алгебраическое тождество или нарушить каноническую форму выражения. В тех случаях, когда требуется взаимодействие с внешними численными системами или вывод десятичного приближения, используется метод `.evalf()` или функция `N()`. Эти методы позволяют явно указать необходимую точность — например, до 50 знаков после запятой, что обеспечивает контролируемый и воспроизводимый переход от точной символьной формы к численному приближению.\n",
        "\n",
        "#### 1.2. Внутренняя Структура Выражений: Древовидная Интерпретация\n",
        "\n",
        "В SymPy любое математическое выражение интерпретируется как древовидная структура — иерархия объектов, где каждый узел представляет операцию, а листья — атомарные символы или числа. Эта структура лежит в основе всех алгоритмов компьютерной алгебры и позволяет системе последовательно применять правила преобразования.\n",
        "\n",
        "Каждый символьный объект обладает двумя ключевыми атрибутами: `func` и `args`. Атрибут `func` указывает на класс операции, определяющей тип узла (например, `Add`, `Mul`, `Pow`), а `args` — это кортеж дочерних узлов. Например, в выражении $x \\cdot y$ атрибут `func` будет ссылаться на класс `Mul`, а `args` — на кортеж `(x, y)`.\n",
        "\n",
        "SymPy применяет принцип алгебраической канонизации, стремясь к минимальному набору базовых операций. Так, операция деления $x/y$ не имеет отдельного класса `Div`; вместо этого она интерпретируется как умножение $x$ на $y^{-1}$, то есть как `Mul(x, Pow(y, -1))`. Аналогично, выражение $\\cos(a + b)$ представляется как объект `Cos`, чьим единственным аргументом является операция сложения `Add(a, b)`. Такая унификация упрощает реализацию преобразований и повышает стабильность системы.\n",
        "\n",
        "В контексте отладки или разработки алгоритмов, требующих явного контроля над порядком операций, может потребоваться предотвращение автоматической оценки. Это достигается либо передачей параметра `evaluate=False` при создании выражения, либо использованием класса `UnevaluatedExpr`. Например, выражение `x * UnevaluatedExpr(1/x)` сохранит свою форму и не будет автоматически сокращено до единицы.\n",
        "\n",
        "#### 1.3. Система Допущений (Assumptions System)\n",
        "\n",
        "Система допущений SymPy — это критически важный механизм, позволяющий выполнять алгебраические преобразования, зависящие от области определения переменных. По умолчанию SymPy работает в наиболее общем домене — комплексных числах. Этот консервативный подход означает, что если не заданы явные ограничения, система не будет выполнять упрощения, которые могут быть неверны для произвольного комплексного числа. Например, упрощение $\\sqrt{y^2}$ до $y$ корректно только при условии, что $y$ — неотрицательное вещественное число.\n",
        "\n",
        "Допущения декларируются при создании символа с помощью ключевых слов, таких как `positive=True`, `real=True` или `integer=True`. Например, объявление `y = Symbol('y', positive=True)` позволяет SymPy автоматически упростить $\\sqrt{y^2}$ до $y$. Без этого допущения результат останется в виде $\\sqrt{y^2}$, чтобы сохранить корректность в комплексной плоскости.\n",
        "\n",
        "Система допущений использует трёхзначную нечёткую логику: запросы о свойствах выражения (например, `expr.is_positive`) могут возвращать `True`, `False` или `None`, где `None` означает, что свойство не может быть однозначно определено. Кроме того, система способна к логической инференции: если символ объявлен как `integer=True`, SymPy автоматически выводит, что он также является рациональным (`rational=True`), поскольку любое целое число — рационально. Аналогично, из `positive=True` следует `negative=False`.\n",
        "\n",
        "В прикладном математическом моделировании — особенно в физике и инженерии — переменные, представляющие физические величины (масса, время, длина), всегда являются положительными вещественными числами. Неиспользование допущений в таких задачах заставляет SymPy придерживаться консервативных правил, что может привести к избыточно сложным результатам. Явное декларирование домена не только упрощает аналитические формы, но и гарантирует, что полученное решение соответствует физической реальности.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 2. Задача 1: Аналитическое Упрощение и Канонические Формы Выражений\n",
        "\n",
        "Символьное упрощение — одна из наиболее востребованных и одновременно методологически сложных задач в компьютерной алгебре. Сложность заключается в том, что понятие «простоты» не имеет универсального определения: для одних задач предпочтительна разложенная (раскрытая) форма, для других — факторизованная, а для третьих — тригонометрическое тождество.\n",
        "\n",
        "#### 2.1. Методологическое Различие: Эвристика vs. Гарантированный Алгоритм\n",
        "\n",
        "SymPy предлагает два подхода к упрощению.\n",
        "\n",
        "Функция `simplify()` — это универсальный эвристический решатель, который пытается применить множество специализированных алгоритмов (тригонометрических, полиномиальных, для специальных функций) и выбирает результат, который, по её мнению, является «наиболее простым». Несмотря на свою мощь, `simplify()` не гарантирует достижения желаемой формы и может быть неэффективной из-за попытки применить широкий спектр преобразований.\n",
        "\n",
        "Поэтому для надёжного аналитического вывода предпочтение отдается **специализированным функциям**, которые гарантируют приведение выражения к определённой алгебраической канонической форме. Такой подход обеспечивает предсказуемость результата, что критически важно при подготовке формул для экспорта, автоматической генерации кода или дальнейшего алгоритмического анализа.\n",
        "\n",
        "#### 2.2. Полиномиальная и Рациональная Алгебра\n",
        "\n",
        "Для работы с полиномами и рациональными функциями SymPy предоставляет набор гарантированных алгоритмов:\n",
        "\n",
        "- **Факторизация (`factor`)** разлагает полином с рациональными коэффициентами на неприводимые множители. Это полезно для нахождения корней или анализа полюсов рациональной функции. Например, `factor(x**2 - 1)` даёт $(x - 1)(x + 1)$.\n",
        "- **Раскрытие (`expand`)** приводит выражение к форме суммы, раскрывая все произведения и степени. Это каноническая форма для многих алгебраических операций: `expand((x + 1)**2)` возвращает $x^2 + 2x + 1$.\n",
        "- **Сбор членов (`collect`)** организует полином по степеням заданной переменной, группируя коэффициенты.\n",
        "- **Общий знаменатель (`together`)** объединяет сумму рациональных функций в одну дробь $P(x)/Q(x)$.\n",
        "- **Разложение на простейшие дроби (`apart`)** выполняет классическое разложение рациональной функции на элементарные слагаемые, что незаменимо в интегрировании и теории управления.\n",
        "\n",
        "#### 2.3. Специализированные Преобразования\n",
        "\n",
        "Для других классов функций используются соответствующие алгоритмы:\n",
        "\n",
        "- **Тригонометрия (`trigsimp`)** применяет тригонометрические тождества. Классический пример: `trigsimp(sin(x)**2 + cos(x)**2)` преобразуется в единицу.\n",
        "- **Степени (`powsimp`)** упрощает выражения, содержащие степени с одинаковыми основаниями или показателями.\n",
        "- **Специальные функции**: SymPy содержит алгоритмы для упрощения выражений с гамма-функцией, дзета-функцией и другими специальными математическими объектами.\n",
        "\n",
        "#### 2.4. Кейс-стади (Инженерия): Аналитическое Упрощение Коэффициентов\n",
        "\n",
        "В инженерии и физике при выводе уравнений движения или передаточных функций часто возникают сложные рациональные выражения, требующие упрощения. Рассмотрим пример:\n",
        "\n",
        "$$E(x) = \\frac{x^3 + x^2 - x - 1}{x^2 + 2x + 1}$$\n",
        "\n",
        "Для получения чистой и вычислительно эффективной формы, пригодной для кодогенерации или дальнейшего анализа, предпочтительны гарантированные методы, а не эвристический `simplify()`.\n",
        "\n",
        "```python\n",
        "from sympy import symbols, factor\n",
        "\n",
        "x = symbols('x')\n",
        "expr = (x**3 + x**2 - x - 1) / (x**2 + 2*x + 1)\n",
        "\n",
        "# Факторизация числителя и знаменателя\n",
        "num = factor(x**3 + x**2 - x - 1)  # (x - 1)*(x + 1)**2\n",
        "den = factor(x**2 + 2*x + 1)       # (x + 1)**2\n",
        "\n",
        "# Сокращение\n",
        "simplified = num / den  # x - 1\n",
        "```\n",
        "\n",
        "Преимущество специализированных функций, таких как `factor()`, заключается в том, что они обеспечивают вывод в предсказуемой канонической форме. При разработке систем, где символьные формулы экспортируются для численного расчёта (например, в C или Fortran), требуется, чтобы выражения были минимальны с точки зрения вычислительной сложности. Специализированные алгоритмы гарантируют получение наиболее эффективной алгебраической формы, чего нельзя сказать об эвристическом подходе.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 3. Задача 2: Решение Алгебраических и Трансцендентных Уравнений\n",
        "\n",
        "Аналитическое решение уравнений — ключевая задача символьных вычислений, позволяющая находить точные корни или параметрические зависимости.\n",
        "\n",
        "#### 3.1. Современный Алгоритмический Подход: `solveset`\n",
        "\n",
        "Исторически SymPy использовал функцию `solve()`, но из-за её неконсистентного интерфейса и неспособности чётко различать типы решений (отсутствие, конечное или бесконечное множество) был разработан новый, методологически строгий интерфейс — `solveset()`.\n",
        "\n",
        "Функция `solveset(equation, variable, domain=S.Complexes)` возвращает строгий математический объект типа `Set`, что позволяет однозначно представлять различные типы решений: `EmptySet` (нет решений), `FiniteSet` (конечное число корней) или `ImageSet` (бесконечное множество, например, для уравнения $\\sin(x) = 0$).\n",
        "\n",
        "Использование `solveset()` требует явного задания области определения. По умолчанию это комплексные числа, но для прикладных задач в физике или экономике чаще всего используется `domain=S.Reals`.\n",
        "\n",
        "#### 3.2. Решение Систем Уравнений\n",
        "\n",
        "Для систем уравнений SymPy предлагает специализированные решатели:\n",
        "\n",
        "- **`linsolve`** применяет матричные методы для надёжного решения линейных систем.\n",
        "- **`nonlinsolve`** предназначен для систем нелинейных или полиномиальных уравнений. Результат возвращается в виде множества кортежей, где каждый кортеж соответствует полному решению по всем переменным.\n",
        "\n",
        "Например, решение системы $a^2 + a = 0$ и $a - b = 0$ с помощью `nonlinsolve` даёт множество $\\{(-1, -1), (0, 0)\\}$.\n",
        "\n",
        "#### 3.3. Кейс-стади (Экономика): Аналитическое Выведение Оптимального Выбора\n",
        "\n",
        "В экономическом моделировании символьные вычисления незаменимы для вывода параметрических формул, описывающих равновесие. Рассмотрим классическую задачу потребительского выбора в модели Кобба-Дугласа: максимизация полезности $U(x_0, x_1) = x_0^\\alpha x_1^{1-\\alpha}$ при бюджетном ограничении $p_0 x_0 + p_1 x_1 = I$.\n",
        "\n",
        "После применения метода множителей Лагранжа получается система нелинейных уравнений — условий первого порядка (FOCs). Эти уравнения передаются в `nonlinsolve`, чтобы найти оптимальные объёмы спроса $x_0^*$ и $x_1^*$ как функции параметров $I, p_0, p_1, \\alpha$.\n",
        "\n",
        "```python\n",
        "from sympy import symbols, nonlinsolve, Eq\n",
        "\n",
        "x0, x1, p0, p1, I, alpha, L = symbols('x0 x1 p0 p1 I alpha Lambda')\n",
        "\n",
        "# Условия первого порядка\n",
        "eq1 = Eq(alpha * x0**(alpha - 1) * x1**(1 - alpha), L * p0)\n",
        "eq2 = Eq((1 - alpha) * x0**alpha * x1**(-alpha), L * p1)\n",
        "eq3 = Eq(p0 * x0 + p1 * x1, I)\n",
        "\n",
        "# Решение системы\n",
        "solution = nonlinsolve([eq1, eq2, eq3], [x0, x1, L])\n",
        "# Аналитический результат:\n",
        "# x0* = I * alpha / p0\n",
        "# x1* = I * (1 - alpha) / p1\n",
        "```\n",
        "\n",
        "Получение точной параметрической формулы спроса — центральный элемент теоретического анализа. В отличие от численного подхода, символьное решение позволяет доказывать фундаментальные теоремы (например, о гомогенности функций спроса) и создаёт основу для эффективных численных реализаций, где градиенты и производные уже выведены аналитически.\n",
        ""
      ],
      "metadata": {
        "id": "ZcnCE0q9nzfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Раздел 4. Задача 3: Символьный Анализ Функций (Дифференциальное Исчисление)\n",
        "\n",
        "Символьное дифференциальное исчисление является основой для анализа изменения функций, линеаризации нелинейных моделей, вычисления градиентов и построения уравнений движения в физике и инженерии.\n",
        "\n",
        "#### 4.1. Дифференцирование: Оператор vs. Результат\n",
        "\n",
        "SymPy обеспечивает гибкость в работе с производными, предоставляя как немедленное вычисление, так и символическое представление оператора. Функция `diff(expr, var)` немедленно вычисляет производную выражения `expr` по переменной `var`. Она поддерживает частные производные, в том числе повторное дифференцирование по одной или нескольким переменным.\n",
        "\n",
        "В тех случаях, когда требуется сохранить структуру дифференциального оператора без немедленного вычисления — например, для построения сложных уравнений в теоретической физике — используется класс `Derivative(expr, var)`. Этот объект представляет собой символическую запись оператора $\\frac{d}{dx}f(x)$. Фактическое вычисление производной выполняется вызовом метода `.doit()` на экземпляре `Derivative`.\n",
        "\n",
        "#### 4.2. Векторное Исчисление\n",
        "\n",
        "Модуль `sympy.vector` реализует оператор Набла ($\\nabla$) через класс `Del()`, который не привязан к конкретной системе координат. Это позволяет символьно вычислять ключевые характеристики скалярных и векторных полей — градиент, дивергенцию и ротор.\n",
        "\n",
        "Градиент скалярного поля создаётся как применение `Del()` к полю, что возвращает выражение с невычисленными операторами `Derivative`. Для получения конкретного результата необходимо вызвать `.doit()`. Аналогично вычисляются дивергенция (`delop.dot(vector_field)`) и ротор (`delop.cross(vector_field)`), что делает SymPy мощным инструментом для аналитической электродинамики, гидродинамики и теории упругости.\n",
        "\n",
        "```python\n",
        "from sympy.vector import CoordSys3D, Del\n",
        "\n",
        "C = CoordSys3D('C')\n",
        "delop = Del()\n",
        "# Скалярное поле\n",
        "scalar_field = C.x * C.y * C.z\n",
        "\n",
        "# Символический градиент\n",
        "gradient_field = delop(scalar_field)\n",
        "# Фактическое вычисление\n",
        "result = gradient_field.doit()\n",
        "# Результат: C.y*C.z*C.i + C.x*C.z*C.j + C.x*C.y*C.k\n",
        "```\n",
        "\n",
        "#### 4.3. Аппроксимация Функций: Ряды Тейлора\n",
        "\n",
        "Символьное вычисление ряда Тейлора — фундаментальный инструмент для локальной аппроксимации функций. Функция `series(formula, variable, center_point, degree)` возвращает разложение вокруг заданной точки. Вывод включает остаточный член вида $O((x - a)^n)$, который символически обозначает все члены более высокого порядка.\n",
        "\n",
        "Для практических целей — например, линеаризации уравнений — необходим чистый полином без остатка. Это достигается вызовом метода `.removeO()`, который удаляет символический остаток и оставляет лишь полиномиальную часть разложения. Такой подход лежит в основе методов возмущений, линеаризации нелинейных систем и анализа устойчивости.\n",
        "\n",
        "#### 4.4. Кейс-стади (Инженерия): Анализ Устойчивости и Линеаризация\n",
        "\n",
        "В динамических системах — будь то механические конструкции, экономические модели или системы управления — часто требуется линеаризовать нелинейные уравнения движения вокруг точки равновесия.\n",
        "\n",
        "Процесс включает три шага. Сначала находится точка равновесия $\\mathbf{x}_e$, где вектор скорости $\\mathbf{f}(\\mathbf{x}_e) = \\mathbf{0}$. Затем вычисляется матрица Якоби $\\mathbf{J} = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$ — это делается символьно с помощью метода `.jacobian()` для объекта `Matrix`. Наконец, анализируются собственные значения $\\lambda_i$ матрицы $\\mathbf{J}$, вычисленные через `.eigenvals()`. Если вещественная часть всех $\\lambda_i$ отрицательна, система устойчива в окрестности $\\mathbf{x}_e$.\n",
        "\n",
        "SymPy позволяет выполнить весь этот процесс в символьной форме. Результат — параметрические выражения для собственных значений — показывает, как устойчивость зависит от физических параметров модели (масс, коэффициентов трения, жёсткости). Это невозможно в рамках чисто численного подхода и делает SymPy незаменимым в теоретическом анализе.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 5. Задача 4: Интегральное Исчисление и Дифференциальные Уравнения\n",
        "\n",
        "#### 5.1. Символьное Интегрирование: Точность и Пределы\n",
        "\n",
        "Функция `integrate()` служит основным инструментом для вычисления первообразных и определённых интегралов. Для неопределённого интеграла достаточно передать выражение и переменную, например, `integrate(cos(x), x)`. Важно отметить, что SymPy **не добавляет константу интегрирования** $C$ автоматически — её необходимо учитывать вручную или использовать `dsolve()` для задач, где константы критичны.\n",
        "\n",
        "Определённый интеграл вычисляется передачей кортежа `(переменная, нижний_предел, верхний_предел)`. SymPy поддерживает символическую бесконечность `oo`, что позволяет вычислять несобственные интегралы, такие как $\\int_0^{\\infty} e^{-x} dx = 1$. Также возможно многократное интегрирование — например, вычисление двойных или тройных интегралов через передачу нескольких кортежей с пределами.\n",
        "\n",
        "#### 5.2. Алгоритм Риша и Неэлементарные Функции\n",
        "\n",
        "SymPy использует детерминированный **алгоритм Риша** для интегрирования элементарных функций. Этот алгоритм обладает уникальным свойством: если первообразная существует в классе элементарных функций, она будет найдена; если нет — алгоритм доказывает неэлементарность интеграла.\n",
        "\n",
        "Например, интеграл $\\int e^{-x^2} dx$ не может быть выражен через элементарные функции, и SymPy оставит его в виде объекта `NonElementaryIntegral` или просто не вычислит, в зависимости от контекста. Ограничение алгоритма Риша — его неприменимость к специальным функциям (Бесселя, гипергеометрическим и др.), которые часто встречаются в физике. В таких случаях требуется расширение базы функций или переход к численным методам.\n",
        "\n",
        "#### 5.3. Решение Обыкновенных Дифференциальных Уравнений (ОДУ)\n",
        "\n",
        "Ключевым инструментом для аналитического решения ОДУ является функция `dsolve()`. Она принимает уравнение (в виде `Eq` или выражения, равного нулю) и искомую функцию. Результат возвращается как объект `Eq`, поскольку решения часто оказываются неявными.\n",
        "\n",
        "Функция `dsolve()` автоматически вводит произвольные константы интегрирования ($C_1, C_2, \\dots$), количество которых соответствует порядку уравнения. Для нахождения частного решения можно передать начальные или краевые условия через параметр `ics`. Например, `dsolve(eq, y, ics={y.subs(t, 0): 1})` задаёт условие $y(0) = 1$.\n",
        "\n",
        "#### 5.4. Кейс-стади (Физика): Аналитическое Решение Динамического Уравнения\n",
        "\n",
        "Рассмотрим классическую задачу радиоактивного распада, описываемую ОДУ первого порядка:\n",
        "$$\n",
        "\\frac{dy(t)}{dt} = - \\lambda y(t)\n",
        "$$\n",
        "где $y(t)$ — количество вещества, $\\lambda$ — константа распада.\n",
        "\n",
        "```python\n",
        "import sympy as sym\n",
        "t, l = sym.symbols('t lambda')\n",
        "y = sym.Function('y')(t)\n",
        "expr = sym.Eq(y.diff(t), -l * y)\n",
        "\n",
        "solution = sym.dsolve(expr, y)\n",
        "# Результат: y(t) = C1*exp(-l*t)\n",
        "```\n",
        "\n",
        "Это решение играет двойную роль. Во-первых, оно даёт точную замкнутую формулу для анализа поведения системы. Во-вторых, оно служит **аналитическим эталоном** для верификации численных методов. Даже если полная модель нелинейна и не поддаётся аналитическому решению, её упрощённая линейная версия может быть решена в SymPy и использована для проверки корректности численного интегратора.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 6. Применение: Сквозное Символическое Моделирование\n",
        "\n",
        "SymPy выходит за рамки отдельных математических операций, обеспечивая поддержку полного цикла аналитического моделирования в прикладных науках.\n",
        "\n",
        "#### 6.1. Аналитический Вывод Уравнений Движения (Физика/Механика)\n",
        "\n",
        "Модуль `sympy.physics.mechanics` позволяет формализовать задачи классической механики. Одним из самых мощных подходов является **метод Лагранжа**: пользователь задаёт кинетическую ($T$) и потенциальную ($U$) энергии, SymPy формирует лагранжиан $L = T - U$ и автоматически генерирует уравнения движения через класс `LagrangesMethod`. Это устраняет многочасовую рутинную алгебру и исключает ошибки при выводе сложных ОДУ. Полученные уравнения могут быть либо решены аналитически, либо преобразованы в численные функции для симуляции.\n",
        "\n",
        "#### 6.2. Символическая Статистика и Вероятность\n",
        "\n",
        "Модуль `sympy.stats` позволяет работать с вероятностными распределениями в аналитической форме. Например, для равномерного распределения $X \\sim U(90, 100)$ функция `E(X)` возвращает точное значение $95$, а не оценку по выборке. Также возможно символьное вычисление плотности вероятности, кумулятивной функции распределения и вероятностей событий вида $P(X < a)$. Это особенно ценно в теоретической статистике и при выводе распределений оценок.\n",
        "\n",
        "#### 6.3. Вычислительная Алгебра и Линейные Системы\n",
        "\n",
        "SymPy предоставляет полный набор инструментов для работы с **символьными матрицами**. Можно вычислять определители, обратные матрицы, собственные значения и векторы, а также приводить матрицы к каноническим формам, таким как жорданова. Это критически важно в теории управления (анализ устойчивости), механике (модальный анализ) и квантовой физике (диагонализация гамильтонианов). Результат — параметрические формулы, показывающие, как свойства системы зависят от её параметров.\n",
        "\n",
        "#### 6.4. Переход к Численным Вычислениям и Визуализация\n",
        "\n",
        "Завершающий этап — **конвертация** символьных выражений в эффективный численный код. Функция `lambdify` преобразует выражение SymPy в быструю Python-функцию (с опциональной поддержкой NumPy или SciPy). Также возможен экспорт в C, Fortran или Julia для интеграции в высокопроизводительные симуляции.\n",
        "\n",
        "Кроме того, SymPy поддерживает **аналитическую визуализацию**: модуль `plot` позволяет строить 2D- и 3D-графики символьных функций, `plot_complex` — отображать комплексные функции методом цветового кодирования (domain coloring), а `plot_vector` — визуализировать векторные поля. Это делает возможным не только вычисление, но и непосредственное восприятие аналитических результатов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Раздел 7. Задача 5: Интегральные Преобразования и Асимптотический Анализ\n",
        "\n",
        "Интегральные преобразования представляют собой мощный аналитический аппарат для решения дифференциальных уравнений, анализа сигналов и изучения поведения функций в предельных режимах. В отличие от локальных методов, таких как ряды Тейлора, интегральные преобразования работают с функцией на всей её области определения, конвертируя дифференциальные операции в алгебраические. SymPy предоставляет символьные реализации ключевых преобразований, что позволяет получать точные аналитические решения и избегать ошибок, связанных с численной аппроксимацией.\n",
        "\n",
        "#### 7.1. Преобразование Лапласа: Решение Линейных ОДУ с Начальными Условиями\n",
        "\n",
        "Преобразование Лапласа является стандартным инструментом в теории управления, электротехнике и механике для анализа линейных динамических систем. Оно переводит функцию времени $f(t)$ в функцию комплексной переменной $F(s)$ по формуле:\n",
        "\\[\n",
        "F(s) = \\mathcal{L}\\{f(t)\\} = \\int_0^\\infty f(t) e^{-st}  dt.\n",
        "\\]\n",
        "Ключевое преимущество заключается в том, что **дифференцирование во временной области** превращается в **умножение на $s$** в частотной:\n",
        "\\[\n",
        "\\mathcal{L}\\{f'(t)\\} = s F(s) - f(0).\n",
        "\\]\n",
        "Это позволяет преобразовать линейное ОДУ с постоянными коэффициентами в алгебраическое уравнение относительно $F(s)$, решить его и затем применить обратное преобразование.\n",
        "\n",
        "SymPy реализует этот процесс через функции `laplace_transform` и `inverse_laplace_transform`. Они корректно обрабатывают начальные условия и возвращают результат в аналитической форме.\n",
        "\n",
        "> **Пример: колебательная система под воздействием ступенчатого сигнала**\n",
        "\n",
        "Рассмотрим уравнение вынужденных колебаний без затухания:\n",
        "\\[\n",
        "\\frac{d^2 y}{dt^2} + \\omega^2 y = u(t), \\quad y(0) = 0, \\quad y'(0) = 0,\n",
        "\\]\n",
        "где $u(t)$ — функция Хевисайда (ступенька).\n",
        "\n",
        "```python\n",
        "import sympy as sym\n",
        "t, s, w = sym.symbols('t s omega', positive=True)\n",
        "y = sym.Function('y')\n",
        "\n",
        "# Уравнение в символьной форме\n",
        "ode = sym.Eq(y(t).diff(t, t) + w**2 * y(t), sym.Heaviside(t))\n",
        "\n",
        "# Прямое преобразование Лапласа\n",
        "Y_s = sym.laplace_transform(ode.lhs, t, s)[0] - sym.laplace_transform(ode.rhs, t, s)[0]\n",
        "# После учёта начальных условий: Y_s = s**2 * Y(s) + w**2 * Y(s) - 1/s\n",
        "\n",
        "# Решение для Y(s)\n",
        "Y_s_solution = 1 / (s * (s**2 + w**2))\n",
        "\n",
        "# Обратное преобразование\n",
        "y_t = sym.inverse_laplace_transform(Y_s_solution, s, t)\n",
        "# Результат: y(t) = (1 - cos(omega*t)) / omega**2\n",
        "```\n",
        "\n",
        "Такой подход не только даёт точное решение, но и позволяет анализировать **передачу сигнала** через систему в частотной области, что является основой для построения диаграмм Боде и анализа устойчивости.\n",
        "\n",
        "#### 7.2. Преобразование Фурье: Анализ Частотного Спектра\n",
        "\n",
        "Преобразование Фурье служит для разложения функции на гармонические компоненты и широко применяется в обработке сигналов, квантовой механике и теории вероятностей. SymPy поддерживает несколько соглашений о нормировке, но по умолчанию использует физическое определение:\n",
        "\\[\n",
        "\\mathcal{F}\\{f(x)\\} = \\int_{-\\infty}^{\\infty} f(x) e^{-i k x}  dx.\n",
        "\\]\n",
        "\n",
        "Функции `fourier_transform` и `inverse_fourier_transform` позволяют точно вычислять спектры даже для обобщённых функций, таких как дельта-функция Дирака $\\delta(x)$ или гауссиан $e^{-x^2}$. Например, преобразование Фурье от гауссиана является гауссианом, что является фундаментальным свойством в теории неопределённости.\n",
        "\n",
        "> **Пример: спектр прямоугольного импульса**\n",
        "\n",
        "```python\n",
        "x, k = sym.symbols('x k', real=True)\n",
        "rect_pulse = sym.Piecewise((1, sym.Abs(x) < 1), (0, True))\n",
        "\n",
        "# Преобразование Фурье\n",
        "F_k = sym.fourier_transform(rect_pulse, x, k)\n",
        "# Результат: 2*sin(k)/k (функция sinc)\n",
        "```\n",
        "\n",
        "Этот результат напрямую связывает ширину импульса во временной области с шириной его спектра — ключевой принцип в теории связи.\n",
        "\n",
        "#### 7.3. Асимптотический Анализ: Поведение Функций в Беспредельных Режимах\n",
        "\n",
        "Помимо локальной аппроксимации (ряд Тейлора), часто требуется понять, как ведёт себя функция при $x \\to \\infty$ или $x \\to 0^+$. Для этого используется **асимптотическое разложение**, которое может включать не только степени, но и логарифмические члены или экспоненциально малые слагаемые.\n",
        "\n",
        "SymPy предоставляет функцию `asymptotic_series`, но на практике чаще используется метод `.asymptotic_expand()` или комбинация `limit` и `series` с указанием направления (`dir='+'` или `dir='-'`).\n",
        "\n",
        "> **Пример: асимптотика интегрального синуса**\n",
        "\n",
        "Интегральный синус $\\text{Si}(x) = \\int_0^x \\frac{\\sin t}{t} dt$ при $x \\to \\infty$ стремится к $\\pi/2$. Асимптотическое разложение показывает, как именно происходит это приближение:\n",
        "\n",
        "```python\n",
        "x = sym.symbols('x', positive=True)\n",
        "Si = sym.Si(x)\n",
        "asympt = Si.asymptotic_expand(x, n=3)  # до 3-го порядка\n",
        "# Результат: pi/2 - cos(x)/x - sin(x)/x**2 + O(1/x**3)\n",
        "```\n",
        "\n",
        "Такое разложение критически важно в физике рассеяния, где необходимо учитывать осциллирующие поправки к предельному значению.\n",
        "\n",
        "#### 7.4. Связь с Дифференциальными Уравнениями и Специальными Функциями\n",
        "\n",
        "Интегральные преобразования неразрывно связаны со специальными функциями. Например, решение уравнения Бесселя или гипергеометрического уравнения часто выражается через функции, чьи интегральные представления являются каноническими. SymPy позволяет не только вычислять преобразования, но и **распознавать** специальные функции в результатах, что обеспечивает согласованность с теоретическими справочниками.\n",
        "\n",
        "Более того, преобразования предоставляют **альтернативный путь к решению ОДУ**, особенно в случае сингулярных коэффициентов или краевых задач на бесконечности, где методы конечных разностей или прямое применение `dsolve` могут оказаться неэффективными.\n",
        "\n",
        "---\n",
        "\n",
        "> **Методологическое значение**  \n",
        "> Интегральные преобразования и асимптотический анализ завершают картину символьного моделирования, переходя от **локального** (дифференцирование, ряды Тейлора) к **глобальному** описанию поведения систем. SymPy, предоставляя точные реализации этих методов, позволяет исследователю не просто получить численный ответ, но и понять **физическую или математическую структуру** решения — его частотный состав, устойчивость, асимптотику и связь с фундаментальными функциями математической физики.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "SymPy является незаменимым инструментом в арсенале математического моделирования, обеспечивая строгую аналитическую базу, недоступную в чисто численных пакетах. Его методологическая сила основана на трёх ключевых принципах.\n",
        "\n",
        "Во-первых, **строгая алгебраическая точность**: использование символьных объектов, рациональных чисел и системы допущений гарантирует, что все выводы корректны в заданной области определения и свободны от ошибок округления.\n",
        "\n",
        "Во-вторых, **древовидная структура и канонизация**: интерпретация выражений как алгебраических деревьев позволяет применять гарантированные алгоритмы — такие как факторизация, алгоритм Риша или символьное дифференцирование, — которые приводят к предсказуемым, эффективным и математически корректным формам.\n",
        "\n",
        "В-третьих, **сквозная аналитика**: SymPy поддерживает полный цикл моделирования — от вывода уравнений движения в механике и градиентов в оптимизации, до решения дифференциальных уравнений и анализа устойчивости. Он выступает в роли **аналитического процессора**, который берёт на себя сложнейшие этапы формального вывода, после чего передаёт точные, верифицированные формулы численной машине.\n",
        "\n",
        "Таким образом, SymPy не просто дополняет численные библиотеки — он создаёт над ними **надёжный теоретический фундамент**, повышая достоверность, воспроизводимость и глубину современного научного и инженерного моделирования."
      ],
      "metadata": {
        "id": "5fSSgCWqecfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 10: SciPy — Научные и Инженерные Вычисления\n",
        "\n",
        "SciPy является фундаментальным компонентом экосистемы научных вычислений на языке Python, предлагая обширную коллекцию высокоуровневых алгоритмов, предназначенных для решения сложных математических, инженерных и научных задач. Этот модуль служит мостом между эффективными структурами данных NumPy и специализированными, проверенными временем численными процедурами, охватывающими оптимизацию, интеграцию, линейную алгебру, обработку сигналов и статистический анализ. Использование SciPy позволяет инженерам и исследователям формулировать сложные вычислительные задачи в виде высокоуровневых Python-команд, полагаясь при этом на скорость и надежность низкоуровневых языков программирования, таких как C и Fortran.\n",
        "\n",
        "Это первая часть в цикле материалов, посвящённых ключевым Python-библиотекам для научных и инженерных вычислений. В ней рассматриваются архитектурные особенности SciPy, его интеграция с другими компонентами экосистемы, а также подробный разбор наиболее важных подмодулей и алгоритмов с акцентом на численную устойчивость и практическое применение.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Фундаментальная Архитектура и Интеграция SciPy\n",
        "\n",
        "### 1.1. Место SciPy в экосистеме Python\n",
        "\n",
        "SciPy функционирует как библиотека, которая значительно расширяет возможности NumPy, предоставляя специализированные подпрограммы, работающие непосредственно с массивами NumPy. Его субмодули — такие как `scipy.optimize`, `scipy.integrate` и `scipy.linalg` — содержат высокоэффективные алгоритмы, необходимые для моделирования и анализа данных. Разделение функций между NumPy (базовая работа с массивами, элементарная линейная алгебра) и SciPy (продвинутые численные методы) обеспечивает модульность и чистоту архитектуры.\n",
        "\n",
        "Эта интеграция позволяет использовать Python для сквозных научных рабочих процессов: от загрузки и манипуляции данными (NumPy/Pandas) до сложного численного анализа (SciPy) и визуализации (Matplotlib). Такой стек обеспечивает полную независимость от закрытых коммерческих систем (например, MATLAB), сохраняя при этом высокую производительность и гибкость.\n",
        "\n",
        "**Пример: Простая интеграция NumPy и SciPy**\n",
        "\n",
        "Предположим, мы хотим решить систему линейных уравнений \\(A\\mathbf{x} = \\mathbf{b}\\), используя массивы NumPy в качестве входных данных и функции линейной алгебры из SciPy для решения:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.linalg import solve\n",
        "\n",
        "# Определяем коэффициенты системы\n",
        "A = np.array([[3, 2], [1, -1]], dtype=float)\n",
        "b = np.array([1, 4], dtype=float)\n",
        "\n",
        "# Решаем систему\n",
        "x = solve(A, b)\n",
        "print(\"Решение:\", x)\n",
        "```\n",
        "\n",
        "*Пояснение:* В этом примере массивы `A` и `b` создаются с помощью NumPy, а функция `solve` из `scipy.linalg` выполняет численно стабильное решение системы без явного вычисления обратной матрицы. SciPy автоматически выбирает оптимальный метод (обычно LU-разложение) в зависимости от структуры матрицы.\n",
        "\n",
        "*После выполнения:* Подобная комбинация демонстрирует элегантность и выразительность научного стека Python: пользователь формулирует задачу на естественном языке программирования, а низкоуровневые оптимизации остаются «под капотом».\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Высокопроизводительная Основа: Наследие Fortran и C\n",
        "\n",
        "Производительность SciPy, особенно в таких критически важных областях, как линейная алгебра и быстрое преобразование Фурье (БПФ), обеспечивается за счёт обёрток над высокооптимизированными библиотеками, написанными на C и Fortran.\n",
        "\n",
        "#### Использование BLAS/LAPACK\n",
        "\n",
        "Функции линейной алгебры в SciPy и NumPy зависят от BLAS (*Basic Linear Algebra Subprograms*) и LAPACK (*Linear Algebra Package*). Эти библиотеки предоставляют эффективные низкоуровневые реализации стандартных алгоритмов. При установке пакетов SciPy и NumPy (например, через `pip` или `conda`) автоматически обнаруживается и выбирается наиболее производительная доступная реализация BLAS/LAPACK — такая как Intel MKL, OpenBLAS или Accelerate (на macOS).\n",
        "\n",
        "Эти оптимизированные реализации активно используют многопоточность и специализированные векторные инструкции процессора (например, AVX-512), что критически важно для скорости вычислений при работе с большими массивами. Порядок поиска и выбора библиотеки определяется конфигурацией сборки, что позволяет достичь максимальной производительности в целевой среде исполнения.\n",
        "\n",
        "**Проверка используемой BLAS-реализации**\n",
        "\n",
        "```python\n",
        "import scipy\n",
        "print(scipy.show_config())\n",
        "```\n",
        "\n",
        "*Пояснение:* Выполнение этой команды выводит информацию о том, какие библиотеки BLAS/LAPACK были обнаружены при компиляции SciPy. Это особенно полезно при диагностике производительности на серверах или кластерах.\n",
        "\n",
        "#### Специализированные Fortran-библиотеки\n",
        "\n",
        "Значительная часть надёжности и точности SciPy основана на обёртках десятилетиями проверенных библиотек Fortran 77, включая:\n",
        "\n",
        "- **QUADPACK** — численное интегрирование;\n",
        "- **FITPACK** — сплайн-интерполяция;\n",
        "- **ODEPACK** — решение обыкновенных дифференциальных уравнений (ОДУ);\n",
        "- **MINPACK** — оптимизация и решение нелинейных систем.\n",
        "\n",
        "Использование этих библиотек гарантирует, что лежащие в основе численные алгоритмы являются стабильными, хорошо изученными и прошедшими многолетнюю проверку в реальных научных и инженерных приложениях.\n",
        "\n",
        "Однако такой архитектурный выбор несёт в себе инженерный компромисс. Высокая производительность и точность SciPy достигаются за счёт сложности сопровождения кода: устаревший Fortran 77 трудно поддерживать, тестировать и компилировать для новых аппаратных платформ (например, Windows on ARM или macOS с архитектурой Apple Silicon) или сред выполнения (таких как Pyodide/WebAssembly).\n",
        "\n",
        "Это обстоятельство указывает на то, что устойчивость и скорость SciPy являются результатом сложного архитектурного баланса между использованием надёжного, проверенного численного ядра и растущей сложностью поддержания его совместимости с современной вычислительной инфраструктурой.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Численная Линейная Алгебра (`scipy.linalg`): Стабильность и Декомпозиции\n",
        "\n",
        "Модуль `scipy.linalg` предлагает высокоуровневые функции для решения систем уравнений, нахождения собственных значений и выполнения матричных декомпозиций. В отличие от `numpy.linalg`, он предоставляет более полный набор алгоритмов и более надёжные реализации, особенно для плохо обусловленных задач.\n",
        "\n",
        "### 2.1. Концепция Обусловленности Матриц\n",
        "\n",
        "Численная стабильность при решении систем линейных уравнений \\(A\\mathbf{x} = \\mathbf{b}\\) напрямую зависит от числа обусловленности матрицы \\(A\\), обозначаемого \\(\\kappa(A)\\).\n",
        "\n",
        "**Математическое определение.** Число обусловленности определяется как  \n",
        "\\[\n",
        "\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|\n",
        "\\]  \n",
        "в некоторой матричной норме (обычно используют спектральную или 2-норму). Матрица считается **хорошо обусловленной**, если \\(\\kappa(A)\\) мало (близко к 1), что означает, что малые изменения во входных данных (матрице \\(A\\) или векторе \\(\\mathbf{b}\\)) приводят лишь к малым изменениям в решении \\(\\mathbf{x}\\).\n",
        "\n",
        "**Интерпретация.** Если \\(\\kappa(A)\\) велико (например, \\(\\gg 10^3\\)), матрица считается **плохо обусловленной** (*ill-conditioned*). В таком случае даже незначительные ошибки округления или шум во входных данных могут вызвать катастрофически большие ошибки в вычисленном решении. Плохая обусловленность часто возникает, когда матрица близка к сингулярной или имеет почти нулевые сингулярные значения.\n",
        "\n",
        "**Пример: Оценка числа обусловленности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.linalg import svdvals, cond\n",
        "\n",
        "# Создаём плохо обусловленную матрицу (например, матрицу Гильберта)\n",
        "def hilbert_matrix(n):\n",
        "    return np.array([[1.0 / (i + j + 1) for j in range(n)] for i in range(n)])\n",
        "\n",
        "A = hilbert_matrix(6)\n",
        "kappa = cond(A)\n",
        "print(f\"Число обусловленности κ(A) = {kappa:.2e}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Матрица Гильберта — классический пример плохо обусловленной матрицы. При \\(n=6\\) её число обусловленности уже превышает \\(10^7\\), что делает решение системы крайне нестабильным в арифметике с плавающей точкой.\n",
        "\n",
        "*После выполнения:* Такие оценки позволяют заранее диагностировать потенциальные проблемы численной точности и выбирать более робастные методы решения (например, SVD с регуляризацией).\n",
        "\n",
        "### 2.2. Основные Декомпозиции и Их Численная Роль\n",
        "\n",
        "Численные декомпозиции позволяют решать линейные задачи более эффективно и стабильно, чем прямое обращение матрицы.\n",
        "\n",
        "- **LU-разложение** (\\(PA = LU\\)).  \n",
        "  Этот метод факторизует квадратную матрицу \\(A\\) на матрицу перестановок \\(P\\), нижнюю треугольную матрицу \\(L\\) (с единичной диагональю) и верхнюю треугольную матрицу \\(U\\). Включение матрицы перестановок \\(P\\) не является просто организационным моментом — она реализует стратегию частичного выбора ведущего элемента (*partial pivoting*), что критически важно для обеспечения численной стабильности разложения в условиях ограниченной точности вычислений.\n",
        "\n",
        "- **Разложение Холецкого** (\\(A = L L^T\\)).  \n",
        "  Является специализированным и наиболее быстрым разложением для решения систем линейных уравнений. Оно применимо только к матрицам, которые являются симметричными и положительно определёнными. Когда это условие выполняется, разложение Холецкого примерно в два раза эффективнее LU-разложения. Оно широко используется в методах Монте-Карло, нелинейной оптимизации и фильтрах Калмана.\n",
        "\n",
        "- **Сингулярное разложение** (SVD, \\(A = U \\Sigma V^T\\)).  \n",
        "  SVD является наиболее робастным инструментом в линейной алгебре, применимым к матрицам любого размера. Его высочайшая численная стабильность делает его незаменимым при работе с плохо обусловленными или сингулярными системами. Диагональная матрица \\(\\Sigma\\) содержит сингулярные значения, которые являются квадратными корнями из собственных значений \\(A^T A\\). Сингулярные значения напрямую связаны с числом обусловленности:  \n",
        "  \\[\n",
        "  \\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\n",
        "  \\]  \n",
        "  где \\(\\sigma_{\\max}\\) и \\(\\sigma_{\\min}\\) — наибольшее и наименьшее сингулярные значения соответственно.\n",
        "\n",
        "  Для обеспечения робастности в инженерных задачах, особенно когда матрица \\(A\\) получена из зашумлённых измерений, необходимо включать SVD в рабочий процесс. Это позволяет не только диагностировать \\(\\kappa(A)\\), но и стабилизировать решение через усечённое SVD или регуляризацию Тихонова.\n",
        "\n",
        "**Пример: Решение плохо обусловленной системы через SVD**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.linalg import svd\n",
        "\n",
        "# Используем ту же матрицу Гильберта\n",
        "A = hilbert_matrix(6)\n",
        "b = np.ones(6)\n",
        "\n",
        "# Обычное решение (нестабильно)\n",
        "x_bad = np.linalg.solve(A, b)\n",
        "\n",
        "# Решение через SVD с отсечением малых сингулярных значений\n",
        "U, s, Vt = svd(A)\n",
        "# Отсекаем сингулярные значения меньше 1e-10\n",
        "s_inv = np.array([1/si if si > 1e-10 else 0 for si in s])\n",
        "x_good = Vt.T @ (s_inv * (U.T @ b))\n",
        "\n",
        "print(\"Норма разности решений:\", np.linalg.norm(x_bad - x_good))\n",
        "```\n",
        "\n",
        "*Пояснение:* Усечённое SVD игнорирует компоненты, соответствующие малым сингулярным значениям, которые усиливают шум. Это — простейшая форма регуляризации.\n",
        "\n",
        "**Таблица 2.1: Сравнение основных методов разложения матриц (`scipy.linalg`)**\n",
        "\n",
        "| Метод | Требования к матрице | Численная роль | Вычислительная эффективность |\n",
        "|-------|----------------------|----------------|-------------------------------|\n",
        "| LU (\\(PA = LU\\)) | Квадратная | Решение общих систем | Хорошая стабильность за счёт \\(P\\), универсальное применение |\n",
        "| Холецкого (\\(LL^T\\)) | Симметричная, положительно определённая | Монте-Карло, оптимизация | В ~2 раза быстрее LU (если применимо) |\n",
        "| SVD (\\(U \\Sigma V^T\\)) | Произвольная (\\(m \\times n\\)) | Диагностика \\(\\kappa(A)\\), псевдоинверсия | Наивысшая стабильность; основной диагностический инструмент |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Интегрирование и Дифференциальные Уравнения (`scipy.integrate`)\n",
        "\n",
        "Модуль `scipy.integrate` предоставляет инструменты как для численной квадратуры (интегрирование функций), так и для решения обыкновенных дифференциальных уравнений (ОДУ).\n",
        "\n",
        "### 3.1. Численная Квадратура (`scipy.integrate.quad`)\n",
        "\n",
        "Функция `quad` предназначена для интегрирования функции одной переменной и является обёрткой над проверенной Fortran-библиотекой **QUADPACK**.\n",
        "\n",
        "**Алгоритмическая база.** `quad` использует методы адаптивной квадратуры, часто основанные на модифицированном методе Кленшоу–Кертиса. Ключевая особенность — адаптивное разбиение интервала: алгоритм итеративно делит интервал интегрирования, концентрируя вычислительные ресурсы (т.е. добавляя больше узлов) в тех областях, где подынтегральная функция имеет высокую вариацию или сингулярности. Этот механизм направлен на минимизацию локальной ошибки и достижение заданного допуска.\n",
        "\n",
        "**Практическое ограничение.** Несмотря на свою робастность, адаптивная квадратура может столкнуться с трудностями. Если функция содержит узкую, но важную особенность (например, острый пик или узкий гауссиан), а интегрирование выполняется на чрезвычайно широком конечном интервале, адаптивная процедура может «пропустить» эту область. В результате алгоритм может ложно оценить ошибку как низкую, давая неточный результат.\n",
        "\n",
        "**Пример: Интегрирование функции с узким пиком**\n",
        "\n",
        "```python\n",
        "from scipy.integrate import quad\n",
        "import numpy as np\n",
        "\n",
        "def narrow_peak(x):\n",
        "    return np.exp(-((x - 1000)**2) / (2 * 0.01**2))\n",
        "\n",
        "# Попытка \"грубого\" интегрирования на широком интервале\n",
        "I1, err1 = quad(narrow_peak, -10000, 10000)\n",
        "print(f\"Широкий интервал: I = {I1:.3e}, оценка ошибки = {err1:.3e}\")\n",
        "\n",
        "# Интегрирование в окрестности пика\n",
        "I2, err2 = quad(narrow_peak, 999, 1001)\n",
        "print(f\"Узкий интервал: I = {I2:.3e}, оценка ошибки = {err2:.3e}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* В первом случае `quad` \"не замечает\" пика и возвращает почти нулевой результат с завышенной уверенностью. Во втором случае мы явно указываем интересующую область — и получаем корректное значение.\n",
        "\n",
        "### 3.2. Решение Задач с Начальными Значениями ОДУ (`scipy.integrate.solve_ivp`)\n",
        "\n",
        "`solve_ivp` — современный и рекомендуемый интерфейс для решения систем ОДУ с начальными условиями:  \n",
        "\\[\n",
        "\\dot{\\mathbf{y}} = \\mathbf{f}(t, \\mathbf{y}), \\quad \\mathbf{y}(t_0) = \\mathbf{y}_0\n",
        "\\]\n",
        "\n",
        "#### Явные методы (Explicit RK)\n",
        "\n",
        "По умолчанию используется метод `'RK45'` — явный метод Рунге–Кутты 5(4)-го порядка. Он быстр и эффективен для **нежёстких** систем. Однако явные методы имеют ограниченную область устойчивости и требуют очень малого временного шага \\(h\\), если система жёсткая.\n",
        "\n",
        "#### Проблема жёсткости (Stiffness)\n",
        "\n",
        "Жёсткость возникает в системах ОДУ, где присутствуют процессы с сильно различающимися временными масштабами (например, в химической кинетике или реакциях горения). При применении явных методов к жёстким системам шаг интегрирования вынужденно уменьшается до значения, определяемого самым быстрым (часто несущественным) процессом, что делает вычисления неэффективными.\n",
        "\n",
        "#### Неявные методы (Implicit Solvers)\n",
        "\n",
        "Для жёстких систем необходимо использовать неявные методы, такие как `'BDF'` (формулы обратного дифференцирования) или `'Radau'`. Эти методы обладают свойством **A-устойчивости**, что позволяет им использовать большие шаги, оставаясь стабильными даже при больших отрицательных собственных значениях якобиана.\n",
        "\n",
        "**Пример: Сравнение решателей на жёсткой системе**\n",
        "\n",
        "```python\n",
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def stiff_system(t, y):\n",
        "    return [-1000 * (y[0] - np.sin(t)) + np.cos(t)]\n",
        "\n",
        "y0 = [0.0]\n",
        "t_span = (0, 1)\n",
        "\n",
        "# Используем BDF для жёсткой системы\n",
        "sol_bdf = solve_ivp(stiff_system, t_span, y0, method='BDF', rtol=1e-6)\n",
        "\n",
        "# Попробуем RK45 (он будет крайне медленным или не сойдётся)\n",
        "try:\n",
        "    sol_rk = solve_ivp(stiff_system, t_span, y0, method='RK45', rtol=1e-6, max_step=1e-4)\n",
        "    print(\"RK45 завершился успешно.\")\n",
        "except Exception as e:\n",
        "    print(\"RK45 не справился:\", e)\n",
        "\n",
        "plt.plot(sol_bdf.t, sol_bdf.y[0], 'b-', label='BDF (жёсткий)')\n",
        "plt.xlabel('t'); plt.ylabel('y(t)'); plt.legend(); plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Явный метод `'RK45'` либо не сходится, либо требует тысяч шагов, тогда как `'BDF'` даёт точное решение за несколько десятков шагов.\n",
        "\n",
        "#### Контроль допусков\n",
        "\n",
        "`solve_ivp` использует адаптивное управление шагом на основе относительных (`rtol`) и абсолютных (`atol`) допусков. Значение `rtol=1e-3` по умолчанию часто недостаточно для научных расчётов. Для высокоточных задач рекомендуется устанавливать `rtol=1e-6` и `atol=1e-9` или даже строже.\n",
        "\n",
        "**Таблица 3.1: Выбор метода для решения ОДУ (`solve_ivp`)**\n",
        "\n",
        "| Метод | Класс | Устойчивость | Применение | Компромисс |\n",
        "|-------|-------|--------------|------------|------------|\n",
        "| `'RK45'` | Явный РК 5(4) | Ограниченная | Нежёсткие системы | Быстрый шаг, но нестабилен при жёсткости |\n",
        "| `'BDF'` | Неявный | A-устойчивость | Жёсткие системы | Более дорогой шаг, но устойчив при больших \\(h\\) |\n",
        "| `'LSODA'` | Гибридный (Адамс/BDF) | Автоматическое переключение | Универсальный выбор | Надёжность, но менее гибкий интерфейс |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Численная Оптимизация (`scipy.optimize`)\n",
        "\n",
        "Модуль `scipy.optimize` предоставляет инструменты для минимизации скалярных функций, нахождения корней уравнений и подгонки кривых.\n",
        "\n",
        "### 4.1. Локальная и Глобальная Оптимизация\n",
        "\n",
        "Основная функция `minimize` предоставляет единый интерфейс для локальной минимизации многомерных скалярных функций.\n",
        "\n",
        "#### Локальные методы\n",
        "\n",
        "1. **BFGS** — квази-Ньютоновский градиентный метод. Использует информацию о градиенте (аналитическом или численном) и эффективно аппроксимирует обратную матрицу Гессе. Требует гладкости целевой функции.\n",
        "2. **Nelder–Mead** — бесградиентный симплекс-метод. Устойчив к шуму и разрывам, но сходится медленнее.\n",
        "\n",
        "#### Глобальная оптимизация\n",
        "\n",
        "Методы глобальной оптимизации, такие как **Differential Evolution** (`differential_evolution`), предназначены для поиска глобального минимума в мультимодальных ландшафтах. Это стохастический алгоритм, не требующий градиента, который эволюционирует популяцию кандидатов.\n",
        "\n",
        "**Компромисс.** Глобальные методы надёжнее, но требуют на порядки больше вычислений. Эффективная стратегия — сначала выполнить грубый глобальный поиск, затем уточнить результат локальным методом.\n",
        "\n",
        "**Пример: Комбинированный подход**\n",
        "\n",
        "```python\n",
        "from scipy.optimize import differential_evolution, minimize\n",
        "import numpy as np\n",
        "\n",
        "def multimodal_func(x):\n",
        "    return np.sin(x[0]) * np.cos(x[1]) + 0.1 * (x[0]**2 + x[1]**2)\n",
        "\n",
        "bounds = [(-5, 5), (-5, 5)]\n",
        "\n",
        "# Шаг 1: глобальный поиск\n",
        "result_de = differential_evolution(multimodal_func, bounds, seed=42)\n",
        "print(\"Глобальный минимум (DE):\", result_de.x)\n",
        "\n",
        "# Шаг 2: локальное уточнение\n",
        "result_local = minimize(multimodal_func, result_de.x, method='BFGS')\n",
        "print(\"Локальный минимум (BFGS):\", result_local.x)\n",
        "print(\"Значение функции:\", result_local.fun)\n",
        "```\n",
        "\n",
        "*Пояснение:* Такой подход сочетает робастность глобального поиска с высокой скоростью сходимости градиентных методов.\n",
        "\n",
        "### 4.2. Нелинейный Метод Наименьших Квадратов — `curve_fit`\n",
        "\n",
        "Функция `curve_fit` подгоняет параметрическую модель \\(f(x; \\theta)\\) к экспериментальным данным, минимизируя сумму квадратов остатков.\n",
        "\n",
        "```python\n",
        "from scipy.optimize import curve_fit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def model(x, a, b, c):\n",
        "    return a * np.exp(-b * x) + c\n",
        "\n",
        "# Синтетические данные с шумом\n",
        "x_data = np.linspace(0, 4, 50)\n",
        "y_true = model(x_data, 2.5, 1.3, 0.5)\n",
        "y_data = y_true + 0.2 * np.random.normal(size=x_data.size)\n",
        "\n",
        "# Подгонка\n",
        "popt, pcov = curve_fit(model, x_data, y_data)\n",
        "perr = np.sqrt(np.diag(pcov))  # стандартные ошибки\n",
        "\n",
        "print(\"Оценённые параметры:\", popt)\n",
        "print(\"Стандартные ошибки:\", perr)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(x_data, y_data, label='Данные')\n",
        "plt.plot(x_data, model(x_data, *popt), 'r-', label='Подгонка')\n",
        "plt.legend(); plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* `pcov` — ковариационная матрица параметров. Её диагональные элементы позволяют оценить доверительные интервалы, что критично для научной интерпретации.\n",
        "\n",
        "### 4.3. Линейное Программирование (`linprog`)\n",
        "\n",
        "Функция `linprog` решает задачи линейного программирования: минимизацию линейной функции при линейных ограничениях.\n",
        "\n",
        "**Стандартная форма.** Задача должна быть приведена к виду:  \n",
        "\\[\n",
        "\\min c^T x \\quad \\text{при} \\quad A_{ub} x \\leq b_{ub},\\ A_{eq} x = b_{eq},\\ x \\geq 0\n",
        "\\]\n",
        "\n",
        "Максимизация \\(z\\) преобразуется в минимизацию \\(-z\\), неравенства \\(\\geq\\) — в \\(\\leq\\) умножением на \\(-1\\).\n",
        "\n",
        "**Пример: Транспортная задача**\n",
        "\n",
        "```python\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# Минимизация стоимости: c = [2, 3, 1, 4]\n",
        "c = [2, 3, 1, 4]\n",
        "\n",
        "# Ограничения по запасам и потребностям\n",
        "A_eq = [\n",
        "    [1, 1, 0, 0],  # Сумма из склада 1\n",
        "    [0, 0, 1, 1],  # Сумма из склада 2\n",
        "    [1, 0, 1, 0],  # Сумма на магазин A\n",
        "    [0, 1, 0, 1]   # Сумма на магазин B\n",
        "]\n",
        "b_eq = [10, 15, 12, 13]  # Запасы и спрос\n",
        "\n",
        "res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=(0, None), method='highs')\n",
        "print(\"Оптимальное решение:\", res.x)\n",
        "print(\"Минимальная стоимость:\", res.fun)\n",
        "```\n",
        "\n",
        "*Пояснение:* `linprog` с методом `'highs'` (новый, высокопроизводительный решатель) эффективно справляется даже со сложными задачами оптимизации ресурсов.\n"
      ],
      "metadata": {
        "id": "9bhVuNA7gTqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично! Ниже — полностью отредактированная, дополненная и стилистически выровненная версия разделов **5–10** вашего текста. Я:\n",
        "\n",
        "- Убрал все посторонние цифры и опечатки (вроде «точек данных.28» → «точек данных»);\n",
        "- Расширил недостаточно раскрытые идеи;\n",
        "- Добавил **практические примеры кода** с пояснениями до и после каждого;\n",
        "- Сохранил академический, но доступный тон, соответствующий первой части;\n",
        "- Убедился, что структура логична, а переходы между разделами плавные.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Интерполяция (`scipy.interpolate`): Избегая Фантомов\n",
        "\n",
        "Интерполяция — это процесс построения функции, которая проходит через заданный набор дискретных точек данных. Модуль `scipy.interpolate` предлагает широкий спектр методов, от простой линейной интерполяции до многомерных сплайнов, позволяя выбирать стратегию, адекватную характеру данных и требованиям к гладкости результата.\n",
        "\n",
        "### 5.1. Феномен Рунге и Необходимость Сплайнов\n",
        "\n",
        "При использовании глобальных полиномов высокой степени для интерполяции большого числа узлов возникает так называемый **феномен Рунге**. Он проявляется в виде неконтролируемых осцилляций, особенно вблизи границ интервала, что приводит к значительным отклонениям от истинного поведения функции, даже если полином точно проходит через все заданные точки.\n",
        "\n",
        "Этот эффект демонстрирует принципиальное ограничение глобальных аппроксимаций: добавление новых точек может ухудшить поведение интерполянта в уже хорошо описанных областях.\n",
        "\n",
        "**Сплайн-интерполяция** решает эту проблему, заменяя единый полином высокой степени на **кусочно-полиномиальные функции низкой степени** (обычно кубические). Узлы интерполяции (*knots*) служат точками соединения этих полиномиальных сегментов. При этом обеспечивается непрерывность не только самой функции, но и её первой и второй производных — так называемая \\(C^2\\)-гладкость.\n",
        "\n",
        "Такой подход даёт локальный контроль: изменение одной точки влияет только на соседние сегменты, а не на всю интерполяционную кривую. Это критически важно при работе с экспериментальными данными, где требуется не просто «провести кривую», а построить физически осмысленное представление процесса, допускающее последующее дифференцирование или интегрирование.\n",
        "\n",
        "**Пример: Феномен Рунге vs. Кубический сплайн**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Истинная функция и узлы интерполяции\n",
        "def runge(x):\n",
        "    return 1 / (1 + 25 * x**2)\n",
        "\n",
        "x_nodes = np.linspace(-1, 1, 11)\n",
        "y_nodes = runge(x_nodes)\n",
        "\n",
        "# Глобальный полином (через NumPy для демонстрации)\n",
        "coeffs = np.polyfit(x_nodes, y_nodes, deg=10)\n",
        "poly_interp = np.poly1d(coeffs)\n",
        "\n",
        "# Кубический сплайн\n",
        "spline = interp1d(x_nodes, y_nodes, kind='cubic')\n",
        "\n",
        "# Точки для построения графика\n",
        "x_plot = np.linspace(-1, 1, 400)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_plot, runge(x_plot), 'k--', label='Истинная функция')\n",
        "plt.plot(x_plot, poly_interp(x_plot), 'r-', label='Полином 10-й степени (Runge)')\n",
        "plt.plot(x_plot, spline(x_plot), 'b-', label='Кубический сплайн')\n",
        "plt.scatter(x_nodes, y_nodes, c='k', zorder=5)\n",
        "plt.legend(); plt.grid(); plt.title('Феномен Рунге и сплайн-интерполяция')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Глобальный полином демонстрирует сильные осцилляции у краёв интервала, в то время как кубический сплайн остаётся близким к истинной функции на всём отрезке.\n",
        "\n",
        "*После выполнения:* Этот пример иллюстрирует, почему в научной практике предпочтение отдаётся сплайнам: они обеспечивают устойчивость, гладкость и естественность формы, не внося артефактов, отсутствующих в исходных данных.\n",
        "\n",
        "### 5.2. Инструменты SciPy на Базе FITPACK\n",
        "\n",
        "Многие высококачественные сплайн-интерполяторы в SciPy основаны на обёртках над старой, но надёжной Fortran-библиотекой **FITPACK**, разработанной Полом Дирксеном.\n",
        "\n",
        "- **1D-интерполяция** (`interp1d`) — удобный класс для быстрого создания интерполирующей функции. Поддерживает методы `'linear'`, `'nearest'`, `'cubic'` и `'quadratic'`.  \n",
        "- **Сплайны напрямую** — классы `UnivariateSpline`, `InterpolatedUnivariateSpline` и процедурные функции вроде `splrep`/`splev` дают более тонкий контроль: например, можно задать степень сглаживания при наличии шума.  \n",
        "- **Многомерная интерполяция** — для нерегулярных данных (точки в произвольных местах) используется `griddata`, основанная на триангуляции Делоне; для данных на регулярной сетке — `RegularGridInterpolator`, которая позволяет эффективно интерполировать в 2D, 3D и выше.\n",
        "\n",
        "Использование кубических сплайнов в науке — это не просто заполнение пробелов в данных; это **методический выбор**, направленный на построение физически осмысленного, гладкого представления процесса. Поскольку сплайны гарантируют непрерывность первой и второй производной, они критически важны для тех областей анализа, где последующее численное дифференцирование или интегрирование по интерполированной кривой должно быть точным и устойчивым.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Обработка Сигналов (`scipy.signal`)\n",
        "\n",
        "Модуль `scipy.signal` предоставляет инструменты для анализа частотного спектра, свёртки и, в особенности, для проектирования и применения цифровых фильтров — ключевых операций в обработке временных рядов, биомедицинских сигналов, астрофизики и многих других областях.\n",
        "\n",
        "### 6.1. FIR против IIR: Математика и Компромиссы\n",
        "\n",
        "Цифровые фильтры классифицируются по характеру их импульсной характеристики.\n",
        "\n",
        "- **FIR** (*Finite Impulse Response*) — фильтры с конечной импульсной характеристикой. Выход зависит только от текущих и прошлых входных значений. Импульсная характеристика обнуляется за конечное время.  \n",
        "  **Преимущество**: могут обеспечивать **строго линейную фазовую характеристику**, что означает одинаковую задержку для всех частотных компонент. Это критично, когда форма сигнала должна сохраняться (например, в нейрофизиологии или аудиообработке).\n",
        "\n",
        "- **IIR** (*Infinite Impulse Response*) — рекурсивные фильтры, где выход зависит как от входа, так и от предыдущих выходов.  \n",
        "  **Преимущество**: значительно более **вычислительно эффективны** — достигают той же частотной избирательности при гораздо меньшем порядке фильтра. Классические проекты (Баттерворта, Чебышева, Эллиптические) аппроксимируют идеальный «прямоугольный» частотный отклик.  \n",
        "  **Недостаток**: их фазовая характеристика, как правило, **нелинейна**, что приводит к искажению формы сигнала.\n",
        "\n",
        "**Пример: Сравнение БИХ и КИХ фильтров**\n",
        "\n",
        "```python\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fs = 1000  # частота дискретизации\n",
        "nyq = 0.5 * fs\n",
        "low = 50 / nyq\n",
        "high = 150 / nyq\n",
        "\n",
        "# IIR: фильтр Баттерворта 5-го порядка\n",
        "b_iir, a_iir = signal.butter(5, [low, high], btype='band')\n",
        "\n",
        "# FIR: фильтр с окном Кайзера (длина 101)\n",
        "b_fir = signal.firwin(101, [low, high], pass_zero=False, window='kaiser', beta=8.6)\n",
        "\n",
        "# АЧХ и ФЧХ\n",
        "w_iir, h_iir = signal.freqz(b_iir, a_iir, fs=fs)\n",
        "w_fir, h_fir = signal.freqz(b_fir, worN=len(w_iir), fs=fs)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(w_iir, 20 * np.log10(abs(h_iir)), 'r', label='IIR (Баттерворт)')\n",
        "plt.plot(w_fir, 20 * np.log10(abs(h_fir)), 'b', label='FIR (Кайзер)')\n",
        "plt.title('АЧХ'); plt.xlabel('Частота (Гц)'); plt.ylabel('Усиление (дБ)'); plt.grid(); plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(w_iir, np.unwrap(np.angle(h_iir)), 'r', label='IIR')\n",
        "plt.plot(w_fir, np.unwrap(np.angle(h_fir)), 'b', label='FIR')\n",
        "plt.title('ФЧХ'); plt.xlabel('Частота (Гц)'); plt.ylabel('Фаза (рад)'); plt.grid(); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* FIR-фильтр (синий) демонстрирует линейную ФЧХ (прямая линия), тогда как IIR (красный) — сильно искривлённую.\n",
        "\n",
        "### 6.2. Решение Проблемы Нелинейной Фазы\n",
        "\n",
        "В инженерных задачах, где важна неискажённая форма сигнала, но требуется высокая эффективность IIR-фильтров (например, при анализе больших массивов данных), используется функция **`scipy.signal.filtfilt`**.\n",
        "\n",
        "Эта функция предназначена для **офлайн-обработки**, когда весь сигнал доступен заранее. `filtfilt` применяет IIR-фильтр дважды: сначала в прямом направлении, затем — в обратном. В результате фазовые искажения компенсируются, и общий фазовый сдвиг становится **нулевым**.\n",
        "\n",
        "**Пример: Устранение фазового сдвига с помощью `filtfilt`**\n",
        "\n",
        "```python\n",
        "t = np.linspace(0, 1, 1000)\n",
        "x = np.sin(2 * np.pi * 10 * t) + np.sin(2 * np.pi * 20 * t)  # Два тона\n",
        "x_noisy = x + 0.5 * np.random.randn(len(t))\n",
        "\n",
        "# Применяем IIR-фильтр обычным способом\n",
        "x_filtered = signal.lfilter(b_iir, a_iir, x_noisy)\n",
        "\n",
        "# И с filtfilt\n",
        "x_filtfilt = signal.filtfilt(b_iir, a_iir, x_noisy)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(t[:200], x_noisy[:200], 'k:', alpha=0.5, label='Шумный сигнал')\n",
        "plt.plot(t[:200], x_filtered[:200], 'r-', label='lfilter (с фазовым сдвигом)')\n",
        "plt.plot(t[:200], x_filtfilt[:200], 'b-', label='filtfilt (нулевая фаза)')\n",
        "plt.legend(); plt.grid(); plt.title('Сравнение lfilter и filtfilt')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Обычный `lfilter` сдвигает пики сигнала во времени, в то время как `filtfilt` сохраняет их положение. Это делает IIR-фильтры **практически универсальными** для аналитических задач, где причинность не требуется.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Статистический Анализ (`scipy.stats`)\n",
        "\n",
        "Модуль `scipy.stats` предоставляет мощный и унифицированный интерфейс для работы с вероятностными распределениями и статистическими тестами.\n",
        "\n",
        "### 7.1. Модели Распределений\n",
        "\n",
        "SciPy включает более 100 непрерывных и 20 дискретных распределений. Все они имеют **единый API**, что упрощает сравнение и подбор моделей:\n",
        "\n",
        "- `.pdf()` / `.pmf()` — плотность вероятности / функция массы;\n",
        "- `.cdf()` — функция распределения;\n",
        "- `.ppf()` — обратная функция распределения (квантили);\n",
        "- `.rvs()` — генерация случайных чисел;\n",
        "- `.fit()` — оценка параметров методом максимального правдоподобия.\n",
        "\n",
        "**Пример: Подбор распределения к данным**\n",
        "\n",
        "```python\n",
        "from scipy.stats import norm, lognorm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Сгенерируем логнормальные данные\n",
        "np.random.seed(42)\n",
        "data = lognorm.rvs(s=0.5, scale=2, size=1000)\n",
        "\n",
        "# Оценим параметры нормального и логнормального распределений\n",
        "params_norm = norm.fit(data)\n",
        "params_lognorm = lognorm.fit(data, floc=0)  # фиксируем сдвиг\n",
        "\n",
        "# Визуализация\n",
        "x = np.linspace(data.min(), data.max(), 200)\n",
        "plt.hist(data, bins=50, density=True, alpha=0.6, label='Данные')\n",
        "plt.plot(x, norm.pdf(x, *params_norm), 'r-', label='Нормальное')\n",
        "plt.plot(x, lognorm.pdf(x, *params_lognorm), 'g-', label='Логнормальное')\n",
        "plt.legend(); plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Метод `.fit()` автоматически оценивает параметры, что позволяет быстро проверять гипотезы о природе данных.\n",
        "\n",
        "### 7.2. Статистические Гипотезы и Робастность\n",
        "\n",
        "Классические тесты (например, t-тест) предполагают **нормальность** и **равенство дисперсий**. При нарушении этих условий выводы становятся ненадёжными.\n",
        "\n",
        "SciPy предлагает **робастные альтернативы**:\n",
        "\n",
        "1. **Поправка Уэлча** — через `equal_var=False` в `ttest_ind`, когда дисперсии различны.\n",
        "2. **Триммированный t-тест** — через параметр `trim` в `ttest_ind`, который отбрасывает экстремальные наблюдения (например, `trim=0.1` удаляет 10% с каждого хвоста).\n",
        "\n",
        "**Пример: Робастный t-тест**\n",
        "\n",
        "```python\n",
        "from scipy.stats import ttest_ind\n",
        "import numpy as np\n",
        "\n",
        "# Две выборки с выбросами\n",
        "np.random.seed(0)\n",
        "a = np.random.normal(0, 1, 100)\n",
        "b = np.random.normal(0.5, 1, 100)\n",
        "a[0] = 100  # выброс\n",
        "\n",
        "# Обычный t-тест\n",
        "t1, p1 = ttest_ind(a, b)\n",
        "\n",
        "# Робастный (триммированный)\n",
        "t2, p2 = ttest_ind(a, b, trim=0.1)\n",
        "\n",
        "print(f\"Обычный t-тест: p = {p1:.3f}\")\n",
        "print(f\"Триммированный: p = {p2:.3f}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Обычный тест может не обнаружить различия из-за выброса, тогда как триммированный остаётся устойчивым.\n",
        "\n",
        "Эти инструменты позволяют принимать **прагматичные решения**, обеспечивая достоверность выводов даже при работе с реальными, зашумлёнными данными.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Специальные Функции и Разреженные Структуры\n",
        "\n",
        "### 8.1. Специальные Функции (`scipy.special`)\n",
        "\n",
        "Модуль `scipy.special` содержит сотни функций, возникающих в физике и инженерии: функции Бесселя, Гамма, интегралы ошибок, эллиптические интегралы и др.\n",
        "\n",
        "**Численная устойчивость.** При больших аргументах стандартные функции (например, `jv` — функция Бесселя первого рода) могут вызывать переполнение или потерю точности. Для этого SciPy предоставляет **масштабированные версии**, такие как `jve`, возвращающие \\(e^{-|z|} J_\\nu(z)\\), что предотвращает переполнение.\n",
        "\n",
        "**Пример: Устойчивое вычисление функции Бесселя**\n",
        "\n",
        "```python\n",
        "from scipy.special import jv, jve\n",
        "import numpy as np\n",
        "\n",
        "z = 1000\n",
        "print(\"jv(0, 1000):\", jv(0, z))        # может быть 0.0 из-за underflow\n",
        "print(\"jve(0, 1000):\", jve(0, z))      # масштабированное значение\n",
        "print(\"Восстановлено:\", jve(0, z) * np.exp(z))  # ≈ jv(0, z), но вычислено устойчиво\n",
        "```\n",
        "\n",
        "### 8.2. Разреженные Матрицы (`scipy.sparse`)\n",
        "\n",
        "Разреженные матрицы — стандарт при решении больших систем уравнений (например, в МКЭ). Хранение только ненулевых элементов экономит память и ускоряет вычисления.\n",
        "\n",
        "**Ключевые форматы:**\n",
        "\n",
        "- **CSR** (*Compressed Sparse Row*) — оптимален для операций по строкам: умножение, сложение, итерационные решатели.\n",
        "- **CSC** (*Compressed Sparse Column*) — оптимален для операций по столбцам: LU-разложение, собственные значения.\n",
        "\n",
        "Выбор формата — **архитектурное решение**, а не техническая деталь. Преобразование между форматами возможно, но требует времени.\n",
        "\n",
        "**Пример: Эффективное решение разреженной системы**\n",
        "\n",
        "```python\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import spsolve\n",
        "import numpy as np\n",
        "\n",
        "# Создаём разреженную матрицу (диагональная + немного шума)\n",
        "n = 10000\n",
        "diagonal = np.ones(n)\n",
        "off_diag = np.full(n-1, 0.01)\n",
        "data = np.concatenate([off_diag, diagonal, off_diag])\n",
        "offsets = [-1, 0, 1]\n",
        "A_sparse = csr_matrix((data, offsets, np.arange(n+1)), shape=(n, n))\n",
        "\n",
        "b = np.random.rand(n)\n",
        "x = spsolve(A_sparse, b)  # Эффективное решение\n",
        "print(\"Решение получено. Норма остатка:\", np.linalg.norm(A_sparse @ x - b))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Сквозной Инженерный Кейс-Стади: Моделирование Жёсткой Реакционной Системы и Оценка Параметров\n",
        "\n",
        "Комплексные научные задачи требуют интеграции нескольких модулей SciPy. Рассмотрим задачу из химической кинетики.\n",
        "\n",
        "### 9.1. Постановка Задачи\n",
        "\n",
        "Система ОДУ описывает концентрации трёх веществ с сильно различающимися временными масштабами (задача Робертсона). Неизвестна константа скорости \\(k\\). Цель — оценить её по зашумлённым измерениям.\n",
        "\n",
        "### 9.2. Фаза 1: Численная Симуляция (`scipy.integrate`)\n",
        "\n",
        "Используем `solve_ivp` с методом `'BDF'` и строгими допусками (`rtol=1e-6`), чтобы гарантировать точность.\n",
        "\n",
        "### 9.3. Фаза 2: Оценка Параметров (`scipy.optimize`)\n",
        "\n",
        "Определяем функцию-обёртку, которая вызывает `solve_ivp` с заданным \\(k\\), и передаём её в `curve_fit`.\n",
        "\n",
        "```python\n",
        "def robertson(t, y, k):\n",
        "    return np.array([\n",
        "        -0.04 * y[0] + 1e4 * y[1] * y[2],\n",
        "        0.04 * y[0] - 1e4 * y[1] * y[2] - k * y[1]**2,\n",
        "        k * y[1]**2\n",
        "    ])\n",
        "\n",
        "def model(t, k):\n",
        "    sol = solve_ivp(robertson, [0, t[-1]], [1, 0, 0], t_eval=t, args=(k,),\n",
        "                    method='BDF', rtol=1e-6, atol=1e-9)\n",
        "    return sol.y[1]  # возвращаем вторую компоненту\n",
        "\n",
        "# Синтетические данные\n",
        "t_data = np.logspace(-2, 6, 50)\n",
        "y_true = model(t_data, k=3e7)\n",
        "y_data = y_true + 0.01 * np.random.randn(len(t_data))\n",
        "\n",
        "# Подгонка\n",
        "k_opt, pcov = curve_fit(model, t_data, y_data, p0=[1e7])\n",
        "print(f\"Оценённая k = {k_opt[0]:.1e}\")\n",
        "```\n",
        "\n",
        "### 9.4. Фаза 3: Анализ и Визуализация\n",
        "\n",
        "- **Неопределённость**: `np.sqrt(np.diag(pcov))` даёт стандартную ошибку.\n",
        "- **Гладкая визуализация**: используем `interp1d` с `kind='cubic'` для построения публикационно-готового графика.\n",
        "\n",
        "\n",
        "## 8.3. Пространственные Структуры и Расстояния (`scipy.spatial`): Геометрия Данных\n",
        "\n",
        "В анализе данных часто требуется не просто обрабатывать числовые значения, но и **понимать взаимное расположение наблюдений в многомерном пространстве признаков**. Расстояния между точками лежат в основе кластеризации, поиска аномалий, рекомендательных систем, снижения размерности и даже оценки качества моделей. Модуль `scipy.spatial` предоставляет эффективные инструменты для работы с геометрией данных, от вычисления метрик до построения иерархических структур.\n",
        "\n",
        "### 8.3.1. Метрики Расстояний: За Пределами Евклида\n",
        "\n",
        "Хотя евклидово расстояние интуитивно понятно, в реальных задачах часто требуются **альтернативные метрики**, лучше отражающие природу данных:\n",
        "\n",
        "- **Манхэттенское расстояние** (\\(L_1\\)) — устойчиво к выбросам, полезно при разреженных данных (например, в NLP).\n",
        "- **Косинусное расстояние** — измеряет угловое сходство, игнорируя длину векторов; идеально для текстов, где важна не частота, а **тематическое направление**.\n",
        "- **Корреляционное расстояние** — основано на коэффициенте Пирсона; полезно при сравнении **временных паттернов** (например, схожесть динамики продаж у двух продуктов).\n",
        "\n",
        "Функции `pdist` (попарные расстояния внутри одного набора) и `cdist` (расстояния между двумя наборами) позволяют эффективно вычислять полные матрицы расстояний без явных циклов.\n",
        "\n",
        "**Пример: Сравнение метрик на текстоподобных данных**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Имитация разреженных TF-IDF векторов (3 документа, 5 признаков)\n",
        "X = np.array([\n",
        "    [0.8, 0.0, 0.2, 0.0, 0.0],  # документ A\n",
        "    [0.0, 0.7, 0.0, 0.3, 0.0],  # документ B\n",
        "    [0.6, 0.0, 0.3, 0.0, 0.1],  # документ A', похожий на A\n",
        "])\n",
        "\n",
        "# Вычисляем матрицы расстояний\n",
        "euclidean = squareform(pdist(X, metric='euclidean'))\n",
        "cosine = squareform(pdist(X, metric='cosine'))\n",
        "manhattan = squareform(pdist(X, metric='cityblock'))\n",
        "\n",
        "# Визуализация\n",
        "metrics = {'Евклидово': euclidean, 'Косинусное': cosine, 'Манхэттен': manhattan}\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
        "for ax, (name, mat) in zip(axes, metrics.items()):\n",
        "    im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)\n",
        "    ax.set_title(name)\n",
        "    ax.set_xticks([0, 1, 2]); ax.set_yticks([0, 1, 2])\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            ax.text(j, i, f\"{mat[i, j]:.2f}\", ha='center', va='center', color='white')\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Косинусное расстояние корректно показывает, что документы A и A' близки (угол мал), тогда как евклидово расстояние преувеличивает разницу из-за различий в «длине» векторов. Это демонстрирует, почему выбор метрики — **существенная часть проектирования признакового пространства**, а не техническая деталь.\n",
        "\n",
        "### 8.3.2. Эффективный Поиск Соседей: KD-деревья\n",
        "\n",
        "При работе с большими наборами данных (десятки или сотни тысяч наблюдений) вычисление полной матрицы расстояний становится **вычислительно неприемлемым** (\\(O(n^2)\\) по времени и памяти). В таких случаях применяются **пространственные индексы**, такие как **KD-дерево** (*k*-dimensional tree).\n",
        "\n",
        "Класс `cKDTree` (оптимизированная C-версия) позволяет находить *k* ближайших соседей или все точки в заданном радиусе за время, близкое к \\(O(\\log n)\\).\n",
        "\n",
        "**Пример: Быстрый поиск похожих клиентов в CRM**\n",
        "\n",
        "```python\n",
        "from scipy.spatial import cKDTree\n",
        "import numpy as np\n",
        "\n",
        "# Условные данные: 50 000 клиентов, 8 числовых признаков\n",
        "np.random.seed(42)\n",
        "customers = np.random.rand(50000, 8)\n",
        "\n",
        "# Строим индекс\n",
        "tree = cKDTree(customers)\n",
        "\n",
        "# Новый клиент (вектор признаков)\n",
        "new_client = np.random.rand(8)\n",
        "\n",
        "# Найти 10 самых похожих клиентов (по евклидову расстоянию)\n",
        "distances, indices = tree.query(new_client, k=10)\n",
        "\n",
        "print(f\"Индексы 10 ближайших клиентов: {indices}\")\n",
        "print(f\"Среднее расстояние: {np.mean(distances):.4f}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Такой подход лежит в основе **рекомендательных систем на основе сходства** (\"пользователям, похожим на вас, понравилось...\"), а также методов **аномалий**: если расстояние до ближайшего соседа аномально велико — объект может быть выбросом.\n",
        "\n",
        "### 8.3.3. Иерархическая Кластеризация: Когда Число Кластеров Неизвестно\n",
        "\n",
        "В отличие от методов вроде K-средних, **иерархическая кластеризация** не требует заранее задавать число кластеров. Она строит **дендрограмму** — древовидную структуру, в которой каждый уровень соответствует определённому порогу объединения кластеров.\n",
        "\n",
        "Процесс начинается с того, что каждая точка — отдельный кластер. Затем на каждом шаге объединяются два **наиболее близких** кластера (стратегия зависит от метода связывания: *single*, *complete*, *average*, *ward*).\n",
        "\n",
        "**Пример: Кластеризация клиентов с визуализацией дендрограммы**\n",
        "\n",
        "```python\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Генерируем данные: 20 точек в 2D (для наглядности)\n",
        "np.random.seed(0)\n",
        "X = np.vstack([\n",
        "    np.random.normal(0, 0.5, (7, 2)),\n",
        "    np.random.normal(3, 0.5, (6, 2)),\n",
        "    np.random.normal([0, 3], 0.5, (7, 2))\n",
        "])\n",
        "\n",
        "# Вычисляем попарные расстояния и строим иерархию (метод Уорда)\n",
        "Z = linkage(X, method='ward')\n",
        "\n",
        "# Строим дендрограмму\n",
        "plt.figure(figsize=(10, 4))\n",
        "dendrogram(Z, color_threshold=4)\n",
        "plt.title('Дендрограмма: иерархическая кластеризация')\n",
        "plt.xlabel('Индекс наблюдения'); plt.ylabel('Расстояние')\n",
        "plt.axhline(y=4, color='r', linestyle='--', label='Порог разреза')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Формируем 3 кластера\n",
        "labels = fcluster(Z, t=4, criterion='distance')\n",
        "print(\"Метки кластеров:\", labels)\n",
        "```\n",
        "\n",
        "*Пояснение:* Дендрограмма позволяет **визуально выбрать оптимальное число кластеров**, анализируя «скачки» в высоте слияния. Метод Уорда (`'ward'`) минимизирует внутрикластерную дисперсию и часто даёт компактные, сферические кластеры — что соответствует интуиции аналитика.\n",
        "\n",
        "*После выполнения:* Такой подход особенно ценен на этапе разведочного анализа данных (EDA), когда исследователь ещё не знает структуры данных, но хочет понять, существуют ли естественные группы.\n",
        "\n",
        "---\n",
        "\n",
        "### Значение для Анализа Данных\n",
        "\n",
        "Модуль `scipy.spatial` превращает абстрактные векторы признаков в **геометрические объекты**, для которых применимы интуитивные понятия «близости», «группировки» и «изоляции». Это не просто вспомогательный инструмент — это **основа геометрического взгляда на данные**, который лежит в сердце большинства методов машинного обучения. Понимание того, как вычисляются расстояния, как строятся индексы и как интерпретируются дендрограммы, позволяет аналитику:\n",
        "\n",
        "- осознанно выбирать метрики сходства;\n",
        "- эффективно работать с большими наборами данных;\n",
        "- визуально обосновывать гипотезы о структуре данных;\n",
        "- строить надёжные системы рекомендаций и обнаружения аномалий.\n",
        "\n",
        "Таким образом, `scipy.spatial` — неотъемлемая часть арсенала современного специалиста по анализу данных.\n",
        "\n",
        "---\n",
        "## 10. Заключение\n",
        "\n",
        "SciPy — это не просто набор функций, а **система численного мышления**, построенная на десятилетиях развития вычислительной математики. Его архитектура позволяет исследователю сосредоточиться на научной сути задачи, не теряя контроля над численной устойчивостью.\n",
        "\n",
        "Ключевые принципы экспертного применения SciPy:\n",
        "\n",
        "1. **Осознанный выбор алгоритма**: не «что работает», а «что стабильно и уместно» — будь то `'BDF'` вместо `'RK45'` для жёстких систем или `'differential_evolution'` для мультимодальных ландшафтов.\n",
        "2. **Контроль точности**: ужесточение `rtol`/`atol`, диагностика обусловленности через SVD, оценка ошибок параметров.\n",
        "3. **Использование архитектурных возможностей**: `filtfilt` для нулевой фазы, CSR/CSC для разреженных систем, масштабированные специальные функции.\n",
        "\n",
        "Сквозные задачи, подобные оценке кинетических параметров, демонстрируют **иерархическую природу численной ошибки**: неточность на уровне ОДУ-решателя разрушает всю последующую оптимизацию. Поэтому мастерство в SciPy — это не только знание API, но и понимание **компромиссов между скоростью, точностью и устойчивостью**.\n",
        ""
      ],
      "metadata": {
        "id": "ykgdYP5vgVKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 12: Scikit-learn — Основы машинного обучения\n",
        "\n",
        "Scikit-learn (sklearn) является краеугольным камнем экосистемы Python для машинного обучения, предоставляя унифицированный, последовательный и методологически строгий интерфейс для реализации классических алгоритмов. Библиотека отличается высококачественными, протестированными реализациями, охватывающими **полный цикл разработки модели**: от предобработки данных и отбора признаков до обучения, оценки, настройки гиперпараметров и развёртывания. Главное преимущество sklearn — не в обилии моделей, а в **строгом соблюдении стандартизированного API**, который делает рабочие процессы модульными, воспроизводимыми и защищёнными от типичных методологических ошибок, таких как утечка данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 1. Фундаментальная Философия Scikit-learn и Единый API\n",
        "\n",
        "### 1.1. Роль и архитектура Scikit-learn\n",
        "\n",
        "В основе архитектуры Scikit-learn лежит базовый класс `BaseEstimator`, от которого наследуются **все** объекты библиотеки — модели, преобразователи, мета-оценщики. Это архитектурное решение обеспечивает единый синтаксис для всех шагов машинного обучения: разработчик может заменить логистическую регрессию на метод опорных векторов, или `StandardScaler` на `QuantileTransformer`, **не меняя структуру основного кода**. Такая унификация превращает машинное обучение из набора разрозненных скриптов в **инженерную дисциплину с воспроизводимыми пайплайнами**.\n",
        "\n",
        "### 1.2. Концепции Estimator и Transformer\n",
        "\n",
        "В Scikit-learn все объекты делятся на две категории в зависимости от их назначения:\n",
        "\n",
        "- **Estimator (Оценщик)** — обучается на данных и делает прогнозы.  \n",
        "  Реализует:  \n",
        "  - `.fit(X, y)` — обучение на признаках `X` и целевой переменной `y` (для регрессии/классификации);  \n",
        "  - `.predict(X)` — генерация прогнозов для новых данных.\n",
        "\n",
        "- **Transformer (Преобразователь)** — модифицирует данные, не используя целевую переменную.  \n",
        "  Реализует:  \n",
        "  - `.fit(X)` — вычисление параметров преобразования (например, среднее и std для стандартизации);  \n",
        "  - `.transform(X)` — применение этих параметров к данным.\n",
        "\n",
        "Некоторые объекты (например, `PCA`) являются **одновременно и Transformer, и Estimator**, так как их можно встраивать в пайплайны и использовать для преобразования, а также оценивать качество через кросс-валидацию.\n",
        "\n",
        "### 1.3. Универсальные методы: `fit()`, `transform()`, `predict()` и их методологическое значение\n",
        "\n",
        "Фундаментальное разделение обязанностей в Scikit-learn отражается в трёх ключевых методах:\n",
        "\n",
        "- `.fit(X, y)` — **единственный** этап, где модель «видит» данные. Здесь она запоминает параметры: веса, пороги, статистики.\n",
        "- `.transform(X)` — применяет **уже выученные** параметры к новым данным.\n",
        "- `.fit_transform(X)` — сокращение для `.fit(X).transform(X)`, **используется только на обучающей выборке**.\n",
        "\n",
        "#### Почему это критически важно?\n",
        "\n",
        "Разделение `fit` и `transform` — не техническое удобство, а **гарантия статистической валидности**. Нарушение этого принципа приводит к **утечке данных** (*data leakage*) — ситуации, когда информация из тестового набора неявно попадает в обучение, что делает оценку производительности **нечестной и оптимистичной**.\n",
        "\n",
        "**Пример: Утечка данных при неправильном масштабировании**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Синтетические данные с сильным разбросом\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(1000, 2)\n",
        "X[:, 0] *= 1000  # первый признак в 1000 раз больше\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
        "\n",
        "# Разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ❌ НЕПРАВИЛЬНО: масштабируем до разделения\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # ИСПОЛЬЗУЕТ ВСЕ ДАННЫЕ!\n",
        "X_train_bad, X_test_bad, _, _ = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ✅ ПРАВИЛЬНО: масштабируем только после разделения\n",
        "scaler = StandardScaler()\n",
        "X_train_good = scaler.fit_transform(X_train)\n",
        "X_test_good = scaler.transform(X_test)  # только transform!\n",
        "\n",
        "# Обучение и оценка\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_bad, y_train)\n",
        "acc_bad = accuracy_score(y_test, model.predict(X_test_bad))\n",
        "\n",
        "model.fit(X_train_good, y_train)\n",
        "acc_good = accuracy_score(y_test, model.predict(X_test_good))\n",
        "\n",
        "print(f\"С утечкой: {acc_bad:.4f}\")\n",
        "print(f\"Без утечки: {acc_good:.4f}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* При утечке модель «знает» статистику тестового набора (например, что первый признак имеет разброс ~1000), и использует это для лучшего масштабирования. В реальности такой информации нет, и производительность падает.\n",
        "\n",
        "*После выполнения:* Разница может быть небольшой на синтетических данных, но **в реальных проектах утечка часто приводит к катастрофическому провалу в продакшене**.\n",
        "\n",
        "Эта строгость лежит в основе **Pipeline** — механизма, объединяющего предобработку и модель в единый объект, который ведёт себя как обычный Estimator:\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "pipe.score(X_test, y_test)  # Масштабирование и прогнозирование — в одном вызове\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 2. Подготовка Данных и Воспроизводимость\n",
        "\n",
        "### 2.1. Базовое деление данных и воспроизводимость\n",
        "\n",
        "Функция `train_test_split` — первый шаг в любом проекте. Ключевые параметры:\n",
        "\n",
        "- `random_state` — **обязателен** для воспроизводимости;\n",
        "- `stratify=y` — сохраняет пропорции классов в обеих выборках (критично при несбалансированных данных).\n",
        "\n",
        "**Пример: Стратификация при несбалансированных классах**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1,\n",
        "                           weights=[0.95, 0.05], random_state=42)\n",
        "\n",
        "print(\"Исходное распределение:\", Counter(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Обучающая выборка:\", Counter(y_train))\n",
        "print(\"Тестовая выборка:\", Counter(y_test))\n",
        "```\n",
        "\n",
        "*Пояснение:* Без `stratify=y` есть риск, что в тестовом наборе не окажется ни одного представителя редкого класса — модель будет невозможно оценить.\n",
        "\n",
        "### 2.2. Продвинутые стратегии валидации: Кросс-Валидация\n",
        "\n",
        "#### `StratifiedKFold` — стандарт для задач классификации\n",
        "\n",
        "Гарантирует, что **во всех фолдах сохраняется пропорция классов**. Это особенно важно при малом числе наблюдений или сильной несбалансированности.\n",
        "\n",
        "#### `TimeSeriesSplit` — единственно корректный выбор для временных рядов\n",
        "\n",
        "Нарушение временного порядка — одна из самых частых ошибок начинающих. `TimeSeriesSplit` строит сплиты так, что **тест всегда идёт после обучения**, имитируя реальный сценарий прогнозирования.\n",
        "\n",
        "**Пример: Визуализация сплитов**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=4)\n",
        "X = np.arange(20).reshape(-1, 1)\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
        "    plt.scatter(train_index, [i]*len(train_index), c='b', label='Train' if i==0 else \"\")\n",
        "    plt.scatter(test_index, [i]*len(test_index), c='r', label='Test' if i==0 else \"\")\n",
        "plt.yticks(range(4), [f\"Split {i+1}\" for i in range(4)])\n",
        "plt.xlabel(\"Индекс наблюдения\"); plt.legend(); plt.title(\"TimeSeriesSplit\")\n",
        "plt.grid(True, axis='x', alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### `ShuffleSplit` — гибкость для диагностики\n",
        "\n",
        "Полезен при построении **кривых обучения**, когда нужно оценить, как производительность зависит от размера обучающего набора.\n",
        "\n",
        "**Таблица 2: Сравнение стратегий кросс-валидации**\n",
        "\n",
        "| Стратегия | Когда использовать | Особенности |\n",
        "|----------|--------------------|-------------|\n",
        "| `KFold` | Регрессия, сбалансированная классификация | Простое разбиение на K частей |\n",
        "| `StratifiedKFold` | Классификация (особенно несбалансированная) | Сохраняет пропорции классов в каждом фолде |\n",
        "| `TimeSeriesSplit` | Временные ряды, последовательные данные | Тест всегда после обучения; нет перемешивания |\n",
        "| `ShuffleSplit` | Диагностика, кривые обучения | Гибкое число сплитов, независимые разбиения |\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 3. Преобразование Признаков I: Масштабирование и Импутация\n",
        "\n",
        "### 3.1. Обработка пропущенных значений\n",
        "\n",
        "`SimpleImputer` — стандартный инструмент. Ключевые рекомендации:\n",
        "\n",
        "- Используйте `strategy='median'` для признаков с выбросами;\n",
        "- Всегда применяйте `fit` только на `X_train`;\n",
        "- Для production-моделей установите `keep_empty_features=True`, чтобы избежать сбоев при полном отсутствии данных по признаку.\n",
        "\n",
        "### 3.2. Стандартизация и нормализация\n",
        "\n",
        "Выбор метода масштабирования зависит от модели:\n",
        "\n",
        "| Метод | Формула | Когда использовать |\n",
        "|------|--------|--------------------|\n",
        "| `StandardScaler` | \\( z = \\frac{x - \\mu}{\\sigma} \\) | SVM, линейные модели, KNN, нейросети |\n",
        "| `MinMaxScaler` | \\( x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\) | Нейросети с сигмоидой, когда нужен диапазон [0,1] |\n",
        "| `MaxAbsScaler` | \\( x' = \\frac{x}{\\max|x|} \\) | Разреженные данные, центрированные признаки |\n",
        "| `QuantileTransformer` | Нелинейное преобразование к равномерному/нормальному распределению | Признаки с выбросами, нелинейные зависимости |\n",
        "\n",
        "**Важно**: модели на основе деревьев (`RandomForest`, `XGBoost`) **не требуют масштабирования** — их можно исключить из пайплайна для ускорения.\n",
        "\n",
        "### 3.3. Пример: Корректное масштабирование (уже включён в основной текст выше)\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 4. Преобразование Признаков II: Кодирование и Извлечение\n",
        "\n",
        "### 4.1. Кодирование категориальных данных\n",
        "\n",
        "`OneHotEncoder` — основной инструмент, но требует внимания к деталям:\n",
        "\n",
        "- `handle_unknown='ignore'` — **обязателен** для production, чтобы модель не падала при новых категориях;\n",
        "- `drop='first'` — предотвращает мультиколлинеарность в линейных моделях.\n",
        "\n",
        "**Альтернативы при высокой кардинальности**:\n",
        "- `OrdinalEncoder` + `embedding` (в нейросетях);\n",
        "- `TargetEncoder` (из `category_encoders` или sklearn ≥1.3);\n",
        "- Кластеризация категорий по целевой переменной.\n",
        "\n",
        "### 4.2. Введение в работу с текстом\n",
        "\n",
        "`TfidfVectorizer` — стандарт для начальной векторизации текста. Он:\n",
        "\n",
        "- Автоматически удаляет стоп-слова (`stop_words='english'`);\n",
        "- Возвращает **разреженную матрицу** (экономит память);\n",
        "- Интегрируется в Pipeline как обычный Transformer.\n",
        "\n",
        "**Пример: Полный пайплайн для текстовой классификации**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Данные\n",
        "texts = [\"I love this movie\", \"This is terrible\", \"Great film!\", \"Worst ever\"]\n",
        "labels = [1, 0, 1, 0]\n",
        "\n",
        "# Пайплайн\n",
        "text_pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "text_pipe.fit(texts, labels)\n",
        "print(\"Прогноз для нового текста:\", text_pipe.predict([\"Amazing!\"]))\n",
        "```\n",
        "\n",
        "*Пояснение:* Весь процесс — от сырых текстов до прогноза — управляется единым объектом. Это позволяет легко настраивать гиперпараметры (`tfidf__max_features`, `clf__C`) через `GridSearchCV`.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ghx95cNtibBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Раздел 5. Сквозной Пайплайн (`Pipeline`) и Работа с Разнородными Данными\n",
        "\n",
        "Создание сквозных рабочих процессов в машинном обучении — это не просто последовательность шагов, а **единая, инкапсулированная система**, защищённая от методологических ошибок. Объекты `Pipeline` и `ColumnTransformer` являются архитектурными краеугольными камнями Scikit-learn, которые обеспечивают эту инкапсуляцию, гарантируя **воспроизводимость**, **масштабируемость** и **защиту от утечки данных**.\n",
        "\n",
        "### 5.1. Преимущества `Pipeline`\n",
        "\n",
        "`Pipeline` последовательно применяет список преобразователей и завершает процесс финальным оценщиком. Его ключевые преимущества:\n",
        "\n",
        "1. **Предотвращение утечки данных**  \n",
        "   Пайплайн гарантирует, что в ходе кросс-валидации **на каждом фолде** преобразования обучаются **только на данных этого фолда**, а затем применяются к его тестовой части. Это исключает даже теоретическую возможность утечки.\n",
        "\n",
        "2. **Инкапсуляция и модульность**  \n",
        "   Весь рабочий процесс становится единым `Estimator`, который можно передавать в `GridSearchCV`, сохранять через `joblib`, или развёртывать в production, не заботясь о порядке операций.\n",
        "\n",
        "3. **Упрощение и читаемость кода**  \n",
        "   Вместо десятков строк ручной предобработки — один объект, чья структура отражает логику проекта.\n",
        "\n",
        "### 5.2. `ColumnTransformer`: Работа с разнородными данными\n",
        "\n",
        "В реальных задачах данные редко бывают однородными: числовые, категориальные и текстовые признаки сосуществуют в одном датафрейме. Для каждого типа требуются свои методы предобработки.\n",
        "\n",
        "`ColumnTransformer` позволяет **применять разные преобразования к разным столбцам**, объединяя результаты в единую числовую матрицу.\n",
        "\n",
        "Структура: список кортежей вида  \n",
        "```python\n",
        "(name, transformer, columns)\n",
        "```\n",
        "\n",
        "- `name` — метка для отладки;\n",
        "- `transformer` — сам объект преобразования (может быть `Pipeline`);\n",
        "- `columns` — список имён, индексов или селекторов столбцов.\n",
        "\n",
        "### 5.3. Практический кейс: Полный пайплайн для смешанных данных\n",
        "\n",
        "Рассмотрим реалистичный сценарий: датасет содержит числовые признаки с пропусками и категориальные признаки с высокой кардинальностью. Нам нужно построить классификатор, защищённый от переобучения и утечек.\n",
        "\n",
        "**Пример: Создание robust-пайплайна**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Генерация синтетического датасета\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, np.nan, 35, 45, 55, np.nan],\n",
        "    'income': [50000, 60000, np.nan, 80000, 90000, 70000],\n",
        "    'city': ['A', 'B', 'A', 'C', 'B', 'A'],\n",
        "    'education': ['BSc', 'MSc', 'PhD', 'BSc', 'MSc', 'BSc']\n",
        "})\n",
        "y = np.array([0, 1, 1, 1, 0, 0])\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, y, test_size=0.5, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Определение селекторов столбцов\n",
        "numerical_cols = make_column_selector(dtype_exclude=['category', 'object'])\n",
        "categorical_cols = make_column_selector(dtype_include=['category', 'object'])\n",
        "\n",
        "# Числовой пайплайн: импутация → масштабирование\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Категориальный пайплайн: OHE → отбор признаков\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first')),\n",
        "    ('selector', SelectPercentile(score_func=chi2, percentile=80))\n",
        "])\n",
        "\n",
        "# Объединение в ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, numerical_cols),\n",
        "    ('cat', categorical_pipeline, categorical_cols)\n",
        "])\n",
        "\n",
        "# Финальный пайплайн: предобработка + модель\n",
        "clf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(C=1.0, max_iter=200))\n",
        "])\n",
        "\n",
        "# Обучение и оценка\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Отчёт классификации:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "*Пояснение до выполнения:*  \n",
        "Этот пайплайн автоматически:\n",
        "- заполняет пропуски в числовых признаках медианой **только по тренировочным данным**;\n",
        "- кодирует категории, игнорируя неизвестные в тесте;\n",
        "- отбирает 80% наиболее значимых бинарных признаков;\n",
        "- обучает логистическую регрессию на объединённом пространстве признаков.\n",
        "\n",
        "*После выполнения:*  \n",
        "Такой подход **полностью исключает утечку данных**, даже при наличии пропусков и новых категорий в тесте. Более того, весь пайплайн можно передать в `GridSearchCV`, чтобы совместно настраивать `classifier__C`, `cat__selector__percentile` и даже `num__imputer__strategy`.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 6. Выбор Эстиматоров I: Регрессионные Модели\n",
        "\n",
        "Линейные модели — основа регрессионного анализа. Их главные достоинства: **интерпретируемость**, **скорость** и **прозрачность**. Однако при большом числе признаков или мультиколлинеарности требуется регуляризация.\n",
        "\n",
        "### 6.1. Линейная регрессия (`LinearRegression`)\n",
        "\n",
        "Реализует метод наименьших квадратов (OLS). Минимизирует сумму квадратов остатков. Коэффициенты показывают **маргинальный вклад** каждого признака.\n",
        "\n",
        "**Ограничения**:  \n",
        "- Чувствительна к выбросам;  \n",
        "- При мультиколлинеарности коэффициенты становятся нестабильными;  \n",
        "- При \\(p > n\\) (признаков больше, чем наблюдений) — не решается.\n",
        "\n",
        "### 6.2. Регуляризация L2 (Ridge)\n",
        "\n",
        "Добавляет к функции потерь штраф \\(\\alpha \\sum w_i^2\\).  \n",
        "- **Не обнуляет** коэффициенты, но **стягивает их к нулю**;  \n",
        "- Стабилизирует решение при мультиколлинеарности;  \n",
        "- Рекомендуется, когда **все признаки потенциально полезны**, но нужно снизить дисперсию.\n",
        "\n",
        "### 6.3. Регуляризация L1 (Lasso)\n",
        "\n",
        "Добавляет штраф \\(\\alpha \\sum |w_i|\\).  \n",
        "- **Обнуляет** коэффициенты малозначимых признаков → **автоматический отбор признаков**;  \n",
        "- Создаёт **разреженную модель**, что повышает интерпретируемость;  \n",
        "- Лучше работает, когда **только небольшое подмножество признаков релевантно**.\n",
        "\n",
        "**Пример: Разреженность Lasso**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Создаём данные: 50 наблюдений, 20 признаков, но только 3 релевантны\n",
        "X, y = make_regression(n_samples=50, n_features=20, n_informative=3, noise=10, random_state=42)\n",
        "\n",
        "# Обучаем Lasso\n",
        "lasso = Lasso(alpha=10.0)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Подсчитываем ненулевые коэффициенты\n",
        "non_zero = np.sum(lasso.coef_ != 0)\n",
        "print(f\"Ненулевых коэффициентов: {non_zero} из 20\")\n",
        "print(\"Истинные релевантные признаки: 0, 1, 2\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Lasso корректно отобрал 3–4 признака, игнорируя остальные 16–17 шумовых переменных. Это демонстрирует его силу как **встроенного метода отбора признаков**.\n",
        "\n",
        "> **Выбор между Ridge и Lasso** — это выбор между **стабильностью** и **интерпретируемостью**. Для бизнес-аналитики, где важно объяснить модель стейкхолдеру, Lasso часто предпочтительнее.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 7. Выбор Эстиматоров II: Классификация и Ансамбли\n",
        "\n",
        "### 7.1. Модели ближайших соседей (`KNeighborsClassifier`)\n",
        "\n",
        "**Принцип**: классифицирует объект по большинству среди *K* ближайших соседей.\n",
        "\n",
        "**Особенности**:  \n",
        "- **Ленивое обучение**: модель = обучающий набор;  \n",
        "- **Критически зависит от масштабирования** — без `StandardScaler` результаты бессмысленны;  \n",
        "- **Чувствителен к размерности** — «проклятие размерности» делает все точки почти равноудалёнными в высоких пространствах.\n",
        "\n",
        "**Когда использовать**:  \n",
        "- Малые наборы данных;  \n",
        "- Когда важна локальная структура;  \n",
        "- Как бейзлайн.\n",
        "\n",
        "### 7.2. Ансамблевые методы\n",
        "\n",
        "#### Случайный лес (`RandomForestClassifier`)\n",
        "\n",
        "- **Бэггинг**: обучение на бутстрэп-выборках + случайный отбор признаков на каждом узле;  \n",
        "- **Снижает дисперсию**, устойчив к выбросам и пропускам;  \n",
        "- Возвращает `feature_importances_` — полезно для EDA;  \n",
        "- **Не требует масштабирования**.\n",
        "\n",
        "#### Градиентный бустинг (`HistGradientBoostingClassifier`)\n",
        "\n",
        "- **Последовательное обучение**: каждое дерево исправляет ошибки предыдущих;  \n",
        "- **Снижает смещение**, достигает высокой точности;  \n",
        "- **`HistGradientBoosting`** — оптимизированная версия:  \n",
        "  - Работает на бинах (гистограммах), а не на сырых значениях → **в 10–100× быстрее**;  \n",
        "  - **Встроенная поддержка пропущенных значений** → можно исключить импутацию из пайплайна;  \n",
        "  - Рекомендуется Scikit-learn для наборов > 10 000 строк.\n",
        "\n",
        "**Выбор ансамбля**:  \n",
        "- **Random Forest** — для устойчивости, интерпретируемости, быстрой настройки;  \n",
        "- **HistGradientBoosting** — для максимальной точности и скорости на больших данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 8. Оценка Производительности Моделей и Метрики\n",
        "\n",
        "Выбор метрики — это **перевод бизнес-цели в математическую форму**. Неправильный выбор приводит к оптимизации модели в неверном направлении.\n",
        "\n",
        "### 8.1. Метрики Классификации\n",
        "\n",
        "| Метрика | Формула | Когда использовать |\n",
        "|--------|--------|--------------------|\n",
        "| **Accuracy** | \\((TP + TN) / (TP + TN + FP + FN)\\) | Сбалансированные данные, равная стоимость ошибок |\n",
        "| **Precision** | \\(TP / (TP + FP)\\) | Минимизация ложных срабатываний (мошенничество, спам) |\n",
        "| **Recall** | \\(TP / (TP + FN)\\) | Минимизация пропусков (медицина, безопасность) |\n",
        "| **F1-Score** | \\(2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\) | Баланс между Precision и Recall |\n",
        "| **ROC AUC** | Площадь под ROC-кривой | Оценка способности к ранжированию, независимо от порога |\n",
        "\n",
        "> **Важно**: при несбалансированных данных **accuracy бесполезна**. Модель, предсказывающая всегда «0» при 99% нулей, даст 99% accuracy, но будет бесполезна.\n",
        "\n",
        "### 8.2. Метрики Регрессии\n",
        "\n",
        "| Метрика | Особенности |\n",
        "|--------|-------------|\n",
        "| **MSE** | Штрафует большие ошибки (квадрат); чувствителен к выбросам |\n",
        "| **MAE** | Линейный штраф; робастен к выбросам |\n",
        "| **RMSE** | В тех же единицах, что и целевая переменная → удобен для интерпретации |\n",
        "| **\\(R^2\\)** | Доля объяснённой дисперсии; 1 = идеально, 0 = не лучше среднего |\n",
        "\n",
        "### 8.3. Кросс-валидационное скорирование\n",
        "\n",
        "В `cross_val_score`, `GridSearchCV`, `RandomizedSearchCV` параметр `scoring` определяет, **что оптимизировать**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'classifier__C': [0.1, 1, 10]}\n",
        "grid = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "- `scoring='precision'` → минимизация ложных срабатываний;\n",
        "- `scoring='f1'` → баланс;\n",
        "- `scoring='roc_auc'` → оценка ранжирования.\n",
        "\n",
        "> **Ключевой навык аналитика**: уметь **сопоставить бизнес-сценарий и метрику**.  \n",
        "> Например:  \n",
        "> - «Мы не можем пропустить ни одного случая заболевания» → **maximize Recall**;  \n",
        "> - «Каждое ложное срабатывание стоит $1000» → **maximize Precision**.\n",
        "\n"
      ],
      "metadata": {
        "id": "DDjA6AYdjaMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Раздел 9. Диагностика: Переобучение, Недообучение и Кривые\n",
        "\n",
        "После обучения модели необходимо провести диагностику её производительности, чтобы понять, страдает ли она от **недообучения** (*high bias*) или **переобучения** (*high variance*). Scikit-learn предоставляет мощные инструменты для визуальной и количественной оценки этого баланса.\n",
        "\n",
        "### 9.1. Диагностика Bias-Variance Trade-off\n",
        "\n",
        "Дилемма «смещения-дисперсии» — центральная концепция машинного обучения:\n",
        "\n",
        "- **Смещение (Bias)** — ошибка, вызванная чрезмерной упрощённостью модели. Модель не может уловить истинные зависимости в данных → **недообучение**.\n",
        "- **Дисперсия (Variance)** — ошибка, вызванная чрезмерной чувствительностью к шуму в обучающих данных. Модель «запоминает» тренировку → **переобучение**.\n",
        "\n",
        "Идеальная модель находится в точке **компромисса**, где сумма смещения и дисперсии минимальна.\n",
        "\n",
        "### 9.2. Кривые обучения (`learning_curve`)\n",
        "\n",
        "Кривые обучения показывают, как **производительность на тренировке и валидации** меняется в зависимости от **размера обучающей выборки**.\n",
        "\n",
        "**Пример: Диагностика через `LearningCurveDisplay`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Генерация данных\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Модель с высокой сложностью (склонна к переобучению)\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Кривая обучения\n",
        "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=42)\n",
        "LearningCurveDisplay.from_estimator(model, X, y, cv=cv, n_jobs=-1)\n",
        "plt.title(\"Кривая обучения: RandomForest (глубокие деревья)\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение до выполнения:*  \n",
        "Мы используем `ShuffleSplit` с 50 сплитами для стабильной оценки. Модель — случайный лес с глубокими деревьями (высокая дисперсия).\n",
        "\n",
        "*После выполнения:*  \n",
        "Если кривые **сильно расходятся** (высокий train score, низкий validation score), это **признак переобучения**. Если обе кривые **низкие и сходятся**, это **недообучение**.\n",
        "\n",
        "**Таблица 3: Диагностика по кривым обучения**\n",
        "\n",
        "| Характеристика кривых | Проблема | Решение |\n",
        "|----------------------|----------|--------|\n",
        "| Высокий Train Score, Низкий Test Score (разошлись) | **Переобучение** (High Variance) | Упростить модель, добавить регуляризацию, собрать больше данных |\n",
        "| Низкий Train Score, Низкий Test Score (сошлись) | **Недообучение** (High Bias) | Использовать более сложную модель, добавить признаки, уменьшить регуляризацию |\n",
        "| Высокий Train Score, Высокий Test Score (сошлись) | **Хорошее обобщение** | Модель готова к использованию |\n",
        "\n",
        "> **Важное следствие**: если валидационная кривая **ещё не вышла на плато**, добавление данных **улучшит** модель. Если кривые сошлись — новые данные **не помогут**.\n",
        "\n",
        "### 9.3. Валидационные кривые (`validation_curve`)\n",
        "\n",
        "Валидационные кривые показывают, как производительность зависит от **одного гиперпараметра**.\n",
        "\n",
        "**Пример: Выбор параметра регуляризации в SVM**\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "param_range = np.logspace(-3, 3, 7)\n",
        "train_scores, val_scores = validation_curve(\n",
        "    SVC(kernel='rbf'), X, y, param_name='C', param_range=param_range, cv=5\n",
        ")\n",
        "\n",
        "# Визуализация\n",
        "plt.semilogx(param_range, np.mean(train_scores, axis=1), 'o-', label='Train')\n",
        "plt.semilogx(param_range, np.mean(val_scores, axis=1), 'o-', label='Validation')\n",
        "plt.xlabel('C (параметр регуляризации)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(); plt.grid()\n",
        "plt.title(\"Валидационная кривая для SVM\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Интерпретация:*  \n",
        "- При **малых C** (сильная регуляризация) → недообучение;  \n",
        "- При **больших C** (слабая регуляризация) → переобучение;  \n",
        "- **Оптимум** — где валидационная кривая максимальна.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 10. Настройка Гиперпараметров и Автоматизированный Поиск\n",
        "\n",
        "### 10.1. Поиск по сетке (`GridSearchCV`)\n",
        "\n",
        "Полный перебор всех комбинаций. Гарантирует нахождение оптимума **в заданной сетке**, но **вычислительно дорог**.\n",
        "\n",
        "### 10.2. Случайный поиск (`RandomizedSearchCV`)\n",
        "\n",
        "Более эффективен при большом пространстве гиперпараметров. Использует **распределения**, а не списки:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import loguniform, randint\n",
        "\n",
        "param_dist = {\n",
        "    'classifier__C': loguniform(1e-4, 1e4),  # лог-равномерное распределение\n",
        "    'classifier__max_iter': [100, 200, 500],\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median']\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    clf, param_dist, n_iter=50, scoring='f1', cv=5, random_state=42, n_jobs=-1\n",
        ")\n",
        "search.fit(X_train, y_train)\n",
        "print(\"Лучшие параметры:\", search.best_params_)\n",
        "```\n",
        "\n",
        "> **Почему `loguniform`?**  \n",
        "> Параметры вроде `C` или `alpha` имеют **логарифмическую шкалу влияния**. Случайный поиск в лог-пространстве эффективнее покрывает диапазон.\n",
        "\n",
        "### 10.3. Интеграция поиска в Pipeline\n",
        "\n",
        "Как уже показано в разделе 5, **весь пайплайн** можно настраивать как единый объект. Это позволяет находить **глобально оптимальную конфигурацию**, а не только лучшую модель.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 11. Практические Кейсы: Несбалансированные Данные и Кластеризация\n",
        "\n",
        "### 11.1. Работа с несбалансированными классами\n",
        "\n",
        "#### Встроенное решение: `class_weight='balanced'`\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_balanced = LogisticRegression(class_weight='balanced')\n",
        "clf_balanced.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Это **минимальное изменение**, которое часто даёт значительный прирост recall для миноритарного класса.\n",
        "\n",
        "#### Продвинутые методы: SMOTE и `imblearn`\n",
        "\n",
        "**Критически важно**: SMOTE должен применяться **только внутри `fit`**, чтобы избежать утечки.\n",
        "\n",
        "**Правильный способ (через `imblearn.pipeline`):**\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Используем ImbPipeline, а не sklearn.pipeline!\n",
        "imb_pipe = ImbPipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "imb_pipe.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Если использовать `sklearn.pipeline`, SMOTE будет применён **до кросс-валидации**, и синтетические точки из валидационного фолда попадут в обучение → **утечка данных**.\n",
        "\n",
        "### 11.2. Основы неконтролируемого обучения: KMeans\n",
        "\n",
        "KMeans минимизирует **инерцию** — сумму квадратов расстояний от точек до центроидов.\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "```\n",
        "\n",
        "### 11.3. Оценка кластеризации: Silhouette Score и Plot\n",
        "\n",
        "Silhouette Score — внутренняя метрика качества кластеров.\n",
        "\n",
        "**Пример: Выбор числа кластеров**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "for n_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    print(f\"K={n_clusters}: Silhouette Score = {score:.3f}\")\n",
        "```\n",
        "\n",
        "**Silhouette Plot** даёт детальную картину:\n",
        "\n",
        "```python\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "fig, ax1 = plt.subplots(1, 1)\n",
        "silhouette_avg = silhouette_score(X, labels)\n",
        "sample_silhouette_values = silhouette_samples(X, labels)\n",
        "\n",
        "y_lower = 10\n",
        "for i in range(n_clusters):\n",
        "    ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                      0, ith_cluster_silhouette_values,\n",
        "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "ax1.set_xlabel('Коэффициент силуэта')\n",
        "ax1.set_ylabel('Кластер')\n",
        "ax1.set_title(f'Silhouette Plot для K={n_clusters}')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Интерпретация:*  \n",
        "- Равномерная ширина полос → сбалансированные кластеры;  \n",
        "- Низкие/отрицательные значения в кластере → плохое разделение.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 12. Интерпретируемость и Сохранение Моделей\n",
        "\n",
        "### 12.1. Встроенная интерпретируемость\n",
        "\n",
        "- **Линейные модели**: `coef_` — прямая интерпретация;\n",
        "- **Деревья**: `feature_importances_` — оценка вклада признака.\n",
        "\n",
        "### 12.2. Пост-хок интерпретируемость: SHAP и LIME\n",
        "\n",
        "Обе библиотеки **нативно работают с моделями из sklearn**:\n",
        "\n",
        "```python\n",
        "import shap\n",
        "\n",
        "# Объяснение через SHAP\n",
        "explainer = shap.TreeExplainer(clf.named_steps['classifier'])\n",
        "shap_values = explainer.shap_values(preprocessor.transform(X_test))\n",
        "shap.summary_plot(shap_values, preprocessor.transform(X_test))\n",
        "```\n",
        "\n",
        "> **SHAP** даёт **глобальные и локальные** объяснения, основанные на теории игр. Это **золотой стандарт** для интерпретации в финансах и медицине.\n",
        "\n",
        "### 12.3. Персистенция моделей\n",
        "\n",
        "**Всегда сохраняйте весь `Pipeline`**:\n",
        "\n",
        "```python\n",
        "from joblib import dump, load\n",
        "\n",
        "# Сохранение\n",
        "dump(clf, 'model_v1.joblib')\n",
        "\n",
        "# Загрузка\n",
        "clf_loaded = load('model_v1.joblib')\n",
        "predictions = clf_loaded.predict(new_data)  # предобработка + прогноз — автоматически\n",
        "```\n",
        "\n",
        "### 12.4. Воспроизводимость окружения (MLOps)\n",
        "\n",
        "- Сохраняйте **`requirements.txt`** или **`environment.yml`**;\n",
        "- Используйте **виртуальные окружения** или **Docker**;\n",
        "- **Pin-версии**: `scikit-learn==1.4.2`, `numpy==1.26.4` и т.д.\n",
        "\n",
        "**Таблица 4: Лучшие практики персистенции**\n",
        "\n",
        "| Задача | Инструмент | Комментарий |\n",
        "|--------|-----------|-------------|\n",
        "| Сохранение модели с NumPy-массивами | `joblib` | Быстрее и компактнее `pickle` |\n",
        "| Сохранение полного ML-процесса | `Pipeline` + `joblib` | Включая предобработку |\n",
        "| Обеспечение идентичности окружения | `pip freeze`, `conda env export` | Обязательно для продакшена |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Модуль 13: Отбор признаков и калибровка вероятностей\n",
        "\n",
        "В процессе построения надёжных и интерпретируемых моделей машинного обучения часто возникают две взаимосвязанные задачи: **управление размерностью признакового пространства** и **обеспечение корректности вероятностных прогнозов**. Хотя современные алгоритмы, такие как регуляризованные линейные модели или ансамбли деревьев, обладают встроенной устойчивостью к избыточным признакам, систематический отбор признаков остаётся важным этапом EDA и оптимизации. Аналогично, многие бизнес-сценарии требуют не просто бинарного прогноза, а **калиброванной оценки вероятности**, что особенно актуально при принятии решений на основе порогов риска. Данный модуль рассматривает обе эти задачи как неотъемлемые компоненты методологически строгого рабочего процесса.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 1. Отбор признаков: Управление размерностью и интерпретируемостью\n",
        "\n",
        "Отбор признаков — это процесс выбора подмножества релевантных переменных для использования в модели. Его цели многообразны:  \n",
        "- снижение вычислительной сложности;  \n",
        "- улучшение обобщающей способности за счёт устранения шума;  \n",
        "- повышение интерпретируемости за счёт фокуса на ключевых драйверах.\n",
        "\n",
        "Scikit-learn предлагает три основных парадигмы отбора признаков, которые различаются по степени взаимодействия с моделью.\n",
        "\n",
        "### 1.1. Фильтрационные методы (Filter Methods)\n",
        "\n",
        "Фильтрационные методы оценивают признаки **независимо от модели**, используя статистические тесты или эвристики. Они вычисляются **до обучения**, что делает их быстрыми и параллелизуемыми.\n",
        "\n",
        "- **`VarianceThreshold`** — удаляет признаки с дисперсией ниже порога. Полезен для исключения константных или почти константных переменных (например, флагов, установленных для 99.9% наблюдений).\n",
        "\n",
        "- **`SelectKBest` / `SelectPercentile`** — отбирают *k* лучших признаков на основе статистики, вычисляемой между признаком и целевой переменной:\n",
        "  - Для **классификации**:  \n",
        "    - `f_classif` — ANOVA F-статистика (предполагает нормальность);  \n",
        "    - `chi2` — хи-квадрат (только для неотрицательных данных, например, TF-IDF);  \n",
        "    - `mutual_info_classif` — взаимная информация (непараметрическая, улавливает нелинейные зависимости).\n",
        "  - Для **регрессии**: `f_regression`, `mutual_info_regression`.\n",
        "\n",
        "**Пример: Отбор признаков на основе взаимной информации**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных: 1000 наблюдений, 50 признаков, только 5 информативны\n",
        "X, y = make_classification(n_samples=1000, n_features=50, n_informative=5,\n",
        "                           n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Вычисление взаимной информации\n",
        "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "\n",
        "# Отбор 10 лучших признаков\n",
        "selector = SelectKBest(mutual_info_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "print(f\"Исходная размерность: {X.shape[1]}\")\n",
        "print(f\"После отбора: {X_selected.shape[1]}\")\n",
        "print(f\"Из 5 истинно информативных признаков выбрано: {np.sum(selector.get_support()[:5])}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Взаимная информация не предполагает линейной зависимости и эффективно выявляет релевантные признаки даже в присутствии шума.\n",
        "\n",
        "### 1.2. Обёрточные методы (Wrapper Methods)\n",
        "\n",
        "Обёрточные методы оценивают подмножества признаков **через производительность конкретной модели**, что делает их более точными, но и **вычислительно дорогими**.\n",
        "\n",
        "- **`RFE` (Recursive Feature Elimination)** — рекурсивно удаляет наименее важные признаки на основе весов модели (например, коэффициентов линейной регрессии или `feature_importances_` в деревьях).\n",
        "- **`RFECV`** — автоматически определяет оптимальное число признаков с помощью кросс-валидации.\n",
        "\n",
        "**Пример: Автоматический отбор признаков через RFECV**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Используем RFECV с кросс-валидацией\n",
        "estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='f1')\n",
        "\n",
        "selector.fit(X, y)\n",
        "\n",
        "print(f\"Оптимальное число признаков: {selector.n_features_}\")\n",
        "print(f\"Поддержка (выбранные признаки): {selector.support_.sum()} из {X.shape[1]}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* `RFECV` находит компромисс между сложностью модели и её производительностью, что особенно ценно при жёстких ограничениях на интерпретируемость.\n",
        "\n",
        "### 1.3. Встроенные методы (Embedded Methods)\n",
        "\n",
        "Некоторые алгоритмы **внутренне** выполняют отбор признаков:\n",
        "\n",
        "- **Lasso** — обнуляет коэффициенты малозначимых признаков (см. Модуль 12, Раздел 6.3);\n",
        "- **Деревья решений и ансамбли** — естественным образом ранжируют признаки по важности.\n",
        "\n",
        "Преимущество встроенных методов — **высокая эффективность**, так как отбор происходит в процессе обучения.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 2. Калибровка вероятностей: От прогноза к надёжной оценке риска\n",
        "\n",
        "Многие алгоритмы машинного обучения выдают **некалиброванные вероятности** — числа в диапазоне \\([0, 1]\\), которые не отражают истинную частоту события. Например, модель может присваивать вероятность 0.8 множеству объектов, но на самом деле только 60% из них принадлежат положительному классу. Это особенно характерно для:\n",
        "\n",
        "- **методов опорных векторов (SVM)** — из-за фокуса на опорных векторах;\n",
        "- **ансамблей деревьев (Random Forest, Gradient Boosting)** — из-за смещения в оценке экстремальных вероятностей.\n",
        "\n",
        "В задачах, где решения принимаются на основе **порогов вероятности** (например, «выдать кредит, если вероятность дефолта < 5%»), некалиброванные прогнозы приводят к систематическим ошибкам.\n",
        "\n",
        "### 2.1. Принципы калибровки\n",
        "\n",
        "Калибровка — это постобработка прогнозов модели с помощью **калибровочной функции** \\( \\hat{p} = f(p_{\\text{raw}}) \\), обученной на валидационных данных.\n",
        "\n",
        "Scikit-learn предоставляет мета-оценщик `CalibratedClassifierCV`, который поддерживает два метода:\n",
        "\n",
        "1. **Плюризация (Platt Scaling)** — аппроксимация сигмоидой:  \n",
        "   \\[\n",
        "   f(p) = \\frac{1}{1 + \\exp(A \\cdot p + B)}\n",
        "   \\]  \n",
        "   Эффективна при **достаточном объёме данных** и **гладких распределениях**.\n",
        "\n",
        "2. **Изотоническая регрессия** — непараметрический метод, представляющий \\(f\\) как **кусочно-постоянную неубывающую функцию**. Более гибок, но склонен к переобучению при малом числе наблюдений.\n",
        "\n",
        "### 2.2. Практическая реализация и диагностика\n",
        "\n",
        "**Пример: Калибровка SVM и визуализация через `CalibrationDisplay`**\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация данных\n",
        "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Некалиброванная SVM\n",
        "svm = SVC(probability=True, random_state=42)  # probability=True для predict_proba\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Калиброванная SVM (используем половину тренировки для калибровки)\n",
        "calibrated_svm = CalibratedClassifierCV(svm, method='isotonic', cv=2)\n",
        "calibrated_svm.fit(X_train, y_train)\n",
        "\n",
        "# Визуализация\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "CalibrationDisplay.from_estimator(svm, X_test, y_test, n_bins=10, ax=ax, name=\"SVM (raw)\")\n",
        "CalibrationDisplay.from_estimator(calibrated_svm, X_test, y_test, n_bins=10, ax=ax, name=\"SVM (calibrated)\")\n",
        "ax.set_title(\"Калибровочные кривые\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Интерпретация графика:*  \n",
        "- **Идеальная калибровка** — это диагональ \\(y = x\\);  \n",
        "- **Некалиброванная SVM** обычно показывает **S-образную кривую**: занижает низкие вероятности и завышает высокие;  \n",
        "- **Калиброванная модель** приближается к диагонали, что означает, что прогноз 0.7 действительно соответствует 70% частоте события.\n",
        "\n",
        "### 2.3. Когда калибровка обязательна?\n",
        "\n",
        "- **Медицинская диагностика**: вероятность заболевания должна быть точной для информированного согласия.\n",
        "- **Кредитный скоринг**: оценка риска дефолта напрямую влияет на условия займа.\n",
        "- **Маркетинг**: если бюджет распределяется по оценке вероятности отклика, некалиброванные оценки приведут к неэффективному расходованию средств.\n",
        "\n",
        "> **Важно**: калибровка **не улучшает** метрики вроде accuracy или AUC, но **делает вероятности надёжными** для принятия решений.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, отбор признаков и калибровка вероятностей — это два финальных штриха, превращающих «рабочую модель» в **надёжный инструмент принятия решений**. Первый обеспечивает **фокус на сути**, устраняя шум и повышая интерпретируемость; второй гарантирует, что **числовая оценка риска соответствует реальности**, что критично в прикладных науках.  \n",
        "\n",
        "Оба подхода органично интегрируются в экосистему Scikit-learn: `SelectKBest`, `RFECV` и `CalibratedClassifierCV` ведут себя как обычные `Estimator` и могут быть встроены в `Pipeline`, сохраняя методологическую строгость и защищённость от утечек данных.  \n",
        "\n",
        "Таким образом, полный цикл машинного обучения в Scikit-learn включает не только обучение и оценку, но и **тонкую настройку под бизнес-контекст**, что и отличает компетентного специалиста по анализу данных от просто пользователя библиотеки.\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Scikit-learn — это не просто библиотека, а **методологический фреймворк**, который учит **думать как инженер машинного обучения**. Его сила — в **дисциплине**: строгом разделении данных, защите от утечек, визуальной диагностике, осознанном выборе метрик и гиперпараметров.\n",
        "\n",
        "Для будущего специалиста по анализу данных освоение Scikit-learn — это не изучение API, а **воспитание культуры надёжного, воспроизводимого и интерпретируемого анализа**, без которой даже самая точная модель остаётся академическим упражнением.\n",
        ""
      ],
      "metadata": {
        "id": "-1e9_MIoks20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jekxi6H6m9cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zCx6tEUYmuvr"
      }
    }
  ]
}