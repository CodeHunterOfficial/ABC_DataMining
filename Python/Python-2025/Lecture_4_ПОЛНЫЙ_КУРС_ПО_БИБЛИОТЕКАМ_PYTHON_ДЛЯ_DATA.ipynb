{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOn5PcbrrJwrYZqt3zwfaKo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/Python/Python-2025/Lecture_4_%D0%9F%D0%9E%D0%9B%D0%9D%D0%AB%D0%99_%D0%9A%D0%A3%D0%A0%D0%A1_%D0%9F%D0%9E_%D0%91%D0%98%D0%91%D0%9B%D0%98%D0%9E%D0%A2%D0%95%D0%9A%D0%90%D0%9C_PYTHON_%D0%94%D0%9B%D0%AF_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 1: NUMPY — ФУНДАМЕНТ НАУЧНЫХ ВЫЧИСЛЕНИЙ\n",
        "\n",
        "## РАЗДЕЛ I. Введение в ndarray и архитектуру NumPy\n",
        "\n",
        "### 1.1. Роль NumPy в экосистеме Python\n",
        "\n",
        "**NumPy** (*Numerical Python*) является краеугольным камнем современной экосистемы научных вычислений на языке Python. Эта библиотека де-факто стала стандартом для эффективных численных операций и служит основой для большинства инструментов в области анализа данных, машинного обучения и научной визуализации.\n",
        "\n",
        "Фундаментальное значение NumPy заключается в его способности преодолевать ограничения интерпретируемого языка Python. За счёт высокооптимизированных вычислительных ядер, написанных на C и Fortran, NumPy предоставляет пользователю простой и элегантный синтаксис для выполнения сложных математических операций, обеспечивая при этом производительность, сопоставимую с низкоуровневыми языками. Эта эффективность критически важна при работе с большими объёмами данных.\n",
        "\n",
        "Области применения NumPy чрезвычайно широки — от академических исследований до промышленного анализа. Например, NumPy сыграл ключевую роль в обработке данных коллаборации LIGO, что привело к подтверждению существования гравитационных волн. В машинном обучении NumPy лежит в основе реализаций таких библиотек, как XGBoost и LightGBM, а также является основой для визуализационных инструментов, включая Matplotlib, Seaborn и Plotly. Вместе с SciPy NumPy формирует обязательный набор инструментов для любого исследователя или разработчика, работающего с числовыми данными.\n",
        "\n",
        "> **Пример: простое сложение массивов — сравнение с Python-списками**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Стандартный список Python\n",
        "python_list = list(range(1000000))\n",
        "%timeit [x + 1 for x in python_list]  # медленно: интерпретируемый цикл\n",
        "\n",
        "# Массив NumPy\n",
        "numpy_array = np.arange(1000000)\n",
        "%timeit numpy_array + 1  # быстро: векторизованная операция\n",
        "```\n",
        "\n",
        "> *Пояснение:* В этом примере демонстрируется разница в производительности между векторизованной операцией над `ndarray` и циклом по обычному списку. Время выполнения векторизованной операции может быть в десятки или сотни раз меньше.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Основы структуры `ndarray`\n",
        "\n",
        "Центральным объектом NumPy является **`ndarray`** (*N-dimensional array*) — контейнер для хранения **гомогенных** данных (все элементы одного типа) в непрерывном или почти непрерывном блоке памяти. Это фундаментальное отличие от стандартных списков Python, которые являются гетерогенными и хранят ссылки на объекты, что значительно снижает эффективность при численных вычислениях.\n",
        "\n",
        "#### Гомогенность и `dtype` (тип данных)\n",
        "\n",
        "Ключевой характеристикой массива является его **тип данных** (`dtype`), который определяет, как элементы интерпретируются и хранятся в памяти. Например, `np.int32` обозначает 32-битное целое, а `np.float64` — 64-битное число с плавающей точкой.\n",
        "\n",
        "Явное управление `dtype` позволяет контролировать потребление памяти и избегать численных ошибок. Например, если сохранить значение `128` в массив типа `np.int8`, диапазон которого ограничен значениями от –128 до 127, результат будет неверным — произойдёт переполнение, и значение «обрежется» до –128. В научных и инженерных расчётах подобные ошибки недопустимы.\n",
        "\n",
        "При операциях между массивами разных типов NumPy автоматически применяет **правила продвижения типов** (*type promotion*), выбирая общий тип, способный вместить все исходные значения без потерь. Например, сложение массивов типов `np.uint32` и `np.int32` приведёт к массиву типа `np.int64`, который безопасно охватывает диапазон обоих исходных типов.\n",
        "\n",
        "> **Пример: контроль типа данных и последствия переполнения**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Опасный пример с переполнением\n",
        "arr_int8 = np.array([127], dtype=np.int8)\n",
        "print(arr_int8 + 1)  # Вывод: [-128] — переполнение!\n",
        "\n",
        "# Безопасный пример с автоматическим продвижением типа\n",
        "arr_uint32 = np.array([1000], dtype=np.uint32)\n",
        "arr_int32 = np.array([-500], dtype=np.int32)\n",
        "result = arr_uint32 + arr_int32\n",
        "print(result, result.dtype)  # Вывод: [500] dtype('int64')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Первый пример иллюстрирует, как переполнение может привести к некорректным результатам. Второй — как NumPy автоматически выбирает безопасный тип при смешанных операциях.\n",
        "\n",
        "#### Архитектурные преимущества\n",
        "\n",
        "Гомогенность и непрерывное хранение позволяют NumPy использовать **C- или Fortran-континуальный порядок** размещения данных в памяти. Это критически важно для эффективной передачи блоков данных в низкоуровневые библиотеки, такие как **BLAS** и **LAPACK**, которые реализуют высокооптимизированные линейные алгебраические операции. Благодаря этому достигается **экспоненциальный выигрыш в скорости** по сравнению с операциями над стандартными списками.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. Сравнение производительности: векторизация\n",
        "\n",
        "**Векторизация** — ключевой принцип высокой производительности в NumPy. Вместо того чтобы писать явные циклы `for` в Python, которые медленны из-за интерпретируемой природы языка, операции применяются сразу ко всему массиву через вызов оптимизированных функций на C или Fortran.\n",
        "\n",
        "На практике это означает, что **арифметические, логические и многие другие операции автоматически распространяются на все элементы массива**. Если же разработчик по неопытности оставляет цикл Python внутри критического участка кода, этот участок неизбежно становится **«узким местом»**, замедляя всю программу.\n",
        "\n",
        "> **Пример: векторизованная функция против цикла**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 10, 1000000)\n",
        "\n",
        "# Невекторизованный (медленный) подход\n",
        "def slow_sin(x):\n",
        "    return np.array([np.sin(val) for val in x])\n",
        "\n",
        "# Векторизованный (быстрый) подход\n",
        "def fast_sin(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "%timeit slow_sin(x)  # медленно\n",
        "%timeit fast_sin(x)  # быстро\n",
        "```\n",
        "\n",
        "> *Пояснение:* Векторизованный вызов `np.sin(x)` выполняется напрямую в C, без итераций в Python. Это делает его значительно быстрее даже для простых функций.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ II. Создание и инициализация массивов\n",
        "\n",
        "### 2.1. Создание массивов из последовательностей (`np.array`)\n",
        "\n",
        "Самый прямой способ создать массив — преобразовать стандартные Python-структуры, такие как списки или кортежи, с помощью функции `np.array(object, dtype=None)`.\n",
        "\n",
        "Уровень вложенности последовательности определяет размерность массива: одномерный список создаёт вектор, список списков — матрицу, и так далее. В научных задачах **рекомендуется явно указывать `dtype`**, особенно если требуется контролировать точность или потребление памяти.\n",
        "\n",
        "> **Пример: создание массивов разной размерности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 1D-массив\n",
        "vector = np.array([1, 2, 3])\n",
        "\n",
        "# 2D-массив\n",
        "matrix = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "\n",
        "# Явное указание типа\n",
        "int_array = np.array([1, 2, 3], dtype=np.int64)\n",
        "float_array = np.array([1, 2, 3], dtype=np.float32)\n",
        "\n",
        "print(\"vector:\", vector)\n",
        "print(\"matrix:\\n\", matrix)\n",
        "print(\"int_array dtype:\", int_array.dtype)\n",
        "print(\"float_array dtype:\", float_array.dtype)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Явное задание типа помогает избежать неожиданных преобразований и экономит память, например, при использовании `float32` вместо `float64`, если задача допускает снижение точности.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Создание массивов с фиксированными значениями\n",
        "\n",
        "Для инициализации вычислительных пространств или создания «заготовок» под результаты используются специализированные функции. `np.zeros(shape, dtype=float64)` создаёт массив, заполненный нулями, `np.ones(shape, dtype=float64)` — единицами, а `np.full(shape, fill_value, dtype=None)` — произвольным значением.\n",
        "\n",
        "Эти функции особенно полезны при настройке начальных условий в численных методах или выделении памяти под промежуточные результаты, когда известна итоговая форма данных, но сами значения ещё не вычислены.\n",
        "\n",
        "> **Пример: инициализация массивов**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "zeros_2d = np.zeros((3, 4))\n",
        "ones_1d = np.ones(5, dtype=np.int32)\n",
        "custom = np.full((2, 2), 7.5)\n",
        "\n",
        "print(\"zeros_2d:\\n\", zeros_2d)\n",
        "print(\"ones_1d:\", ones_1d)\n",
        "print(\"custom:\\n\", custom)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такие массивы часто используются в алгоритмах, где нужно «собирать» результаты постепенно — например, при построении матрицы корреляций или накоплении градиентов в итеративных методах оптимизации.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. Создание регулярных последовательностей: `arange` и `linspace`\n",
        "\n",
        "Для генерации одномерных числовых последовательностей NumPy предоставляет две основные функции.\n",
        "\n",
        "**`np.arange(start, stop, step)`** создаёт последовательность с фиксированным шагом и является аналогом встроенной функции `range`, но возвращает `ndarray`. Однако её **не рекомендуется использовать с дробным шагом** из-за накопления ошибок округления, присущих арифметике с плавающей точкой.\n",
        "\n",
        "**`np.linspace(start, stop, num)`** создаёт **ровно `num` точек**, равномерно распределённых между `start` и `stop`, включая обе границы. Эта функция предпочтительна при построении численных сеток, дискретизации функций и любых задачах, где важен точный контроль над количеством и расположением точек.\n",
        "\n",
        "> **Пример: сравнение `arange` и `linspace`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# arange: риск неточности при float-шаге\n",
        "arr1 = np.arange(0, 1, 0.1)\n",
        "print(\"arange:\", arr1)\n",
        "\n",
        "# linspace: гарантированное количество точек\n",
        "arr2 = np.linspace(0, 1, 10)\n",
        "print(\"linspace:\", arr2)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `linspace` более надёжен при работе с вещественными числами, особенно в задачах, требующих точного контроля над границами и количеством точек, таких как построение графиков или решение дифференциальных уравнений.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Воспроизводимая генерация случайных чисел (RNG)\n",
        "\n",
        "Начиная с версии **1.17**, NumPy использует современный API генерации псевдослучайных чисел через объекты **`Generator`**. Этот подход основан на более быстрых и статистически надёжных алгоритмах, таких как **PCG64**, по сравнению со старым классом `RandomState`.\n",
        "\n",
        "#### Управление воспроизводимостью\n",
        "\n",
        "Для воспроизводимости экспериментов и симуляций необходимо инициализировать генератор с фиксированным **зерном** (*seed*):\n",
        "\n",
        "```python\n",
        "rng = np.random.default_rng(seed=42)\n",
        "random_array = rng.random((2, 3))  # массив 2×3 из [0, 1)\n",
        "```\n",
        "\n",
        "#### Роль `SeedSequence`\n",
        "\n",
        "Внутри `Generator` использует **`SeedSequence`** — механизм, который «перемешивает» входное зерно в надёжное начальное состояние. Это позволяет избегать проблем с «плохими» зёрнами, создавать **независимые подпотоки случайности** через метод `.spawn()` и безопасно использовать генерацию в распределённых вычислениях.\n",
        "\n",
        "#### Векторизованные случайные операции\n",
        "\n",
        "`Generator` поддерживает не только базовые распределения, но и **векторизованные операции**, такие как `rng.permuted(x, axis=1)`, которая перемешивает срезы массива вдоль указанной оси, или `rng.shuffle(x)`, которая перемешивает массив *in-place*.\n",
        "\n",
        "> **Пример: генерация и перестановка**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "# Генерация случайных чисел\n",
        "data = rng.normal(loc=0.0, scale=1.0, size=(3, 4))\n",
        "print(\"Исходные данные:\\n\", data)\n",
        "\n",
        "# Перемешивание по строкам\n",
        "shuffled = rng.permuted(data, axis=1)\n",
        "print(\"После перемешивания по строкам:\\n\", shuffled)\n",
        "\n",
        "# In-place перемешивание (по умолчанию — по первой оси)\n",
        "copy_data = data.copy()\n",
        "rng.shuffle(copy_data)\n",
        "print(\"In-place shuffle (по строкам):\\n\", copy_data)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такие функции широко применяются в статистике — например, при бутстреппинге, кросс-валидации или генерации случайных разбиений данных для обучения моделей машинного обучения."
      ],
      "metadata": {
        "id": "x9mLyuvqebLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## РАЗДЕЛ III. Манипуляции формой и осями\n",
        "\n",
        "### 3.1. Форма и описание массива\n",
        "\n",
        "Каждый массив в NumPy характеризуется набором неизменяемых атрибутов, которые полностью описывают его структуру и содержимое. Ключевыми из них являются: `.shape` — кортеж, задающий количество элементов вдоль каждой оси (например, `(3, 4)` означает 3 строки и 4 столбца); `.ndim` — целое число, указывающее ранг массива (количество измерений); `.size` — общее число элементов, равное произведению всех компонент `.shape`; и `.dtype` — тип данных элементов, такой как `int64` или `float32`.\n",
        "\n",
        "Эти атрибуты доступны только для чтения и фиксированы для данного объекта `ndarray`. Любое изменение формы требует создания нового представления или копии данных, но не модификации исходного объекта.\n",
        "\n",
        "> **Пример: основные атрибуты массива**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12).reshape(3, 4)\n",
        "print(\"Массив a:\\n\", a)\n",
        "print(\"shape:\", a.shape)      # (3, 4)\n",
        "print(\"ndim:\", a.ndim)        # 2\n",
        "print(\"size:\", a.size)        # 12\n",
        "print(\"dtype:\", a.dtype)      # int64 (на большинстве систем)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти атрибуты являются первым шагом в диагностике и понимании структуры любого числового массива.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Изменение формы массива (`reshape` и `ravel`)\n",
        "\n",
        "NumPy позволяет эффективно **переинтерпретировать** данные в памяти без их физического перемещения, при условии, что общее количество элементов сохраняется. Это достигается за счёт изменения метаданных о форме и порядке размещения.\n",
        "\n",
        "Функция `np.reshape(a, newshape)` возвращает массив с новой формой. Один из размеров может быть задан как `-1`, что позволяет NumPy автоматически вычислить его на основе `a.size`. Параметр `order` управляет порядком: `'C'` (построчный, по умолчанию) или `'F'` (постолбцовый), что критично при работе с данными, поступающими из Fortran-кода или внешних библиотек.\n",
        "\n",
        "> **Пример: reshape с автоматическим размером**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12)               # shape: (12,)\n",
        "b = a.reshape(3, -1)            # shape: (3, 4)\n",
        "c = a.reshape(-1, 2, 2)         # shape: (3, 2, 2)\n",
        "\n",
        "print(\"Исходный массив:\", a)\n",
        "print(\"После reshape(3, -1):\\n\", b)\n",
        "print(\"После reshape(-1, 2, 2):\\n\", c)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `-1` устраняет необходимость ручного расчёта размеров и снижает вероятность ошибок.\n",
        "\n",
        "Для преобразования многомерного массива в одномерный существуют две функции. `np.ravel(a)` возвращает **представление (view)**, если это возможно, не копируя данные и обеспечивая высокую производительность. В отличие от него, `a.flatten()` всегда создаёт **новую копию** данных, что гарантирует независимость от исходного массива, но увеличивает потребление памяти и время выполнения.\n",
        "\n",
        "> **Пример: разница между ravel и flatten**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "flat_view = np.ravel(a)\n",
        "flat_copy = a.flatten()\n",
        "\n",
        "# Изменяем view — исходный массив тоже изменится\n",
        "flat_view[0] = 999\n",
        "print(\"После изменения view:\\n\", a)        # [[999, 2], [3, 4]]\n",
        "\n",
        "# Копия не влияет на оригинал\n",
        "flat_copy[0] = 0\n",
        "print(\"После изменения копии:\\n\", a)       # всё ещё [[999, 2], [3, 4]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* В вычислительно интенсивных задачах стоит отдавать предпочтение `ravel()`, чтобы избежать ненужного копирования памяти.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Управление осями (транспонирование и пермутация)\n",
        "\n",
        "Изменение порядка осей — частая операция при подготовке данных для матричных операций, нейросетевых архитектур или визуализации. Для 2D-массивов классическое матричное транспонирование выполняется через атрибут `.T`. Для массивов произвольного ранга используется функция `np.transpose(a)`, которая по умолчанию инвертирует порядок всех осей, но может принимать явный кортеж `axes`, задающий новую перестановку.\n",
        "\n",
        "Обе операции возвращают **представление**, если структура памяти позволяет, что делает их крайне эффективными.\n",
        "\n",
        "> **Пример: транспонирование**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.random.rand(2, 3, 4)  # shape: (2, 3, 4)\n",
        "b = a.T                      # shape: (4, 3, 2)\n",
        "c = np.transpose(a, axes=(2, 0, 1))  # явный порядок: (4, 2, 3)\n",
        "\n",
        "print(\"Исходная форма:\", a.shape)\n",
        "print(\"После .T:\", b.shape)\n",
        "print(\"После transpose(2,0,1):\", c.shape)\n",
        "```\n",
        "\n",
        "Для более гибкого перемещения отдельных осей применяется функция `np.moveaxis(a, source, destination)`. Она позволяет переместить одну или несколько осей в новые позиции, сохранив относительный порядок остальных. Это особенно полезно при работе с тензорами, например, при преобразовании формата изображений из `(batch, height, width, channels)` в `(batch, channels, height, width)` для совместимости с фреймворками глубокого обучения.\n",
        "\n",
        "> **Пример: moveaxis**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Тензор: (batch, height, width, channels) → хотим (batch, channels, height, width)\n",
        "x = np.random.rand(10, 64, 64, 3)\n",
        "x_moved = np.moveaxis(x, source=3, destination=1)  # перемещаем ось 3 на позицию 1\n",
        "print(\"Новая форма:\", x_moved.shape)  # (10, 3, 64, 64)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `moveaxis` делает код читаемее и безопаснее, чем ручное перечисление всех осей в `transpose`.\n",
        "\n",
        "Ещё один важный приём — добавление новой оси размером 1 с помощью `np.newaxis` (псевдоним для `None`). Это ключевой механизм для подготовки массивов к бродкастингу. Например, вектор формы `(n,)` можно превратить в столбец `(n, 1)` или строку `(1, n)`, что позволяет выполнять операции, иначе запрещённые из-за несовместимости форм.\n",
        "\n",
        "> **Пример: превращение вектора в столбец**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(4)           # shape: (4,)\n",
        "b = a[:, np.newaxis]       # shape: (4, 1)\n",
        "c = a[np.newaxis, :]       # shape: (1, 4)\n",
        "\n",
        "print(\"Исходный:\", a.shape)\n",
        "print(\"Столбец:\", b.shape)\n",
        "print(\"Строка:\", c.shape)\n",
        "\n",
        "# Теперь можно, например, вычесть вектор из каждой строки матрицы\n",
        "matrix = np.random.rand(4, 5)\n",
        "result = matrix - b  # broadcasting: (4,5) - (4,1) → (4,5)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без добавления оси такие операции были бы невозможны, что подчёркивает центральную роль `np.newaxis` в векторизованных вычислениях.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ IV. Доступ к элементам: индексация и маскирование\n",
        "\n",
        "NumPy предоставляет несколько мощных и взаимодополняющих механизмов доступа к данным, каждый из которых имеет свои особенности с точки зрения производительности, гибкости и поведения в памяти.\n",
        "\n",
        "### 4.1. Базовая индексация и срезы\n",
        "\n",
        "Базовая индексация включает доступ к отдельным элементам и создание срезов с использованием целых чисел и стандартного синтаксиса срезов (`start:stop:step`). Эта форма индексации всегда возвращает **представление (view)**, то есть новый объект массива, который разделяет память с исходным. Это делает операцию чрезвычайно быстрой, но требует осторожности: любое изменение среза напрямую влияет на исходный массив.\n",
        "\n",
        "> **Пример: срез как view**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "sub = a[0, :]  # первая строка — view\n",
        "sub[0] = 999\n",
        "print(\"Изменённый массив:\\n\", a)  # [[999, 2, 3], [4, 5, 6]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Если изоляция данных необходима, следует явно вызывать метод `.copy()`.\n",
        "\n",
        "### 4.2. Булева индексация (маскирование)\n",
        "\n",
        "Булева индексация использует массив логических значений (`True`/`False`) той же формы, что и исходный массив, для выбора элементов, где маска равна `True`. Этот механизм является основным инструментом для фильтрации данных по произвольному условию, замены значений или удаления пропущенных (NaN) или некорректных наблюдений.\n",
        "\n",
        "В отличие от базовой индексации, булево маскирование **всегда возвращает копию** данных в виде **одномерного массива**, независимо от того, к каким осям применяется маска.\n",
        "\n",
        "> **Пример: фильтрация и замена**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2, np.nan, 3])\n",
        "\n",
        "# Замена отрицательных чисел на 0\n",
        "x[x < 0] = 0\n",
        "print(\"После замены отрицательных:\", x)\n",
        "\n",
        "# Удаление NaN (возвращает копию!)\n",
        "clean_x = x[~np.isnan(x)]\n",
        "print(\"Без NaN:\", clean_x)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Булево маскирование является основой для условной обработки данных в векторизованном стиле.\n",
        "\n",
        "### 4.3. Продвинутая (fancy) целочисленная индексация\n",
        "\n",
        "Продвинутая индексация использует массивы или списки целых чисел для выбора произвольных, не обязательно смежных или упорядоченных элементов. Элементы могут повторяться, а их порядок в результате будет точно соответствовать порядку индексов.\n",
        "\n",
        "При передаче массивов индексов для нескольких осей они **согласуются (broadcastятся)** и комбинируются поэлементно. Например, если передать два массива длины 3 для строк и столбцов, будет выбрано ровно 3 элемента — на пересечении `(row[0], col[0])`, `(row[1], col[1])` и так далее.\n",
        "\n",
        "Этот тип индексации **всегда создаёт копию** данных и возвращает массив, форма которого определяется результатом broadcasting индексных массивов.\n",
        "\n",
        "> **Пример: выбор конкретных позиций**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12).reshape(3, 4)\n",
        "print(\"Массив a:\\n\", a)\n",
        "\n",
        "# Выбор элементов (0,1), (1,2), (2,3)\n",
        "rows = np.array([0, 1, 2])\n",
        "cols = np.array([1, 2, 3])\n",
        "selected = a[rows, cols]\n",
        "print(\"Выбранные элементы:\", selected)  # [1, 6, 11]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Результат представляет собой одномерный массив, даже если исходный массив был многомерным, что важно учитывать при проектировании алгоритмов.\n",
        "\n",
        "### 4.4. Комбинированная индексация: функция `np.ix_`\n",
        "\n",
        "Прямая передача двух отдельных массивов индексов для строк и столбцов не даёт полной подматрицы, а выбирает только диагональные пары. Для получения **всех комбинаций** индексов — то есть подматрицы на пересечении заданных строк и столбцов — используется функция `np.ix_`.\n",
        "\n",
        "Эта функция преобразует одномерные массивы индексов в совместимые формы: первый массив превращается в столбец `(n, 1)`, второй — в строку `(1, m)`. Это позволяет механизму бродкастинга создать полную двумерную сетку индексов.\n",
        "\n",
        "> **Пример: выбор подматрицы с помощью `np.ix_`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[ 0,  1,  2],\n",
        "              [ 3,  4,  5],\n",
        "              [ 6,  7,  8],\n",
        "              [ 9, 10, 11]])\n",
        "\n",
        "# Строки с чётной суммой\n",
        "rows_mask = (x.sum(axis=1) % 2 == 0)  # [True, False, True, False]\n",
        "# Столбцы 0 и 2\n",
        "cols = np.array([0, 2])\n",
        "\n",
        "# Правильный способ: создать полную подматрицу\n",
        "subset = x[np.ix_(rows_mask, cols)]\n",
        "print(\"Подматрица на пересечении:\\n\", subset)\n",
        "# Результат:\n",
        "# [[0  2]\n",
        "#  [6  8]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.ix_` является незаменимым инструментом для сложной фильтрации по нескольким измерениям и обеспечивает полный контроль над структурой результата.\n",
        "\n",
        "---\n",
        "\n",
        "> **Примечание:** Эта часть завершает вводный обзор ключевых возможностей `ndarray`. В следующем модуле будут рассмотрены **математические функции**, **агрегации**, **broadcasting** и **производительность** в NumPy."
      ],
      "metadata": {
        "id": "TMDG0VuWec0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## РАЗДЕЛ V. Векторизованные операции и бродкастинг\n",
        "\n",
        "### 5.1. Универсальные функции (UFuncs)\n",
        "\n",
        "**Универсальные функции** (*universal functions*, или **UFuncs**) составляют основу векторизованных вычислений в NumPy. Это функции, которые выполняют **поэлементные операции** над массивами с высокой скоростью, поскольку их внутренние циклы реализованы на C и не зависят от интерпретируемой природы Python. К этому классу относятся арифметические операции (`np.add`, `np.multiply`), элементарные математические функции (`np.sin`, `np.exp`, `np.log`) и логические операторы (`np.greater`, `np.equal`, `np.logical_and`). Все UFuncs автоматически применяют правила бродкастинга, что позволяет им корректно работать с массивами разной, но совместимой формы, обеспечивая при этом максимальную производительность и экономию памяти.\n",
        "\n",
        "> **Пример: UFunc в действии**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "# Поэлементное умножение через UFunc\n",
        "result = np.multiply(a, b)  # эквивалентно a * b\n",
        "print(result)  # [4 10 18]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Благодаря UFuncs операции над массивами выглядят как обычные арифметические выражения, но выполняются на уровне C — без циклов Python.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Теория бродкастинга (Broadcasting Theory)\n",
        "\n",
        "**Бродкастинг** — это мощный механизм, позволяющий NumPy выполнять арифметические операции над массивами **разной формы**, не копируя данные физически. Вместо дублирования памяти он логически «растягивает» меньший массив при доступе к элементам. Это критически важно для эффективного использования памяти и производительности, особенно при работе с большими тензорами.\n",
        "\n",
        "Совместимость форм определяется строгими правилами. Сравнение начинается с последней (самой правой) оси и движется влево. Для каждой пары осей допускаются два случая: либо их размеры равны, либо один из них равен единице. Если один массив имеет меньше осей, он виртуально дополняется осями размером 1 слева. Если ни одно из условий не выполняется для хотя бы одной пары осей, возникает исключение `ValueError: operands could not be broadcast together`.\n",
        "\n",
        "> **Пример: проверка совместимости**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Формы: (3, 4) и (4,) → совместимы: (3,4) vs (1,4) → (3,4)\n",
        "A = np.ones((3, 4))\n",
        "b = np.array([1, 2, 3, 4])\n",
        "C = A + b  # OK\n",
        "\n",
        "# Формы: (2, 3) и (3, 2) → НЕсовместимы: 3 ≠ 2 и ни одно ≠ 1\n",
        "try:\n",
        "    D = np.ones((2, 3)) + np.ones((3, 2))\n",
        "except ValueError as e:\n",
        "    print(\"Ошибка:\", e)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Бродкастинг — это не копирование, а **логическая «растяжка»** данных при доступе к памяти.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Практические примеры бродкастинга\n",
        "\n",
        "Наиболее простой случай — операция массива со скаляром. Скаляр «виртуально растягивается» до формы массива, и операция применяется поэлементно. Более сложный и часто встречающийся сценарий — добавление одномерного вектора к каждой строке двумерной матрицы. Если длина вектора совпадает с числом столбцов матрицы, он автоматически добавляется к **каждой строке** без необходимости явного цикла или копирования.\n",
        "\n",
        "Для создания **всех возможных комбинаций** между двумя векторами используется приём с `np.newaxis`. Преобразуя один вектор в столбец `(n, 1)`, а другой оставляя строкой `(m,)`, можно построить двумерную матрицу результатов `(n, m)`, что эквивалентно внешнему произведению, но применимо к любой бинарной операции.\n",
        "\n",
        "> **Пример: внешняя сумма**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([0, 10, 20])    # (3,)\n",
        "b = np.array([1, 2])         # (2,)\n",
        "\n",
        "# Превращаем a в столбец: (3, 1)\n",
        "outer_sum = a[:, np.newaxis] + b  # (3,1) + (2,) → (3,2)\n",
        "print(\"Внешняя сумма:\\n\", outer_sum)\n",
        "# [[ 1  2]\n",
        "#  [11 12]\n",
        "#  [21 22]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой приём часто используется для построения сеток значений, вычисления попарных расстояний или ядерных функций.\n",
        "\n",
        "Ключевое преимущество бродкастинга — **экономия памяти**. Например, при умножении изображения формы `(256, 256, 3)` на скаляр NumPy не создаёт копию объёмом в сотни мегабайт, а выполняет операцию на лету, что делает его незаменимым в задачах компьютерного зрения и обработки сигналов.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Условные операции: `np.where()`\n",
        "\n",
        "Функция `np.where(condition, x, y)` представляет собой **векторизованный аналог конструкции `if-else`**. Для каждого элемента результирующего массива выбирается значение из `x` или `y` в зависимости от соответствующего логического условия. Эта функция возвращает новый массив, что делает её поведение предсказуемым и безопасным по сравнению с in-place операциями.\n",
        "\n",
        "> **Пример: замена значений по условию**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "# Заменить отрицательные на 0, положительные — на 1, нули оставить\n",
        "result = np.where(x < 0, 0, np.where(x > 0, 1, x))\n",
        "print(result)  # [0 0 0 1 1]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.where` особенно полезен в сложных условиях, где требуется несколько уровней ветвления, и позволяет избежать цепочек булевых масок.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ VI. Математика и статистика массивов\n",
        "\n",
        "NumPy предоставляет богатый набор **векторизованных агрегационных функций**, которые могут применяться ко всему массиву или вдоль заданной оси. Эти функции реализованы на C и обеспечивают высокую производительность даже для больших наборов данных.\n",
        "\n",
        "### 6.1. Статистические агрегации\n",
        "\n",
        "К базовым статистическим функциям относятся `np.sum`, `np.mean`, `np.std`, `np.min` и `np.max`. Все они поддерживают параметр `axis`, который определяет, **вдоль какой оси «схлопывается»** массив. Например, при `axis=0` операция выполняется по строкам (результат — вектор по столбцам), а при `axis=1` — по столбцам (результат — вектор по строкам).\n",
        "\n",
        "> **Пример: агрегация по осям**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "\n",
        "print(\"Сумма по всему массиву:\", np.sum(A))        # 21\n",
        "print(\"Сумма по строкам (axis=1):\", np.sum(A, axis=1))  # [6, 15]\n",
        "print(\"Сумма по столбцам (axis=0):\", np.sum(A, axis=0)) # [5, 7, 9]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Параметр `axis` является ключевым для многомерного анализа и часто используется в предобработке данных для машинного обучения.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Кумулятивные операции: `np.cumsum`\n",
        "\n",
        "Функция `np.cumsum(a, axis=None)` возвращает массив с **накопленными суммами**. Важно понимать, что из-за особенностей машинной арифметики с плавающей точкой, последний элемент результата `np.cumsum(a)[-1]` **может не совпадать** с `np.sum(a)`. Причина в том, что `np.sum` использует **оптимизированные алгоритмы суммирования** (например, pairwise summation), которые минимизируют ошибку округления, тогда как `cumsum` выполняет строгую последовательную аккумуляцию, накапливая ошибку на каждом шаге.\n",
        "\n",
        "> **Пример: разница в точности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Массив с очень малыми и большими числами\n",
        "x = np.array([1e10, 1, -1e10, 1])\n",
        "\n",
        "total_sum = np.sum(x)\n",
        "cumsum_last = np.cumsum(x)[-1]\n",
        "\n",
        "print(\"np.sum(x):\", total_sum)           # 2.0\n",
        "print(\"cumsum(x)[-1]:\", cumsum_last)     # 0.0 — потеря точности!\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для итоговых сумм предпочтительнее `np.sum`; `cumsum` следует использовать только если требуется вся последовательность накопленных значений.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. Вычисление квантилей: `np.percentile` и `np.quantile`\n",
        "\n",
        "Функция `np.percentile(a, q, axis=None, method='linear')` вычисляет **перцентили** (квантили, умноженные на 100). Значение `q=50` соответствует медиане, `q=25` и `q=75` — первому и третьему квартилям. Параметр `method` позволяет выбрать алгоритм интерполяции между соседними точками данных. Доступны такие варианты, как `'linear'` (по умолчанию), `'lower'`, `'higher'`, `'midpoint'` и `'inverted_cdf'`, последний из которых соответствует классическому статистическому определению квантиля.\n",
        "\n",
        "> **Пример: медиана и квартили**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "median = np.percentile(data, 50)\n",
        "q1 = np.percentile(data, 25)\n",
        "q3 = np.percentile(data, 75)\n",
        "\n",
        "print(\"Медиана:\", median)  # 5.0\n",
        "print(\"Q1, Q3:\", q1, q3)   # 3.0, 7.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Выбор метода должен соответствовать статистической методологии исследования — особенно при сравнении результатов с другими пакетами, такими как R или pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.4. Управление агрегацией по осям: `axis` и `keepdims`\n",
        "\n",
        "При выполнении агрегации по оси соответствующее измерение **удаляется** из результата. Это может вызвать трудности при последующих операциях, например, при вычитании среднего значения из исходного массива, поскольку формы перестают быть совместимыми. Решение — использование параметра `keepdims=True`, который **сохраняет оси размером 1** в результирующем массиве, делая его совместимым с исходным для бродкастинга.\n",
        "\n",
        "> **Пример: стандартизация данных**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Исходные данные: 10 наблюдений, 5 признаков\n",
        "X = np.random.rand(10, 5)\n",
        "\n",
        "# Без keepdims: среднее — вектор (5,)\n",
        "mean_bad = np.mean(X, axis=0)\n",
        "# X - mean_bad → работает благодаря бродкастингу, но не всегда очевидно\n",
        "\n",
        "# С keepdims: среднее — матрица (1, 5)\n",
        "mean_good = np.mean(X, axis=0, keepdims=True)\n",
        "std_good = np.std(X, axis=0, keepdims=True)\n",
        "\n",
        "# Стандартизация: каждая строка центрируется и масштабируется\n",
        "X_standardized = (X - mean_good) / std_good\n",
        "\n",
        "print(\"Форма X:\", X.shape)               # (10, 5)\n",
        "print(\"Форма mean_good:\", mean_good.shape)  # (1, 5)\n",
        "print(\"Форма X_standardized:\", X_standardized.shape)  # (10, 5)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `keepdims=True` делает код **более явным, устойчивым к ошибкам** и совместимым с последующими бродкастинг-операциями. Это особенно важно в машинном обучении, где центрирование и масштабирование — стандартные этапы предобработки.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:** Векторизованные операции, UFuncs и бродкастинг — это три кита, на которых стоит эффективная работа с данными в NumPy. Понимание этих механизмов позволяет писать код, который не только короче и читабельнее, но и **на порядки быстрее** и **экономичнее по памяти**, чем эквивалент, написанный с использованием циклов Python."
      ],
      "metadata": {
        "id": "aHuYl7A9ehWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## РАЗДЕЛ VII. Линейная алгебра (подмодуль `numpy.linalg`)\n",
        "\n",
        "Подмодуль **`numpy.linalg`** предоставляет доступ к **высокооптимизированным реализациям** стандартных операций линейной алгебры, основанным на промышленных библиотеках **BLAS** и **LAPACK**. Эти функции гарантируют как скорость, так и численную надёжность при работе с матрицами.\n",
        "\n",
        "### 7.1. Матричные произведения\n",
        "\n",
        "NumPy предлагает два основных способа выполнения матричного умножения, различающихся семантикой и областью применения. Функция **`np.dot(A, B)`** является универсальной: для одномерных массивов она возвращает скалярное произведение, для двумерных — выполняет классическое матричное умножение, а для массивов более высокого ранга — суммирует по последней оси первого аргумента и предпоследней оси второго.\n",
        "\n",
        "В отличие от неё, оператор **`A @ B`** или функция **`np.matmul(A, B)`** предназначены строго для матричного умножения. Они не поддерживают скалярное произведение одномерных векторов (выбрасывая исключение `ValueError`), но корректно обрабатывают **«стеки» матриц**, например, тензоры форм `(N, M, K)` и `(N, K, L)`, результатом умножения которых будет тензор `(N, M, L)`. Этот подход строже следует правилам линейной алгебры и делает намерения кода более явными.\n",
        "\n",
        "> **Пример: сравнение `dot` и `matmul`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Скалярное произведение\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "print(\"np.dot(a, b):\", np.dot(a, b))  # 32\n",
        "\n",
        "try:\n",
        "    print(\"a @ b:\", a @ b)  # Ошибка: 1D @ 1D не поддерживается\n",
        "except ValueError as e:\n",
        "    print(\"Ошибка @ с 1D:\", e)\n",
        "\n",
        "# Матричное умножение\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "print(\"A @ B:\\n\", A @ B)\n",
        "\n",
        "# Стеки матриц\n",
        "C = np.random.rand(2, 3, 4)  # 2 матрицы 3×4\n",
        "D = np.random.rand(2, 4, 5)  # 2 матрицы 4×5\n",
        "E = C @ D  # результат: (2, 3, 5)\n",
        "print(\"Форма результата стека:\", E.shape)\n",
        "```\n",
        "\n",
        "> *Пояснение:* В современном коде **предпочтителен оператор `@`**, так как он делает намерения разработчика более явными и безопасными.\n",
        "\n",
        "### 7.2. Решение систем линейных уравнений (СЛУ)\n",
        "\n",
        "Для решения системы вида **`A x = B`**, где `A` — квадратная матрица, используется функция `np.linalg.solve(A, B)`. Она требует, чтобы матрица `A` была **квадратной** и **несингулярной** (полного ранга). Если система **переопределена** (уравнений больше, чем неизвестных) или **недоопределена**, следует использовать **метод наименьших квадратов** через функцию `np.linalg.lstsq(A, B, rcond=None)`.\n",
        "\n",
        "> **Проверка решения:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[3, 1], [1, 2]])\n",
        "B = np.array([9, 8])\n",
        "x = np.linalg.solve(A, B)\n",
        "\n",
        "print(\"Решение x:\", x)\n",
        "print(\"Проверка A @ x ≈ B:\", np.allclose(A @ x, B))  # True\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.allclose` учитывает погрешности машинной арифметики и корректно сравнивает результаты с плавающей точкой.\n",
        "\n",
        "### 7.3. Обратная матрица и численная стабильность\n",
        "\n",
        "Функция `np.linalg.inv(A)` вычисляет обратную матрицу **`A⁻¹`**. Однако **прямое обращение — плохая практика** в большинстве приложений. Если матрица **плохо обусловлена** (близка к сингулярной), результат будет **числово неточным**. Число обусловленности, вычисляемое как `np.linalg.cond(A) = σ_max / σ_min`, количественно оценивает эту чувствительность: чем больше значение, тем выше риск ошибки.\n",
        "\n",
        "Рекомендуется вместо выражения `x = inv(A) @ B` всегда использовать **`x = solve(A, B)`** — это не только быстрее, но и значительно стабильнее с точки зрения численной математики.\n",
        "\n",
        "> **Пример: сравнение точности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Плохо обусловленная матрица Гильберта\n",
        "A = np.array([[1, 1/2], [1/2, 1/3]], dtype=np.float64)\n",
        "B = np.array([1, 1])\n",
        "\n",
        "# Плохой способ\n",
        "x_bad = np.linalg.inv(A) @ B\n",
        "\n",
        "# Хороший способ\n",
        "x_good = np.linalg.solve(A, B)\n",
        "\n",
        "true_x = np.array([4, -6])  # точное решение\n",
        "print(\"Ошибка через inv:\", np.linalg.norm(x_bad - true_x))\n",
        "print(\"Ошибка через solve:\", np.linalg.norm(x_good - true_x))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Даже на малых матрицах разница может быть значимой. В реальных задачах (машинное обучение, физика) предпочтение `solve` критично.\n",
        "\n",
        "### 7.4. Сингулярное разложение (SVD)\n",
        "\n",
        "**Сингулярное разложение (SVD)** — одна из самых мощных техник линейной алгебры. Любая матрица **`A`** может быть разложена как:\n",
        "\\[\n",
        "A = U \\cdot S \\cdot V^H\n",
        "\\]\n",
        "где `U` — левые сингулярные векторы, `S` — диагональная матрица сингулярных значений (в NumPy — вектор `s`), а `V^H` — сопряжённо-транспонированные правые сингулярные векторы.\n",
        "\n",
        "Функция `U, s, Vh = np.linalg.svd(A, full_matrices=False)` возвращает **усечённое SVD**, что экономит память и используется в задачах **PCA**, сжатия данных и регуляризации. Все функции `linalg`, включая `svd`, `solve` и `inv`, поддерживают работу с **N-мерными массивами**, применяя операции к последним двум осям — что идеально для обработки батчей в машинном обучении.\n",
        "\n",
        "> **Пример: реконструкция и PCA-подобное сжатие**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.random.rand(5, 4)\n",
        "U, s, Vh = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "# Реконструкция\n",
        "A_rec = U @ np.diag(s) @ Vh\n",
        "print(\"Ошибка реконструкции:\", np.linalg.norm(A - A_rec))  # ~1e-15\n",
        "\n",
        "# Сжатие: оставить только 2 главных компоненты\n",
        "k = 2\n",
        "A_approx = U[:, :k] @ np.diag(s[:k]) @ Vh[:k, :]\n",
        "print(\"Сжатая форма:\", A_approx.shape)  # (5, 4), но ранг ≈ 2\n",
        "```\n",
        "\n",
        "> *Пояснение:* SVD лежит в основе **метода главных компонент (PCA)**, рекомендательных систем и решения некорректных СЛУ.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ VIII. Производительность, продвинутые методы и практическое применение\n",
        "\n",
        "### 8.1. Продвинутая векторизация: конвенция Эйнштейна (`np.einsum`)\n",
        "\n",
        "Для сложных тензорных операций, которые неудобно выражать через `@` или `dot`, NumPy предоставляет **`np.einsum`** — реализацию **конвенции суммирования Эйнштейна**. Синтаксис функции задаётся строкой вида `'индексы_входов->индексы_выхода'`, что делает код читаемым и близким к математической записи.\n",
        "\n",
        "Например, матричное умножение записывается как `'ij,jk->ik'`, скалярное произведение — как `'i,i->'`, а извлечение диагонали — как `'ii->i'`. Преимущества `np.einsum` многообразны: высокая читаемость, гибкость в выражении сложных свёрток и перестановок, а также возможность автоматической оптимизации порядка операций через параметр `optimize=True`, что особенно важно для больших тензоров. Эта функция лежит в основе тензорных операций в современных фреймворках, таких как TensorFlow и PyTorch.\n",
        "\n",
        "> **Пример: ускорение через оптимизацию**\n",
        "\n",
        "```python\n",
        "X = np.random.rand(100, 50, 20)\n",
        "Y = np.random.rand(50, 20, 80)\n",
        "\n",
        "# Без оптимизации — медленно\n",
        "%timeit np.einsum('ijk,jkl->il', X, Y)\n",
        "\n",
        "# С оптимизацией — может быть в разы быстрее\n",
        "%timeit np.einsum('ijk,jkl->il', X, Y, optimize=True)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.einsum` — ключевой инструмент в библиотеках вроде **TensorFlow**, **PyTorch** (через `torch.einsum`) и **JAX**.\n",
        "\n",
        "### 8.2. Производительность: бенчмаркинг и профилирование\n",
        "\n",
        "Векторизованный код на NumPy **на порядки быстрее** циклов Python. Однако «островки» не-векторизованного кода легко становятся **узкими местами**. Для поиска и устранения таких проблем рекомендуется использовать `%timeit` для измерения времени выполнения отдельных участков и профилировщики, такие как `cProfile` или `line_profiler`. Основной стратегией оптимизации остаётся минимизация явных циклов `for` в пользу UFuncs, `np.where` и `np.einsum`.\n",
        "\n",
        "> **Пример: векторизация vs цикл**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(1000000)\n",
        "\n",
        "# Медленно\n",
        "def slow_log(x):\n",
        "    return np.array([np.log(val) if val > 0 else 0 for val in x])\n",
        "\n",
        "# Быстро\n",
        "def fast_log(x):\n",
        "    out = np.zeros_like(x)\n",
        "    mask = x > 0\n",
        "    out[mask] = np.log(x[mask])\n",
        "    return out\n",
        "\n",
        "%timeit slow_log(x)  # ~100 ms\n",
        "%timeit fast_log(x)  # ~1 ms\n",
        "```\n",
        "\n",
        "> *Пояснение:* Разница в 100 раз — типична для перехода от Python-циклов к векторизации.\n",
        "\n",
        "### 8.3. Практика 1: обработка изображений\n",
        "\n",
        "Изображение — это **3D-массив** формы `(высота, ширина, каналы)`. NumPy позволяет выполнять базовые операции **без сторонних библиотек**. Например, нормализацию к диапазону `[0, 1]`, геометрические преобразования (поворот, отражение) и центрирование по каналам можно реализовать с помощью стандартных функций и бродкастинга.\n",
        "\n",
        "> **Пример: нормализация и геометрические преобразования**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Имитация цветного изображения 100×100×3\n",
        "img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "# Нормализация к [0, 1]\n",
        "img_norm = img.astype(np.float32) / 255.0\n",
        "\n",
        "# Поворот на 90° по часовой стрелке\n",
        "img_rot = np.rot90(img, k=-1)\n",
        "\n",
        "# Отражение по горизонтали\n",
        "img_flip = np.fliplr(img)\n",
        "\n",
        "# Удаление среднего по каналам\n",
        "mean_per_channel = img_norm.mean(axis=(0, 1), keepdims=True)\n",
        "img_centered = img_norm - mean_per_channel\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для сложных операций (размытие, градиенты, морфология) используется **`scipy.ndimage`**, но **NumPy — основа** всех этих преобразований.\n",
        "\n",
        "### 8.4. Практика 2: фильтрация сигналов и симуляции\n",
        "\n",
        "Временные ряды и физические модели представляют собой **одно- или двумерные массивы**, которые идеально подходят для NumPy. Например, скользящее среднее можно эффективно вычислить через кумулятивную сумму, избегая циклов. В симуляциях динамических систем все обновления состояний (позиций, скоростей) должны выполняться векторизованно, что обеспечивает высокую производительность и читаемость кода.\n",
        "\n",
        "> **Пример: скользящее среднее через кумулятивную сумму**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def moving_average(x, window):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (cumsum[window:] - cumsum[:-window]) / window\n",
        "\n",
        "signal = np.random.randn(1000)\n",
        "smoothed = moving_average(signal, window=50)\n",
        "```\n",
        "\n",
        "> **Симуляции:**  \n",
        "> Используйте `Generator` для воспроизводимости, а все обновления — через векторизованные операции:\n",
        "\n",
        "```python\n",
        "rng = np.random.default_rng(42)\n",
        "positions = rng.normal(size=(100, 2))  # 100 частиц в 2D\n",
        "velocities = np.zeros_like(positions)\n",
        "\n",
        "for _ in range(1000):\n",
        "    forces = -positions  # упрощённая сила (пружина)\n",
        "    velocities += forces * 0.01\n",
        "    positions += velocities * 0.01\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой подход лежит в основе **численного интегрирования**, **машинного обучения с подкреплением**, **моделирования погоды** и др.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**NumPy — это не просто библиотека, а философия эффективных научных вычислений.** Его архитектура строится на трёх китах:\n",
        "\n",
        "1. **`ndarray`** — гомогенный, непрерывный контейнер, позволяющий использовать оптимизированные ядра C/Fortran.\n",
        "2. **Векторизация и бродкастинг** — механизм, устраняющий циклы Python и экономящий память.\n",
        "3. **Численно устойчивые алгоритмы** в `numpy.linalg` и `numpy.random` — основа для надёжных расчётов.\n",
        "\n",
        "Однако эффективность требует **осознанного подхода**:\n",
        "- Предпочитайте **`view` над `copy`** (`ravel` вместо `flatten`).\n",
        "- Используйте **`keepdims=True`** при агрегации для корректного бродкастинга.\n",
        "- Избегайте **явного обращения матриц** — выбирайте `solve`.\n",
        "- Применяйте **`np.newaxis`** для подготовки к бродкастингу.\n",
        "- Используйте **`np.einsum`** для сложных тензорных операций.\n",
        "\n",
        "С освоением NumPy вы получаете **единый, мощный и стандартизированный язык** для работы с данными — язык, на котором говорят **SciPy**, **pandas**, **scikit-learn**, **Matplotlib**, и даже **PyTorch** и **TensorFlow** (через совместимость с `ndarray`).\n",
        "\n",
        "> Таким образом, овладение NumPy — это не первый шаг в data science, а **фундамент всего здания современных вычислений на Python**."
      ],
      "metadata": {
        "id": "xtlNUalxhZcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 2: Библиотека Pandas — Комплексный анализ и обработка табличных данных\n",
        "\n",
        "Библиотека **Pandas** является краеугольным камнем современной экосистемы обработки данных на языке Python. Она предоставляет высокопроизводительные, удобные в использовании структуры данных и инструменты для анализа, делая Python мощным инструментом для работы с табличными данными — на уровне таких систем, как **R** или **SQL**.\n",
        "\n",
        "В рамках этого модуля рассматриваются ключевые концепции и практики работы с Pandas, необходимые для построения надёжных **ETL-конвейеров** (Extract, Transform, Load): от загрузки и создания структур данных до очистки, трансформации и сохранения результатов.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Фундаментальные структуры данных Pandas\n",
        "\n",
        "Pandas строится на трёх основных структурах: **`Series`**, **`DataFrame`** и **`Index`**. Понимание их взаимосвязи и внутренних механизмов — особенно **принципа выравнивания данных (Data Alignment)** — критически важно для эффективной и предсказуемой работы с библиотекой.\n",
        "\n",
        "### I.1. Обзор: `Series`, `DataFrame`, `Index`\n",
        "\n",
        "#### **`Series` — одномерный индексированный массив**\n",
        "\n",
        "Объект `Series` представляет собой одномерный массив с **явно заданными метками** (индексом). Его можно рассматривать как **индексированный аналог NumPy-массива** или как **высокопроизводительный словарь с фиксированным порядком ключей**.\n",
        "\n",
        "> **Пример: создание Series**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ser = pd.Series([10, 20, 30], index=['a', 'b', 'c'], name='values')\n",
        "print(\"Series:\\n\", ser)\n",
        "# Вывод:\n",
        "# a    10\n",
        "# b    20\n",
        "# c    30\n",
        "# Name: values, dtype: int64\n",
        "```\n",
        "\n",
        "> *Пояснение:* В отличие от обычного словаря, `Series` поддерживает векторизованные операции и интеграцию с `DataFrame`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`DataFrame` — двумерная таблица с метками**\n",
        "\n",
        "`DataFrame` — основная структура данных в Pandas. Это двумерная, изменяемая таблица с **метками строк (индекс)** и **метками столбцов (колонки)**. Концептуально `DataFrame` можно представить как **словарь из `Series`**, где каждый столбец — это отдельный `Series`, а все они разделяют общий индекс.\n",
        "\n",
        "> **Пример: `DataFrame` из `Series`**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ser = pd.Series([0, 1, 2], index=['a', 'b', 'c'], name='ser_data')\n",
        "df_from_ser = pd.DataFrame(ser)\n",
        "\n",
        "print(\"DataFrame из Series:\\n\", df_from_ser)\n",
        "# Вывод:\n",
        "#    ser_data\n",
        "# a         0\n",
        "# b         1\n",
        "# c         2\n",
        "```\n",
        "\n",
        "> *Пояснение:* Имя `Series` автоматически становится именем столбца. Индекс сохраняется.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Принцип выравнивания данных (Data Alignment)**\n",
        "\n",
        "Ключевое отличие Pandas от NumPy — **автоматическое выравнивание по меткам** при выполнении операций. Арифметические и логические операции выполняются **по совпадающим строкам и столбцам**, а отсутствующие метки заполняются `NaN`.\n",
        "\n",
        "Это гарантирует, что вы всегда работаете с **соответствующими элементами**, независимо от порядка или полноты данных.\n",
        "\n",
        "> **Пример: выравнивание при сложении**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
        "s2 = pd.Series([10, 20], index=['b', 'c'])\n",
        "\n",
        "result = s1 + s2\n",
        "print(\"Результат сложения с выравниванием:\\n\", result)\n",
        "# Вывод:\n",
        "# a    NaN\n",
        "# b    12.0\n",
        "# c    23.0\n",
        "# dtype: float64\n",
        "```\n",
        "\n",
        "> *Пояснение:* Элемент `'a'` отсутствует в `s2`, поэтому результат — `NaN`. Это предотвращает ошибки, характерные для «слепых» операций над массивами без меток.\n",
        "\n",
        "---\n",
        "\n",
        "### I.2. Атрибуты объектов: `shape`, `dtypes`, `index`, `columns`\n",
        "\n",
        "Эти атрибуты предоставляют метаданные, необходимые для анализа структуры и качества данных.\n",
        "\n",
        "- **`.shape`** — кортеж вида `(число строк, число столбцов)`, совпадает с соглашением NumPy.\n",
        "- **`.dtypes`** — `Series`, показывающий тип данных каждого столбца. Тип `object` часто указывает на строки или смешанные типы — это сигнал к проверке данных.\n",
        "- **`.index`** — метки строк (`Index`-объект).\n",
        "- **`.columns`** — метки столбцов (`Index`-объект).\n",
        "\n",
        "> **Пример: доступ к атрибутам**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'ID': [101, 102, 103, 104, 105],\n",
        "    'Value': [10.5, 20.1, 30.7, 40.2, 50.8],\n",
        "    'Category': ['A', 'B', 'A', 'C', 'B']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data, index=['r1', 'r2', 'r3', 'r4', 'r5'])\n",
        "\n",
        "print(\"Форма (shape):\", df.shape)  # (5, 3)\n",
        "print(\"\\nТипы данных (dtypes):\\n\", df.dtypes)\n",
        "# ID           int64\n",
        "# Value      float64\n",
        "# Category    object\n",
        "\n",
        "print(\"\\nИндекс строк:\", df.index)\n",
        "# Index(['r1', 'r2', 'r3', 'r4', 'r5'], dtype='object')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Анализ `.dtypes` помогает выявить неоптимальное хранение (например, числа как `object`) и спланировать преобразования.\n",
        "\n",
        "---\n",
        "\n",
        "### I.3. Создание объектов из различных источников\n",
        "\n",
        "Pandas поддерживает гибкое создание структур из Python-объектов.\n",
        "\n",
        "#### 1. Из словаря списков\n",
        "\n",
        "Наиболее распространённый способ: ключи → имена столбцов, значения → данные.\n",
        "\n",
        "```python\n",
        "dict_data = {\n",
        "    'City': ['Moscow', 'Kazan', 'Saint Petersburg'],\n",
        "    'Population': [12600000, 1270000, 5400000]\n",
        "}\n",
        "df_dict = pd.DataFrame(dict_data)\n",
        "print(\"Из словаря списков:\\n\", df_dict)\n",
        "```\n",
        "\n",
        "#### 2. Из словаря `Series` (демонстрация выравнивания)\n",
        "\n",
        "```python\n",
        "data_series = {\n",
        "    'Col_A': pd.Series([10, 20], index=['x', 'y']),\n",
        "    'Col_B': pd.Series([100, 200, 300], index=['y', 'z', 'x'])\n",
        "}\n",
        "df_aligned = pd.DataFrame(data_series)\n",
        "print(\"Выравнивание Series:\\n\", df_aligned)\n",
        "# Вывод:\n",
        "#    Col_A  Col_B\n",
        "# x   10.0  300.0\n",
        "# y   20.0  100.0\n",
        "# z    NaN  200.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Pandas автоматически объединил все уникальные метки (`x`, `y`, `z`) и вставил `NaN` там, где данных нет.\n",
        "\n",
        "#### 3. Из NumPy-массива\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "numpy_array = np.array([[1, 2, 3],\n",
        "                        [4, 5, 6],\n",
        "                        [7, 8, 9]])\n",
        "\n",
        "df_numpy = pd.DataFrame(\n",
        "    numpy_array,\n",
        "    index=['r1', 'r2', 'r3'],\n",
        "    columns=['c1', 'c2', 'c3']\n",
        ")\n",
        "print(\"Из NumPy-массива:\\n\", df_numpy)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без явного указания `index` и `columns` будут использованы целочисленные метки по умолчанию.\n",
        "\n",
        "---\n",
        "\n",
        "## II. Операции ввода-вывода (I/O) и оптимизация загрузки\n",
        "\n",
        "Эффективность анализа данных начинается с **быстрой и корректной загрузки**. Pandas предлагает мощные инструменты для работы с CSV, Excel, JSON, Parquet и другими форматами.\n",
        "\n",
        "### II.1. Загрузка данных: `pd.read_csv()`\n",
        "\n",
        "Функция `pd.read_csv()` — основной инструмент для чтения табличных данных. Она поддерживает локальные файлы, URL (`http`, `s3`, `gs`), а также потоки (`StringIO`).\n",
        "\n",
        "#### Ключевые параметры для оптимизации:\n",
        "\n",
        "| Параметр | Назначение | Зачем это важно |\n",
        "|--------|-----------|----------------|\n",
        "| `dtype` | Явное указание типов данных | Предотвращает ошибки угадывания (`object` вместо `int`), экономит память и ускоряет операции |\n",
        "| `usecols` | Загрузка только нужных столбцов | Снижает потребление памяти и время парсинга в разы |\n",
        "| `index_col` | Назначение столбца(ов) как индекса | Упрощает последующий анализ и фильтрацию |\n",
        "| `parse_dates` + `date_format` | Парсинг дат с явным форматом | Ускоряет обработку временных меток и избегает неоднозначности |\n",
        "\n",
        "> **Пример: оптимизированная загрузка**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Предположим, файл содержит: Timestamp (строка), Cost (целое), Description (текст), Region (категория)\n",
        "file_path = 'data/large_data.csv'\n",
        "\n",
        "data_types = {\n",
        "    'Cost': 'int32',          # меньше памяти, чем int64\n",
        "    'Region': 'category'      # идеально для строк с небольшим числом уникальных значений\n",
        "}\n",
        "\n",
        "df_optimized = pd.read_csv(\n",
        "    file_path,\n",
        "    usecols=['Timestamp', 'Cost', 'Region'],  # только нужное\n",
        "    dtype=data_types,\n",
        "    parse_dates=['Timestamp'],                # преобразуем в datetime\n",
        "    index_col='Timestamp'                     # делаем индексом\n",
        ")\n",
        "\n",
        "print(\"Типы после загрузки:\\n\", df_optimized.dtypes)\n",
        "# Timestamp    datetime64[ns]\n",
        "# Cost                  int32\n",
        "# Region             category\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `'category'` для `Region` может сократить потребление памяти в 10–100 раз по сравнению с `'object'`.\n",
        "\n",
        "---\n",
        "\n",
        "### II.2. Сохранение данных: `DataFrame.to_csv()`\n",
        "\n",
        "Эта функция завершает этап **Load** в ETL-процессе.\n",
        "\n",
        "#### Ключевые параметры:\n",
        "\n",
        "- **`index=False`** — не сохранять индекс (если он просто `0,1,2,...`).\n",
        "- **`compression='gzip'`** — сжимать «на лету» (поддержка `infer` по расширению: `.csv.gz`).\n",
        "- **`date_format='%Y-%m-%d'`** — контролировать формат дат при экспорте.\n",
        "\n",
        "> **Пример: сохранение с оптимизацией**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Создаём данные с датами\n",
        "dates = pd.date_range('2023-01-01', periods=5)\n",
        "df_result = pd.DataFrame({'Sales': [100, 150, 200, 180, 220]}, index=dates)\n",
        "df_result.index.name = 'Date'\n",
        "\n",
        "# Сохраняем в сжатый CSV\n",
        "df_result.to_csv(\n",
        "    'results/sales_summary.csv.gz',\n",
        "    index=True,               # сохраняем даты как индекс\n",
        "    date_format='%Y-%m-%d',\n",
        "    compression='gzip'\n",
        ")\n",
        "\n",
        "print(\"Данные сохранены в сжатый файл: results/sales_summary.csv.gz\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Сжатие особенно важно при работе с большими объёмами данных — файлы могут быть в 3–10 раз меньше.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas превращает неструктурированные и полуобработанные данные в **аналитически пригодные структуры**, обеспечивая надёжность через выравнивание, гибкость через метки и производительность через интеграцию с NumPy. Освоение базовых структур и оптимизированного I/O — первый шаг к построению промышленных конвейеров обработки данных.\n"
      ],
      "metadata": {
        "id": "-Vtjc_dPiDTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## III. Индексирование и селекция данных\n",
        "\n",
        "Эффективная выборка и модификация данных — основа любого преобразования в Pandas. Библиотека предоставляет **специализированные аксессоры**, оптимизированные под разные сценарии: от массовой фильтрации до сверхбыстрого доступа к отдельным ячейкам.\n",
        "\n",
        "### III.1. Выборка по меткам и позициям: `loc` и `iloc`\n",
        "\n",
        "#### `loc` — индексирование по **меткам**\n",
        "\n",
        "`df.loc[rows, cols]` выбирает данные **исключительно по именам строк и столбцов**.  \n",
        "\n",
        "**Важная особенность:** при использовании срезов (`:`) с метками **конечная метка включается** в результат (инклюзивное срезание).\n",
        "\n",
        "> **Пример: `loc` с инклюзивным срезом**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    np.arange(12).reshape(3, 4),\n",
        "    index=['r_a', 'r_b', 'r_c'],\n",
        "    columns=['c1', 'c2', 'c3', 'c4']\n",
        ")\n",
        "\n",
        "# Выбираем строки от 'r_a' до 'r_c' (включительно) и столбцы от 'c2' до 'c4' (включительно)\n",
        "result_loc = df.loc['r_a':'r_c', 'c2':'c4']\n",
        "print(\"Селекция через .loc (инклюзивно):\\n\", result_loc)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "     c2  c3  c4\n",
        "r_a   1   2   3\n",
        "r_b   5   6   7\n",
        "r_c   9  10  11\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такое поведение интуитивно для аналитиков: «от A до C» обычно включает и C.\n",
        "\n",
        "---\n",
        "\n",
        "#### `iloc` — индексирование по **целочисленным позициям**\n",
        "\n",
        "`df.iloc[rows, cols]` работает **только с целыми индексами** (0, 1, 2, ...), как в NumPy.  \n",
        "\n",
        "**Семантика срезов стандартная:** конечный индекс **не включается** (эксклюзивное срезание).\n",
        "\n",
        "> **Пример: `iloc` с эксклюзивным срезом**\n",
        "\n",
        "```python\n",
        "# Те же данные\n",
        "result_iloc = df.iloc[0:2, 1:3]  # строки 0–1, столбцы 1–2\n",
        "print(\"\\nСелекция через .iloc (эксклюзивно):\\n\", result_iloc)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "     c2  c3\n",
        "r_a   1   2\n",
        "r_b   5   6\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это важно помнить при переходе от меток к позициям — границы ведут себя по-разному.\n",
        "\n",
        "---\n",
        "\n",
        "### III.2. Оптимизированный скалярный доступ: `at` и `iat`\n",
        "\n",
        "Для **чтения или записи одного значения** используйте `at` (по метке) и `iat` (по позиции). Они **значительно быстрее**, чем `loc`/`iloc`, так как не создают промежуточных объектов.\n",
        "\n",
        "> **Пример: высокоскоростной доступ к ячейкам**\n",
        "\n",
        "```python\n",
        "# Изменяем значение по метке\n",
        "df.at['r_b', 'c3'] = 999\n",
        "value_at = df.at['r_b', 'c3']\n",
        "\n",
        "# Изменяем значение по позиции: строка 2 ('r_c'), столбец 3 ('c4')\n",
        "df.iat[2, 3] = 1000\n",
        "value_iat = df.iat[2, 3]\n",
        "\n",
        "print(f\"\\nЗначение в [r_b, c3] после .at: {value_at}\")\n",
        "print(f\"Значение в [r_c, c4] после .iat: {value_iat}\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти методы критичны в редких случаях, когда без итерации не обойтись (например, в сложной логике с зависимыми условиями). Однако **векторизация всегда предпочтительнее**.\n",
        "\n",
        "---\n",
        "\n",
        "### III.3. Булево индексирование и логические операторы\n",
        "\n",
        "Фильтрация по условиям выполняется через **булевы маски**. При объединении условий **обязательно используйте побитовые операторы**:\n",
        "\n",
        "- `&` вместо `and`\n",
        "- `|` вместо `or`\n",
        "- `~` вместо `not`\n",
        "\n",
        "> **Пример: сложная фильтрация**\n",
        "\n",
        "```python\n",
        "data_bool = {\n",
        "    'Age': [25, 35, 45, 65],\n",
        "    'Salary': [35000, 75000, 85000, 95000]\n",
        "}\n",
        "df_cond = pd.DataFrame(data_bool)\n",
        "\n",
        "# Условие: Возраст > 30 ИЛИ (Зарплата < 80000 И возраст ≤ 60)\n",
        "mask = (df_cond['Age'] > 30) | ((df_cond['Salary'] < 80000) & ~(df_cond['Age'] > 60))\n",
        "\n",
        "filtered = df_cond.loc[mask]\n",
        "print(\"\\nФильтрация через булеву маску:\\n\", filtered)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Age  Salary\n",
        "1   35   75000\n",
        "2   45   85000\n",
        "```\n",
        "\n",
        "> *Пояснение:* Скобки **обязательны** из-за приоритета операторов: `&` имеет более высокий приоритет, чем `|`.\n",
        "\n",
        "---\n",
        "\n",
        "### III.4. Высокопроизводительный запрос: метод `query()`\n",
        "\n",
        "Метод `df.query('условие')` позволяет писать фильтры в виде **строковых выражений**, используя синтаксис, близкий к SQL.\n",
        "\n",
        "#### Преимущества:\n",
        "- Использует библиотеку **NumExpr**, которая вычисляет выражения в C — без участия интерпретатора Python.\n",
        "- **Значительно быстрее** при работе с большими DataFrame (обычно > 50 000 строк).\n",
        "- Поддерживает **внешние переменные** через `@`.\n",
        "\n",
        "#### Синтаксис:\n",
        "- Столбцы с пробелами: `` `Column Name` ``\n",
        "- Внешние переменные: `@var_name`\n",
        "\n",
        "> **Пример: использование `query()`**\n",
        "\n",
        "```python\n",
        "target_age = 30\n",
        "target_salary = 90000\n",
        "\n",
        "# Фильтрация с внешними переменными\n",
        "result_query = df_cond.query('Age > @target_age and Salary < @target_salary')\n",
        "print(\"\\nФильтрация через .query():\\n\", result_query)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Age  Salary\n",
        "1   35   75000\n",
        "2   45   85000\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для небольших данных `query()` может быть **медленнее** из-за накладных расходов на парсинг строки. Используйте его осознанно.\n",
        "\n",
        "---\n",
        "\n",
        "#### Сравнительная таблица методов доступа\n",
        "\n",
        "| Метод      | Основа          | Возвращаемое значение     | Скорость (скаляр) | Основное применение |\n",
        "|-----------|------------------|----------------------------|-------------------|----------------------|\n",
        "| `.loc`    | Метка            | Series / DataFrame / Scalar| Умеренная         | Фильтрация по именам, диапазоны (включая конец) |\n",
        "| `.iloc`   | Позиция          | Series / DataFrame / Scalar| Умеренная         | Селекция по индексу (исключая конец) |\n",
        "| `.at`     | Метка            | **Скаляр**                 | **Высокая**       | Быстрое чтение/запись одной ячейки |\n",
        "| `.iat`    | Позиция          | **Скаляр**                 | **Высокая**       | То же по позиции |\n",
        "| `.query()`| Строка-выражение | DataFrame                  | Высокая (на больших данных) | Читаемые, сложные фильтры |\n",
        "\n",
        "---\n",
        "\n",
        "## IV. Методы очистки и подготовки данных\n",
        "\n",
        "Работа с пропущенными значениями — обязательный этап ETL. В Pandas отсутствующие данные обозначаются как **`np.nan`** (или `pd.NA` для новых nullable-типов). Стратегии обработки делятся на три категории.\n",
        "\n",
        "### IV.1. Теоретические основы обработки пропусков\n",
        "\n",
        "1. **Удаление (`dropna`)** — простой, но радикальный метод. Оправдан при малой доле пропусков.\n",
        "2. **Вменение (`fillna`)** — замена на константу, среднее, медиану. Сохраняет объём данных.\n",
        "3. **Интерполяция (`interpolate`)** — оценка пропусков на основе соседей. Идеальна для временных рядов.\n",
        "\n",
        "> **Выбор стратегии зависит от:**\n",
        "> - природы данных,\n",
        "> - доли пропусков,\n",
        "> - наличия структуры (например, временной упорядоченности).\n",
        "\n",
        "---\n",
        "\n",
        "### IV.2. Идентификация и удаление пропусков\n",
        "\n",
        "- **`isna()` / `notna()`** — возвращают булеву маску.\n",
        "- **`dropna()`** — удаляет строки/столбцы с пропусками.\n",
        "\n",
        "Параметры:\n",
        "- `axis=0` — удалять строки (по умолчанию), `axis=1` — столбцы.\n",
        "- `how='any'` — удалить при **любом** `NaN`; `how='all'` — только если **все** значения `NaN`.\n",
        "\n",
        "> **Пример: работа с пропусками**\n",
        "\n",
        "```python\n",
        "df_na = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [5, np.nan, np.nan, 8],\n",
        "    'C': [np.nan, np.nan, np.nan, np.nan]\n",
        "})\n",
        "\n",
        "print(\"Пропуски по столбцам:\\n\", df_na.isna().sum())\n",
        "# A: 1, B: 2, C: 4\n",
        "\n",
        "# Удалить строки с хотя бы одним NaN\n",
        "print(\"\\nПосле dropna(axis=0, how='any'):\\n\", df_na.dropna())\n",
        "\n",
        "# Удалить столбцы с хотя бы одним NaN\n",
        "print(\"\\nПосле dropna(axis=1, how='any'):\\n\", df_na.dropna(axis=1))\n",
        "# Пустой DataFrame — все столбцы содержат NaN\n",
        "```\n",
        "\n",
        "> *Пояснение:* Столбец `C` полностью пуст — его часто удаляют на этапе предварительного анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### IV.3. Заполнение пропусков: `fillna()`\n",
        "\n",
        "#### 1. Скалярные значения и статистика\n",
        "\n",
        "```python\n",
        "# Восстановим исходный df_na\n",
        "df_na = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [5, np.nan, np.nan, 8]\n",
        "})\n",
        "\n",
        "# Замена средним\n",
        "df_na['A'] = df_na['A'].fillna(df_na['A'].mean())\n",
        "\n",
        "# Замена константой\n",
        "df_na = df_na.fillna(0)\n",
        "print(\"После fillna:\\n\", df_na)\n",
        "```\n",
        "\n",
        "#### 2. Пропагация значений (`ffill`, `bfill`)\n",
        "\n",
        "- `method='ffill'` — заполнить предыдущим значением (forward fill).\n",
        "- `method='bfill'` — заполнить следующим значением (backward fill).\n",
        "- `limit` — ограничить количество заполняемых подряд пропусков.\n",
        "\n",
        "> **Пример: пропагация**\n",
        "\n",
        "```python\n",
        "ser = pd.Series([1.0, np.nan, np.nan, 5.0, np.nan, 7.0])\n",
        "\n",
        "# Ffill с лимитом — заполнит только один пропуск\n",
        "ffill_limited = ser.fillna(method='ffill', limit=1)\n",
        "print(\"Ffill (limit=1):\\n\", ffill_limited)\n",
        "# [1.0, 1.0, nan, 5.0, 5.0, 7.0]\n",
        "\n",
        "# Bfill — заполняет в обратном направлении\n",
        "bfill_full = ser.fillna(method='bfill')\n",
        "print(\"\\nBfill:\\n\", bfill_full)\n",
        "# [1.0, 5.0, 5.0, 5.0, 7.0, 7.0]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `limit` предотвращает «размазывание» значения на слишком большой промежуток.\n",
        "\n",
        "---\n",
        "\n",
        "### IV.4. Интерполяция данных: `interpolate()`\n",
        "\n",
        "Интерполяция **оценивает пропущенные значения**, основываясь на соседях. По умолчанию — **линейная**.\n",
        "\n",
        "> **Пример: линейная интерполяция**\n",
        "\n",
        "```python\n",
        "ser_interp = pd.Series([0, 10, np.nan, np.nan, 40, 50])\n",
        "result = ser_interp.interpolate(method='linear')\n",
        "print(\"Линейная интерполяция:\\n\", result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "0     0.0\n",
        "1    10.0\n",
        "2    20.0\n",
        "3    30.0\n",
        "4    40.0\n",
        "5    50.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Особенно полезно для **временных рядов**, **геоданных**, **датчиков**, где данные упорядочены и гладки.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas предоставляет **полный арсенал инструментов** для точечного и массового доступа к данным, а также для гибкой обработки пропусков. Осознанное использование `loc`/`iloc`, `at`/`iat`, `query`, `fillna` и `interpolate` позволяет строить **надёжные, читаемые и производительные** конвейеры очистки и трансформации данных.\n"
      ],
      "metadata": {
        "id": "UPm9PAamjFW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## V. Векторизация, применение пользовательских функций и цепочки вызовов\n",
        "\n",
        "Основа **высокопроизводительного кода в Pandas** — **векторизация**: делегирование вычислений оптимизированным ядрам C/NumPy вместо медленных циклов Python. Однако при необходимости применения пользовательской логики важно выбирать правильный инструмент — от простого `map` до декларативного `assign` и оптимизированного `eval`.\n",
        "\n",
        "---\n",
        "\n",
        "### V.1. Векторизация и применение функций: `map`, `apply`, `applymap`\n",
        "\n",
        "Эти методы различаются по гибкости, производительности и области применения.\n",
        "\n",
        "> **Важно:** все они **медленнее чисто векторизованных операций** (`+`, `np.log`, `str.upper` и др.). Используйте их только тогда, когда векторизация невозможна.\n",
        "\n",
        "#### V.1.1. `Series.map()`\n",
        "\n",
        "Применяется **только к `Series`**, работает поэлементно.  \n",
        "**Идеален для:**\n",
        "- замены значений по словарю (например, кодирование категорий),\n",
        "- простых преобразований через лямбда-функции.\n",
        "\n",
        "> **Ограничение:** не поддерживает передачу дополнительных аргументов.\n",
        "\n",
        "#### V.1.2. `Series.apply()`\n",
        "\n",
        "Более гибкий, чем `map`.  \n",
        "**Позволяет:**\n",
        "- передавать `args` и `kwargs`,\n",
        "- возвращать сложные объекты (например, `Series` → `DataFrame`).\n",
        "\n",
        "#### V.1.3. `DataFrame.apply()`\n",
        "\n",
        "Применяет функцию **к строкам (`axis=1`) или столбцам (`axis=0`)**.\n",
        "\n",
        "- **`axis=0`** (по умолчанию): функция получает каждый **столбец** как `Series` → полезно для статистики.\n",
        "- **`axis=1`**: функция получает каждую **строку** как `Series` → полезно для вычисления признаков из нескольких столбцов.\n",
        "\n",
        "> **Пример: `map` и `apply(axis=1)`**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_func = pd.DataFrame({\n",
        "    'Status_Code': [1, 2, 3, 1],\n",
        "    'Height': [170, 165, 180, 175],   # в см\n",
        "    'Weight': [70, 60, 90, 80]        # в кг\n",
        "})\n",
        "\n",
        "# 1. Замена кодов статусов на метки\n",
        "status_map = {1: 'Active', 2: 'Inactive', 3: 'Pending'}\n",
        "df_func['Status'] = df_func['Status_Code'].map(status_map)\n",
        "\n",
        "# 2. Расчёт BMI построчно\n",
        "def calculate_bmi(row):\n",
        "    height_m = row['Height'] / 100.0\n",
        "    return row['Weight'] / (height_m ** 2)\n",
        "\n",
        "df_func['BMI'] = df_func.apply(calculate_bmi, axis=1)\n",
        "\n",
        "print(\"DataFrame после map() и apply(axis=1):\\n\", df_func)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Status_Code  Height  Weight    Status        BMI\n",
        "0            1     170      70    Active  24.221474\n",
        "1            2     165      60  Inactive  22.038567\n",
        "2            3     180      90   Pending  27.777778\n",
        "3            1     175      80    Active  26.122449\n",
        "```\n",
        "\n",
        "> *Пояснение:* `map` использован для **кодирования**, `apply(axis=1)` — для **расчёта составного признака**. Оба метода создают **новые столбцы**, не изменяя исходные данные.\n",
        "\n",
        "---\n",
        "\n",
        "### V.2. Создание новых признаков: `assign()` и `eval()`\n",
        "\n",
        "#### V.2.1. `DataFrame.assign()` — декларативное создание столбцов\n",
        "\n",
        "Метод `assign()` возвращает **новый DataFrame** с добавленными столбцами. Его главное преимущество — **интеграция в цепочки вызовов** (method chaining), что улучшает читаемость и функциональность кода.\n",
        "\n",
        "```python\n",
        "df_new = df_func.assign(\n",
        "    BMI_rounded = lambda x: x['BMI'].round(1),\n",
        "    Is_Overweight = lambda x: x['BMI'] > 25\n",
        ")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `lambda` позволяет ссылаться на **уже существующие столбцы** в том же вызове `assign`.\n",
        "\n",
        "#### V.2.2. `pandas.eval()` — высокоскоростные вычисления через строку\n",
        "\n",
        "Функция `pd.eval()` использует движок **NumExpr** для выполнения арифметических и логических выражений **в C-слое**, минимизируя overhead Python.\n",
        "\n",
        "> **Когда использовать?**  \n",
        "> Только для **очень больших DataFrame** (обычно > 100 000 строк). Для малых данных накладные расходы на парсинг строки перевешивают выгоду.\n",
        "\n",
        "> **Пример: `eval` с множественными выражениями**\n",
        "\n",
        "```python\n",
        "N = 500_000\n",
        "df_large = pd.DataFrame(\n",
        "    np.random.randint(1, 100, size=(N, 3)),\n",
        "    columns=['A', 'B', 'C']\n",
        ")\n",
        "\n",
        "# Два новых признака за один вызов\n",
        "df_large.eval(\n",
        "    \"D = (A + B) / C; E = A * B - C\",\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "print(\"После eval (первые 3 строки):\\n\", df_large.head(3))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Разделение выражений точкой с запятой позволяет выполнить **несколько операций без промежуточных копий** — это ключ к максимальной производительности.\n",
        "\n",
        "---\n",
        "\n",
        "### V.3. Построение конвейеров: `pipe()`\n",
        "\n",
        "Метод `pipe()` позволяет строить **читаемые, последовательные конвейеры**, где результат предыдущей функции передаётся как первый аргумент в следующую.\n",
        "\n",
        "> **Преимущества:**\n",
        "> - Избегает вложенности: `f(g(h(df)))` → `df.pipe(h).pipe(g).pipe(f)`\n",
        "> - Поддерживает любые пользовательские функции\n",
        "> - Упрощает отладку и тестирование\n",
        "\n",
        "> **Пример: ETL-конвейер через `pipe()`**\n",
        "\n",
        "```python\n",
        "def filter_high_value(df, threshold):\n",
        "    return df[df['Sales'] > threshold]\n",
        "\n",
        "def add_bonus(df, rate):\n",
        "    return df.assign(Bonus=df['Sales'] * rate)\n",
        "\n",
        "data_pipe = pd.DataFrame({\n",
        "    'Agent': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'Sales': [800, 1200, 950, 1500],\n",
        "    'Region': ['North', 'South', 'North', 'South']\n",
        "})\n",
        "\n",
        "df_transformed = (\n",
        "    data_pipe\n",
        "    .pipe(filter_high_value, threshold=1000)\n",
        "    .pipe(add_bonus, rate=0.1)\n",
        "    .sort_values('Sales', ascending=False)\n",
        ")\n",
        "\n",
        "print(\"Результат конвейера через .pipe():\\n\", df_transformed)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "    Agent  Sales Region   Bonus\n",
        "3   Diana   1500  South   150.0\n",
        "1     Bob   1200  South   120.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* `pipe()` — это **синтаксический сахар**, не дающий прироста скорости. Производительность зависит от **векторизации внутри функций**.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Группировка, агрегация и реструктуризация\n",
        "\n",
        "Эти инструменты позволяют выполнять **сводный анализ**, обогащать данные и приводить их к нужному формату для моделирования или визуализации.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.1. Принцип Split-Apply-Combine: объект `GroupBy`\n",
        "\n",
        "Pandas реализует классическую парадигму:\n",
        "\n",
        "1. **Split** — разбиение на группы по одному или нескольким ключам.\n",
        "2. **Apply** — применение функции к каждой группе (агрегация, трансформация, фильтрация).\n",
        "3. **Combine** — сбор результатов в единый объект.\n",
        "\n",
        "> **Создание группы:** `df.groupby('column')` или `df.groupby(['col1', 'col2'])`\n",
        "\n",
        "---\n",
        "\n",
        "### VI.2. Агрегация данных: `agg()`\n",
        "\n",
        "Метод `agg()` (или `aggregate()`) вычисляет сводную статистику **по группам** и **возвращает результат меньшей размерности**.\n",
        "\n",
        "> **Пример: множественная агрегация по столбцам**\n",
        "\n",
        "```python\n",
        "df_group = pd.DataFrame({\n",
        "    'Department': ['HR', 'IT', 'IT', 'Sales', 'HR', 'IT'],\n",
        "    'Salary': [45000, 70000, 80000, 75000, 50000, 70000],\n",
        "    'Experience': [2, 5, 10, 6, 3, 7]\n",
        "})\n",
        "\n",
        "summary = df_group.groupby('Department').agg({\n",
        "    'Salary': ['mean', 'median'],\n",
        "    'Experience': ['sum', 'max']\n",
        "})\n",
        "\n",
        "print(\"Множественная агрегация:\\n\", summary)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "           Salary            Experience      \n",
        "             mean   median        sum max\n",
        "Department                                 \n",
        "HR        47500.0   47500.0          5   3\n",
        "IT        73333.3   70000.0         22  10\n",
        "Sales     75000.0   75000.0          6   6\n",
        "```\n",
        "\n",
        "> *Пояснение:* Результат имеет **MultiIndex в столбцах**, что позволяет точно идентифицировать каждую агрегацию.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.3. Трансформация данных: `transform()`\n",
        "\n",
        "`transform()` применяет функцию к группе, но **возвращает объект той же формы и индекса**, что и исходный DataFrame.\n",
        "\n",
        "> **Сценарий:** добавление групповой статистики к каждой строке **без слияния**.\n",
        "\n",
        "> **Пример: нормализация внутри групп**\n",
        "\n",
        "```python\n",
        "# Z-оценка зарплаты внутри отдела\n",
        "df_group['Salary_Z'] = df_group.groupby('Department')['Salary'].transform(\n",
        "    lambda x: (x - x.mean()) / x.std(ddof=1)\n",
        ")\n",
        "\n",
        "print(\"Трансформация (Z-оценка):\\n\", df_group[['Department', 'Salary', 'Salary_Z']])\n",
        "```\n",
        "\n",
        "> *Пояснение:* `transform` — это «невидимая сила» ETL: он обогащает данные, сохраняя их структуру.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.4. Фильтрация групп: `filter()`\n",
        "\n",
        "Метод `filter()` удаляет **целые группы**, если они не удовлетворяют условию.\n",
        "\n",
        "> **Условие:** функция должна возвращать **одно булево значение** на группу.\n",
        "\n",
        "> **Пример: оставить только отделы с >2 сотрудниками**\n",
        "\n",
        "```python\n",
        "df_filtered = df_group.groupby('Department').filter(lambda x: len(x) > 2)\n",
        "print(\"После фильтрации групп:\\n\", df_filtered['Department'].unique())  # ['HR' 'IT']\n",
        "```\n",
        "\n",
        "> *Пояснение:* Отдел `Sales` (1 сотрудник) исключён.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.5. Объединение таблиц\n",
        "\n",
        "#### VI.5.1. `pd.merge()` — реляционные соединения\n",
        "\n",
        "Аналог **SQL JOIN**. Ключевые параметры:\n",
        "- `on` — столбец-ключ,\n",
        "- `how` — тип соединения (`inner`, `left`, `right`, `outer`).\n",
        "\n",
        "> **Пример: LEFT и INNER JOIN**\n",
        "\n",
        "```python\n",
        "df_left = pd.DataFrame({\n",
        "    'Key': ['K0', 'K1', 'K2', 'K3'],\n",
        "    'A': ['A0', 'A1', 'A2', 'A3']\n",
        "})\n",
        "\n",
        "df_right = pd.DataFrame({\n",
        "    'Key': ['K1', 'K3', 'K4', 'K5'],\n",
        "    'B': ['B1', 'B3', 'B4', 'B5']\n",
        "})\n",
        "\n",
        "left_join = pd.merge(df_left, df_right, on='Key', how='left')\n",
        "inner_join = pd.merge(df_left, df_right, on='Key', how='inner')\n",
        "\n",
        "print(\"LEFT JOIN:\\n\", left_join)\n",
        "print(\"\\nINNER JOIN:\\n\", inner_join)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `left` сохраняет все строки из левой таблицы, `inner` — только совпадающие.\n",
        "\n",
        "#### VI.5.2. `pd.concat()` — конкатенация\n",
        "\n",
        "«Склеивает» объекты вдоль оси:\n",
        "- `axis=0` — вертикально (добавление строк),\n",
        "- `axis=1` — горизонтально (добавление столбцов).\n",
        "\n",
        "> **Параметр `join`:**\n",
        "> - `'outer'` (по умолчанию) — объединение индексов,\n",
        "> - `'inner'` — пересечение.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.6. Изменение формы данных\n",
        "\n",
        "#### Wide vs. Long Format\n",
        "\n",
        "- **Wide**: одна строка = одно наблюдение, переменные — отдельные столбцы.\n",
        "- **Long**: одна строка = одно измерение, переменные и значения — в двух столбцах.\n",
        "\n",
        "> **Long предпочтителен** для `groupby`, `seaborn`, `plotly`.\n",
        "\n",
        "#### `melt()` — Wide → Long\n",
        "\n",
        "```python\n",
        "df_wide = pd.DataFrame({\n",
        "    'ID': [1, 2],\n",
        "    'Score_Math': [90, 85],\n",
        "    'Score_Science': [88, 92]\n",
        "})\n",
        "\n",
        "df_long = df_wide.melt(\n",
        "    id_vars='ID',\n",
        "    var_name='Subject',\n",
        "    value_name='Score'\n",
        ")\n",
        "print(\"Wide → Long:\\n\", df_long)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   ID        Subject  Score\n",
        "0   1     Score_Math     90\n",
        "1   2     Score_Math     85\n",
        "2   1  Score_Science     88\n",
        "3   2  Score_Science     92\n",
        "```\n",
        "\n",
        "#### `pivot_table()` — Long → Wide\n",
        "\n",
        "```python\n",
        "df_pivot = df_long.pivot_table(\n",
        "    index='ID',\n",
        "    columns='Subject',\n",
        "    values='Score',\n",
        "    aggfunc='mean'  # обработка дубликатов\n",
        ")\n",
        "print(\"Long → Wide:\\n\", df_pivot)\n",
        "```\n",
        "\n",
        "#### `stack()` / `unstack()` — работа с MultiIndex\n",
        "\n",
        "- `unstack()`: переносит уровень индекса в столбцы.\n",
        "- `stack()`: обратная операция.\n",
        "\n",
        "> **Пример: unstack**\n",
        "\n",
        "```python\n",
        "index = pd.MultiIndex.from_tuples(\n",
        "    [('X', 'a'), ('X', 'b'), ('Y', 'a'), ('Y', 'b')],\n",
        "    names=['Level1', 'Level2']\n",
        ")\n",
        "df_multi = pd.DataFrame({'Data': [1, 2, 3, 4]}, index=index)\n",
        "\n",
        "df_unstacked = df_multi.unstack('Level2')\n",
        "print(\"Unstacked:\\n\", df_unstacked)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas предоставляет мощный и гибкий инструментарий для **анализа, трансформации и реструктуризации** данных. Освоение `groupby`, `merge`, `melt`, `assign` и `pipe` позволяет строить **промышленные ETL-конвейеры**, сочетающие читаемость, производительность и надёжность.\n"
      ],
      "metadata": {
        "id": "Ui7lqQuLj8UL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## VII. Работа с временными рядами и категориальными данными\n",
        "\n",
        "Работа с **временными рядами** и оптимизация памяти через **категориальные типы** — ключевые навыки для анализа больших и структурированных данных. Pandas предоставляет специализированные инструменты, превращающие эти задачи из «проблем» в «возможности».\n",
        "\n",
        "---\n",
        "\n",
        "### VII.1. Обработка временных данных: `pd.to_datetime()`\n",
        "\n",
        "Функция `pd.to_datetime()` преобразует строки, целые числа (Unix timestamp) или другие представления в объекты типа **`datetime64[ns]`** — основу всей временной аналитики в Pandas.\n",
        "\n",
        "> **Критически важно:** при работе с большими файлами всегда указывайте **`format`** явно. Это отключает медленный механизм «угадывания» формата и ускоряет парсинг в **10–100 раз**.\n",
        "\n",
        "> **Пример: безопасная конвертация дат**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "date_strings = ['2023/10/25', '2023/10/26', '2023/10/27']\n",
        "ser_dates = pd.Series(date_strings)\n",
        "\n",
        "# Явное указание формата — быстро и надёжно\n",
        "dates_dti = pd.to_datetime(ser_dates, format='%Y/%m/%d')\n",
        "print(\"Конвертация в DatetimeIndex:\\n\", dates_dti)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без `format` Pandas пытается проанализировать каждую строку — это недопустимо при загрузке миллионов записей.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.2. Передискретизация (Resampling): `DataFrame.resample()`\n",
        "\n",
        "Метод `resample()` изменяет **частоту временного ряда**, требуя **`DatetimeIndex`** в качестве индекса.\n",
        "\n",
        "- **Downsampling** (понижение частоты): `D → M` → требует **агрегации** (`sum`, `mean`).\n",
        "- **Upsampling** (повышение частоты): `M → D` → требует **заполнения** (`fillna`, `interpolate`).\n",
        "\n",
        "> **Пример: ежедневные данные → еженедельная сумма**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Ежедневный временной ряд\n",
        "daily_index = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n",
        "ts_daily = pd.Series(np.random.randint(10, 50, size=10), index=daily_index)\n",
        "\n",
        "# Downsample: сумма за неделю\n",
        "weekly_sum = ts_daily.resample('W').sum()\n",
        "print(\"\\nЕженедельная сумма:\\n\", weekly_sum)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `resample` возвращает объект `Resampler`, к которому можно применять любые агрегационные функции.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.3. Оконные функции: `DataFrame.rolling()`\n",
        "\n",
        "Метод `rolling(window=N)` создаёт **скользящее окно** фиксированного размера для вычисления локальных статистик.\n",
        "\n",
        "> **Пример: 3-дневное скользящее среднее**\n",
        "\n",
        "```python\n",
        "values = [10, 20, 30, 40, 50]\n",
        "df_ts = pd.DataFrame({'Value': values})\n",
        "\n",
        "# Скользящее среднее (первые 2 значения — NaN)\n",
        "df_ts['Rolling_Mean'] = df_ts['Value'].rolling(window=3).mean()\n",
        "print(\"\\nСкользящее среднее:\\n\", df_ts)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Value  Rolling_Mean\n",
        "0     10           NaN\n",
        "1     20           NaN\n",
        "2     30          20.0\n",
        "3     40          30.0\n",
        "4     50          40.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Скользящие окна — основа технического анализа, сглаживания шума и выявления трендов.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.4. Оптимизация памяти: тип данных `category`\n",
        "\n",
        "Столбцы с **низкой кардинальностью** (мало уникальных значений) — идеальные кандидаты на конвертацию в `category`.\n",
        "\n",
        "> **Как это работает?**  \n",
        "> Вместо хранения строк «HR», «IT», «HR»... Pandas хранит:\n",
        "> - словарь: `{0: 'HR', 1: 'IT'}`,\n",
        "> - массив целых: `[0, 1, 0, ...]`.\n",
        "\n",
        "> **Эффект:** сокращение памяти **в 10–20 раз**.\n",
        "\n",
        "> **Пример: сравнение потребления памяти**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Создаём 100 000 строк с 3 категориями\n",
        "np.random.seed(42)\n",
        "df_mem = pd.DataFrame({\n",
        "    'Category': np.random.choice(['HR', 'IT', 'Sales'], size=100_000),\n",
        "    'Values': np.random.randint(1, 100, size=100_000)\n",
        "})\n",
        "\n",
        "print(\"Память до (object):\")\n",
        "print(df_mem.memory_usage(deep=True) // 1024)  # в КБ\n",
        "\n",
        "# Конвертация в category\n",
        "df_mem['Category'] = df_mem['Category'].astype('category')\n",
        "\n",
        "print(\"\\nПамять после (category):\")\n",
        "print(df_mem.memory_usage(deep=True) // 1024)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это один из самых простых и эффективных способов **масштабировать Pandas** на большие данные.\n",
        "\n",
        "---\n",
        "\n",
        "## VIII. Комплексная аналитика и оптимизация производительности\n",
        "\n",
        "Профессиональный анализ требует **интеграции всех инструментов** в единый, производительный и читаемый конвейер.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.1. Пример ETL-конвейера на основе Pandas\n",
        "\n",
        "> **Сценарий:** анализ журнала продаж — от загрузки до агрегации.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. EXTRACTION ---\n",
        "df_raw = pd.DataFrame({\n",
        "    'Order_ID': [101, 102, 103, 104, 105],\n",
        "    'Date_Str': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n",
        "    'Amount': [100.5, np.nan, 250.0, 150.0, 300.5],\n",
        "    'Client_Type': ['New', 'Old', 'New', 'Old', 'New']\n",
        "})\n",
        "\n",
        "# Преобразуем даты\n",
        "df_raw['Date'] = pd.to_datetime(df_raw['Date_Str'], format='%Y-%m-%d')\n",
        "df_raw.drop(columns=['Date_Str'], inplace=True)\n",
        "\n",
        "# --- 2. TRANSFORMATION ---\n",
        "# 2.1. Заполнение пропусков\n",
        "df_raw['Amount'].fillna(df_raw['Amount'].median(), inplace=True)\n",
        "\n",
        "# 2.2. Извлечение признаков из даты\n",
        "df_raw['DayOfWeek'] = df_raw['Date'].dt.dayofweek  # 0=Пн, 6=Вс\n",
        "\n",
        "# 2.3. Агрегация: продажи по дням\n",
        "daily_sales = df_raw.groupby('Date').agg(Total_Amount=('Amount', 'sum'))\n",
        "\n",
        "# 2.4. Обогащение: добавляем дневную сумму к каждой строке\n",
        "df_raw['Daily_Total'] = df_raw.groupby('Date')['Amount'].transform('sum')\n",
        "\n",
        "# --- 3. LOADING ---\n",
        "df_raw.to_csv('results/sales_etl_output.csv', index=False)\n",
        "print(\"ETL завершён. Первые 3 строки:\\n\", df_raw.head(3))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Конвейер демонстрирует **полный цикл**: загрузка → очистка → feature engineering → агрегация → сохранение.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.2. Продвинутая разработка признаков (Feature Engineering)\n",
        "\n",
        "#### 1. Извлечение из дат\n",
        "```python\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5\n",
        "```\n",
        "\n",
        "#### 2. Бинаризация (Binning)\n",
        "```python\n",
        "bins = [0, 100, 500, 1000, np.inf]\n",
        "labels = ['Low', 'Medium', 'High', 'Very High']\n",
        "df['Amount_Category'] = pd.cut(df['Amount'], bins=bins, labels=labels)\n",
        "```\n",
        "\n",
        "#### 3. One-Hot Encoding\n",
        "```python\n",
        "df_encoded = pd.get_dummies(df, columns=['Client_Type'], prefix='Type')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти методы критичны для **подготовки данных к машинному обучению**.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.3. Ускорение пользовательских функций: Numba\n",
        "\n",
        "Когда векторизация невозможна, **Numba** компилирует Python-код в машинные инструкции.\n",
        "\n",
        "> **Пример: ускорение поэлементной функции**\n",
        "\n",
        "```python\n",
        "import numba\n",
        "import numpy as np\n",
        "\n",
        "@numba.vectorize\n",
        "def custom_transform(x):\n",
        "    return np.sqrt(x) if x > 0 else 0.0\n",
        "\n",
        "# Применяем к NumPy-массиву\n",
        "series = pd.Series(np.random.rand(1_000_000) * 1000)\n",
        "result = custom_transform(series.to_numpy())  # возвращает ndarray\n",
        "```\n",
        "\n",
        "> *Пояснение:* Numba работает **только с NumPy-массивами**, не с Pandas-объектами. Передавайте `.to_numpy()`.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.4. Параллельные вычисления: Dask и Swifter\n",
        "\n",
        "- **Dask**: разбивает DataFrame на **partitions**, распределяет вычисления по ядрам/кластеру. API похож на Pandas, но с отложенным выполнением.\n",
        "- **Swifter**: автоматически выбирает между **векторизацией** и **параллелизацией**:\n",
        "  ```python\n",
        "  df['new_col'] = df['col'].swifter.apply(complex_function)\n",
        "  ```\n",
        "\n",
        "> *Пояснение:* Используйте Dask/Swifter, когда данные **не помещаются в память** или когда `apply` слишком медлен.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Pandas — это методология, а не просто библиотека.** Её мощь раскрывается через **стратегический выбор инструментов**:\n",
        "\n",
        "| Уровень оптимизации             | Инструменты                                  | Цель                              |\n",
        "|-------------------------------|---------------------------------------------|-----------------------------------|\n",
        "| **I/O**                       | `dtype`, `usecols`, `parse_dates`           | Быстрая и безопасная загрузка     |\n",
        "| **Память**                    | `category`, `pd.Int64`                      | Снижение потребления RAM          |\n",
        "| **Массовые вычисления**       | `eval()`, `query()`, векторизованные UFuncs | Высокая скорость на больших данных|\n",
        "| **Пользовательская логика**   | `Numba`, `Cython`                           | Ускорение нетривиальных функций   |\n",
        "| **Архитектура кода**          | `pipe()`, `assign()`, `loc`                 | Читаемость, модульность, надёжность |\n",
        "\n",
        "Таким образом, Pandas предоставляет **полную экосистему** для построения промышленных конвейеров обработки данных — от первичной очистки до подготовки данных для машинного обучения. Освоение его принципов позволяет не просто «работать с таблицами», а **строить масштабируемые, воспроизводимые и производительные аналитические системы**.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "2VkX1-FPkkv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# МОДУЛЬ 3: Библиотека Polars — Архитектура высокопроизводительной обработки данных\n",
        "\n",
        "**Polars** — это не просто ещё одна библиотека для работы с табличными данными, а **архитектурная эволюция** аналитических вычислений в Python. В отличие от традиционных инструментов, Polars переносит вычислительную нагрузку за пределы интерпретатора Python, опираясь на современные стандарты памяти и системное программирование. Это позволяет ему достигать **порядков прироста в скорости** и **линейного масштабирования** на многоядерных системах, особенно при работе с большими объёмами данных.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Архитектурные принципы Polars: Фундамент производительности\n",
        "\n",
        "Производительность Polars не является результатом «улучшенной обёртки» вокруг Pandas, а обусловлена **глубинной перестройкой стека**: от языка реализации до модели хранения данных.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1. Двуединый языковой стек: Rust и Apache Arrow\n",
        "\n",
        "#### 1.1.1. Ядро на Rust: скорость без компромиссов\n",
        "\n",
        "Polars полностью написан на **Rust** — языке системного программирования, сочетающем:\n",
        "\n",
        "- **Безопасность памяти** без сборщика мусора,\n",
        "- **Высокую производительность** за счёт компиляции в нативный код,\n",
        "- **Нативную многопоточность** без блокировок.\n",
        "\n",
        "Ключевое преимущество: **обход Global Interpreter Lock (GIL)** Python. В то время как Pandas-операции часто выполняются в одном потоке, ядро Polars **автоматически распределяет работу по всем ядрам CPU**, используя пулы потоков и SIMD-векторизацию. Это обеспечивает **линейное ускорение** при увеличении числа ядер — особенно при агрегациях, фильтрации и трансформациях.\n",
        "\n",
        "#### 1.1.2. Apache Arrow: колоночная память как стандарт\n",
        "\n",
        "Внутреннее представление данных в Polars строится на **Apache Arrow** — отраслевом стандарте для **колоночного хранения данных в оперативной памяти**.\n",
        "\n",
        "> **Почему это важно?**  \n",
        "> Большинство аналитических операций (например, `groupby`, `filter`, `sum`) работают **со столбцами**, а не со строками. Колоночный формат:\n",
        "> - минимизирует промахи кэша CPU,\n",
        "> - позволяет загружать в процессор только нужные данные,\n",
        "> - исключает накладные расходы на упаковку/распаковку объектов.\n",
        "\n",
        "В отличие от Pandas (где `object`-столбцы хранят ссылки на Python-объекты), Arrow хранит **непрерывные блоки однотипных данных**, что делает доступ к ним чрезвычайно быстрым.\n",
        "\n",
        "> **Zero-Copy Interoperability**  \n",
        "> Поскольку Polars строго следует спецификации Arrow, он может **обмениваться данными без копирования** с другими Arrow-совместимыми системами (Apache Spark, DuckDB, PyArrow, Vaex), просто передавая указатели на буферы памяти.\n",
        "\n",
        "> **Важно:** Polars использует **собственную реализацию буферов и вычислений на Rust**, а не обёртку вокруг PyArrow. Это даёт полный контроль над оптимизациями и избегает внешних зависимостей.\n",
        "\n",
        "---\n",
        "\n",
        "### Сравнение архитектур: Polars vs Pandas\n",
        "\n",
        "| Критерий                     | Polars                                      | Pandas (стандартный стек)                |\n",
        "|-----------------------------|---------------------------------------------|------------------------------------------|\n",
        "| **Язык ядра**               | Rust (компилируемый, безопасный)            | Python / C (NumPy)                       |\n",
        "| **Модель памяти**           | Apache Arrow (колоночная, непрерывная)      | NumPy (часто строковая или блочная)      |\n",
        "| **Параллелизм**             | Нативная многопоточность + SIMD             | Преимущественно однопоточный (GIL)       |\n",
        "| **Модель выполнения**       | Eager **и** Lazy (с оптимизатором запросов) | Только Eager                             |\n",
        "| **Типизация**               | Строгая (тип выводится из выражений)        | Гибкая (неявные приведения, `object`)    |\n",
        "| **Стратегия копирования**   | Минимизация (Zero-Copy при совместимости)   | Частые копии (особенно при `copy()` и `fillna`) |\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Параллелизм и распределение нагрузки\n",
        "\n",
        "#### 1.2.1. «Embarrassingly Parallel» по дизайну\n",
        "\n",
        "Polars изначально спроектирован как **лёгкораспараллеливаемая система** (*Embarrassingly Parallel*). Это означает:\n",
        "\n",
        "- Рабочая нагрузка **автоматически делится** между всеми доступными ядрами.\n",
        "- Независимые выражения (например, два новых столбца в `select`) вычисляются **параллельно**.\n",
        "- При `group_by().agg()` каждая группа может обрабатываться **отдельным потоком**.\n",
        "\n",
        "> **Пример:**  \n",
        "> Запрос вида  \n",
        "> ```python\n",
        "> df.select([\n",
        ">     pl.col(\"A\").mean(),\n",
        ">     pl.col(\"B\").std()\n",
        "> ])\n",
        "> ```  \n",
        "> будет выполнен в **двух потоках одновременно**, без участия пользователя.\n",
        "\n",
        "#### 1.2.2. Совместимость с Python multiprocessing\n",
        "\n",
        "Внутренняя многопоточность на Rust накладывает **ограничения на использование `multiprocessing` в Python**:\n",
        "\n",
        "- В Unix-системах метод `fork` (по умолчанию) **копирует состояние всех потоков Rust**, что может привести к **нестабильности**.\n",
        "- **Рекомендация:** при использовании `multiprocessing` всегда устанавливайте контекст `spawn` или `forkserver`:\n",
        "  ```python\n",
        "  import multiprocessing as mp\n",
        "  if __name__ == \"__main__\":\n",
        "      mp.set_start_method(\"spawn\")  # или \"forkserver\"\n",
        "      # ... запуск процессов\n",
        "  ```\n",
        "\n",
        "> *Пояснение:* Это не недостаток, а признак того, что Polars — **независимый вычислительный движок**, а не «тонкая обёртка».\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Модели выполнения запросов: Eager vs Lazy\n",
        "\n",
        "Polars поддерживает два режима работы: **Eager** (немедленное выполнение) и **Lazy** (отложенное с оптимизацией). **Lazy API — предпочтительный подход** для производительного анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1. Концепции Eager и Lazy\n",
        "\n",
        "#### 2.1.1. Eager API — как в Pandas\n",
        "\n",
        "Каждая операция выполняется **сразу**, результат возвращается как `DataFrame`.\n",
        "\n",
        "```python\n",
        "import polars as pl\n",
        "\n",
        "# Eager: данные загружаются немедленно\n",
        "df = pl.read_csv(\"data.csv\")\n",
        "filtered = df.filter(pl.col(\"value\") > 100)\n",
        "```\n",
        "\n",
        "> **Плюсы:** простота, интерактивность.  \n",
        "> **Минусы:** нет глобальной оптимизации, возможны избыточные вычисления.\n",
        "\n",
        "#### 2.1.2. Lazy API — сила оптимизатора\n",
        "\n",
        "Операции строят **логический план запроса** (`LogicalPlan`), но **не выполняются** до вызова `.collect()`.\n",
        "\n",
        "```python\n",
        "# Lazy: создаётся план, данные не загружаются\n",
        "lf = pl.scan_csv(\"large_data.csv\")\n",
        "result = (\n",
        "    lf\n",
        "    .filter(pl.col(\"value\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"price\").mean())\n",
        "    .collect()  # ← запуск выполнения\n",
        ")\n",
        "```\n",
        "\n",
        "> **Преимущество:** движок видит **весь запрос целиком** и может его **оптимизировать глобально**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Оптимизатор запросов Polars\n",
        "\n",
        "Оптимизатор преобразует `LogicalPlan` в **физический план выполнения**, применяя мощные правила:\n",
        "\n",
        "#### Predicate Pushdown (проталкивание фильтров)\n",
        "\n",
        "Фильтры применяются **на этапе чтения данных**, а не после загрузки.  \n",
        "**Результат:** чтение **только релевантных строк** из файла → меньше I/O, меньше памяти.\n",
        "\n",
        "> **До оптимизации:**  \n",
        "> `read → group_by → filter`  \n",
        "> **После:**  \n",
        "> `read + filter → group_by`\n",
        "\n",
        "#### Projection Pushdown (проталкивание проекций)\n",
        "\n",
        "Загружаются **только нужные столбцы**.  \n",
        "Если в запросе используются 3 из 50 столбцов — читаются только эти 3.\n",
        "\n",
        "#### Другие ключевые оптимизации\n",
        "\n",
        "- **Join Ordering** — перестановка соединений для минимизации промежуточных размеров и предотвращения OOM.\n",
        "- **Common Subplan Elimination** — кэширование повторяющихся подзапросов.\n",
        "- **Expression Simplification** — свёртка констант, упрощение логики.\n",
        "- **Type Coercion** — приведение типов к минимально достаточным (например, `Int32` вместо `Int64`).\n",
        "\n",
        "> **Практическое значение:**  \n",
        "> Пользователю **не нужно** вручную оптимизировать порядок операций (например, «фильтровать до сортировки»). Polars делает это **автоматически**.\n",
        "\n",
        "---\n",
        "\n",
        "### Ключевые оптимизации Polars Query Optimizer\n",
        "\n",
        "| Оптимизация                  | Принцип работы                                      | Влияние                              |\n",
        "|-----------------------------|-----------------------------------------------------|--------------------------------------|\n",
        "| **Predicate Pushdown**      | Фильтрация на уровне источника данных               | ↓ I/O, ↓ RAM, ↑ скорость             |\n",
        "| **Projection Pushdown**     | Загрузка только используемых столбцов               | ↓ Потребление памяти                 |\n",
        "| **Join Ordering**           | Выбор порядка JOIN для минимизации промежуточных данных | ↓ Риск OOM, ↑ стабильность         |\n",
        "| **Common Subplan Elimination** | Повторное использование вычисленных подвыражений  | ↓ Избыточные вычисления              |\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Polars переосмысливает обработку данных, заменяя «интерпретируемую» модель Pandas на **компилируемый, колоночный, многопоточный движок**. Его архитектура — ответ на вызовы современной аналитики: **скорость**, **масштабируемость** и **эффективность памяти**. Освоение Lazy API и понимание оптимизаций — ключ к раскрытию всего потенциала библиотеки.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "jG65vH4wlAg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Система выражений (DSL) Polars\n",
        "\n",
        "Сердце Polars — это **декларативный предметно-ориентированный язык (DSL)** на основе объектов типа `pl.Expr`. Выражения не просто трансформируют данные — они описывают **логический план вычислений**, который движок Polars анализирует, оптимизирует и выполняет параллельно в Rust.\n",
        "\n",
        "### 3.1. Выражения как строительные блоки\n",
        "\n",
        "Каждое выражение:\n",
        "- принимает **столбец (`Series`)** на входе,\n",
        "- возвращает **новый столбец** на выходе,\n",
        "- является **компонуемым**: можно строить цепочки без промежуточных копий.\n",
        "\n",
        "> **Пример: цепочка трансформаций**\n",
        "\n",
        "```python\n",
        "import polars as pl\n",
        "\n",
        "expr = (\n",
        "    pl.col(\"Revenue\")\n",
        "    .mul(100)                  # Умножить на 100\n",
        "    .log()                     # Натуральный логарифм\n",
        "    .alias(\"Scaled_Log_Revenue\")  # Переименовать\n",
        ")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эта конструкция **не вычисляется сразу**. Она становится частью логического плана, который будет оптимизирован и выполнен позже.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Сложные агрегации в `group_by().agg()`\n",
        "\n",
        "Выражения позволяют встраивать **фильтрацию, сортировку и даже подзапросы** непосредственно в агрегацию — без пользовательских функций (UDF).\n",
        "\n",
        "> **Пример: условная агрегация по группе**\n",
        "\n",
        "```python\n",
        "df = pl.DataFrame({\n",
        "    \"ID\": [1, 1, 2, 2],\n",
        "    \"Val\": [10, 15, 18, 20],\n",
        "    \"Flag\": [5, 12, 10, 18]\n",
        "})\n",
        "\n",
        "result = df.group_by(\"ID\").agg(\n",
        "    pl.col(\"Val\")\n",
        "    .filter(pl.col(\"Flag\") > pl.mean(\"Flag\"))  # Только где Flag > среднего в группе\n",
        "    .max()\n",
        "    .alias(\"Conditional_Max\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (2, 2)\n",
        "┌─────┬─────────────────┐\n",
        "│ ID  ┆ Conditional_Max │\n",
        "│ --- ┆ ---             │\n",
        "│ i64 ┆ i64             │\n",
        "╞═════╪═════════════════╡\n",
        "│ 1   ┆ 15              │\n",
        "│ 2   ┆ 20              │\n",
        "└─────┴─────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* В первой группе среднее `Flag = 8.5`. Только вторая строка (`Flag=12`) удовлетворяет условию, поэтому максимум `Val = 15`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Условная логика: `pl.when().then().otherwise()`\n",
        "\n",
        "Это **SIMD-оптимизированная**, безветвёвая реализация условий — аналог `CASE WHEN` в SQL или `np.where` в NumPy, но **значительно быстрее**.\n",
        "\n",
        "> **Пример: условное среднее**\n",
        "\n",
        "```python\n",
        "df_cond = pl.DataFrame({\n",
        "    \"age\": [25, 35, 45, 55],\n",
        "    \"height\": [170, 175, 180, 165]\n",
        "})\n",
        "\n",
        "cutoff = 30\n",
        "result = df_cond.select(\n",
        "    pl.when(pl.col(\"age\") < cutoff)\n",
        "    .then(pl.lit(1.0))\n",
        "    .otherwise(pl.col(\"height\"))\n",
        "    .mean()\n",
        "    .alias(\"Whenthen_Mean\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (1, 1)\n",
        "┌────────────────┐\n",
        "│ Whenthen_Mean  │\n",
        "│ ---            │\n",
        "│ f64            │\n",
        "╞════════════════╡\n",
        "│ 173.333333     │  # (1 + 175 + 180 + 165) / 4\n",
        "└────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Благодаря **branchless-коду** и **SIMD**, эта операция выполняется на порядки быстрее, чем аналог с циклами или медленными UDF.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Продвинутые методы обработки данных\n",
        "\n",
        "### 4.1. Оконные функции: `.over()`\n",
        "\n",
        "Метод `over()` реализует **оконные функции**, аналогичные `OVER (PARTITION BY ...)` в SQL. Он позволяет вычислять агрегаты **внутри групп**, не сворачивая DataFrame.\n",
        "\n",
        "> **Пример: скользящее среднее по региону**\n",
        "\n",
        "```python\n",
        "data = pl.DataFrame({\n",
        "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"South\"],\n",
        "    \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
        "    \"sales\": [100, 150, 200, 250, 300]\n",
        "}).with_columns(\n",
        "    pl.col(\"date\").str.to_date(\"%Y-%m-%d\")  # Преобразуем в Date\n",
        ").sort([\"region\", \"date\"])\n",
        "\n",
        "# Добавляем среднее по региону ко всем строкам\n",
        "result = data.with_columns(\n",
        "    pl.col(\"sales\").mean().over(\"region\").alias(\"Avg_Sales_By_Region\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (5, 4)\n",
        "┌────────┬────────────┬───────┬─────────────────────────┐\n",
        "│ region ┆ date       ┆ sales ┆ Avg_Sales_By_Region     │\n",
        "│ ---    ┆ ---        ┆ ---   ┆ ---                     │\n",
        "│ str    ┆ date       ┆ i64   ┆ f64                     │\n",
        "╞════════╪════════════╪═══════╪═════════════════════════╡\n",
        "│ North  ┆ 2023-01-01 ┆ 100   ┆ 125.0                   │\n",
        "│ North  ┆ 2023-01-02 ┆ 150   ┆ 125.0                   │\n",
        "│ South  ┆ 2023-01-01 ┆ 200   ┆ 250.0                   │\n",
        "│ South  ┆ 2023-01-02 ┆ 250   ┆ 250.0                   │\n",
        "│ South  ┆ 2023-01-03 ┆ 300   ┆ 250.0                   │\n",
        "└────────┴────────────┴───────┴─────────────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Каждая строка «видит» среднее своей группы, но сохраняет свою позицию.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Обработка временных рядов: Asof Join\n",
        "\n",
        "`join_asof` — **специализированное соединение для временных рядов**, где точные совпадения времени не требуются.\n",
        "\n",
        "> **Параметры:**\n",
        "> - `strategy`: `'backward'` (по умолчанию), `'forward'`, `'nearest'`,\n",
        "> - `tolerance`: максимальное допустимое отклонение (например, `'1h'`, `'5min'`).\n",
        "\n",
        "> **Пример: присоединение курсов к транзакциям**\n",
        "\n",
        "```python\n",
        "df_transactions = pl.DataFrame({\n",
        "    \"tx_time\": [\n",
        "        pl.datetime(2023, 1, 1, 10, 0),\n",
        "        pl.datetime(2023, 1, 1, 10, 30)\n",
        "    ],\n",
        "    \"amount\": [1000, 1500]\n",
        "})\n",
        "\n",
        "df_fx_rates = pl.DataFrame({\n",
        "    \"fx_time\": [\n",
        "        pl.datetime(2023, 1, 1, 9, 50),   # Ближайший до 10:00\n",
        "        pl.datetime(2023, 1, 1, 10, 15)   # Ближайший до 10:30\n",
        "    ],\n",
        "    \"rate\": [1.1, 1.2]\n",
        "})\n",
        "\n",
        "result = df_transactions.join_asof(\n",
        "    df_fx_rates,\n",
        "    left_on=\"tx_time\",\n",
        "    right_on=\"fx_time\",\n",
        "    strategy=\"backward\",\n",
        "    tolerance=\"1h\"\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (2, 3)\n",
        "┌─────────────────────┬────────┬──────┐\n",
        "│ tx_time             ┆ amount ┆ rate │\n",
        "│ ---                 ┆ ---    ┆ ---  │\n",
        "│ datetime[μs]        ┆ i64    ┆ f64  │\n",
        "╞═════════════════════╪════════╪══════╡\n",
        "│ 2023-01-01 10:00:00 ┆ 1000   ┆ 1.1  │\n",
        "│ 2023-01-01 10:30:00 ┆ 1500   ┆ 1.2  │\n",
        "└─────────────────────┴────────┴──────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это стандартный паттерн в финтехе, IoT и лог-аналитике.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Производительность, память и интеграция\n",
        "\n",
        "### 5.1. Оптимизация типов данных\n",
        "\n",
        "Полный контроль над типами — ключ к эффективному использованию памяти.\n",
        "\n",
        "| Исходный тип               | Рекомендуемый тип Polars      | Эффект                          |\n",
        "|---------------------------|-------------------------------|----------------------------------|\n",
        "| Строки с низкой кардинальностью | `pl.Categorical` или `pl.Enum` | ↓ память в 10–100×, ↑ скорость сравнения |\n",
        "| `float64`                 | `pl.Float32`                  | ↓ память в 2×                   |\n",
        "| `int64` (малый диапазон)  | `pl.Int32`, `pl.Int16`        | ↓ память, ↑ кэш-эффективность   |\n",
        "\n",
        "> **Пример: загрузка с оптимизацией типов**\n",
        "\n",
        "```python\n",
        "df = pl.read_csv(\n",
        "    \"large_file.csv\",\n",
        "    dtypes={\n",
        "        \"user_id\": pl.Int32,\n",
        "        \"category\": pl.Categorical,\n",
        "        \"price\": pl.Float32\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Streaming API: обработка данных «вне памяти»\n",
        "\n",
        "Для датасетов **больше RAM** используйте **Streaming API**:\n",
        "\n",
        "```python\n",
        "# LazyFrame\n",
        "lf = pl.scan_csv(\"huge_file.csv\")\n",
        "\n",
        "# Обработка потоками\n",
        "result = (\n",
        "    lf\n",
        "    .filter(pl.col(\"value\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"price\").mean())\n",
        "    .collect(streaming=True)  # ← ключевой параметр\n",
        ")\n",
        "```\n",
        "\n",
        "> **Как это работает?**  \n",
        "> Polars разбивает запрос на этапы и обрабатывает данные **пакетами**, никогда не загружая всё в память. Если операция не поддерживает streaming (например, глобальная сортировка), Polars автоматически откатывается к in-memory режиму.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Интеграция с ML: `to_numpy()` и Zero-Copy\n",
        "\n",
        "Для передачи данных в `scikit-learn` или `PyTorch`:\n",
        "\n",
        "```python\n",
        "X = df.select(pl.col([\"feature1\", \"feature2\"])).to_numpy()\n",
        "```\n",
        "\n",
        "> **Zero-Copy достигается, если:**\n",
        "> - все столбцы — одного числового типа (`Float32` или `Int64`),\n",
        "> - нет пропущенных значений (`null`),\n",
        "> - данные хранятся в одном блоке (chunk),\n",
        "> - порядок памяти — колоночный (Fortran-style, по умолчанию в Polars).\n",
        "\n",
        "Если условия не выполнены, Polars **автоматически выполнит копирование и приведение типов** — но это будет чётко и безопасно.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Сравнение производительности: Polars vs Pandas\n",
        "\n",
        "Независимые бенчмарки (включая [**db-benchmark**](https://h2oai.github.io/db-benchmark/)) демонстрируют:\n",
        "\n",
        "| Операция                | Преимущество Polars       | Типичный прирост скорости |\n",
        "|------------------------|----------------------------|----------------------------|\n",
        "| Чтение + фильтрация    | Predicate Pushdown + параллелизм | **3–4×**                  |\n",
        "| `group_by().agg()`     | Параллельная агрегация по группам | **4–5×**                  |\n",
        "| `join`                 | Join Ordering + Arrow      | **до 14×**                |\n",
        "| Условная логика        | SIMD + branchless          | **2–10×** (в зависимости от сложности) |\n",
        "\n",
        "> **Пояснение:** Выигрыш особенно заметен **на данных > 1 млн строк**, где накладные расходы Pandas (GIL, копирование, отсутствие глобальной оптимизации) становятся критичными.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Polars — это архитектурный прорыв** в обработке табличных данных. Он не пытается «ускорить Pandas», а предлагает **новую парадигму**:\n",
        "\n",
        "- **Вычисления** — в компилируемом, многопоточном ядре на **Rust**,\n",
        "- **Память** — в эффективном колоночном формате **Apache Arrow**,\n",
        "- **Оптимизация** — через **Lazy API** и **логический план запроса**,\n",
        "- **Выразительность** — через **DSL на основе выражений**.\n",
        "\n",
        "Эти принципы позволяют Polars:\n",
        "- обрабатывать **миллионы и миллиарды строк** на одном компьютере,\n",
        "- выполнять **сложные аналитические запросы** без написания UDF,\n",
        "- масштабироваться **линейно с числом ядер**,\n",
        "- интегрироваться с **экосистемой Arrow** без копирования данных.\n",
        "\n",
        "Таким образом, Polars не просто альтернатива Pandas — это **следующее поколение фреймворка для аналитики данных**, объединяющее скорость системного программирования, выразительность DSL и удобство Python.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "TY2jtxOpmstR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 4. Dask: Архитектура, методология и масштабирование вычислений в экосистеме Python\n",
        "\n",
        "## Введение: Масштабирование PyData и архитектурный вызов\n",
        "\n",
        "Экосистема Scientific Python — с её столпами **NumPy** и **Pandas** — давно стала де-факто стандартом для анализа данных и научных вычислений. Эти библиотеки достигают высокой производительности за счёт **векторизованных операций**, реализованных на C/C++, и оптимизированных под работу **в оперативной памяти (in-memory)**.\n",
        "\n",
        "Однако на практике данные часто:\n",
        "- **превышают объём RAM** (out-of-core),\n",
        "- требуют **интенсивных CPU-вычислений**, которые не могут быть ускорены одним ядром.\n",
        "\n",
        "В этих сценариях традиционный стек PyData сталкивается с фундаментальными ограничениями — в первую очередь, из-за **Global Interpreter Lock (GIL)** в CPython, который блокирует истинный параллелизм на уровне потоков.\n",
        "\n",
        "**Dask** был создан как гибкая платформа параллельных вычислений, которая **расширяет**, а не заменяет, экосистему PyData. Он состоит из двух компонентов:\n",
        "\n",
        "1. **Низкоуровневого планировщика задач**, управляющего исполнением графа вычислений,\n",
        "2. **Высокоуровневых коллекций** (`Dask Array`, `Dask DataFrame`, `Dask Bag`), которые имитируют интерфейсы NumPy, Pandas и итераторов Python.\n",
        "\n",
        "Ключевое преимущество Dask — **масштабируемость вниз и вверх**:\n",
        "- **Вниз**: запуск на ноутбуке для обработки 100 ГБ данных с диска,\n",
        "- **Вверх**: распределённый кластер с тысячами ядер для обработки петабайтов.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Фундаментальные принципы архитектуры Dask\n",
        "\n",
        "### 1.1. Ленивые вычисления (Lazy Evaluation)\n",
        "\n",
        "Все операции в Dask **ленивы**: вызов метода не выполняет расчёты, а лишь **строит граф задач**. Фактическое выполнение запускается только при вызове терминальных методов:\n",
        "\n",
        "- `.compute()` — возвращает итоговый результат в памяти (например, как `pandas.DataFrame`),\n",
        "- `.persist()` — сохраняет промежуточные результаты в распределённой памяти (полезно для интерактивных сессий).\n",
        "\n",
        "Этот подход позволяет Dask **анализировать весь план вычислений целиком** и применять оптимизации: удалять избыточные операции, минимизировать передачу данных, выбирать порядок выполнения.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Графы задач (Task Graphs)\n",
        "\n",
        "Все алгоритмы в Dask кодируются как **ориентированные ациклические графы (DAG)**:\n",
        "\n",
        "- **Узлы** — функции или операции,\n",
        "- **Рёбра** — зависимости между результатами.\n",
        "\n",
        "Этот граф служит **универсальным промежуточным представлением (IR)** для всех коллекций.\n",
        "\n",
        "> **Пример: построение графа с `dask.delayed`**\n",
        "\n",
        "```python\n",
        "import dask\n",
        "\n",
        "@dask.delayed\n",
        "def calculate_mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "# Создаём отложенные объекты\n",
        "data1 = [1, 2, 3, 4]\n",
        "data2 = [10, 20, 30]\n",
        "\n",
        "mean1 = calculate_mean(data1)\n",
        "mean2 = calculate_mean(data2)\n",
        "\n",
        "# Складываем результаты\n",
        "final_sum = dask.delayed(lambda x, y: x + y)(mean1, mean2)\n",
        "\n",
        "# Вычисление запускается здесь\n",
        "result = final_sum.compute()\n",
        "print(\"Результат:\", result)  # 20.5\n",
        "```\n",
        "\n",
        "> *Пояснение:* Каждый вызов отложенной функции добавляет узел в граф. Dask выполняет их **параллельно**, если зависимости позволяют.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. Динамический планировщик (Scheduler)\n",
        "\n",
        "Планировщик — «мозг» Dask. Он:\n",
        "- распределяет задачи по исполнителям (workers),\n",
        "- управляет зависимостями,\n",
        "- оптимизирует **локальность данных** (стремится выполнять задачи там, где уже находятся входные данные).\n",
        "\n",
        "#### Типы планировщиков:\n",
        "\n",
        "| Тип | Описание | Использование |\n",
        "|-----|----------|---------------|\n",
        "| **Single-machine** | Локальный пул потоков/процессов | Быстрый старт, небольшие данные |\n",
        "| **Distributed** | Полноценный кластер (даже на одной машине через `LocalCluster`) | Масштабирование, дашборд, отказоустойчивость |\n",
        "\n",
        "> **Дашборд (Dashboard)** — одно из главных преимуществ распределённого планировщика: визуализация графа, загрузки CPU, объёма памяти, сетевого трафика в реальном времени.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.4. Накладные расходы и гранулярность задач\n",
        "\n",
        "Dask спроектирован с минимальными накладными расходами (~1 мс на задачу), но **это не бесплатно**. Если задача выполняется < 100 мс, накладные расходы на планирование и передачу данных могут **перевесить выгоду от параллелизма**.\n",
        "\n",
        "> **Рекомендация:**  \n",
        "> Размер чанка (chunk/partition) должен быть таким, чтобы **время выполнения одной задачи ≥ 100 мс**.\n",
        "\n",
        "Неправильный выбор гранулярности — частая ошибка новичков, приводящая к **деградации производительности** по сравнению с Pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### Сравнение высокоуровневых коллекций Dask\n",
        "\n",
        "| Коллекция | Основа | Принцип | Использование |\n",
        "|----------|--------|--------|----------------|\n",
        "| **Dask Array** | `numpy.ndarray` | Блочная (chunked) структура | Научные расчёты, многомерные данные, линейная алгебра |\n",
        "| **Dask DataFrame** | `pandas.DataFrame` | Разбиение по строкам (partitioning) | ETL, агрегация, обработка больших таблиц |\n",
        "| **Dask Bag** | Python-итераторы | Параллельные коллекции объектов | Логи, JSON, неструктурированные данные |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dask DataFrame: параллельная обработка табличных данных\n",
        "\n",
        "### 2.1. Архитектура и секционирование\n",
        "\n",
        "`Dask DataFrame` — это **коллекция обычных `pandas.DataFrame`**, разбитых по строкам. Каждый «кусок» (partition) обрабатывается независимо.\n",
        "\n",
        "- **Преимущество**: может обрабатывать **100 ГБ на ноутбуке** или **100 ТБ на кластере**.\n",
        "- **Ограничение**: операции, требующие **перетасовки (shuffle)**, становятся медленными из-за межпроцессной коммуникации.\n",
        "\n",
        "> **Совет:** если вы часто фильтруете или группируете по определённому столбцу, **разбивайте данные по этому столбцу заранее** (например, при сохранении в Parquet).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Совместимость с Pandas и ленивое исполнение\n",
        "\n",
        "API `Dask DataFrame` **почти идентичен Pandas**:\n",
        "\n",
        "```python\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Ленивая загрузка (данные не читаются!)\n",
        "df = dd.read_parquet(\"data/*.parquet\")\n",
        "\n",
        "# Ленивые трансформации\n",
        "filtered = df[df.value > 0]\n",
        "aggregated = filtered.groupby(\"category\").value.mean()\n",
        "\n",
        "# Фактическое вычисление\n",
        "result = aggregated.compute()  # → pandas.Series\n",
        "```\n",
        "\n",
        "> **Важно:** `compute()` возвращает **обычный Pandas-объект**. Если результат не помещается в память — используйте `.to_parquet()` или другие методы сохранения без материализации.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. Производительность: что работает быстро, а что — нет\n",
        "\n",
        "- ✅ **Быстро**: `groupby().sum()`, `groupby().mean()` — **декомпозируемые агрегации** (MapReduce).\n",
        "- ❌ **Медленно**: `groupby().apply(custom_func)` — требует **shuffle**, так как все строки одной группы должны быть на одном worker'е.\n",
        "\n",
        "> **Пример: избегайте `apply`, если можно**\n",
        "\n",
        "```python\n",
        "# ПЛОХО: медленно из-за shuffle\n",
        "df.groupby(\"user\").apply(lambda x: fit_model(x))\n",
        "\n",
        "# ХОРОШО: используйте встроенные агрегаты или перепишите логику через map_partitions\n",
        "```\n",
        "\n",
        "> *Пояснение:* Dask не может оптимизировать произвольные функции. Старайтесь оставаться в рамках векторизованных операций.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Dask Array: масштабирование многомерных массивов\n",
        "\n",
        "### 3.1. Архитектура и чанкинг\n",
        "\n",
        "`Dask Array` — это блочная структура поверх `numpy.ndarray`. Данные разбиваются на **чанки** — небольшие NumPy-массивы, которые помещаются в память одного worker'а.\n",
        "\n",
        "> **Как выбрать размер чанка?**\n",
        "> - Слишком мал → много накладных расходов,\n",
        "> - Слишком велик → не помещается в память,\n",
        "> - **Идея**: 10–100 МБ на чанк, время выполнения ≥ 100 мс.\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "\n",
        "# Создаём массив 10 000×10 000, разбитый на чанки 1000×1000\n",
        "x = da.random.random((10_000, 10_000), chunks=(1000, 1000))\n",
        "y = x + x.T  # Ленивые операции\n",
        "result = y.sum().compute()  # Фактическое вычисление\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Совместимость с NumPy\n",
        "\n",
        "Dask Array поддерживает **большую часть API NumPy**:\n",
        "- Универсальные функции (`sin`, `log`, `+`, `*`) — применяются к каждому чанку,\n",
        "- Редукции (`sum`, `mean`) — координируются между чанками,\n",
        "- Линейная алгебра (`dot`, `svd`) — реализована с учётом распределённости.\n",
        "\n",
        "> **Важно:** не все функции NumPy доступны. Если операция требует глобального контекста (например, `np.argsort`), Dask либо выдаст ошибку, либо предложит альтернативу.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Пользовательские функции: `map_blocks`\n",
        "\n",
        "Для внедрения собственной логики используется `map_blocks`:\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "import numpy as np\n",
        "\n",
        "x = da.arange(1000, chunks=100)\n",
        "\n",
        "def block_max(block):\n",
        "    return np.array([block.max()])  # 100 элементов → 1\n",
        "\n",
        "# Указываем форму выходного чанка\n",
        "result = x.map_blocks(block_max, chunks=(1,), dtype=x.dtype)\n",
        "\n",
        "print(result.compute())  # [99, 199, 299, ..., 999]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `map_blocks` — точка расширения для высокопроизводительных кернелов (Numba, Cython), которые можно масштабировать на весь массив.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Dask Bag: параллелизм для неструктурированных данных\n",
        "\n",
        "### 4.1. Архитектура и использование\n",
        "\n",
        "`Dask Bag` — коллекция **произвольных Python-объектов** (словари, строки, JSON). Используется на ранних этапах ETL:\n",
        "\n",
        "```python\n",
        "import dask.bag as db\n",
        "import json\n",
        "\n",
        "# Чтение и параллельная обработка логов\n",
        "bag = db.read_text(\"logs/*.json.gz\").map(json.loads)\n",
        "\n",
        "# Пайплайн: фильтрация → извлечение → агрегация\n",
        "top_jobs = (\n",
        "    bag\n",
        "    .filter(lambda r: r.get(\"age\", 0) > 30)\n",
        "    .map(lambda r: r[\"job\"])\n",
        "    .frequencies()\n",
        "    .topk(5, key=lambda x: x[1])\n",
        "    .compute()\n",
        ")\n",
        "```\n",
        "\n",
        "> **Преимущество:** гибкость для «грязных» данных без схемы.\n",
        "\n",
        "> **Недостаток:** высокие накладные расходы на **сериализацию** (каждый объект pickle'ится при передаче между процессами).\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Методология: Bag — только на входе\n",
        "\n",
        "**Рекомендация:** используйте `Dask Bag` **только для первоначальной очистки и парсинга**. Как только данные становятся структурированными — **конвертируйте в `Dask DataFrame`**:\n",
        "\n",
        "```python\n",
        "# После парсинга JSON\n",
        "df = bag.to_dataframe()  # или bag.to_delayed() → обработка → dd.from_delayed()\n",
        "```\n",
        "\n",
        "Так вы получите преимущества **типизированной памяти**, **векторизации** и **оптимизированного планировщика**.\n",
        "\n"
      ],
      "metadata": {
        "id": "XExFt3xhmv9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Запуск и управление вычислениями: планирование и диагностика\n",
        "\n",
        "### 5.1. Модель «Клиент–Планировщик–Работник»\n",
        "\n",
        "Распределённый планировщик Dask (`Dask Distributed`) использует классическую трёхзвенную архитектуру, применимую как на локальной машине, так и в кластере:\n",
        "\n",
        "1. **Клиент (Client)** — интерфейс пользователя. Отправляет граф задач планировщику и получает результаты.\n",
        "2. **Планировщик (Scheduler)** — центральный узел. Управляет зависимостями, распределяет задачи, отслеживает состояние работников и локальность данных.\n",
        "3. **Работник (Worker)** — исполнитель. Выполняет задачи, хранит промежуточные результаты в памяти и может обмениваться данными с другими работниками напрямую (по указанию планировщика), минимизируя задержки.\n",
        "\n",
        "Эта архитектура обеспечивает **масштабируемость**, **отказоустойчивость** и **гибкое управление ресурсами**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Локальный запуск с `LocalCluster`\n",
        "\n",
        "Даже при работе на одном компьютере рекомендуется использовать **распределённый режим** через `LocalCluster` — он предоставляет доступ к **асинхронному API** и, что особенно важно, к **диагностическому дашборду**.\n",
        "\n",
        "> **Пример: инициализация локального кластера**\n",
        "\n",
        "```python\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "# Запускает планировщик и несколько worker-процессов\n",
        "cluster = LocalCluster(\n",
        "    n_workers=4,        # количество процессов\n",
        "    threads_per_worker=2,\n",
        "    memory_limit=\"2GB\"  # лимит памяти на worker\n",
        ")\n",
        "\n",
        "# Подключаем клиент\n",
        "client = Client(cluster)\n",
        "\n",
        "# URL дашборда выводится автоматически (например, http://127.0.0.1:8787)\n",
        "print(\"Дашборд:\", client.dashboard_link)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `LocalCluster` использует **процессы** (а не потоки), чтобы обойти GIL и обеспечить истинный параллелизм даже на одном ядре.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Интерактивная диагностика: Dask Dashboard\n",
        "\n",
        "Дашборд — **главный инструмент профилирования** в Dask. Он построен на Bokeh и предоставляет в реальном времени:\n",
        "\n",
        "#### **Task Stream**\n",
        "- Каждый прямоугольник — задача на одном потоке.\n",
        "- **Цвета** — тип операции (`read-parquet`, `groupby-sum` и т.д.).\n",
        "- **Красные полосы** — передача данных между работниками (**shuffle**). Много красного = проблема с чанкингом или избыточной коммуникацией.\n",
        "- **Белые промежутки** — простой потока = несбалансированная нагрузка или блокировки.\n",
        "\n",
        "#### **Memory Usage**\n",
        "- **Синий** — безопасный уровень памяти,\n",
        "- **Оранжевый** — данные начинают сбрасываться на диск (spilling),\n",
        "- **Красный** — worker приостановлен из-за нехватки памяти.\n",
        "\n",
        "> **Методология:**  \n",
        "> Эффективный разработчик Dask **не просто пишет код**, а **анализирует Task Stream** после каждого запуска, корректируя:\n",
        "> - размер чанков,\n",
        "> - структуру графа,\n",
        "> - выбор операций (избегая `apply` и `shuffle`).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Интеграция с машинным обучением: Dask-ML\n",
        "\n",
        "Библиотека **`dask-ml`** расширяет экосистему Scikit-learn для работы с **out-of-core** и **распределёнными** данными.\n",
        "\n",
        "### 6.1. Параллельная предобработка\n",
        "\n",
        "Модули `dask_ml.preprocessing` предоставляют трансформеры, совместимые с `sklearn`:\n",
        "- `StandardScaler`, `MinMaxScaler`,\n",
        "- `OneHotEncoder` с поддержкой `CategoricalDtype`.\n",
        "\n",
        "Все они работают **лениво** и **параллельно** на `Dask DataFrame` и `Dask Array`.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Масштабирование обучения: мета-оценщики\n",
        "\n",
        "#### **`ParallelPostFit`**\n",
        "Оборачивает обученную модель и позволяет **параллельно применять** `predict`/`transform` к большим данным.\n",
        "\n",
        "#### **`Incremental`**\n",
        "Для моделей, поддерживающих `partial_fit` (например, `SGDClassifier`), обучает **блок за блоком**, не загружая всё в память.\n",
        "\n",
        "> **Пример: обучение на 1 млрд записей**\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from dask_ml.wrappers import Incremental\n",
        "\n",
        "# Большие данные (out-of-core)\n",
        "X = da.random.normal(size=(1_000_000_000, 10), chunks=(100_000, 10))\n",
        "y = (X.sum(axis=1) > 0).astype(int)\n",
        "\n",
        "# Обучение по блокам\n",
        "model = Incremental(SGDClassifier(random_state=42))\n",
        "model.fit(X, y)  # каждый чанк → partial_fit\n",
        "\n",
        "# Параллельный прогноз\n",
        "predictions = model.predict(X).compute()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой подход делает возможным обучение на данных, которые **никогда не помещаются в RAM**.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. Поиск гиперпараметров: инкрементальные методы\n",
        "\n",
        "- **`IncrementalSearchCV`** и **`HyperbandSearchCV`** — аналоги `GridSearchCV`, но с **ранней остановкой**.\n",
        "- Модели, показывающие плохую сходимость, **отбрасываются досрочно**, что экономит ресурсы.\n",
        "\n",
        "> **Ограничение:** требует поддержки `partial_fit` от модели.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Комплексные практические кейсы\n",
        "\n",
        "### 7.1. Методология out-of-core ETL/ELT\n",
        "\n",
        "Оптимальный пайплайн в Dask включает:\n",
        "\n",
        "1. **Параллельная загрузка**: `dd.read_parquet(\"s3://bucket/data*.parquet\")`.\n",
        "2. **Ленивая трансформация**: фильтрация, очистка, feature engineering.\n",
        "3. **Персистенция**: если за шагом следует несколько дорогих операций, вызовите `.persist()`, чтобы **закешировать промежуточный результат** в распределённой памяти.\n",
        "4. **Сохранение**: `.to_parquet()`, `.to_csv()` или запись в БД — **без `.compute()`**, чтобы избежать материализации.\n",
        "\n",
        "> **Пример:**\n",
        "\n",
        "```python\n",
        "df = dd.read_parquet(\"raw_data/\")\n",
        "clean = df[df.value.notnull()].assign(...)\n",
        "clean = clean.persist()  # ← кешируем\n",
        "\n",
        "# Несколько независимых агрегаций\n",
        "agg1 = clean.groupby(\"cat\").value.mean()\n",
        "agg2 = clean.groupby(\"cat\").value.std()\n",
        "\n",
        "# Сохраняем без полной загрузки в память\n",
        "agg1.to_parquet(\"results/mean.parquet\")\n",
        "agg2.to_parquet(\"results/std.parquet\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7.2. Методологическая карта перехода к Dask\n",
        "\n",
        "**Не используйте Dask «на всякий случай»**. Переход оправдан **только при соблюдении критериев**:\n",
        "\n",
        "| Критерий | Dask **НЕ рекомендован** | Dask **рекомендован** | Обоснование |\n",
        "|--------|--------------------------|------------------------|-------------|\n",
        "| **Размер данных** | Помещаются в RAM | Превышают RAM (out-of-core) | Накладные расходы не окупаются |\n",
        "| **Длительность вычисления** | < 1 секунды | > 1–2 секунд | Минимальная задача ≥ 100 мс |\n",
        "| **Диагностика** | Не требуется | Нужен контроль памяти и производительности | Дашборд — ключ к оптимизации |\n",
        "\n",
        "> **Важно:** на малых данных Dask может быть **в 10–100 раз медленнее** Pandas из-за инициализации графа и планировщика.\n",
        "\n",
        "---\n",
        "\n",
        "### 7.3. Кейс: гибридный пайплайн временного ряда\n",
        "\n",
        "**Задача:** обработать 50 ГБ метеоданных → фильтрация → FFT → агрегация.\n",
        "\n",
        "**Архитектура:**\n",
        "\n",
        "1. **Табличная фильтрация (Dask DataFrame)**  \n",
        "   ```python\n",
        "   df = dd.read_parquet(\"weather/\")\n",
        "   filtered = df[(df.temp > 0) & (df.time >= \"2020\")]\n",
        "   ```\n",
        "\n",
        "2. **Переход к численным данным (Dask Array)**  \n",
        "   ```python\n",
        "   signal = filtered.temp.values  # → dask.array\n",
        "   signal = signal.rechunk(chunks=(\"auto\",))  # оптимизация чанков под FFT\n",
        "   ```\n",
        "\n",
        "3. **Численный анализ (Dask Array)**  \n",
        "   ```python\n",
        "   fft_result = da.fft.fft(signal)\n",
        "   power = da.abs(fft_result) ** 2\n",
        "   ```\n",
        "\n",
        "4. **Финальная агрегация (обратно в DataFrame)**  \n",
        "   ```python\n",
        "   result_df = dd.from_dask_array(power, columns=[\"power\"])\n",
        "   daily_stats = result_df.groupby(result_df.index // 86400).mean()\n",
        "   daily_stats.to_parquet(\"fft_stats/\")\n",
        "   ```\n",
        "\n",
        "> **Ключевой принцип:**  \n",
        "> Данные **мигрируют между коллекциями** в зависимости от задачи:\n",
        "> - `DataFrame` — для структурированной фильтрации,\n",
        "> - `Array` — для HPC-операций.\n",
        ">\n",
        "> Минимизируйте переходы и **выравнивайте чанки**, чтобы избежать дорогостоящего `rechunk`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Dask — это зрелая, гибкая и диагностически прозрачная платформа** для масштабирования аналитики данных в экосистеме Python. Его сила — не в «автомагическом» ускорении, а в **осознанном управлении вычислениями**:\n",
        "\n",
        "- через **ленивые графы**,\n",
        "- через **блочную память**,\n",
        "- через **интерактивную диагностику**.\n",
        "\n",
        "Использование Dask требует **методологической дисциплины**: понимания накладных расходов, гранулярности задач, архитектуры данных и инструментов профилирования.\n",
        "\n",
        "При правильном применении Dask позволяет:\n",
        "- обрабатывать **петабайты данных** на кластере,\n",
        "- выполнять **численные расчёты на миллиардах точек**,\n",
        "- обучать **ML-модели на out-of-core данных**,\n",
        "- и всё это — **в знакомом синтаксисе PyData**.\n",
        "\n",
        "Таким образом, Dask завершает эволюцию от **in-memory аналитики** (Pandas) к **масштабируемой, распределённой и диагностируемой** вычислительной платформе для современных задач данных.\n",
        "\n",
        "\n",
        "\n",
        "✅ **Цикл полностью завершён.**  \n",
        "Вы прошли путь от основ (NumPy) → аналитики (Pandas) → высокой производительности (Polars) → распределённых вычислений (Dask).\n",
        ""
      ],
      "metadata": {
        "id": "Q9UOCsmzoXq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 5: Apache Spark и PySpark — Архитектура и практика распределённой обработки Big Data\n",
        "\n",
        "## Введение\n",
        "\n",
        "В эпоху Big Data обработка объёмов информации, превышающих возможности единичной вычислительной машины, стала неотъемлемой частью научных исследований, промышленного анализа и разработки интеллектуальных систем. Apache Spark представляет собой одну из наиболее зрелых и широко применяемых платформ для решения подобных задач. В отличие от библиотек, ориентированных на in-memory вычисления (таких как Pandas или Polars), Spark изначально спроектирован как **распределённый вычислительный движок**, способный масштабироваться от локального режима до кластеров, охватывающих тысячи узлов.\n",
        "\n",
        "Архитектурная целостность Spark обеспечивается не только его способностью распараллеливать вычисления, но и глубокой интеграцией механизмов отказоустойчивости, оптимизации запросов и эффективного управления памятью. Понимание этих механизмов — от жизненного цикла приложения до работы оптимизатора Catalyst — является необходимым условием для построения производительных и надёжных систем обработки больших данных.\n",
        "\n",
        "Настоящий модуль посвящён систематическому изложению архитектурных основ Spark, эволюции его программных интерфейсов и принципов функционирования его вычислительного ядра. Особое внимание уделено специфике взаимодействия PySpark с JVM-базой Spark, что критически важно для разработчиков, использующих Python в качестве основного языка аналитики. Все теоретические положения сопровождаются практическими примерами, демонстрирующими их применение в реальных сценариях.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Архитектурные основы распределённой модели Spark\n",
        "\n",
        "### 1.1. Диспетчер, Исполнители и Рабочие Узлы: Формальные определения\n",
        "\n",
        "Распределённое приложение Spark функционирует на основе трёх взаимосвязанных компонентов: Диспетчера программы (Driver Program), Исполнителей (Executors) и Рабочих Узлов (Worker Nodes).\n",
        "\n",
        "**Диспетчер программы** является центральным управляющим элементом любого Spark-приложения. Он инициализируется при создании объекта `SparkSession` и может размещаться либо на клиентской машине (в режиме client), либо на одном из узлов кластера (в режиме cluster). Основные функции Диспетчера включают: преобразование последовательности пользовательских преобразований и действий в направленный ациклический граф (DAG), который служит логическим планом вычислений; взаимодействие с кластерным менеджером для запроса и выделения вычислительных ресурсов в виде Исполнителей; мониторинг статуса выполнения задач на Исполнителях и обеспечение отказоустойчивости; сбор и агрегация окончательных результатов вычислений.\n",
        "\n",
        "**Исполнители** представляют собой рабочие процессы, запускаемые на Рабочих Узлах кластера. Каждый Исполнитель получает в своё распоряжение выделенный объём оперативной памяти и набор процессорных ядер (Cores), которые служат минимальными единицами параллельного исполнения. Исполнители отвечают за непосредственное выполнение задач, назначенных им Диспетчером, над партициями данных.\n",
        "\n",
        "**Рабочие Узлы** — это физические или виртуальные машины, составляющие вычислительный кластер. На каждом Рабочем Узле может быть запущено один или несколько Исполнителей.\n",
        "\n",
        "> **Пример: Инициализация SparkSession и базовое распределённое вычисление**\n",
        "\n",
        "В следующем примере демонстрируется создание Spark-приложения и выполнение простой операции подсчёта. Даже в локальном режиме (`master=\"local[*]\"`) Spark создаёт Диспетчер и один Исполнитель (внутри того же процесса), что позволяет изучать его архитектуру на одной машине.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Инициализация Диспетчера (Driver)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Architecture Example\") \\\n",
        "    .master(\"local[*]\") \\  # Локальный режим со всеми доступными ядрами\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание RDD из диапазона чисел. Данные автоматически разбиваются на партиции.\n",
        "numbers = spark.sparkContext.parallelize(range(1, 1000001), numSlices=4)\n",
        "\n",
        "# Действие (Action): запускает DAG и возвращает результат Диспетчеру\n",
        "total = numbers.reduce(lambda a, b: a + b)\n",
        "print(f\"Сумма чисел от 1 до 1000000: {total}\")\n",
        "\n",
        "# Завершение работы приложения\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Вызов `parallelize` создаёт RDD с 4 партициями. Метод `reduce` является действием (Action), которое инициирует вычисление. Диспетчер разбивает задачу на 4 подзадачи, которые выполняются параллельно на Исполнителе (в локальном режиме — в том же процессе). Итоговый результат агрегируется и возвращается в Диспетчер.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.5. Влияние замыканий (Closures) на состояние Driver\n",
        "\n",
        "При разработке распределённых приложений на PySpark крайне важно понимать механизм передачи кода и данных от Диспетчера к Исполнителям.\n",
        "\n",
        "> **Пример: Демонстрация неизменности переменной Driver из Исполнителя**\n",
        "\n",
        "Следующий код иллюстрирует классическую ошибку, связанную с непониманием замыканий в распределённой среде. Разработчик пытается инкрементировать переменную `counter` из функции, выполняемой на Исполнителе. Однако результат оказывается неожиданным.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Closure Example\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Глобальная переменная в процессе Диспетчера\n",
        "counter = 0\n",
        "\n",
        "def increment_counter(value):\n",
        "    global counter\n",
        "    counter += 1  # Эта операция изменяет ЛОКАЛЬНУЮ копию переменной на Исполнителе\n",
        "    return value\n",
        "\n",
        "# Создаём RDD и применяем функцию\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.map(increment_counter).collect()  # Запускаем действие\n",
        "\n",
        "print(f\"Значение counter в Диспетчере: {counter}\")  # Вывод: 0\n",
        "\n",
        "# Правильный способ: использование аккумулятора\n",
        "acc_counter = sc.accumulator(0)\n",
        "\n",
        "def increment_accumulator(value):\n",
        "    global acc_counter\n",
        "    acc_counter.add(1)  # Аккумулятор гарантирует агрегацию в Диспетчере\n",
        "    return value\n",
        "\n",
        "rdd.map(increment_accumulator).collect()\n",
        "print(f\"Значение аккумулятора: {acc_counter.value}\")  # Вывод: 5\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* В первом случае переменная `counter` внутри `increment_counter` является независимой копией на каждом Исполнителе. Изменения не отражаются в Диспетчере. Во втором случае используется специальный объект `Accumulator`, который предназначен для безопасной агрегации информации из Исполнителей в Диспетчер.\n",
        "\n",
        "---\n",
        "\n",
        "## II. Эволюция Abstraction API: От RDD к структурированной обработке\n",
        "\n",
        "### 2.2. DataFrame API (PySpark)\n",
        "\n",
        "В современной практике, где подавляющее большинство данных имеет табличную структуру, DataFrame API является стандартом де-факто.\n",
        "\n",
        "> **Пример: Чтение данных, трансформации и анализ с помощью DataFrame**\n",
        "\n",
        "В этом примере показано, как с помощью DataFrame API можно выполнить типичный ETL-пайплайн: загрузку данных из CSV-файла, фильтрацию, агрегацию и анализ плана выполнения.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame API Example\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 1. Чтение данных в DataFrame\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .csv(\"sales_data.csv\")\n",
        "\n",
        "# Показать схему данных\n",
        "df.printSchema()\n",
        "# root\n",
        "#  |-- product: string (nullable = true)\n",
        "#  |-- category: string (nullable = true)\n",
        "#  |-- price: double (nullable = true)\n",
        "#  |-- quantity: integer (nullable = true)\n",
        "\n",
        "# 2. Применение преобразований (ленивые операции)\n",
        "filtered_df = df.filter(col(\"price\") > 10.0)\n",
        "aggregated_df = filtered_df.groupBy(\"category\").agg(avg(\"price\").alias(\"avg_price\"))\n",
        "\n",
        "# 3. Анализ физического плана выполнения\n",
        "print(\"Физический план:\")\n",
        "aggregated_df.explain(\"formatted\")\n",
        "\n",
        "# 4. Выполнение действия и сбор результата\n",
        "result = aggregated_df.collect()\n",
        "for row in result:\n",
        "    print(f\"Категория: {row['category']}, Средняя цена: {row['avg_price']:.2f}\")\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Все операции до вызова `collect()` являются преобразованиями (Transformations) и лишь строят логический план. Метод `explain(\"formatted\")` выводит читаемый физический план, в котором можно увидеть использование оптимизаций Catalyst, таких как `Filter` перед `Scan` (Predicate Pushdown).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Взаимодействие PySpark и JVM: Сериализационный барьер и Apache Arrow\n",
        "\n",
        "Для устранения сериализационного барьера в Spark была интегрирована библиотека **Apache Arrow**.\n",
        "\n",
        "> **Пример: Сравнение производительности UDF с и без Arrow**\n",
        "\n",
        "Следующий пример демонстрирует, как включить Arrow и создать векторизованную UDF, которая работает с целыми столбцами данных за один вызов, а не построчно.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Arrow UDF Example\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание тестового DataFrame\n",
        "df = spark.range(0, 1000000).toDF(\"id\")\n",
        "df = df.withColumn(\"value\", col(\"id\") * 2.0)\n",
        "\n",
        "# Скалярная UDF (медленная, без Arrow)\n",
        "def slow_udf(x):\n",
        "    return x * 1.1\n",
        "\n",
        "spark.udf.register(\"slow_udf\", slow_udf, DoubleType())\n",
        "start = time.time()\n",
        "df.selectExpr(\"slow_udf(value) as new_value\").collect()\n",
        "slow_time = time.time() - start\n",
        "\n",
        "# Векторизованная UDF с Arrow (быстрая)\n",
        "@pandas_udf(DoubleType())\n",
        "def fast_udf(v: pd.Series) -> pd.Series:\n",
        "    return v * 1.1\n",
        "\n",
        "start = time.time()\n",
        "df.select(fast_udf(col(\"value\")).alias(\"new_value\")).collect()\n",
        "fast_time = time.time() - start\n",
        "\n",
        "print(f\"Скалярная UDF: {slow_time:.2f} секунд\")\n",
        "print(f\"Векторизованная (Arrow) UDF: {fast_time:.2f} секунд\")\n",
        "print(f\"Ускорение: {slow_time / fast_time:.2f}x\")\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Векторизованная UDF, отмеченная декоратором `@pandas_udf`, получает и возвращает целые `pandas.Series`. Благодаря Arrow, передача данных между JVM и Python происходит без сериализации, что приводит к значительному ускорению.\n",
        "\n",
        "---\n",
        "\n",
        "## III. Движок исполнения Spark: Catalyst и Tungsten (Глубокий анализ)\n",
        "\n",
        "### 3.2. Физический план: Выбор стратегий и стоимостное моделирование\n",
        "\n",
        "Умение анализировать физический план является ключевым навыком для оптимизации запросов.\n",
        "\n",
        "> **Пример: Анализ плана выполнения операции Join**\n",
        "\n",
        "В этом примере создаются два DataFrame и выполняется операция соединения. Анализ плана позволяет понять, какой алгоритм Join был выбран оптимизатором.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Join Plan Analysis\").getOrCreate()\n",
        "\n",
        "# Создание двух небольших DataFrame\n",
        "df1 = spark.createDataFrame([(i, f\"user_{i}\") for i in range(1, 101)], [\"id\", \"name\"])\n",
        "df2 = spark.createDataFrame([(i, f\"category_{i % 5}\") for i in range(1, 101)], [\"id\", \"category\"])\n",
        "\n",
        "# Выполнение Join\n",
        "joined_df = df1.join(df2, on=\"id\")\n",
        "\n",
        "# Вывод расширенного плана\n",
        "print(\"Расширенный план выполнения:\")\n",
        "joined_df.explain(\"extended\")\n",
        "\n",
        "# В реальных сценариях для больших таблиц Spark может выбрать\n",
        "# BroadcastHashJoin или SortMergeJoin в зависимости от статистики.\n",
        "```\n",
        "\n",
        "> *Пояснение:* В выводе `explain` можно увидеть физический оператор, например, `BroadcastHashJoin`. Это означает, что Catalyst определил, что одна из таблиц достаточно мала, чтобы быть переданной (broadcasted) всем Исполнителям, что избегает дорогостоящей операции shuffle.\n",
        "\n"
      ],
      "metadata": {
        "id": "C-opDJc48ICU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IV. Практика PySpark: Код, оптимизация и анализ плана выполнения\n",
        "\n",
        "В производственных задачах PySpark используется для создания надёжных и масштабируемых ETL-пайплайнов, что требует не только знания синтаксиса DataFrame, но и глубокого понимания того, как операции высокого уровня трансформируются в низкоуровневые распределённые события. Настоящий раздел посвящён переходу от теории к практике: мы рассмотрим канонические примеры, проанализируем их физические планы и продемонстрируем методы оптимизации, применяемые в реальных инженерных задачах.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1. Идиоматичное использование DataFrame API: Пример ETL с оконными функциями\n",
        "\n",
        "Оконные функции (Window Functions) являются мощным инструментом для выполнения сложных аналитических операций, таких как ранжирование, кумулятивные суммы и скользящие средние, без необходимости выполнять дорогостоящую глобальную агрегацию.\n",
        "\n",
        "> **Пример: Ранжирование сотрудников по зарплате внутри отдела**\n",
        "\n",
        "Рассмотрим типичную задачу из корпоративной аналитики: необходимо для каждого отдела проранжировать сотрудников по убыванию заработной платы. В реляционных базах данных эта задача решается с помощью аналитических функций `RANK() OVER (PARTITION BY ... ORDER BY ...)`. В PySpark аналогичный результат достигается с помощью модуля `pyspark.sql.window`.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Инициализация сессии\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Window Function Example\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание тестового набора данных\n",
        "data = [\n",
        "    (1, \"Alice\", 10, 5000),\n",
        "    (2, \"Bob\", 10, 6000),\n",
        "    (3, \"Charlie\", 20, 7000),\n",
        "    (4, \"David\", 10, 5000),\n",
        "    (5, \"Eve\", 20, 8000)\n",
        "]\n",
        "columns = [\"empno\", \"ename\", \"deptno\", \"sal\"]\n",
        "emp_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Определение оконной спецификации:\n",
        "# — партиционирование по отделу (deptno),\n",
        "# — сортировка внутри партиции по зарплате (убывание)\n",
        "window_spec = Window.partitionBy(\"deptno\").orderBy(F.col(\"sal\").desc())\n",
        "\n",
        "# Применение оконной функции ранжирования\n",
        "result_df = emp_df.withColumn(\"rank\", F.rank().over(window_spec))\n",
        "\n",
        "print(\"Результат ранжирования сотрудников:\")\n",
        "result_df.show()\n",
        "\n",
        "# Анализ физического плана выполнения\n",
        "print(\"\\nФизический план выполнения:\")\n",
        "result_df.explain(\"formatted\")\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "+-----+-------+------+----+----+\n",
        "|empno|  ename|deptno| sal|rank|\n",
        "+-----+-------+------+----+----+\n",
        "|    2|    Bob|    10|6000|   1|\n",
        "|    1|  Alice|    10|5000|   2|\n",
        "|    4|  David|    10|5000|   2|\n",
        "|    5|    Eve|    20|8000|   1|\n",
        "|    3|Charlie|    20|7000|   2|\n",
        "+-----+-------+------+----+----+\n",
        "```\n",
        "\n",
        "> *Пояснение:* В физическом плане, который выводится командой `explain(\"formatted\")`, можно наблюдать следующую последовательность:\n",
        "> 1. **ShuffleExchange** по столбцу `deptno` — все строки с одинаковым `deptno` перераспределяются на один исполнитель.\n",
        "> 2. **Sort** внутри каждой партиции по `sal DESC`.\n",
        "> 3. **Window** — применение функции `rank()`.\n",
        ">\n",
        "> Несмотря на лаконичность кода, операция требует **shuffle**, что делает её потенциально дорогой при большом объёме данных. Это подчёркивает важный принцип: даже высокоуровневые API скрывают низкоуровневые распределённые операции, которые необходимо учитывать при проектировании пайплайнов.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Кейс 1: Устранение дорогостоящих операций Shuffle\n",
        "\n",
        "Shuffle — это операция перераспределения данных по ключу, которая неизбежна при `JOIN`, `GROUP BY` и оконных функциях с партиционированием. Её стоимость обусловлена сериализацией, передачей данных по сети и десериализацией.\n",
        "\n",
        "> **Оптимизация: Broadcast Join для малых таблиц**\n",
        "\n",
        "Наиболее эффективный способ избежать shuffle — использовать **Broadcast Hash Join**, когда одна из таблиц мала (например, справочник регионов). Spark транслирует малую таблицу в память каждого исполнителя, что позволяет выполнять соединение локально.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Broadcast Join Example\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"104857600\") \\  # 100 МБ\n",
        "    .getOrCreate()\n",
        "\n",
        "# Большая таблица фактов (например, транзакции)\n",
        "large_df = spark.range(0, 1000000).toDF(\"id\").withColumn(\"region_id\", F.col(\"id\") % 10)\n",
        "\n",
        "# Малая справочная таблица (регионы)\n",
        "small_df = spark.createDataFrame(\n",
        "    [(i, f\"Region_{i}\") for i in range(10)],\n",
        "    [\"region_id\", \"region_name\"]\n",
        ")\n",
        "\n",
        "# Принудительный broadcast для гарантии\n",
        "result = large_df.join(\n",
        "    F.broadcast(small_df),\n",
        "    on=\"region_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "print(\"Результат соединения (первые 5 строк):\")\n",
        "result.show(5)\n",
        "\n",
        "# Анализ плана: в выводе будет BroadcastExchange и BroadcastHashJoin\n",
        "print(\"\\nФизический план (фрагмент):\")\n",
        "result.explain(\"simple\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* В выводе `explain` появится строка вида `*(2) BroadcastHashJoin`, что подтверждает использование broadcast-стратегии. Это означает, что **shuffle для большой таблицы не происходит**, и вся операция выполняется за счёт локальных вычислений на каждом исполнителе. В производственной среде рекомендуется **явно указывать `broadcast()`**, даже если автоматическая оптимизация включена, чтобы избежать неожиданного переключения на shuffle-join при изменении размера данных.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3. Кейс 2: Методы борьбы с Data Skew (перекосом данных)\n",
        "\n",
        "**Data Skew** возникает, когда распределение ключей крайне неравномерно — например, 90% транзакций относятся к одному клиенту. Это приводит к тому, что одна партиция обрабатывается значительно дольше остальных, что замедляет весь этап.\n",
        "\n",
        "> **Техника салтинга (Salting) для агрегации**\n",
        "\n",
        "Салтинг — это метод искусственного разбиения «горячего» ключа на несколько подключей с помощью случайного суффикса («соли»). Это позволяет распределить нагрузку по нескольким партициям.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Salting Example\").getOrCreate()\n",
        "\n",
        "# Создание данных с перекосом: 90% записей имеют key=1\n",
        "skewed_data = (\n",
        "    [(1, 1.0)] * 900000 +  # \"горячий\" ключ\n",
        "    [(i, 1.0) for i in range(2, 10001)]  # остальные ключи\n",
        ")\n",
        "skewed_df = spark.createDataFrame(skewed_data, [\"skewed_key\", \"value\"])\n",
        "\n",
        "# Параметр: количество \"бакетов соли\"\n",
        "N = 10\n",
        "\n",
        "# Шаг 1: Добавление соли и частичная агрегация\n",
        "salted_df = (\n",
        "    skewed_df\n",
        "    .withColumn(\"salt\", (F.rand() * N).cast(IntegerType()))\n",
        "    .groupBy(\"skewed_key\", \"salt\")\n",
        "    .agg(F.sum(\"value\").alias(\"partial_sum\"))\n",
        ")\n",
        "\n",
        "# Шаг 2: Финальная агрегация (без соли)\n",
        "final_df = (\n",
        "    salted_df\n",
        "    .groupBy(\"skewed_key\")\n",
        "    .agg(F.sum(\"partial_sum\").alias(\"total_value\"))\n",
        ")\n",
        "\n",
        "print(\"Результат агрегации после салтинга:\")\n",
        "final_df.show(5)\n",
        "\n",
        "# Анализ плана: два этапа агрегации вместо одного\n",
        "print(\"\\nФизический план:\")\n",
        "final_df.explain(\"simple\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без салтинга вся работа по ключу `1` выполнялась бы на одном исполнителе, что привело бы к «застреванию» задачи. Салтинг разбивает её на 10 партиций, что ускоряет выполнение в 5–8 раз в реальных сценариях. В физическом плане видны **два этапа агрегации**: `HashAggregate` → `Exchange` → `HashAggregate`, что подтверждает корректность двойного подхода.\n",
        "\n",
        "---\n",
        "\n",
        "## V. Управление ресурсами и производственный контекст\n",
        "\n",
        "Оптимальная работа Spark в production зависит не только от кода, но и от настройки ресурсов, выбора форматов хранения и использования современных архитектур.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1. Настройка ресурсов кластера для production\n",
        "\n",
        "Неправильная конфигурация ресурсов — частая причина низкой производительности. Основной принцип: **избегать больших JVM-процессов**.\n",
        "\n",
        "> **Пример: Запуск через `spark-submit` с оптимальными параметрами**\n",
        "\n",
        "```bash\n",
        "spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --num-executors 20 \\\n",
        "  --executor-cores 4 \\\n",
        "  --executor-memory 12g \\\n",
        "  --driver-memory 4g \\\n",
        "  --conf spark.sql.adaptive.enabled=true \\\n",
        "  --conf spark.sql.autoBroadcastJoinThreshold=104857600 \\\n",
        "  --conf spark.shuffle.service.enabled=true \\\n",
        "  your_etl_script.py\n",
        "```\n",
        "\n",
        "> *Пояснение:*  \n",
        "> - `--executor-cores 4` — обеспечивает баланс между параллелизмом и GC-паузами.  \n",
        "> - `--executor-memory 12g` — достаточно для большинства задач без спиллинга на диск.  \n",
        "> - Включение `spark.shuffle.service.enabled` — обязательно для dynamic allocation и отказоустойчивости.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Современные форматы хранения: от Parquet к Delta Lake\n",
        "\n",
        "> **Пример: Работа с Delta Lake**\n",
        "\n",
        "```python\n",
        "from delta import DeltaTable\n",
        "\n",
        "# Запись в Delta-таблицу\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/sales_delta\")\n",
        "\n",
        "# Создание DeltaTable для транзакционных операций\n",
        "delta_table = DeltaTable.forPath(spark, \"/data/sales_delta\")\n",
        "\n",
        "# Операция MERGE (upsert)\n",
        "new_data = spark.createDataFrame([(101, 1500.0)], [\"id\", \"amount\"])\n",
        "delta_table.alias(\"t\").merge(\n",
        "    new_data.alias(\"s\"),\n",
        "    \"t.id = s.id\"\n",
        ").whenMatchedUpdate(set={\"amount\": \"s.amount\"}) \\\n",
        " .whenNotMatchedInsert(values={\"id\": \"s.id\", \"amount\": \"s.amount\"}) \\\n",
        " .execute()\n",
        "\n",
        "# Time Travel: чтение предыдущей версии\n",
        "historical_df = spark.read.format(\"delta\") \\\n",
        "    .option(\"versionAsOf\", 0) \\\n",
        "    .load(\"/data/sales_delta\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Delta Lake добавляет **ACID-транзакции**, **MERGE**, **Time Travel** и **оптимизированную статистику** поверх Parquet. Это делает его стандартом для современных Lakehouse-архитектур.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Мониторинг и тюнинг производительности (Production Ready)\n",
        "\n",
        "### 6.2. Диагностика через Spark UI\n",
        "\n",
        "> **Как читать метрики:**\n",
        "> - **Skew**: если 99-й перцентиль времени выполнения задач в 10–100 раз больше медианы — есть перекос.\n",
        "> - **Spill to Disk**: наличие значений в колонке `Spill (Memory)` или `Spill (Disk)` означает нехватку памяти.\n",
        "> - **GC Time**: если время GC превышает 10–15% от общего времени выполнения — уменьшайте `--executor-cores`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Мастерство работы с PySpark заключается в способности **предсказывать распределённое поведение** на основе высокоуровневого кода. Это требует:\n",
        "- понимания жизненного цикла приложения (Driver, Executors, DAG),\n",
        "- умения анализировать физический план (`explain`),\n",
        "- знания методов оптимизации (Broadcast Join, Salting),\n",
        "- владения современными инструментами (Delta Lake, Arrow-UDF),\n",
        "- системного подхода к мониторингу (Spark UI, метрики).\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dDMl-R0kBh-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 6: Веб-скрейпинг и парсинг данных — от статических страниц до распределённых динамических приложений\n",
        "\n",
        "### Введение: Фундамент сбора данных в сети\n",
        "\n",
        "Веб-скрейпинг (Web Scraping) представляет собой методический процесс автоматизированного извлечения больших объёмов неструктурированных или полуструктурированных данных с веб-сайтов. В контексте современной обработки данных этот процесс является неотъемлемой частью фазы Extract в общем цикле ETL (Extract, Transform, Load). В отличие от использования официального программного интерфейса (API), скрейпинг требует активного анализа и парсинга сырого HTML-кода и DOM-структуры, поскольку целевой ресурс не предоставляет гарантированно стабильного и структурированного формата данных. Эта особенность делает скрейпинг одновременно гибким и уязвимым инструментом, требующим глубокого понимания как веб-технологий, так и этико-правовых рамок.\n",
        "\n",
        "### 1.1. Определение Веб-скрейпинга и его Место в ETL-процессах\n",
        "\n",
        "Профессиональный скрейпинг всегда рассматривается как высоконагруженный ETL-процесс. Фаза Extract заключается в непосредственном сборе данных, который может осуществляться посредством прямых HTTP-запросов или эмуляции поведения веб-браузера. Фаза Transform включает очистку, валидацию, нормализацию и дедупликацию извлечённых данных. Фаза Load завершает цикл — данные сохраняются в целевое хранилище: реляционную или документную базу данных, файловую систему, облачное хранилище или потоковую платформу. Архитектура промышленных фреймворков, таких как Scrapy, напрямую отражает этот цикл: задачи сбора данных делегируются «паукам» (Spiders), а обработка и сохранение — конвейерам элементов (Item Pipelines).\n",
        "\n",
        "### 1.2. Этические и Правовые Границы\n",
        "\n",
        "Прежде чем приступать к сбору данных, необходимо провести тщательный юридический и этический аудит. Правовое поле веб-скрейпинга неоднозначно и сильно зависит от юрисдикции, характера собираемых данных и условий использования целевого сайта. Условия предоставления услуг (Terms of Service, ToS) имеют приоритетное значение: если ToS явно запрещают автоматизированный сбор данных, выполнение скрейпинга может повлечь юридическую ответственность и техническую блокировку IP-адресов. Регламент GDPR (General Data Protection Regulation) строго регулирует сбор и обработку идентифицируемых персональных данных (PII). Сбор таких данных без явного согласия пользователя представляет собой высокий юридический риск, даже если информация публично доступна. Промышленные системы сбора данных обязаны включать процедуры обработки запросов на удаление персональной информации. Файл `robots.txt` является де-факто стандартом для коммуникации между веб-мастерами и автоматизированными агентами. Профессиональные скрейперы должны неукоснительно соблюдать директивы `Disallow` и уважать параметр `Crawl-delay`. «Дружественный» скрейпинг означает, что процесс должен быть незаметным, не нарушать нормальное функционирование целевого сервера и не создавать чрезмерную нагрузку на его ресурсы.\n",
        "\n",
        "### 1.3. Архитектура Современных Веб-приложений\n",
        "\n",
        "Сложность инструментария, необходимого для сбора данных, напрямую определяется архитектурой целевого сайта. Статический HTML — самый простой случай: весь контент (текст, ссылки, таблицы) полностью содержится в исходном коде, полученном в ответ на HTTP-запрос. Для такого сайта достаточно базовых HTTP-клиентов и HTML-парсеров. Server-Side Rendering (SSR) представляет промежуточную сложность: основной контент генерируется на сервере, но отдельные элементы (рейтинги, комментарии, рекомендации) могут подгружаться асинхронно через AJAX. В таких случаях полнота данных может потребовать анализа сетевых запросов или частичного рендеринга. Наибольшую сложность представляют приложения с Client-Side Rendering (CSR) и архитектурой Single Page Application (SPA). Такие сайты отдают минимальный HTML-каркас и набор JavaScript-скриптов, а вся визуализация и формирование DOM-дерева происходят на стороне клиента. Для извлечения данных с подобных ресурсов требуется полная эмуляция браузерного окружения с выполнением JavaScript — для этого используются такие инструменты, как Playwright, Puppeteer или Selenium.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 1: Основы парсинга статического контента (BeautifulSoup4 + Requests)\n",
        "\n",
        "Для работы со статическими или SSR-страницами основным инструментарием в экосистеме Python являются библиотека `requests` для выполнения HTTP-запросов и `BeautifulSoup4` (BS4) для парсинга HTML-документов. Эта связка обеспечивает простоту, читаемость и достаточную гибкость для большинства задач начального и среднего уровня.\n",
        "\n",
        "### 2.1. Теория Клиент-Серверного Взаимодействия\n",
        "\n",
        "Протокол HTTP (Hypertext Transfer Protocol) лежит в основе взаимодействия между клиентом и сервером. Будучи протоколом без сохранения состояния (stateless), HTTP не запоминает контекст предыдущих запросов. Для поддержания сессий (авторизации, корзины, навигации) используются заголовки (Headers) и куки (Cookies). Библиотека `requests` позволяет эффективно управлять этим состоянием через объект `requests.Session()`. Сессия автоматически сохраняет полученные куки и прикрепляет их к последующим запросам, что позволяет имитировать поведение реального браузера и обеспечивает устойчивость к перенаправлениям, CSRF-токенам и другим механизмам защиты.\n",
        "\n",
        "### 2.2. Устойчивые HTTP-запросы с requests\n",
        "\n",
        "При скрейпинге крайне важна маскировка и устойчивость. Многие сайты анализируют HTTP-заголовки и блокируют запросы с подозрительными сигнатурами. Наличие реалистичного заголовка `User-Agent`, имитирующего популярный браузер (например, Chrome или Firefox), а также заголовка `Referer`, указывающего на предыдущую страницу, значительно снижает вероятность обнаружения и блокировки.\n",
        "\n",
        "Не менее важна обработка ошибок. В промышленном скрейпинге неудача при получении ответа — например, HTTP-код 429 «Too Many Requests» или 5xx «Server Error» — не должна приводить к немедленному повторному запросу. Такое поведение усугубляет нагрузку на сервер и гарантирует блокировку. Профессиональным решением является использование стратегии **экспоненциального замедления** (Exponential Backoff). Эта методика предусматривает постепенное увеличение задержки между повторными попытками: вместо фиксированной паузы задержка растёт с каждой неудачной попыткой, например, по формуле $D = \\text{backoff\\_factor} \\cdot (2^{(R - 1)})$, где $D$ — задержка, а $R$ — номер попытки. Такой подход демонстрирует «дружественное» поведение, даёт серверу время на восстановление и повышает общую надёжность скрипта.\n",
        "\n",
        "Пример реализации устойчивой сессии с поддержкой экспоненциального замедления:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def create_resilient_session(max_retries=5, backoff_factor=1):\n",
        "    \"\"\"Создаёт сессию requests с логикой повторных попыток и экспоненциальным замедлением.\"\"\"\n",
        "    retry_strategy = Retry(\n",
        "        total=max_retries,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        backoff_factor=backoff_factor,\n",
        "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session = requests.Session()\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    return session\n",
        "\n",
        "# Пример использования\n",
        "session = create_resilient_session()\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "try:\n",
        "    response = session.get('https://example.com/data', headers=headers, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    print(\"Успешно получен статус:\", response.status_code)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Критическая ошибка после всех попыток: {e}\")\n",
        "```\n",
        "\n",
        "Этот код создаёт HTTP-сессию, которая автоматически повторяет запросы при временных ошибках, постепенно увеличивая паузу между попытками. Такая практика является стандартом для production-скриптов.\n",
        "\n",
        "### 2.3. Построение DOM-Дерева и Навигация (BeautifulSoup)\n",
        "\n",
        "После получения HTML-контента необходимо преобразовать его из плоского текста в иерархическую структуру — дерево объектов (DOM-дерево). Библиотека BeautifulSoup4 (BS4) является наиболее популярным инструментом для этой задачи благодаря своей толерантности к невалидному HTML и интуитивно понятному API. BS4 позволяет легко находить элементы по тегам, атрибутам, текстовому содержимому и CSS-селекторам.\n",
        "\n",
        "Однако важно учитывать производительность. BS4 сама по себе является обёрткой над внешними парсерами. Наиболее эффективный выбор — использовать `lxml` в качестве бэкенда. Библиотека `lxml` основана на высокоскоростных C-библиотеках `libxml2` и `libxslt`, что делает её значительно быстрее встроенного `html.parser`, особенно при обработке больших объёмов данных. Кроме того, `lxml` поддерживает мощный язык запросов XPath, который позволяет точно навигировать по сложным и глубоко вложенным структурам. Сама BS4 не поддерживает XPath напрямую, но при использовании `lxml` как парсера можно комбинировать подходы или перейти к более продвинутым библиотекам, таким как `parsel` (используется в Scrapy).\n",
        "\n",
        "Пример парсинга с использованием BS4 и `lxml`:\n",
        "\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://example-news-site.com\"\n",
        "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0...\"})\n",
        "soup = BeautifulSoup(response.content, 'lxml')  # Используем lxml для скорости\n",
        "\n",
        "# Извлечение заголовков статей с помощью CSS-селектора\n",
        "headlines = [h.get_text(strip=True) for h in soup.select('h2.article-title')]\n",
        "print(\"Найдено заголовков:\", len(headlines))\n",
        "```\n",
        "\n",
        "В этом примере используется CSS-селектор `h2.article-title` для точного извлечения заголовков, а `lxml` обеспечивает быструю обработку даже при большом объёме HTML.\n",
        "\n",
        "### 2.4. Практика: Парсинг и Обход Пагинации\n",
        "\n",
        "Обход пагинации — одна из самых распространённых задач при сборе данных. Сайты реализуют пагинацию по-разному: через параметры URL (например, `?page=2`), смещение (`?offset=20`), или динамическую подгрузку по клику на кнопку «Далее». В случае статической пагинации процесс сводится к циклическому формированию URL и извлечению данных с каждой страницы.\n",
        "\n",
        "Ключевые шаги: сначала анализируется структура URL или HTML-кнопки перехода, затем реализуется цикл, который последовательно запрашивает каждую страницу. Важно соблюдать «дружественные» практики: добавлять задержки между запросами, использовать устойчивую сессию и обрабатывать возможные ошибки.\n",
        "\n",
        "Пример обхода пагинации:\n",
        "\n",
        "```python\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def scrape_paginated_site(base_url, total_pages):\n",
        "    all_data = []\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\"User-Agent\": \"Mozilla/5.0...\"})\n",
        "    \n",
        "    for page_num in range(1, total_pages + 1):\n",
        "        url = f\"{base_url}?page={page_num}\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            \n",
        "            # Извлечение элементов с помощью CSS-селектора\n",
        "            items = soup.select('div.product-item h3')\n",
        "            for item in items:\n",
        "                title = item.get_text(strip=True)\n",
        "                all_data.append(title)\n",
        "                \n",
        "            print(f\"Обработана страница {page_num}\")\n",
        "            time.sleep(1.5)  # Уважительная задержка\n",
        "            \n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Ошибка на странице {page_num}: {e}\")\n",
        "            break\n",
        "            \n",
        "    return all_data\n",
        "\n",
        "# Запуск сбора\n",
        "data = scrape_paginated_site(\"https://example-store.com/products\", total_pages=10)\n",
        "print(f\"Всего собрано элементов: {len(data)}\")\n",
        "```\n",
        "\n",
        "Этот код демонстрирует полный цикл: инициализация сессии, постраничный запрос, извлечение данных и уважительная задержка. Такой подход легко масштабируется и адаптируется под различные схемы пагинации.\n",
        "\n",
        "### 2.5. Резюме Части 1\n",
        "\n",
        "Инструментарий `requests` и `BeautifulSoup4` (предпочтительно с парсером `lxml`) идеально подходит для быстрого прототипирования, сбора данных с небольших статических или SSR-сайтов, а также для первичного анализа структуры веб-ресурсов. Его преимущества — простота, читаемость кода и низкий порог входа. Однако у этого подхода есть чёткие границы применимости: он является синхронным, не масштабируется для высоконагруженных задач и совершенно неспособен обрабатывать контент, генерируемый JavaScript. При переходе к промышленным объёмам, динамическим SPA или требованию высокой пропускной способности необходимо переходить к асинхронным фреймворкам, таким как Scrapy, или к решениям с полной эмуляцией браузера.\n",
        ""
      ],
      "metadata": {
        "id": "sNG32N6rFAhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Часть 2: Промышленный фреймворк для скрейпинга (Scrapy)\n",
        "\n",
        "Scrapy — это мощный, полнофункциональный фреймворк для веб-скрейпинга, написанный на Python. Он представляет собой промышленный стандарт для высоконагруженного сбора данных с сайтов, не требующих выполнения JavaScript. Благодаря своей архитектуре, Scrapy обеспечивает не только высокую производительность, но и чёткую структуру для всего ETL-цикла — от извлечения до загрузки.\n",
        "\n",
        "### 3.1. Архитектура Scrapy: Асинхронное Ядро\n",
        "\n",
        "Ключевое архитектурное преимущество Scrapy заключается в использовании асинхронного фреймворка Twisted в качестве основы. Этот подход позволяет эффективно управлять тысячами одновременных сетевых операций в одном потоке, избегая блокировок и достигая высокой скорости обхода. В отличие от синхронных решений (например, `requests`), Scrapy не ждёт завершения каждого запроса, а продолжает обрабатывать другие задачи, что делает его особенно эффективным при работе с большим числом URL.\n",
        "\n",
        "Архитектура Scrapy состоит из нескольких взаимосвязанных компонентов. Ядро (Scrapy Engine) выступает главным контроллером, координирующим обмен данными между всеми частями системы. Планировщик (Scheduler) отвечает за управление очередью запросов: он получает их от пауков, упорядочивает по приоритету и гарантирует, что одна и та же страница не будет запрошена дважды благодаря встроенному механизму дедупликации. Менеджер загрузок (Downloader) выполняет асинхронные HTTP-запросы и возвращает HTML-ответы. Пауки (Spiders) содержат логику обхода сайта и извлечения данных. Item Pipelines отвечают за обработку и сохранение структурированных данных, а Downloader Middleware позволяет вмешиваться в процесс обработки запросов и ответов на низком уровне — например, для ротации прокси или заголовков.\n",
        "\n",
        "### 3.2. Structuring ETL: Items, Pipelines и Middleware\n",
        "\n",
        "В Scrapy данные структурируются с помощью классов `scrapy.Item`. Эти контейнеры похожи на словари, но имеют строго определённые поля (`Field`), что обеспечивает типобезопасность и предсказуемость на всех этапах обработки. Например, можно явно указать, что каждый товар должен иметь `title`, `price`, `url` и `sku`.\n",
        "\n",
        "Пример определения структуры данных:\n",
        "\n",
        "```python\n",
        "# items.py\n",
        "import scrapy\n",
        "\n",
        "class ProductItem(scrapy.Item):\n",
        "    title = scrapy.Field()\n",
        "    price = scrapy.Field()\n",
        "    url = scrapy.Field()\n",
        "    sku = scrapy.Field()  # Артикул для дедупликации\n",
        "```\n",
        "\n",
        "Item Pipelines — это последовательность классов, через которые проходят все извлечённые элементы. Каждый этап конвейера выполняет определённую задачу, соответствующую фазам Transform и Load ETL-цикла. На первом этапе данные могут быть очищены от HTML-тегов, преобразованы в числовые типы или проверены на наличие обязательных полей. Невалидные элементы можно отбрасывать с помощью исключения `DropItem`. На следующем этапе реализуется дедупликация — например, по уникальному `sku` или `url`, чтобы избежать дублирования в хранилище. Наконец, данные сохраняются в выбранный формат: JSON Lines, CSV, или напрямую в базу данных с использованием SQLAlchemy или другого ORM.\n",
        "\n",
        "Downloader Middleware — это мощный механизм для настройки поведения запросов. Он позволяет реализовать ротацию заголовков `User-Agent`, автоматическую смену прокси-серверов и гибкую логику повторных попыток. Scrapy включает встроенный `RetryMiddleware`, который автоматически повторяет запросы при временных ошибках (например, HTTP 500, 503, 504). Количество попыток и коды ошибок настраиваются через параметры `RETRY_TIMES` и `RETRY_HTTP_CODES`. Неудачные запросы возвращаются в очередь с пониженным приоритетом, что обеспечивает устойчивость к временным сбоям сети или сервера.\n",
        "\n",
        "### 3.3. Создание Паука (Spider) и Обход\n",
        "\n",
        "Паук (Spider) — это сердце любого Scrapy-проекта. Он определяет, с каких URL начинать обход, как извлекать данные и как переходить по ссылкам. Scrapy использует библиотеку `parsel` (основанную на `lxml`) для навигации по HTML с помощью CSS-селекторов и XPath.\n",
        "\n",
        "Пример паука для каталога товаров:\n",
        "\n",
        "```python\n",
        "# spiders/product_spider.py\n",
        "import scrapy\n",
        "from myproject.items import ProductItem\n",
        "\n",
        "class ProductSpider(scrapy.Spider):\n",
        "    name = \"product_spider\"\n",
        "    start_urls = ['https://example-store.com/catalog']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Извлечение карточек товаров\n",
        "        for card in response.css('div.product-card'):\n",
        "            item = ProductItem()\n",
        "            item['title'] = card.css('h2.title::text').get()\n",
        "            item['price'] = card.css('span.price::text').re_first(r'(\\d+)')\n",
        "            item['url'] = response.urljoin(card.css('a::attr(href)').get())\n",
        "            yield item\n",
        "\n",
        "        # Переход на следующую страницу пагинации\n",
        "        next_page = response.css('a.pagination-next::attr(href)').get()\n",
        "        if next_page:\n",
        "            yield response.follow(next_page, self.parse)\n",
        "```\n",
        "\n",
        "Этот код демонстрирует три ключевых паттерна Scrapy: извлечение данных с текущей страницы, генерация структурированного элемента и рекурсивный обход по ссылкам. Все запросы обрабатываются асинхронно, а дубликаты URL автоматически фильтруются планировщиком.\n",
        "\n",
        "### 3.4. Масштабирование и Распределённый Скрейпинг\n",
        "\n",
        "При сборе данных в промышленных масштабах (миллионы и миллиарды страниц) одного сервера недостаточно. Для горизонтального масштабирования используется расширение **Scrapy-Redis**. Оно заменяет локальный планировщик Scrapy на распределённый, использующий Redis в качестве централизованного хранилища состояния.\n",
        "\n",
        "В такой архитектуре все рабочие узлы (воркеры) подключаются к одному экземпляру Redis. Запросы, сгенерированные любым пауком, помещаются в общую очередь, откуда их может взять любой свободный узел. Глобальный механизм дедупликации на основе отпечатков (fingerprints) хранится в Redis, что гарантирует, что одна и та же страница не будет обработана дважды, даже если генерация запроса происходила на разных машинах. При сбое одного из воркеров его задачи автоматически перераспределяются, что обеспечивает отказоустойчивость.\n",
        "\n",
        "Для обхода сайтов с динамическим контентом Scrapy можно интегрировать с инструментами рендеринга JavaScript. Например, **Scrapy-Playwright** или **Scrapy-Splash** работают как специальные Downloader Middleware: они перехватывают запросы, требующие выполнения JavaScript, отправляют их во внешний браузерный движок, а затем возвращают полностью отрендеренный HTML в паук для обычного парсинга. Это позволяет сохранить преимущества асинхронной архитектуры Scrapy даже при работе с SPA.\n",
        "\n",
        "Для команд, не желающих заниматься инфраструктурой, существуют облачные платформы, такие как **Zyte** (ранее Scrapy Cloud). Они предоставляют управляемую среду для деплоя, мониторинга и автоматического масштабирования пауков, а также включают встроенные инструменты для обхода антибот-систем, ротации прокси и решения CAPTCHA.\n",
        "\n",
        "### 3.5. Резюме Части 2\n",
        "\n",
        "Scrapy — это зрелый, масштабируемый фреймворк, идеально подходящий для крупномасштабного сбора данных со статических и SSR-сайтов. Его архитектура обеспечивает высокую производительность, а модульная структура — чёткое разделение ответственности между этапами ETL. Однако он не предназначен для сайтов, где основной контент полностью генерируется JavaScript на клиенте. В таких случаях требуется полная эмуляция браузера, что выводит нас за пределы возможностей классического Scrapy.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 3: Автоматизация браузера для сложного JavaScript (Selenium)\n",
        "\n",
        "Когда сайт построен по архитектуре Single Page Application (SPA), традиционный HTTP-скрейпинг теряет смысл: сервер возвращает только пустой HTML-каркас и JavaScript-файлы, а весь контент формируется в браузере. Для извлечения данных с таких ресурсов требуется эмуляция поведения реального пользователя — именно эту задачу решает **Selenium WebDriver**.\n",
        "\n",
        "### 4.1. Теория: Принципы работы WebDriver\n",
        "\n",
        "Selenium использует протокол W3C WebDriver для взаимодействия между кодом на Python и физическим браузером (например, Chrome или Firefox). Архитектура состоит из трёх уровней: скрипт на Python → драйвер браузера (например, `chromedriver`) → сам браузер. Все команды передаются через HTTP-запросы, что делает архитектуру универсальной, но вносит задержки. Важное отличие от статического парсинга: Selenium не просто получает HTML — он запускает полноценный браузер, выполняет весь JavaScript, загружает ресурсы и рендерит DOM, как это сделал бы человек.\n",
        "\n",
        "### 4.2. Практика: Настройка и Управление\n",
        "\n",
        "Работа с Selenium требует установки соответствующего драйвера для выбранного браузера. Для упрощения управления рекомендуется использовать менеджеры драйверов, такие как `webdriver-manager`, которые автоматически скачивают нужную версию.\n",
        "\n",
        "Одной из главных сложностей при работе с динамическим контентом является **синхронизация**. Selenium не может автоматически определить, когда JavaScript завершил рендеринг или когда AJAX-запрос вернул данные. Попытка взаимодействовать с элементом до его появления приведёт к исключению. Для решения этой проблемы используются **явные ожидания** (Explicit Waits) через класс `WebDriverWait`.\n",
        "\n",
        "Пример безопасного ожидания динамического элемента:\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example.com/dynamic-content\")\n",
        "\n",
        "try:\n",
        "    # Ожидание появления элемента с ID 'result' в течение 10 секунд\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.ID, \"result\"))\n",
        "    )\n",
        "    print(\"Данные загружены:\", element.text)\n",
        "except Exception as e:\n",
        "    print(\"Элемент не появился вовремя:\", e)\n",
        "finally:\n",
        "    driver.quit()\n",
        "```\n",
        "\n",
        "Этот подход делает скрипты надёжными: вместо фиксированных задержек (`time.sleep()`) код ждёт именно нужного состояния страницы.\n",
        "\n",
        "### 4.3. Реализация Сложных Сценариев\n",
        "\n",
        "Selenium позволяет эмулировать действия пользователя: кликать по кнопкам, вводить текст в формы, прокручивать страницу и даже загружать файлы. Это критически важно для скрейпинга сайтов с ленивой загрузкой контента или многошаговыми формами.\n",
        "\n",
        "Например, для загрузки скрытых товаров в интернет-магазине можно прокручивать страницу вниз до тех пор, пока не перестанут появляться новые элементы:\n",
        "\n",
        "```python\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "```\n",
        "\n",
        "Для отладки сложных сценариев полезно сохранять скриншоты или HTML-код страницы в момент ошибки — это помогает понять, на каком этапе скрипт отклонился от ожидаемого поведения.\n",
        "\n",
        "\n",
        "### 4.4. Обход CAPTCHA\n",
        "\n",
        "Столкновение с системами CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) — одна из наиболее частых и сложных проблем при автоматизации веб-скрейпинга. Современные реализации, такие как **reCAPTCHA v2/v3** от Google или **hCaptcha**, интегрированы в форму отправки данных и активируются при подозрении на автоматизированное поведение. Хотя полностью обойти CAPTCHA без внешней помощи невозможно, её можно интегрировать в автоматизированный workflow с использованием специализированных сервисов распознавания.\n",
        "\n",
        "Общий сценарий обхода CAPTCHA состоит из нескольких этапов:  \n",
        "— сначала скрипт должен обнаружить наличие CAPTCHA на странице;  \n",
        "— затем извлечь её идентификаторы и параметры (в первую очередь `sitekey`);  \n",
        "— передать эти данные в сторонний сервис решения (например, 2Captcha, Anti-Captcha или CapMonster);  \n",
        "— дождаться получения токена-ответа;  \n",
        "— ввести этот токен в скрытое поле формы;  \n",
        "— и только после этого выполнить отправку.\n",
        "\n",
        "Рассмотрим каждый шаг на примере обхода **reCAPTCHA v2** с использованием Selenium и сервиса **2Captcha**.\n",
        "\n",
        "#### Шаг 1: Обнаружение CAPTCHA\n",
        "\n",
        "Скрипт должен уметь определять, появилась ли CAPTCHA. Для reCAPTCHA это делается через поиск iframe с определённым идентификатором или проверку наличия скрытого поля с атрибутом `data-sitekey`.\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example-form-with-recaptcha.com\")\n",
        "\n",
        "# Ожидание появления reCAPTCHA на странице\n",
        "try:\n",
        "    captcha_iframe = WebDriverWait(driver, 5).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[src*='recaptcha']\"))\n",
        "    )\n",
        "    print(\"Обнаружена reCAPTCHA v2.\")\n",
        "    has_captcha = True\n",
        "except:\n",
        "    has_captcha = False\n",
        "    print(\"CAPTCHA не обнаружена.\")\n",
        "```\n",
        "\n",
        "#### Шаг 2: Извлечение параметров\n",
        "\n",
        "Если CAPTCHA обнаружена, необходимо извлечь её ключ — `sitekey`. Он обычно содержится в атрибуте `data-sitekey` у самого элемента `<div class=\"g-recaptcha\">` или в URL iframe.\n",
        "\n",
        "```python\n",
        "if has_captcha:\n",
        "    # Извлечение sitekey из DOM\n",
        "    recaptcha_div = driver.find_element(By.CSS_SELECTOR, \"div.g-recaptcha\")\n",
        "    sitekey = recaptcha_div.get_attribute(\"data-sitekey\")\n",
        "    page_url = driver.current_url\n",
        "    print(f\"Извлечён sitekey: {sitekey}\")\n",
        "```\n",
        "\n",
        "#### Шаг 3: Отправка задачи на решение\n",
        "\n",
        "Сервисы вроде 2Captcha предоставляют REST API для решения CAPTCHA. Для reCAPTCHA v2 требуется отправить POST-запрос с вашим API-ключом, `sitekey` и URL страницы.\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import time\n",
        "\n",
        "API_KEY = \"ваш_ключ_2captcha\"\n",
        "CAPTCHA_METHOD = \"userrecaptcha\"\n",
        "\n",
        "# Отправка задачи на решение\n",
        "task_resp = requests.post(\"http://2captcha.com/in.php\", data={\n",
        "    'key': API_KEY,\n",
        "    'method': CAPTCHA_METHOD,\n",
        "    'googlekey': sitekey,\n",
        "    'pageurl': page_url,\n",
        "    'json': 1\n",
        "})\n",
        "\n",
        "task_data = task_resp.json()\n",
        "if task_data.get(\"status\") == 1:\n",
        "    captcha_id = task_data[\"request\"]\n",
        "    print(f\"Задача отправлена, ID: {captcha_id}\")\n",
        "else:\n",
        "    raise Exception(f\"Ошибка создания задачи: {task_data.get('request')}\")\n",
        "\n",
        "# Ожидание результата (обычно 10–30 секунд)\n",
        "while True:\n",
        "    time.sleep(5)\n",
        "    result_resp = requests.get(\n",
        "        f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "    )\n",
        "    result_data = result_resp.json()\n",
        "    if result_data.get(\"status\") == 1:\n",
        "        captcha_token = result_data[\"request\"]\n",
        "        print(\"Токен CAPTCHA получен.\")\n",
        "        break\n",
        "    elif result_data[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "        continue\n",
        "    else:\n",
        "        raise Exception(f\"Ошибка получения результата: {result_data['request']}\")\n",
        "```\n",
        "\n",
        "#### Шаг 4: Вставка токена и отправка формы\n",
        "\n",
        "reCAPTCHA v2 ожидает, что токен будет помещён в скрытое поле формы с именем `g-recaptcha-response`. Иногда это поле изначально отсутствует и создаётся динамически — в таком случае его нужно вставить в DOM вручную.\n",
        "\n",
        "```python\n",
        "# Найти или создать скрытое поле для токена\n",
        "try:\n",
        "    token_field = driver.find_element(By.NAME, \"g-recaptcha-response\")\n",
        "except:\n",
        "    # Если поле не существует, создаём его\n",
        "    driver.execute_script(\"\"\"\n",
        "        var response = document.createElement('textarea');\n",
        "        response.name = 'g-recaptcha-response';\n",
        "        response.style.display = 'none';\n",
        "        document.querySelector('form').appendChild(response);\n",
        "    \"\"\")\n",
        "    token_field = driver.find_element(By.NAME, \"g-recaptcha-response\")\n",
        "\n",
        "# Вставить токен\n",
        "driver.execute_script(\"arguments[0].value = arguments[1];\", token_field, captcha_token)\n",
        "\n",
        "# Отправить форму\n",
        "submit_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n",
        "submit_button.click()\n",
        "\n",
        "print(\"Форма отправлена с решённой CAPTCHA.\")\n",
        "```\n",
        "\n",
        "#### Альтернатива: Ручной ввод\n",
        "\n",
        "В исследовательских или низкочастотных сценариях можно приостановить выполнение и запросить у оператора ввод решения вручную. Это особенно полезно при отладке или при работе с дорогостоящими CAPTCHA.\n",
        "\n",
        "```python\n",
        "if has_captcha:\n",
        "    input(\"CAPTCHA обнаружена. Пройдите проверку вручную и нажмите Enter...\")\n",
        "    # После ввода оператором скрипт продолжает выполнение\n",
        "```\n",
        "\n",
        "Такой подход не масштабируем, но исключительно надёжен и не требует финансовых затрат.\n",
        "\n",
        "#### Важные нюансы\n",
        "\n",
        "Стоимость решения одной reCAPTCHA v2 через 2Captcha составляет около \\$0.5–\\$1 за 1000 решений, что делает такой подход экономически оправданным только при высокой ценности данных. Для reCAPTCHA v3, которая не требует визуального взаимодействия, сервисы имитируют поведенческий профиль и возвращают оценку `score` в виде токена. Кроме того, некоторые сайты используют «невидимую» CAPTCHA, которая срабатывает фоново — в таких случаях необходимо эмулировать поведение пользователя (движения мыши, задержки) до отправки формы, иначе токен может быть отклонён.\n",
        "\n",
        "Таким образом, обход CAPTCHA — это не обход в прямом смысле, а **делегирование** задачи распознавания человеку или машинному сервису с последующей интеграцией ответа в автоматизированный процесс. Это требует тщательной обработки ошибок, управления временем ожидания и соблюдения этических и юридических норм при использовании внешних сервисов.\n",
        "\n",
        "\n",
        "### 4.5. Резюме Части 3\n",
        "\n",
        "Selenium — мощный инструмент для работы с динамическими, JavaScript-интенсивными сайтами. Он обеспечивает полную эмуляцию поведения пользователя, что делает его незаменимым для сложных сценариев. Однако его архитектура на основе HTTP-коммуникации с внешним браузером приводит к высокой ресурсоёмкости, низкой скорости и потенциальной нестабильности. Эти недостатки делают Selenium менее подходящим для высокоскоростного, массового скрейпинга, где предпочтение отдаётся более лёгким и управляемым решениям, таким как Playwright или Puppeteer в headless-режиме.\n",
        ""
      ],
      "metadata": {
        "id": "_m23d5ZYGPXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Часть 4: Современный подход к браузерной автоматизации (Playwright)\n",
        "\n",
        "Playwright — это современный фреймворк для автоматизации браузеров, разработанный Microsoft и быстро ставший промышленным стандартом для скрейпинга динамических веб-приложений. В отличие от устаревших решений, Playwright устраняет ключевые архитектурные недостатки Selenium и обеспечивает высокую производительность, надёжность и удобство разработки.\n",
        "\n",
        "### 5.1. Теория: Архитектурные Преимущества\n",
        "\n",
        "Фундаментальное отличие Playwright заключается в способе взаимодействия с браузерными движками. В то время как Selenium опирается на HTTP-протокол и промежуточные драйверы (например, `chromedriver`), Playwright устанавливает прямое, постоянное соединение с ядром браузера через WebSocket. Такой подход обеспечивает нативный контроль над Chromium, Firefox и WebKit, минимизируя задержки и накладные расходы на сериализацию запросов. Это не просто архитектурное улучшение — это кардинальное повышение эффективности.\n",
        "\n",
        "Одной из самых значимых инноваций Playwright является механизм **автоматического ожидания** (Auto-Wait). Перед выполнением любого действия — клика, ввода текста, извлечения содержимого — Playwright автоматически проверяет, что целевой элемент присутствует в DOM, видим, стабилен и готов к взаимодействию. Это устраняет необходимость вручную настраивать сложные условия ожидания, как это требуется в Selenium, где разработчик должен явно указывать, на что именно стоит ждать (`visibility_of_element_located`, `element_to_be_clickable` и т.д.). В результате код становится короче, чище и устойчивее к колебаниям времени загрузки страницы.\n",
        "\n",
        "### 5.2. Практика Playwright: Скорость и Эффективность\n",
        "\n",
        "Playwright предоставляет унифицированный и лаконичный API как для синхронного, так и для асинхронного использования. Он поддерживает headless-режим по умолчанию, что делает его идеальным для автоматизированных задач сбора данных.\n",
        "\n",
        "Пример базового скрипта для извлечения динамического контента:\n",
        "\n",
        "```python\n",
        "from playwright.sync_api import sync_playwright\n",
        "\n",
        "def scrape_dynamic_data(url):\n",
        "    with sync_playwright() as p:\n",
        "        browser = p.chromium.launch(headless=True)\n",
        "        page = browser.new_page()\n",
        "        page.goto(url)\n",
        "        \n",
        "        # Автоматическое ожидание появления элемента\n",
        "        page.wait_for_selector(\".dynamic-content\", state=\"visible\")\n",
        "        \n",
        "        # Взаимодействие с элементом\n",
        "        page.click(\"button#load-more\")\n",
        "        \n",
        "        # Извлечение текста из всех элементов с классом .result-item\n",
        "        data = page.locator(\".result-item\").all_text_contents()\n",
        "        \n",
        "        browser.close()\n",
        "        return data\n",
        "```\n",
        "\n",
        "Этот код демонстрирует ключевые преимущества Playwright: отсутствие явных ожиданий, интуитивный синтаксис (`page.click`, `page.locator`) и встроенную поддержку headless-режима. Playwright также позволяет легко эмулировать различные условия — например, мобильные устройства или конкретные геолокации — через создание контекстов с заданными параметрами. Это особенно полезно при сборе данных, которые варьируются в зависимости от региона или типа устройства.\n",
        "\n",
        "### 5.3. Перехват и Модификация Сетевых Запросов (Оптимизация)\n",
        "\n",
        "Одной из самых мощных функций Playwright является возможность перехвата и модификации всего сетевого трафика. Это открывает путь к радикальной оптимизации производительности при работе с SPA-приложениями.\n",
        "\n",
        "При загрузке типичного современного сайта до 80% времени и ресурсов уходит на получение ненужных для скрейпинга ресурсов: изображений, шрифтов, рекламных скриптов, аналитики и метрик. Playwright позволяет блокировать такие запросы на лету, что значительно сокращает время загрузки, потребление памяти и сетевой трафик.\n",
        "\n",
        "Пример блокировки ненужных ресурсов:\n",
        "\n",
        "```python\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def run_scraper_optimized(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch()\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Блокировка изображений и шрифтов\n",
        "        await page.route(\n",
        "            \"**/*.{png,jpg,jpeg,gif,webp,woff,woff2,ttf,eot}\",\n",
        "            lambda route: route.abort()\n",
        "        )\n",
        "\n",
        "        await page.goto(url)\n",
        "        content = await page.locator(\"div.main-content\").inner_text()\n",
        "        await browser.close()\n",
        "        return content\n",
        "```\n",
        "\n",
        "Кроме блокировки, перехват запросов позволяет получать данные напрямую из AJAX-вызовов. Например, если SPA загружает данные через `fetch()` в формате JSON, можно перехватить этот ответ и извлечь структурированные данные, минуя рендеринг DOM и парсинг HTML. Это не только быстрее, но и надёжнее, поскольку структура JSON-ответа обычно стабильнее, чем разметка страницы.\n",
        "\n",
        "### 5.4. Организация Параллельного Выполнения\n",
        "\n",
        "Playwright эффективно использует системные ресурсы, позволяя создавать множество изолированных браузерных контекстов в рамках одного процесса. Каждый контекст представляет собой независимую сессию с собственными куками, локальным хранилищем и настройками. Это обеспечивает высокую степень параллелизации без необходимости запускать отдельный экземпляр браузера на каждый запрос, как это часто делают в Selenium. В результате пропускная способность скрейпера возрастает на порядки.\n",
        "\n",
        "### 5.5. Резюме Части 4\n",
        "\n",
        "Playwright представляет собой современное, архитектурно превосходящее решение для скрейпинга динамических сайтов. Его нативное взаимодействие с браузерными движками, автоматические ожидания, возможность перехвата и фильтрации сетевых запросов, а также поддержка эффективной параллелизации делают его наиболее производительным и надёжным инструментом для работы со сложными SPA. В промышленной практике Playwright постепенно вытесняет Selenium как основной выбор для задач, требующих выполнения JavaScript.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 5: Продвинутые техники обхода ограничений\n",
        "\n",
        "Современные веб-сайты активно защищаются от автоматизированного сбора данных, используя многоуровневые системы обнаружения ботов. Это требует от промышленных скрейперов применения адаптивных, многослойных стратегий защиты и обхода.\n",
        "\n",
        "### 6.1. Теория: Эволюция Систем Защиты\n",
        "\n",
        "Современные решения, такие как Cloudflare, Akamai или PerimeterX, используют комплексные эвристики для идентификации нечеловеческого трафика. Основные методы включают ограничение скорости запросов с одного IP-адреса (Rate Limiting), что приводит к ответам с кодом 429 «Too Many Requests». Более изощрённые системы применяют **фингерпринтинг** — создание уникального «отпечатка» браузера на основе сотен параметров: версии движка, списка плагинов, характеристик Canvas и WebGL, поведения при рендеринге и даже временных задержек в JavaScript. Отдельное внимание уделяется обнаружению признаков автоматизации: во многих браузерных движках, управляемых через WebDriver, устанавливается скрытое свойство `window.webdriver`, которое легко детектируется. Наконец, системы могут анализировать поведение пользователя: неестественно быстрый скроллинг, отсутствие движений мыши, идеально точные клики — всё это может быть признаком бота.\n",
        "\n",
        "### 6.2. Построение Надёжной Системы Ротации\n",
        "\n",
        "Для успешного обхода защит требуется динамическая смена идентификационных данных. Ключевой компонент — это ротация HTTP-заголовков, в первую очередь `User-Agent`. Использование библиотек вроде `fake-useragent` позволяет генерировать реалистичные, постоянно обновляемые строки, имитирующие популярные браузеры и устройства. Ещё важнее — управление IP-адресами. Датацентровые прокси, хотя и дешевы, легко блокируются, так как их IP-адреса принадлежат известным диапазонам центров обработки данных. В то же время резидентские прокси, использующие IP-адреса реальных домашних или мобильных пользователей, значительно эффективнее обходят современные системы защиты. В Scrapy ротация прокси и заголовков реализуется через Downloader Middleware, а в Playwright — через параметры при создании нового контекста (`browser.new_context(proxy=...)`).\n",
        "\n",
        "### 6.3. Маскировка Автоматизации (Stealth Techniques)\n",
        "\n",
        "Даже при использовании резидентских прокси и реалистичных заголовков автоматизированный браузер может выдать себя через специфические JavaScript-свойства. Для решения этой проблемы применяются **stealth-техники** — инъекция скриптов, которые модифицируют или удаляют признаки WebDriver до загрузки целевой страницы. Например, можно переопределить `navigator.webdriver`, подменить WebGL-рендерер, скрыть автоматическое разрешение экрана и многое другое. В экосистеме Playwright существуют специализированные библиотеки, такие как `playwright-stealth`, которые автоматически применяют десятки проверенных модификаций, делая автоматизированный браузер практически неотличимым от обычного.\n",
        "\n",
        "### 6.4. Многоуровневый Алгоритм Реагирования на Блокировки\n",
        "\n",
        "Для обеспечения промышленной устойчивости необходимо внедрить иерархическую систему реагирования на ошибки. На первом уровне — временные сбои (HTTP 429, 5xx) — применяется стратегия экспоненциального замедления и повторная попытка. Если повторный запрос также завершается ошибкой, система переходит ко второму уровню: выполняется ротация заголовков и, при необходимости, смена cookies. На третьем уровне, при устойчивых блокировках (HTTP 403 Forbidden), запрос перенаправляется через новый IP-адрес из пула резидентских прокси. Наконец, если сайт выдаёт CAPTCHA, запрос автоматически передаётся в сторонний сервис решения (например, 2Captcha), и полученный токен вводится в форму через тот же Playwright. Такой многоступенчатый подход позволяет поддерживать высокий уровень успешности даже при работе с наиболее защищёнными ресурсами.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 6: Инструменты и правовые аспекты промышленного скрейпинга\n",
        "\n",
        "### 7.1. Сравнительный Анализ Производительности Парсеров\n",
        "\n",
        "Выбор парсера напрямую влияет на производительность промышленного скрейпинга. Библиотека `lxml`, основанная на высокоскоростных C-библиотеках `libxml2` и `libxslt`, является безусловным лидером по скорости парсинга HTML и XML. Она поддерживает мощный язык XPath, что делает её незаменимой для навигации по сложным структурам. `Parsel` — это обёртка над `lxml`, используемая в Scrapy для унификации селекторов; она сохраняет всю производительность `lxml`, добавляя поддержку CSS-селекторов. В отличие от этого, `BeautifulSoup4`, хотя и превосходит конкурентов в устойчивости к невалидному HTML, значительно уступает в скорости и рекомендуется только для прототипирования или небольших задач, где важна простота кода, а не пропускная способность.\n",
        "\n",
        "### 7.2. Алгоритм Принятия Решения: Scraping vs Official API\n",
        "\n",
        "Стратегический выбор между использованием официального API и разработкой собственного скрейпера должен основываться на комплексной оценке. Официальный API всегда предпочтителен: он предоставляет структурированные, стабильные данные, минимизирует юридические риски и не требует постоянного сопровождения из-за изменений в DOM-структуре. Скрейпинг оправдан только в ситуациях, когда API отсутствует, непомерно дорог, накладывает жёсткие ограничения на объём или частоту запросов, либо не предоставляет доступ к историческим данным, необходимым для анализа. Таким образом, скрейпинг — это вынужденная мера, а не предпочтительный путь.\n",
        "\n",
        "### 7.3. Юридические Прецеденты и Практика Соблюдения\n",
        "\n",
        "Хотя судебная практика в некоторых юрисдикциях (например, в США по делу *hiQ Labs v. LinkedIn*) подтверждает право на сбор публично доступных данных, это не отменяет обязательств перед условиями предоставления услуг (ToS) и требованиями регуляторов. Соблюдение файла `robots.txt` остаётся минимальным условием «дружественного» скрейпинга. При работе с любыми данными, которые могут быть связаны с физическим лицом — даже если они публичны, как профили в соцсетях — необходимо учитывать требования GDPR. Это включает разработку внутренних процедур на случай получения запроса на удаление персональной информации («право на забвение»).\n",
        "\n",
        "### 7.4. Обзор Управляемых Инструментов\n",
        "\n",
        "Сложность современных систем защиты привела к росту популярности управляемых решений. **Zyte API** (ранее Scrapy Cloud) предлагает не только платформу для деплоя и мониторинга пауков, но и функции обхода блокировок как услугу: автоматическая ротация прокси, решение CAPTCHA, защита от фингерпринтинга — всё это скрыто за простым HTTP-интерфейсом. Это позволяет разработчикам сосредоточиться исключительно на логике извлечения данных. Для менее требовательных задач может подойти **requests-html** — лёгкая библиотека, сочетающая `requests` и `lxml` с ограниченной возможностью рендеринга JavaScript через headless-браузер. Она полезна для случаев, где требуется минимальное выполнение скриптов без перехода к полной архитектуре Playwright или Scrapy.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение: Скрейпинг как Архитектурный Вызов\n",
        "\n",
        "Эффективный и надёжный веб-скрейпинг в Python — это, прежде всего, архитектурная задача. Выбор инструментария должен определяться двумя ключевыми факторами: природой целевого сайта и требуемым масштабом операции.\n",
        "\n",
        "Если сайт состоит из статического или серверно-рендеренного контента и объём данных невелик, оптимальным выбором будет связка `requests` и `BeautifulSoup4` с парсером `lxml`. Для промышленного сбора с таких ресурсов следует использовать **Scrapy** — его асинхронная архитектура, система Item Pipelines и поддержка распределённого выполнения через **Scrapy-Redis** обеспечивают надёжность и масштабируемость. В случае с динамическими SPA-приложениями, где контент формируется на клиенте, требуется полная эмуляция браузера, и здесь **Playwright** становится предпочтительным решением благодаря своей скорости, автоматическим ожиданиям и мощным инструментам оптимизации.\n",
        "\n",
        "Будущее веб-скрейпинга лежит в области высокопроизводительной браузерной автоматизации и интеграции с управляемыми API, которые берут на себя всю сложность обхода постоянно эволюционирующих систем защиты. Инженер данных должен не просто уметь писать парсеры, но и понимать, как устроены современные веб-приложения, как работают системы антибот-защиты и как проектировать устойчивые, этичные и масштабируемые архитектуры сбора данных.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "L9NKBmWQG3Rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 7: Библиотека Matplotlib — основы построения научной визуализации\n",
        "\n",
        "### Раздел 1: Архитектура Matplotlib и Приоритет OO-Стиля для Научной Визуализации\n",
        "\n",
        "Matplotlib является фундаментальной библиотекой Python для создания статических, анимированных и интерактивных визуализаций данных. Для исследователей, стремящихся к высокой степени контроля, воспроизводимости и стандартизации своих графиков для публикации, необходимо глубокое понимание внутренней архитектуры библиотеки. Это понимание позволяет перейти от быстрого прототипирования к созданию изображений публикационного качества.\n",
        "\n",
        "#### 1.1. Фундаментальная Объектная Иерархия (The Anatomy of a Plot)\n",
        "\n",
        "Архитектура Matplotlib строится на чёткой иерархии объектов, что критически важно для эффективного использования объектно-ориентированного (OO) API. В основе этой иерархии лежат три ключевых компонента: **Figure**, **Axes** и **Artist**.\n",
        "\n",
        "**Figure** представляет собой самый верхний контейнер — «холст», на котором размещаются все элементы визуализации. Он управляет дочерними объектами Axes, а также глобальными элементами, такими как общий заголовок (`fig.suptitle`), легенда на уровне всей фигуры или цветовая шкала (`fig.colorbar`). Типичный способ создания фигуры — вызов `fig = plt.figure()` для пустого холста или использование функции `plt.subplots()`, которая одновременно создаёт Figure и один или несколько объектов Axes.\n",
        "\n",
        "**Axes** — это не отдельная ось, а именно система координат, в которой отображаются данные. Именно на уровне Axes выполняется подавляющее большинство операций: построение линий, гистограмм, облаков точек, добавление подписей и легенд. Один Figure может содержать несколько Axes (например, в случае многопанельных графиков), и каждый из них полностью независим: имеет собственные оси, данные и оформление.\n",
        "\n",
        "**Artist** — это самая общая концепция в архитектуре Matplotlib. Любой видимый элемент на графике — линия, текст, изображение, метка, тик, сам Axes или даже Figure — является объектом Artist. OO-стиль работы с Matplotlib состоит в том, чтобы вызывать методы этих Artist-объектов для точной настройки их внешнего вида.\n",
        "\n",
        "#### 1.2. Сравнение Двух Интерфейсов: Pyplot vs. Object-Oriented (OO) API\n",
        "\n",
        "Matplotlib предоставляет два основных способа взаимодействия с графиками: **Pyplot API** и **Object-Oriented API**.\n",
        "\n",
        "**Pyplot API** (модуль `matplotlib.pyplot`, обычно импортируемый как `plt`) работает через механизм неявного состояния. Функции вроде `plt.plot()`, `plt.title()` или `plt.xlabel()` автоматически создают и управляют текущими объектами Figure и Axes «за кулисами». Пользователь не ссылается на эти объекты напрямую. Такой подход удобен для быстрого интерактивного анализа в Jupyter Notebook или при создании простых графиков в несколько строк кода.\n",
        "\n",
        "**Object-Oriented API** требует явного создания объектов Figure и Axes, например, через `fig, ax = plt.subplots()`. Все последующие действия — построение данных, настройка подписей, добавление легенд — выполняются через вызов методов этих объектов: `ax.plot()`, `ax.set_title()`, `fig.colorbar()`. Этот подход не полагается на глобальное состояние и предоставляет полный контроль над каждым элементом визуализации.\n",
        "\n",
        "Важно понимать, что Pyplot API на самом деле является обёрткой над OO-интерфейсом. Например, вызов `plt.plot(x, y)` эквивалентен последовательности `ax = plt.gca(); ax.plot(x, y)`. Аналогично, `plt.title()` преобразуется в `plt.gca().set_title()`. Таким образом, OO-стиль — это не альтернатива, а основа, на которой построен весь Matplotlib.\n",
        "\n",
        "#### 1.3. Ключевой Вывод для Научной Визуализации: Приоритет OO-Стиля\n",
        "\n",
        "Для создания сложных многопанельных графиков, написания функций, предназначенных для повторного использования в рамках крупного проекта, и обеспечения максимальной воспроизводимости в научных публикациях настоятельно рекомендуется использовать OO-стиль. Явное управление объектами `fig` и `ax` устраняет зависимость от внутреннего «текущего состояния» Matplotlib, которое может вести себя непредсказуемо при создании множества фигур в цикле или в асинхронной среде. OO-стиль обеспечивает чёткость, модульность и предсказуемость кода — качества, без которых невозможно строить надёжные научные pipeline’ы.\n",
        "\n",
        "#### 1.4. Практика: Использование `plt.subplots()` как Вход в OO-Мир\n",
        "\n",
        "Наиболее идиоматическим способом начать работу в OO-стиле является функция `plt.subplots()`. Для одиночного графика она возвращает кортеж `(fig, ax)`, где `fig` — объект Figure, а `ax` — единственный объект Axes. Для многопанельного макета вызов `fig, axs = plt.subplots(nrows=2, ncols=3)` создаёт фигуру и двумерный массив `axs` из шести объектов Axes, каждый из которых можно настраивать независимо.\n",
        "\n",
        "Пример простого графика в OO-стиле:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.sin(x)\n",
        "\n",
        "# Создание фигуры и осей в OO-стиле\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "# Построение данных\n",
        "ax.plot(x, y, color='steelblue', linewidth=2, label='sin(x)')\n",
        "\n",
        "# Настройка элементов графика\n",
        "ax.set_xlabel('Время (с)', fontsize=12)\n",
        "ax.set_ylabel('Амплитуда', fontsize=12)\n",
        "ax.set_title('Гармоническое колебание', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Отображение\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код демонстрирует ключевые принципы OO-стиля: явное создание `fig` и `ax`, вызов методов на `ax` для построения и настройки, и полный контроль над каждым аспектом визуализации.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 2: Базовые Инструменты Научной Визуализации в OO-Стиле\n",
        "\n",
        "Научная визуализация требует инструментов, способных точно представлять зависимости, распределения данных и сопутствующую им неопределённость. Все эти построения в OO-стиле осуществляются через методы, вызываемые на объекте Axes.\n",
        "\n",
        "#### 2.1. Отображение Зависимостей: Линейные и Точечные Графики\n",
        "\n",
        "Линейные и точечные графики — основа для демонстрации взаимосвязей между переменными. Метод `ax.plot()` используется для отображения функциональных зависимостей, временных рядов или любых упорядоченных данных. Он позволяет настраивать цвет (`color`), стиль линии (`linestyle`), ширину (`linewidth`) и маркеры (`marker`), что делает его гибким инструментом для отображения нескольких наборов данных на одном графике.\n",
        "\n",
        "Метод `ax.scatter()` предназначен для визуализации парных распределений. Он особенно полезен при анализе корреляций, выявлении кластеров и обнаружении выбросов. При работе с большими объёмами данных ключевым параметром становится `alpha` — прозрачность точек. Низкое значение `alpha` (например, 0.3) позволяет визуально выделить области с высокой плотностью точек, тогда как перекрывающиеся точки в стандартном режиме (`alpha=1`) создают «тёмные пятна», искажающие восприятие.\n",
        "\n",
        "Пример сравнения линейного и точечного графиков:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Синтетические данные с шумом\n",
        "x = np.linspace(0, 4, 100)\n",
        "y_true = np.exp(-x) * np.cos(2 * np.pi * x)\n",
        "y_obs = y_true + 0.1 * np.random.randn(len(x))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Линейный график\n",
        "ax1.plot(x, y_true, 'k--', label='Истинная модель')\n",
        "ax1.plot(x, y_obs, 'o', color='crimson', markersize=3, alpha=0.6, label='Наблюдения')\n",
        "ax1.set_title('Линейный + точечный (модель vs данные)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Чистый точечный график с прозрачностью\n",
        "ax2.scatter(x, y_obs, c=y_obs, cmap='viridis', alpha=0.6, edgecolors='none')\n",
        "ax2.set_title('Точечный график с прозрачностью')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот пример иллюстрирует, как `alpha` и цветовая кодировка помогают раскрыть структуру данных, которая была бы скрыта при стандартном отображении.\n",
        "\n",
        "#### 2.2. Визуализация Распределений и Статистики\n",
        "\n",
        "Для анализа формы распределения Matplotlib предоставляет несколько взаимодополняющих инструментов. Метод `ax.hist()` строит гистограмму — базовый способ визуализации частотного распределения. Важно выбирать адекватное количество бинов (`bins`), так как слишком мелкое или грубое разбиение может исказить представление о данных.\n",
        "\n",
        "Метод `ax.boxplot()` создаёт «ящик с усами» — компактное изображение, отражающее пять ключевых статистик: минимум, первый квартиль (Q1), медиану, третий квартиль (Q3) и максимум (с исключением выбросов). Box plot идеален для сравнения распределений между группами, но скрывает детали формы распределения.\n",
        "\n",
        "Более информативной альтернативой является **скрипичный график** (`ax.violinplot()`), который отображает ядерную оценку плотности распределения (KDE). Он сохраняет все преимущества box plot, но дополнительно показывает, является ли распределение унимодальным, бимодальным или скошенным. Для научных публикаций, где форма распределения имеет значение (например, при проверке нормальности остатков), скрипичный график часто предпочтительнее.\n",
        "\n",
        "Пример сравнения box plot и violin plot:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация синтетических выборок\n",
        "np.random.seed(42)\n",
        "data_A = np.random.normal(0, 1, 200)\n",
        "data_B = np.concatenate([np.random.normal(-2, 0.8, 100), np.random.normal(2, 0.8, 100)])  # бимодальное\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Box plots\n",
        "ax1.boxplot([data_A, data_B], labels=['Выборка A', 'Выборка B'])\n",
        "ax1.set_title('Box Plot')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Violin plots\n",
        "ax2.violinplot([data_A, data_B], showmedians=True)\n",
        "ax2.set_xticks([1, 2])\n",
        "ax2.set_xticklabels(['Выборка A', 'Выборка B'])\n",
        "ax2.set_title('Violin Plot')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "На этом примере видно, что box plot для выборки B выглядит как стандартный симметричный «ящик», тогда как violin plot явно демонстрирует наличие двух пиков — информацию, критически важную для интерпретации.\n",
        "\n",
        "#### 2.3. Добавление Элементов Ошибки (Error Bars)\n",
        "\n",
        "Научная визуализация неполна без количественной оценки неопределённости. Метод `ax.errorbar()` позволяет отображать погрешности — будь то стандартное отклонение, стандартная ошибка среднего или доверительный интервал. Это обязательный элемент для публикаций в рецензируемых журналах.\n",
        "\n",
        "Элементы ошибок могут быть симметричными (`yerr=0.1`) или асимметричными (`yerr=[[низ, низ], [верх, верх]]`). Кроме того, можно одновременно отображать ошибки по X и по Y (`xerr`, `yerr`), а также настраивать их внешний вид: цвет, ширину штрихов (`capsize`), стиль линий.\n",
        "\n",
        "Пример с элементами ошибок:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array([2.1, 3.9, 6.0, 8.2])\n",
        "yerr = np.array([0.2, 0.3, 0.25, 0.4])  # стандартная ошибка\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.errorbar(x, y, yerr=yerr, fmt='o', color='darkgreen', ecolor='lightgray',\n",
        "            elinewidth=2, capsize=5, markersize=6, label='Измерения ± SE')\n",
        "ax.set_xlabel('Независимая переменная')\n",
        "ax.set_ylabel('Зависимая переменная')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь `fmt='o'` задаёт стиль маркеров, `ecolor` — цвет полос ошибок, а `capsize` добавляет «шапочки» на концы, что улучшает читаемость. Такой график не только передаёт данные, но и честно демонстрирует степень уверенности в них.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "HQ15P4cSGLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Раздел 3: Тонкая Настройка Axes для Публикационного Качества (OO Customization)\n",
        "\n",
        "Достижение публикационного качества в научной визуализации требует не просто отображения данных, а точной и осознанной настройки каждого визуального элемента. Объектно-ориентированный (OO) стиль Matplotlib предоставляет явные, предсказуемые методы-сеттеры, которые позволяют полностью контролировать заголовки, подписи осей, тики, лимиты и декоративные элементы.\n",
        "\n",
        "#### 3.1. Управление Заголовками и Подписями Осей\n",
        "\n",
        "Для обеспечения ясности и профессионального вида графика необходимо чётко разделять заголовки подграфиков и общие заголовки всей фигуры. Метод `ax.set_title(\"Название графика\")` устанавливает заголовок конкретного объекта Axes, что особенно важно при работе с многопанельными композициями. Аналогично, `ax.set_xlabel(\"Ось X\")` и `ax.set_ylabel(\"Ось Y\")` задают метки осей с полным контролем над их текстом, шрифтом и положением. Если фигура содержит несколько подграфиков, а требуется передать общую тему исследования, используется метод `fig.suptitle(\"Общее название\")`, который размещает заголовок над всеми Axes и не привязан к какому-либо отдельному подграфику.\n",
        "\n",
        "Пример настройки заголовков в многопанельном графике:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "x = np.linspace(0, 5, 100)\n",
        "ax1.plot(x, np.sin(x), label='sin(x)')\n",
        "ax2.plot(x, np.cos(x), label='cos(x)', color='tab:orange')\n",
        "\n",
        "# Заголовки подграфиков\n",
        "ax1.set_title('Синусоида')\n",
        "ax2.set_title('Косинусоида')\n",
        "\n",
        "# Подписи осей\n",
        "ax1.set_xlabel('Угол (рад)')\n",
        "ax1.set_ylabel('Амплитуда')\n",
        "ax2.set_xlabel('Угол (рад)')\n",
        "\n",
        "# Общий заголовок фигуры\n",
        "fig.suptitle('Тригонометрические функции', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код демонстрирует чёткое разделение ответственности: каждый подграфик управляет своими локальными подписями, а фигура — общей темой.\n",
        "\n",
        "#### 3.2. Детальная Работа с Тиками и Лимитами Осей\n",
        "\n",
        "Автоматическая разметка осей, предлагаемая Matplotlib по умолчанию, часто не соответствует требованиям научной публикации. Исследователь должен иметь возможность точно определять, какие значения отображаются на осях и как они подписаны. Методы `ax.set_xlim()` и `ax.set_ylim()` позволяют ограничить отображаемый диапазон данных, исключая выбросы или нерелевантные области и фокусируя внимание читателя на ключевой зоне.\n",
        "\n",
        "Ещё более важна настройка тиков. Методы `ax.set_xticks()` и `ax.set_yticks()` принимают два аргумента: позиции тиков и, опционально, их текстовые метки. Это особенно полезно при работе с физическими величинами, денежными единицами или научной нотацией. Например, можно заменить числовые значения на подписи вида «\\$1.0 M» или «1.2 × 10⁴», что значительно улучшает читаемость.\n",
        "\n",
        "Пример ручной настройки тиков с форматированными метками:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Данные в миллионных единицах\n",
        "years = [2020, 2021, 2022, 2023]\n",
        "revenue = [1.2, 1.8, 2.5, 3.1]  # миллионы долларов\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.plot(years, revenue, marker='o')\n",
        "\n",
        "# Установка тиков по годам и форматированных меток по доходу\n",
        "ax.set_xticks(years)\n",
        "ax.set_yticks([1, 2, 3])\n",
        "ax.set_yticklabels(['\\$1.0 M', '\\$2.0 M', '\\$3.0 M'])\n",
        "\n",
        "ax.set_xlabel('Год')\n",
        "ax.set_ylabel('Выручка')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой подход гарантирует, что оси не только технически точны, но и интерпретируемы без дополнительных пояснений.\n",
        "\n",
        "#### 3.3. Легенды, Сетки и Стилизация\n",
        "\n",
        "Легенда (`ax.legend()`) играет ключевую роль при визуализации нескольких наборов данных на одном графике. В сложных композициях её расположение должно быть тщательно продумано, чтобы избежать перекрытия с данными. Это достигается с помощью параметра `loc` (например, `'upper right'`) или более точного управления через `bbox_to_anchor`, который позволяет размещать легенду в произвольной точке относительно Axes.\n",
        "\n",
        "Сетка (`ax.grid(True)`) облегчает точное считывание значений, особенно на графиках с плотным расположением точек. Однако её стиль должен быть ненавязчивым: рекомендуется использовать прерывистые линии (`linestyle='--'`) и пониженную прозрачность (`alpha=0.5`), чтобы сетка служила фоном, а не отвлекала от данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 4: Создание Сложных Многопанельных Макетов с GridSpec\n",
        "\n",
        "Для научных отчётов часто требуются несимметричные, иерархические композиции, где подграфики имеют разный размер и расположение. Стандартный подход `plt.subplots(nrows, ncols)` ограничен равномерными сетками. Для создания произвольной геометрии используется класс `GridSpec`.\n",
        "\n",
        "#### 4.1. Введение в GridSpec\n",
        "\n",
        "`GridSpec` определяет логическую сетку внутри объекта Figure, а затем позволяет объединять ячейки этой сетки с помощью синтаксиса срезов Python. Это превращает проектирование макета в декларативный процесс, управляемый индексами.\n",
        "\n",
        "Инициализация выполняется как `gs = GridSpec(nrows, ncols, figure=fig)`. После этого объекты Axes создаются вызовом `fig.add_subplot(gs[срез])`. Например, `gs[0, :]` охватывает всю первую строку, а `gs[1:, -1]` — последний столбец, начиная со второй строки. Такой подход особенно мощен при построении составных графиков, где, например, гистограмма распределения по X должна быть размещена под основным точечным графиком, а гистограмма по Y — справа от него.\n",
        "\n",
        "Пример асимметричного макета:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(0)\n",
        "x = np.random.randn(1000)\n",
        "y = 1.2 * x + np.random.randn(1000) * 0.5\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8), layout=\"constrained\")\n",
        "gs = gridspec.GridSpec(3, 3, figure=fig)\n",
        "\n",
        "# Основной scatter plot (занимает 2x2 в левом верхнем углу)\n",
        "ax_main = fig.add_subplot(gs[:2, :2])\n",
        "ax_main.scatter(x, y, alpha=0.6)\n",
        "ax_main.set_xlabel('X')\n",
        "ax_main.set_ylabel('Y')\n",
        "\n",
        "# Гистограмма по X (внизу)\n",
        "ax_hist_x = fig.add_subplot(gs[2, :2], sharex=ax_main)\n",
        "ax_hist_x.hist(x, bins=30, color='steelblue')\n",
        "ax_hist_x.set_ylabel('Частота')\n",
        "\n",
        "# Гистограмма по Y (справа)\n",
        "ax_hist_y = fig.add_subplot(gs[:2, 2], sharey=ax_main)\n",
        "ax_hist_y.hist(y, bins=30, orientation='horizontal', color='crimson')\n",
        "ax_hist_y.set_xlabel('Частота')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код создаёт классический «scatter plot с маргинальными гистограммами», где все подграфики точно выровнены, а их размеры определяются логикой анализа, а не техническими ограничениями.\n",
        "\n",
        "#### 4.2. Вложенные GridSpec (Nested GridSpec)\n",
        "\n",
        "Для ещё более сложных композиций, где разные области фигуры требуют независимых внутренних сеток, используется вложенная структура GridSpec. Сначала создаётся основной `GridSpec`, затем для выбранной области вызывается метод `subgridspec()`, который определяет дочернюю сетку. Это позволяет, например, разделить фигуру на две колонки, а в каждой — создать свою независимую композицию из нескольких графиков.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 5: Обеспечение Публикационного Качества: Constrained Layout и Сохранение\n",
        "\n",
        "Создание сложного макета часто сопровождается проблемой наложения или обрезания подписей, заголовков и легенд. Решение этой проблемы — использование современного механизма автоматической компоновки.\n",
        "\n",
        "#### 5.1. Современное Решение: Constrained Layout\n",
        "\n",
        "Исторически для этой цели использовалась функция `plt.tight_layout()`, но она имеет ограничения при работе со сложными элементами, такими как цветовые шкалы или многоуровневые легенды. Современный и рекомендуемый подход — **Constrained Layout**. Он активируется при создании фигуры через параметр `layout=\"constrained\"` и использует внутренний решатель для расчёта необходимого пространства под все элементы графика. Constrained Layout полностью совместим с `GridSpec` и вложенными макетами, обеспечивая гармоничную компоновку даже в самых сложных сценариях.\n",
        "\n",
        "#### 5.2. Интеграция Цветовых Шкал (Colorbars)\n",
        "\n",
        "Цветовые шкалы — частый источник проблем в макетировании. При вызове `fig.colorbar(im, ax=ax)` в сочетании с Constrained Layout система автоматически уменьшает размер указанных Axes и выделяет место для шкалы, предотвращая перекрытия. Это особенно важно при сравнении нескольких тепловых карт с общей цветовой шкалой — задача, типичная для научных публикаций в физике, биологии и геоинформатике.\n",
        "\n",
        "#### 5.3. Сохранение Графиков для Публикации: `fig.savefig()`\n",
        "\n",
        "Финальный шаг — экспорт графика в формате, пригодном для публикации. Метод `fig.savefig()` предоставляет ключевые параметры для контроля качества:\n",
        "\n",
        "- **Разрешение (dpi):** Для печати требуется не менее 300 DPI. По умолчанию Matplotlib использует 100 DPI, что недостаточно для журналов.\n",
        "- **Формат файла:** Векторные форматы (PDF, SVG) предпочтительны для академических публикаций, так как они масштабируются без потерь. Растровые форматы (PNG, JPG) используются для веба.\n",
        "- **Обрезка (bbox_inches='tight'):** Этот параметр автоматически удаляет избыточные белые поля, гарантируя, что сохранённый файл содержит только необходимые элементы.\n",
        "\n",
        "Пример экспорта:\n",
        "\n",
        "```python\n",
        "fig.savefig(\n",
        "    'scientific_plot.pdf',\n",
        "    format='pdf',\n",
        "    dpi=300,\n",
        "    bbox_inches='tight',\n",
        "    transparent=False\n",
        ")\n",
        "```\n",
        "\n",
        "Такой файл будет соответствовать требованиям большинства научных журналов и сохранит все детали визуализации при любом масштабе.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Matplotlib предоставляет строгую, иерархическую модель для создания научной визуализации публикационного качества. Ключ к успеху — системный подход, основанный на трёх принципах.\n",
        "\n",
        "Во-первых, **контроль через ОО-стиль**: начинайте с `plt.subplots()` и используйте явные методы `ax.set_*()` для настройки каждого элемента. Это исключает зависимость от глобального состояния и обеспечивает воспроизводимость.\n",
        "\n",
        "Во-вторых, **сложное макетирование через GridSpec**: при необходимости асимметричных или иерархических композиций переходите от простых сеток к срезам `GridSpec`, что позволяет программно определять пространственные отношения между графиками.\n",
        "\n",
        "В-третьих, **гарантия качества через Constrained Layout и правильный экспорт**: активируйте `layout=\"constrained\"` при создании фигуры, чтобы автоматически избежать наложений, и сохраняйте результат в векторном формате с `dpi=300` и `bbox_inches='tight'`.\n",
        "\n",
        "Освоение этих принципов превращает Matplotlib из инструмента быстрого прототипирования в мощную платформу для создания визуализаций, соответствующих самым строгим стандартам научной и технической коммуникации."
      ],
      "metadata": {
        "id": "-SrdkFwqJTse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6: Продвинутые возможности Matplotlib — анимация, 3D-визуализация, аннотации и интеграция\n",
        "\n",
        "### Раздел 1: Анимация данных для демонстрации динамики\n",
        "\n",
        "Статическая визуализация не всегда способна передать эволюцию процесса во времени. Matplotlib предоставляет мощные инструменты для создания анимаций, которые позволяют наглядно демонстрировать динамические системы, сходимость алгоритмов или изменение распределений. Основой анимации служит класс `FuncAnimation` из модуля `matplotlib.animation`.\n",
        "\n",
        "В отличие от построения серии отдельных кадров, `FuncAnimation` оптимизирует рендеринг, обновляя только те части графика, которые изменились. Это достигается за счёт механизма **blitting**, который сохраняет фон и перерисовывает только движущиеся элементы.\n",
        "\n",
        "Пример анимации гармонического осциллятора:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.set_xlim(0, 4 * np.pi)\n",
        "ax.set_ylim(-1.2, 1.2)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "x = np.linspace(0, 4 * np.pi, 200)\n",
        "line, = ax.plot([], [], 'b-', lw=2)\n",
        "point, = ax.plot([], [], 'ro', markersize=8)\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    point.set_data([], [])\n",
        "    return line, point\n",
        "\n",
        "def animate(frame):\n",
        "    t = x[:frame]\n",
        "    y = np.sin(t)\n",
        "    line.set_data(t, y)\n",
        "    if frame > 0:\n",
        "        point.set_data(t[-1], y[-1])\n",
        "    return line, point\n",
        "\n",
        "anim = FuncAnimation(\n",
        "    fig, animate, init_func=init, frames=len(x),\n",
        "    interval=30, blit=True, repeat=False\n",
        ")\n",
        "\n",
        "# Для сохранения: anim.save('oscillation.mp4', fps=30)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код иллюстрирует ключевые компоненты анимации: функцию инициализации (`init`), которая задаёт начальное состояние, и функцию обновления (`animate`), вызываемую для каждого кадра. Анимации особенно ценны в образовательных материалах и при представлении результатов моделирования, где важна временная последовательность событий.\n",
        "\n",
        "### Раздел 2: Трёхмерная визуализация научных данных\n",
        "\n",
        "Для анализа многомерных зависимостей или пространственных структур Matplotlib поддерживает 3D-графику через модуль `mpl_toolkits.mplot3d`. Объект `Axes3D` расширяет стандартный `Axes`, добавляя методы для построения поверхностей, облаков точек и контурных сечений в трёхмерном пространстве.\n",
        "\n",
        "Ключевыми методами являются `plot_surface` для отображения гладких функций двух переменных, `scatter` для визуализации трёхмерных наборов данных и `contour`/`contourf` для построения изолиний на плоскостях. Важно помнить, что 3D-графики в Matplotlib остаются статическими в формате PDF или PNG; интерактивное вращение возможно только в интерактивных средах (Jupyter, Qt).\n",
        "\n",
        "Пример построения поверхности:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Генерация сетки\n",
        "x = np.linspace(-5, 5, 50)\n",
        "y = np.linspace(-5, 5, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "# Построение поверхности с цветовой картой\n",
        "surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.9, edgecolor='none')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=10, label='Амплитуда')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Трёхмерная визуализация требует особой осторожности: перегруженные графики сложно интерпретировать на печати. Рекомендуется использовать прозрачность (`alpha`), упрощённую геометрию и вспомогательные проекции (например, контуры на дне графика).\n",
        "\n",
        "### Раздел 3: Аннотации и произвольные графические примитивы\n",
        "\n",
        "Научная визуализация часто требует выделения ключевых точек, добавления пояснительных стрелок или визуального подчёркивания определённых областей. Matplotlib предоставляет богатый набор инструментов для таких задач.\n",
        "\n",
        "Метод `ax.annotate()` позволяет размещать текст с указателем, направленным на конкретную координату. Это особенно полезно для подписи экстремумов, точек пересечения или выбросов. Для выделения областей используются методы вроде `ax.axhspan()` (горизонтальная полоса), `ax.axvline()` (вертикальная линия) или `ax.fill_between()` (заливка между кривыми).\n",
        "\n",
        "Пример аннотации экстремума:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = x * np.exp(-x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.plot(x, y, 'b-', lw=2)\n",
        "\n",
        "# Найдём максимум\n",
        "x_max = 1.0\n",
        "y_max = x_max * np.exp(-x_max)\n",
        "\n",
        "# Аннотация с изогнутой стрелкой\n",
        "ax.annotate(\n",
        "    f'Максимум\\n({x_max:.1f}, {y_max:.2f})',\n",
        "    xy=(x_max, y_max),\n",
        "    xytext=(4, 0.3),\n",
        "    arrowprops=dict(\n",
        "        arrowstyle='->',\n",
        "        connectionstyle='arc3,rad=0.3',\n",
        "        color='red'\n",
        "    ),\n",
        "    fontsize=11,\n",
        "    ha='center'\n",
        ")\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('f(x) = x·e⁻ˣ')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Дополнительно, через модуль `matplotlib.patches` можно добавлять геометрические фигуры: круги, прямоугольники, эллипсы и многоугольники. Это позволяет строить схематические диаграммы, выделять зоны неопределённости или создавать кастомные визуальные элементы.\n",
        "\n",
        "### Раздел 4: Интеграция с экосистемой Python и кастомизация стиля\n",
        "\n",
        "Хотя Matplotlib предоставляет низкоуровневый контроль, на практике часто используется совместно с библиотеками высокого уровня. Например, метод `.plot()` объектов pandas DataFrame и Series является обёрткой над `ax.plot()`, автоматически использующей индексы как X-координаты и имена столбцов как метки. Это значительно ускоряет анализ временных рядов и табличных данных.\n",
        "\n",
        "Для более сложной статистической визуализации (парные графики, регрессионные полосы, тепловые карты корреляций) исследователи часто прибегают к библиотеке **seaborn**, которая построена поверх Matplotlib и использует те же объекты Figure и Axes. Это означает, что любой график seaborn можно донастроить с помощью ОО-методов Matplotlib, сочетая удобство высокоуровневого API с гибкостью низкоуровневого.\n",
        "\n",
        "Кроме того, Matplotlib поддерживает систему стилей, позволяющую глобально изменять внешний вид всех графиков. Стиль можно активировать через `plt.style.use('seaborn-v0_8')` или загрузить из пользовательского файла `.mplstyle`. Это обеспечивает единообразие визуализации в рамках одного проекта или публикации.\n",
        "\n",
        "### Раздел 5: Поддержка математической нотации и работа с изображениями\n",
        "\n",
        "Для научных публикаций критически важна корректная отрисовка математических формул. Matplotlib встроенно поддерживает подмножество LaTeX через механизм **mathtext**. Достаточно заключить выражение в символы `$...$`, и библиотека отобразит его в соответствии с типографскими правилами математики.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```python\n",
        "ax.set_xlabel(r'Время $t$ (с)')\n",
        "ax.set_ylabel(r'Амплитуда $\\psi(t) = A e^{-\\gamma t} \\sin(\\omega t + \\phi)$')\n",
        "```\n",
        "\n",
        "Для отображения растровых данных (изображений, тепловых карт, спектрограмм) используются методы `ax.imshow()` и `ax.pcolormesh()`. Первый интерпретирует массив как изображение с пиксельной семантикой, второй — как дискретизированную функцию двух переменных. Оба метода поддерживают произвольные цветовые карты (`cmap`), нормализацию значений и добавление цветовых шкал, что делает их незаменимыми в обработке сигналов, компьютерном зрении и физическом моделировании.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение модуля**  \n",
        "> Matplotlib — это не только инструмент для построения статических линейных графиков, но и полноценная платформа для научной визуализации любого уровня сложности. От анимации динамических процессов и трёхмерного моделирования до точной типографики и интеграции с экосистемой Python — библиотека предоставляет исследователю исчерпывающий набор возможностей. Освоение этих продвинутых функций позволяет переходить от простого отображения данных к созданию выразительных, информативных и публикационно-готовых научных иллюстраций."
      ],
      "metadata": {
        "id": "CWbEhsjmJR9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Модуль 8: Статистическая Визуализация Высокого Уровня в Python (Seaborn)\n",
        "\n",
        "### 1. Методологические Основы Seaborn и Принцип «Опрятных Данных»\n",
        "\n",
        "#### 1.1. Роль Seaborn в конвейере EDA (Exploratory Data Analysis)\n",
        "\n",
        "Seaborn представляет собой высокоуровневую библиотеку для статистической визуализации, построенную поверх Matplotlib и тесно интегрированную с экосистемой pandas. В отличие от низкоуровневого Matplotlib, где требуется явное управление осями, метками и элементами графика, Seaborn предоставляет декларативный, ориентированный на данные API. Аналитик задаёт семантические отношения между переменными — например, указывает, что столбец `'region'` должен определять цвет (`hue`), а `'time'` — ось X, — и библиотека автоматически выполняет необходимую агрегацию, трассировку и отрисовку.\n",
        "\n",
        "Этот подход кардинально ускоряет процесс эксплораторного анализа данных (EDA). Вместо того чтобы вручную группировать данные, вычислять средние или строить отдельные кривые для каждой категории, исследователь формулирует гипотезу на языке переменных, и Seaborn мгновенно предоставляет визуальное подтверждение или опровержение. Таким образом, Seaborn заполняет критический пробел между сырыми табличными данными и статистическим пониманием, превращая визуализацию в инструмент прямого познания структуры данных.\n",
        "\n",
        "#### 1.2. Философия Tidy Data: Преимущества длинного формата данных (Long-Form Data)\n",
        "\n",
        "Эффективное использование Seaborn требует, чтобы данные соответствовали принципам «опрятных данных» (Tidy Data), предложенным Хадли Уикэмом. В этом формате каждая переменная занимает отдельный столбец, каждое наблюдение — отдельную строку, а каждое значение — одну ячейку. Такой подход противопоставляется широкому формату (wide-form), где, например, продажи по регионам могут быть разбросаны по разным столбцам (`sales_EU`, `sales_US`, `sales_APAC`).\n",
        "\n",
        "Длинный формат не является просто эстетическим предпочтением — он является архитектурной необходимостью для Seaborn. Ключевые компоненты, такие как `FacetGrid`, полагаются на возможность семантического сопоставления имени переменной с графическим атрибутом. Если уровни категории хранятся в одном столбце (например, `'region'` со значениями `'EU'`, `'US'`, `'APAC'`), система может автоматически создать фасеты по этим уровням. В широком формате такая информация теряется: библиотека не «знает», что три столбца относятся к одной и той же переменной.\n",
        "\n",
        "Преобразование данных в длинний формат легко выполняется с помощью метода `pandas.melt()`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Исходные данные в широком формате\n",
        "wide_df = pd.DataFrame({\n",
        "    'product': ['A', 'B'],\n",
        "    'Q1': [100, 150],\n",
        "    'Q2': [120, 160],\n",
        "    'Q3': [110, 155]\n",
        "})\n",
        "\n",
        "# Преобразование в длинний формат\n",
        "long_df = wide_df.melt(\n",
        "    id_vars='product',\n",
        "    value_vars=['Q1', 'Q2', 'Q3'],\n",
        "    var_name='quarter',\n",
        "    value_name='sales'\n",
        ")\n",
        "\n",
        "print(long_df)\n",
        "```\n",
        "\n",
        "Результат — таблица, где каждая строка представляет одно наблюдение (продажи продукта в квартале), что делает её идеальной для передачи в любую функцию Seaborn.\n",
        "\n",
        "#### 1.3. Эстетические основы: Управление стилями и выбор палитр\n",
        "\n",
        "Seaborn не только статистически, но и визуально улучшает стандартный вывод Matplotlib. Вызов `sns.set_theme()` активирует одну из встроенных тем (`'darkgrid'`, `'whitegrid'`, `'ticks'`), которая настраивает фон, сетку, шрифты и отступы, обеспечивая профессиональный вид «из коробки».\n",
        "\n",
        "Особое внимание в Seaborn уделяется **цветовым палитрам**, поскольку цвет является мощным, но и极易 вводящим в заблуждение каналом передачи информации. Библиотека предоставляет палитры, спроектированные с учётом перцептивной равномерности — то есть равные шаги в данных соответствуют равным шагам в восприятии.\n",
        "\n",
        "Различают три методологических типа палитр:\n",
        "\n",
        "- **Категориальные палитры** (например, `husl`, `Set1`) используют максимально различимые цвета для дискретных групп без внутреннего порядка.\n",
        "- **Последовательные палитры** (например, `Blues`, `viridis`) отображают монотонный градиент от низких к высоким значениям.\n",
        "- **Дивергентные палитры** (например, `vlag`, `coolwarm`) критически важны для данных с центральной точкой (обычно нулём или средним). Они используют контрастные цвета (синий/красный) для обозначения отклонений в противоположных направлениях.\n",
        "\n",
        "Некорректный выбор палитры может искажать интерпретацию. Например, при визуализации матрицы корреляций использование последовательной палитры (`Blues`) скроет знак коэффициентов: отрицательная корреляция будет выглядеть как «менее интенсивная», а не как противоположная по смыслу.\n",
        "\n",
        "Пример корректного выбора палитры для heatmap:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Загрузка примера данных\n",
        "flights = sns.load_dataset(\"flights\")\n",
        "flights_wide = flights.pivot(\"month\", \"year\", \"passengers\")\n",
        "\n",
        "# Использование последовательной палитры для положительных данных\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(flights_wide, cmap=\"YlGnBu\", annot=False, cbar_kws={'label': 'Пассажиры'})\n",
        "plt.title('Пассажиропоток по месяцам и годам')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь `YlGnBu` — последовательная палитра, уместная для неотрицательных данных. Для корреляций следовало бы выбрать `vlag`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Архитектура Сетки: Метод Малых Мультиплов (Small Multiples)\n",
        "\n",
        "#### 2.1. Концептуальное значение малых мультиплов\n",
        "\n",
        "Метод «малых мультиплов» (small multiples) — один из самых мощных приёмов в визуальном анализе многомерных данных. Он предполагает создание серии графиков одинакового типа, где каждый график отображает условный срез данных (например, по региону, году или категории). Поскольку визуальная кодировка остаётся постоянной, зритель может легко сравнивать формы распределений, тренды или отношения между переменными в разных условиях.\n",
        "\n",
        "#### 2.2. Класс FacetGrid и его измерения\n",
        "\n",
        "В Seaborn за реализацию малых мультиплов отвечает класс `FacetGrid`. Он создаёт сетку из объектов Axes, где структура определяется категориальными переменными. Основные измерения:\n",
        "\n",
        "- **`row` и `col`** задают физическое размещение подграфиков в двумерной сетке. Каждый уникальный уровень переменной порождает отдельную строку или столбец.\n",
        "- **`hue`** добавляет третье измерение через цвет: разные категории отображаются разными цветами на одном и том же подграфике.\n",
        "\n",
        "Таким образом, `FacetGrid` позволяет одновременно анализировать до четырёх переменных: две непрерывные (X и Y), одна для фасетирования по сетке и одна для цветового кодирования.\n",
        "\n",
        "#### 2.3. Взаимодействие с высокоуровневыми функциями\n",
        "\n",
        "Seaborn предоставляет два типа функций: **уровня фигуры** (Figure-Level) и **уровня осей** (Axes-Level).\n",
        "\n",
        "Функции уровня фигуры — `relplot()`, `displot()`, `catplot()`, `lmplot()` — автоматически создают `FacetGrid` и применяют к нему соответствующую функцию уровня осей (`scatterplot`, `histplot`, `boxplot`, `regplot`). Они идеальны для быстрого многомерного анализа.\n",
        "\n",
        "Функции уровня осей работают с уже существующим объектом Axes, что даёт полный контроль, но требует ручного управления макетом.\n",
        "\n",
        "Пример использования `relplot` для анализа по категориям:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Анализ зависимости чаевых от счёта, разбитый по полу и курению\n",
        "g = sns.relplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"sex\", col=\"smoker\", row=\"time\",\n",
        "    height=4, aspect=1\n",
        ")\n",
        "g.set_axis_labels(\"Счёт ($)\", \"Чаевые ($)\")\n",
        "g.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код создаёт сетку 2×2 графиков за одну строку, демонстрируя силу Figure-Level API. Если бы мы использовали `scatterplot`, пришлось бы вручную создавать сетку через `plt.subplots()` и писать цикл для заполнения.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Визуализация и Статистическая Интерпретация Одномерных Распределений\n",
        "\n",
        "Seaborn предлагает три взаимодополняющих метода визуализации одномерных распределений, каждый со своими статистическими достоинствами.\n",
        "\n",
        "#### 3.1. Гистограммы (`histplot`)\n",
        "\n",
        "Гистограмма разбивает диапазон значений на интервалы (бины) и отображает частоту или плотность наблюдений в каждом бине. Главный недостаток — зависимость от выбора количества и ширины бинов. Seaborn смягчает эту проблему, позволяя добавить к гистограмме кривую оценки плотности ядра (`kde=True`), что даёт более плавное представление формы распределения.\n",
        "\n",
        "#### 3.2. Оценка плотности ядра (`kdeplot`)\n",
        "\n",
        "KDE-график строит гладкую оценку функции плотности вероятности, суммируя ядра (обычно гауссовы) вокруг каждой точки данных. Ключевой параметр — **полоса пропускания** (bandwidth): слишком широкая скрывает мультимодальность, слишком узкая — усиливает шум. В Seaborn она настраивается через параметр `bw_adjust`.\n",
        "\n",
        "#### 3.3. Эмпирическая кумулятивная функция распределения (`ecdfplot`)\n",
        "\n",
        "ECDF-график показывает долю наблюдений, не превышающих заданное значение. Его главное преимущество — **отсутствие настраиваемых параметров**. Каждая точка данных отображается напрямую, что делает ECDF объективным инструментом для сравнения распределений. Хотя форма менее интуитивна, чем KDE, она свободна от субъективного сглаживания.\n",
        "\n",
        "Пример сравнения трёх методов:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "body_mass = penguins[\"body_mass_g\"].dropna()\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Гистограмма с KDE\n",
        "sns.histplot(body_mass, kde=True, ax=axs[0])\n",
        "axs[0].set_title('Гистограмма + KDE')\n",
        "\n",
        "# Чистый KDE\n",
        "sns.kdeplot(body_mass, ax=axs[1])\n",
        "axs[1].set_title('KDE')\n",
        "\n",
        "# ECDF\n",
        "sns.ecdfplot(body_mass, ax=axs[2])\n",
        "axs[2].set_title('ECDF')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "В академическом анализе, где важна воспроизводимость и объективность, ECDF следует рассматривать как основу, а гистограммы и KDE — как вспомогательные инструменты для интуитивного понимания.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Анализ Групп и Категориальных Переменных\n",
        "\n",
        "#### 4.1. Сводные распределения: `boxplot` и `violinplot`\n",
        "\n",
        "Boxplot предоставляет компактное резюме распределения: медиана, квартили, whiskers и выбросы. Он устойчив к выбросам и идеален для быстрого сравнения локации и разброса.\n",
        "\n",
        "Violinplot дополняет эту информацию, отображая полную форму распределения через KDE. Он может выявить бимодальность или асимметрию, которые boxplot скрывает.\n",
        "\n",
        "#### 4.2. Точечные представления: `stripplot` и `swarmplot`\n",
        "\n",
        "Swarmplot отображает каждую точку данных, избегая наложения за счёт небольшого смещения вдоль категориальной оси. Это позволяет видеть не только центральную тенденцию, но и плотность, количество и точное расположение наблюдений.\n",
        "\n",
        "#### 4.3. Комбинирование графиков для комплексного анализа\n",
        "\n",
        "Наиболее информативный подход — комбинировать методы. Например, наложить `swarmplot` на `violinplot`:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.violinplot(data=penguins, x=\"species\", y=\"body_mass_g\", inner=None, color=\".8\")\n",
        "sns.swarmplot(data=penguins, x=\"species\", y=\"body_mass_g\", size=4)\n",
        "plt.title('Масса пингвинов по видам')\n",
        "plt.ylabel('Масса (г)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь серый violinplot показывает форму распределения, а точки — фактические наблюдения. Это даёт полную картину: и статистическую, и эмпирическую.\n",
        "\n",
        "Кроме того, важно **упорядочивать категории осмысленно**. Если категории не имеют естественного порядка, их можно сортировать, например, по медиане:\n",
        "\n",
        "```python\n",
        "order = penguins.groupby(\"species\")[\"body_mass_g\"].median().sort_values().index\n",
        "sns.boxplot(data=penguins, x=\"species\", y=\"body_mass_g\", order=order)\n",
        "```\n",
        "\n",
        "Такой подход накладывает на визуализацию статистически обоснованную структуру, что улучшает интерпретацию.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение главы**  \n",
        "> Seaborn — это не просто библиотека для «красивых графиков», а инструмент для **статистического мышления через визуализацию**. Его архитектура, основанная на принципах tidy data, малых мультиплов и перцептивно обоснованных палитр, направляет исследователя к методологически корректному анализу. Освоение различий между Figure-Level и Axes-Level функциями, понимание сильных и слабых сторон каждого типа графика распределения, а также умение комбинировать визуальные методы позволяют превратить EDA из рутинной проверки в процесс глубокого познания данных. В руках внимательного аналитика Seaborn становится мостом между сырыми числами и научным выводом."
      ],
      "metadata": {
        "id": "-tDirS3oQCsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 5. Визуализация Отношений и Регрессионный Анализ\n",
        "\n",
        "### 5.1. Реляционные графики (`relplot`)\n",
        "\n",
        "Функция `relplot()` является ключевым инструментом Seaborn для визуализации взаимосвязей между переменными. Как функция уровня фигуры, она автоматически управляет сеткой подграфиков и поддерживает два основных типа графиков: точечные (`kind=\"scatter\"`) и линейные (`kind=\"line\"`). Главное преимущество `relplot` — его мощная семантическая кодировка, позволяющая одновременно отображать до пяти переменных: две позиционные (X и Y), а также цвет (`hue`), форму маркера (`style`), размер (`size`) и условные фасеты (`row`/`col`). Это превращает простой scatter plot в многомерный аналитический инструмент.\n",
        "\n",
        "Пример визуализации сложного отношения в данных о чаевых:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Отображение связи между счётом и чаевыми с учётом пола, курения и времени\n",
        "sns.relplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"sex\", style=\"smoker\", size=\"size\",\n",
        "    sizes=(40, 200),  # диапазон размеров маркеров\n",
        "    alpha=0.7,\n",
        "    height=5, aspect=1.2\n",
        ")\n",
        "plt.title('Многомерный анализ чаевых')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот график позволяет одновременно оценить, как размер счёта влияет на сумму чаевых, и как это соотношение изменяется в зависимости от пола клиента, привычки к курению и количества людей в группе. Такая визуализация служит отправной точкой для формулировки гипотез.\n",
        "\n",
        "### 5.2. Построение регрессионных моделей (`regplot` и `lmplot`)\n",
        "\n",
        "Для визуального представления линейной зависимости Seaborn предоставляет две функции. `regplot()` — это функция уровня осей, которая строит точечный график с наложенной линией регрессии и доверительным интервалом. `lmplot()` — это её обёртка уровня фигуры, интегрированная с `FacetGrid`, что позволяет строить отдельные регрессионные модели для каждого уровня категориальной переменной.\n",
        "\n",
        "Важно подчеркнуть, что Seaborn не предназначен для формального статистического вывода — для оценки коэффициентов, p-значений или критериев качества модели следует использовать библиотеки вроде `statsmodels` или `scikit-learn`. Регрессионные графики в Seaborn служат **визуальным руководством**: они помогают оценить силу, направление и линейность связи, а также выявить потенциальные выбросы или нелинейные паттерны.\n",
        "\n",
        "Пример сравнения регрессий по категориям:\n",
        "\n",
        "```python\n",
        "# Отдельная регрессия для курящих и некурящих\n",
        "sns.lmplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"smoker\",\n",
        "    height=5, aspect=1.2\n",
        ")\n",
        "plt.title('Регрессия чаевых по группам курильщиков')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой график сразу показывает, различается ли наклон регрессионной линии между группами — важный признак взаимодействия переменных.\n",
        "\n",
        "### 5.3. Диагностика модели: Применение `residplot()`\n",
        "\n",
        "Визуализация самой регрессии недостаточна — необходимо проверить адекватность модели. Для этого используется `residplot()`, который отображает остатки (разницу между наблюдаемыми и предсказанными значениями) в зависимости от предиктора.\n",
        "\n",
        "Для корректной линейной модели остатки должны быть **случайно рассеяны** вокруг горизонтальной линии `y = 0`. Любая структура — например, парабола, волна или «веер» (расширяющаяся дисперсия) — указывает на нарушение допущений: нелинейность, гетероскедастичность или пропущенную переменную.\n",
        "\n",
        "Seaborn позволяет углубить диагностику, подгоняя нелинейные модели. Например, параметр `order=2` строит квадратичную регрессию, а `lowess=True` добавляет сглаживающую кривую без параметрических допущений:\n",
        "\n",
        "```python\n",
        "# Диагностика линейной модели\n",
        "sns.residplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    lowess=True,  # непараметрическая линия тренда\n",
        "    scatter_kws={'alpha': 0.6}\n",
        ")\n",
        "plt.title('Остатки регрессионной модели')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Если кривая LOWESS явно отклоняется от нуля, это сигнал: линейная модель неадекватна, и следует рассмотреть нелинейные преобразования или добавление полиномиальных признаков.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Методология Отображения Статистической Неопределенности (`errorbar`)\n",
        "\n",
        "Начиная с версии 0.12, Seaborn ввёл унифицированный параметр `errorbar`, который строго разделяет два фундаментально разных типа неопределённости: **неопределённость оценки** и **разброс данных**.\n",
        "\n",
        "### 6.1. Два типа интервалов ошибки\n",
        "\n",
        "**Неопределённость оценки** отражает, насколько точно выборочная статистика (например, среднее) оценивает параметр генеральной совокупности. Этот интервал (доверительный интервал, CI, или стандартная ошибка, SE) **уменьшается с ростом размера выборки**.\n",
        "\n",
        "**Разброс данных** (стандартное отклонение, SD, или процентильный интервал, PI) описывает изменчивость самих наблюдений вокруг центра. Он **не зависит от размера выборки** и отражает дисперсию популяции.\n",
        "\n",
        "Смешивание этих понятий — серьёзная методологическая ошибка. Например, отображение SD вместо CI в графике средних по группам создаёт ложное впечатление, что различия между группами статистически значимы, даже если они нет.\n",
        "\n",
        "### 6.2. Методы построения доверительных интервалов\n",
        "\n",
        "Seaborn поддерживает два подхода к оценке неопределённости:\n",
        "\n",
        "- **Параметрический**: предполагает нормальность данных и использует аналитические формулы (например, `errorbar=(\"se\", 1)` для одной стандартной ошибки).\n",
        "- **Непараметрический (бутстрап)**: многократно ресэмплирует данные с замещением, строит эмпирическое распределение статистики и определяет интервал по процентилям (например, `errorbar=(\"ci\", 95)` для 95% доверительного интервала).\n",
        "\n",
        "Бутстрап особенно ценен при нарушении нормальности, наличии выбросов или сложных статистик, где аналитическая оценка дисперсии затруднена.\n",
        "\n",
        "Сравнение методов на практике:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# График средних с разными типами ошибок\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=(\"ci\", 95), ax=axes[0])\n",
        "axes[0].set_title('95% CI (бутстрап)')\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=\"se\", ax=axes[1])\n",
        "axes[1].set_title('Стандартная ошибка')\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=\"sd\", ax=axes[2])\n",
        "axes[2].set_title('Стандартное отклонение')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Первый график отвечает на вопрос: «В каком диапазоне, вероятно, лежит истинное среднее для всей популяции?». Третий — на вопрос: «Насколько сильно варьируются чаевые в группе?».\n",
        "\n",
        "### 6.3. Контроль прозрачности и извлечение данных\n",
        "\n",
        "При построении линейных графиков с доверительными интервалами (`lineplot`, `regplot`) неопределённость отображается в виде заштрихованной области. Её прозрачность регулируется параметром `err_kws={'alpha': 0.3}`. Это особенно важно при наложении нескольких линий, чтобы избежать визуального перегруза.\n",
        "\n",
        "Хотя Seaborn не предоставляет прямого API для извлечения численных значений границ CI, их можно получить из объекта Axes Matplotlib, что позволяет проводить дальнейший анализ или настраивать отображение.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Матричный Анализ: Корреляции и Иерархическая Кластеризация\n",
        "\n",
        "### 7.1. Тепловые карты (`heatmap`)\n",
        "\n",
        "Функция `heatmap()` предназначена для визуализации двумерных матриц, чаще всего — корреляционных. Использование **дивергентной палитры** (например, `vlag` или `coolwarm`) критически важно: она интуитивно разделяет положительные (тёплые цвета) и отрицательные (холодные) корреляции, а нейтральный центр (обычно белый или серый) обозначает отсутствие связи.\n",
        "\n",
        "Для научной публикации рекомендуется включать численные значения через `annot=True` и управлять их форматом через `fmt`:\n",
        "\n",
        "```python\n",
        "# Визуализация корреляционной матрицы\n",
        "numeric_vars = tips.select_dtypes(include=\"number\")\n",
        "corr_matrix = numeric_vars.corr()\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"vlag\",\n",
        "    center=0,\n",
        "    square=True,\n",
        "    cbar_kws={\"label\": \"Коэффициент корреляции Пирсона\"}\n",
        ")\n",
        "plt.title('Матрица корреляций')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой график позволяет мгновенно оценить силу и направление всех попарных линейных связей.\n",
        "\n",
        "### 7.2. Иерархическая кластеризация (`clustermap`)\n",
        "\n",
        "Функция `clustermap()` расширяет `heatmap`, добавляя **иерархическую кластеризацию** строк и столбцов. Алгоритм переупорядочивает элементы матрицы на основе их сходства (например, `1 - |ρ|` для корреляций), а вдоль осей отображает дендрограммы, показывающие иерархию объединения кластеров.\n",
        "\n",
        "`clustermap` — инструмент для **генерации гипотез**, а не для окончательного вывода. Выявленные кластеры требуют подтверждения формальными методами. Однако визуальная группировка сильно коррелирующих признаков или схожих наблюдений чрезвычайно полезна на этапе EDA.\n",
        "\n",
        "Важно: в отличие от большинства функций Seaborn, `heatmap` и `clustermap` работают с **широким форматом** (матрицей), хотя `clustermap` поддерживает tidy data через параметр `pivot_kws`.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```python\n",
        "# Кластеризация признаков по корреляции\n",
        "sns.clustermap(\n",
        "    corr_matrix,\n",
        "    cmap=\"vlag\",\n",
        "    center=0,\n",
        "    metric=\"correlation\",  # расстояние = 1 - |корреляция|\n",
        "    method=\"average\",      # метод связывания\n",
        "    figsize=(8, 8)\n",
        ")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот график выявляет, например, что `'total_bill'` и `'tip'` образуют один кластер, а `'size'` — другой, что может навести на мысль о латентных факторах (например, «размер группы» vs «щедрость»).\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Комплексное Применение Seaborn: Практические Кейсы EDA\n",
        "\n",
        "### 8.1. Пошаговый рабочий процесс EDA (набор данных `tips`)\n",
        "\n",
        "Эффективный EDA следует структурированному итеративному процессу.\n",
        "\n",
        "**Шаг 1: Унивариантный анализ.**  \n",
        "Изучение распределения ключевых переменных. Например, `sns.histplot(tips[\"total_bill\"], kde=True)` показывает, что распределение счёта скошено вправо, что может потребовать логарифмического преобразования перед регрессионным анализом.\n",
        "\n",
        "**Шаг 2: Создание производных метрик и сравнение групп.**  \n",
        "Аналитик вводит новую переменную — процент чаевых: `tips[\"tip_pct\"] = tips[\"tip\"] / tips[\"total_bill\"]`. Затем сравнивает его по категориям:  \n",
        "```python\n",
        "sns.boxplot(data=tips, x=\"day\", y=\"tip_pct\", hue=\"smoker\")\n",
        "```  \n",
        "Boxplot выявляет, что в выходные дни курящие оставляют меньший процент чаевых, чем некурящие.\n",
        "\n",
        "**Шаг 3: Многомерный анализ отношений.**  \n",
        "Семантический scatterplot позволяет одновременно учесть несколько факторов:  \n",
        "```python\n",
        "sns.scatterplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip_pct\",\n",
        "    hue=\"time\", style=\"sex\", size=\"size\"\n",
        ")\n",
        "```  \n",
        "График может показать, что в ужины (dinner) связь между размером счёта и процентом чаевых слабее, чем в обеды.\n",
        "\n",
        "**Шаг 4: Регрессия и корреляция.**  \n",
        "Построение `regplot` и матрицы корреляций завершает обзор, подтверждая или опровергая наблюдаемые тенденции количественно.\n",
        "\n",
        "### 8.2. Заключительные рекомендации по эффективному дизайну\n",
        "\n",
        "Эффективная статистическая визуализация требует методологической дисциплины:\n",
        "\n",
        "- **Чётко указывайте, что означают полосы ошибок** — CI, SE или SD. В подписи к графику пишите: «Планки ошибок: 95% доверительный интервал (бутстрап)».\n",
        "- **Используйте перцептивно корректные палитры**: дивергентные для корреляций, последовательные для положительных величин.\n",
        "- **Избегайте перегрузки**: не используйте одновременно `hue`, `style`, `size`, `row` и `col`, если это не критично для гипотезы.\n",
        "- **Помните о Matplotlib**: для тонкой настройки (аннотации, кастомные тики, LaTeX-формулы) используйте методы `ax.set_*()` после построения графика Seaborn.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Выводы и Методологическое Заключение\n",
        "\n",
        "Seaborn — это не просто инструмент для «красивых картинок», а **методологическая платформа для статистического мышления через визуализацию**. Его архитектура, основанная на принципах tidy data и малых мультиплов, направляет исследователя к структурированному, воспроизводимому анализу.\n",
        "\n",
        "Процесс EDA в Seaborn итеративен: гистограмма генерирует вопрос о распределении, scatterplot — о связи, а `residplot` — о валидности модели. Каждый график — не конечный результат, а **диалог с данными**.\n",
        "\n",
        "Особое внимание библиотека уделяет **статистической честности**. Разделение неопределённости оценки и разброса данных, использование робастных методов вроде бутстрапа, визуальная диагностика моделей — всё это защищает от поспешных выводов.\n",
        "\n",
        "Таким образом, Seaborn служит критически важным мостом между сырыми данными и обоснованным научным выводом. Его декларативный API позволяет исследователю сосредоточиться на содержании анализа, а не на технических деталях отрисовки, делая статистическую визуализацию неотъемлемой частью мышления аналитика."
      ],
      "metadata": {
        "id": "x6VZzGRlSqvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 9. Интерактивная Визуализация и Веб-Дашборды в Python: Методическая Лекция по Plotly и Dash\n",
        "\n",
        "### Раздел I. Архитектурные Основы Plotly: От Данных к Интерактивному Графику\n",
        "\n",
        "#### 1.1. Фигура Plotly как Фундаментальная Структура Данных\n",
        "\n",
        "Центральным элементом визуализации в библиотеке Plotly является объект `plotly.graph_objects.Figure`. Этот объект представляет собой декларативный контейнер, полностью описывающий график: его данные, внешний вид и интерактивные возможности. На самом низком уровне фигура Plotly структурирована как словарь Python, который последовательно транслируется в JSON-схему, интерпретируемую фронтенд-библиотекой **Plotly.js**.\n",
        "\n",
        "Такая архитектура превращает Plotly Python API в генератор строго формализованной спецификации. Любое изменение, внесённое через методы Python, неизбежно отражается в конкретных ключах и значениях этой схемы. Это обеспечивает высокую предсказуемость и воспроизводимость при кастомизации. Для разработчика, стремящегося к точному контролю, понимание иерархии этой структуры — не опция, а необходимость.\n",
        "\n",
        "Объект `Figure` состоит из трёх компонентов:\n",
        "\n",
        "- **`data`** — список следов (`traces`), каждый из которых описывает один набор данных и способ его отображения (точки, столбцы, поверхность и т.д.);\n",
        "- **`layout`** — объект, содержащий все стилистические настройки, не зависящие от данных: заголовки, оси, отступы, легенды, параметры 3D-сцены;\n",
        "- **`frames`** — список кадров, используемых для анимаций, где каждый кадр определяет новое состояние `data` и/или `layout`.\n",
        "\n",
        "#### 1.2. Объектная Модель Plotly: Trace и Layout\n",
        "\n",
        "Для построения и модификации фигур используются два класса объектов: **Trace** и **Layout**.\n",
        "\n",
        "Каждый **след** (`go.Scatter`, `go.Bar`, `go.Surface` и др.) соответствует определённому типу визуализации и инкапсулирует массивы данных (например, `x`, `y`, `z`) и параметры отображения (например, `mode='lines+markers'`). Plotly поддерживает сотни типов следов, включая геопространственные (`Choroplethmapbox`), 3D (`Surface`) и специализированные (`Sankey`, `Sunburst`). После создания фигуру можно динамически изменять: метод `Figure.add_traces()` добавляет новые следы, а `Figure.update_traces()` — массово обновляет свойства существующих (например, меняет цвет всех линий).\n",
        "\n",
        "Объект **`Layout`** управляет глобальным видом графика. Его свойства настраиваются через `Figure.update_layout()`. Для создания многопанельных композиций используется функция `make_subplots()`, которая генерирует предварительно настроенную фигуру с сеткой подграфиков. При добавлении следов в такую фигуру явно указывается их позиция (`row=1, col=2`).\n",
        "\n",
        "Важный методологический нюанс: при построении линейных графиков (например, временных рядов) **данные должны быть отсортированы по оси X**. Если этого не сделать, Plotly соединит точки в порядке их следования в массиве, что может привести к визуально искажённой, «путающейся» линии, не отражающей истинный тренд. Это не ошибка библиотеки, а следствие некорректной подготовки данных.\n",
        "\n",
        "#### 1.3. Сравнение Plotly Express (px) и Graph Objects (go)\n",
        "\n",
        "В Plotly существует два уровня API: высокоуровневый **Plotly Express** (`px`) и низкоуровневый **Graph Objects** (`go`).\n",
        "\n",
        "**Plotly Express** — это декларативный интерфейс, оптимизированный для быстрого создания типовых статистических графиков. Он принимает pandas DataFrame и автоматически назначает цвета, легенды, оси и даже анимации. Например, `px.scatter(df, x=\"income\", y=\"spending\", color=\"region\")` за одну строку строит многоцветный scatter plot. Это идеальный инструмент для разведочного анализа (EDA) и прототипирования.\n",
        "\n",
        "**Graph Objects** предоставляет полный контроль над каждым элементом фигуры. Он требует больше кода, но позволяет создавать сложные, нетиповые композиции: например, 3D-поверхность с наложенными контурами и точками, или интерактивную карту с несколькими слоями. При работе с `go` разработчик явно создаёт каждый след и настраивает каждый параметр макета.\n",
        "\n",
        "Выбор между `px` и `go` — это выбор между скоростью и гибкостью. Часто используется гибридный подход: график создаётся через `px`, а затем детально донастраивается через `fig.update_layout()` и `fig.update_traces()`.\n",
        "\n",
        "Пример гибридного подхода:\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Быстрое создание через px\n",
        "fig = px.line(\n",
        "    data_frame=df_sorted,\n",
        "    x=\"date\", y=\"value\",\n",
        "    color=\"category\",\n",
        "    title=\"Динамика показателей по категориям\"\n",
        ")\n",
        "\n",
        "# Детальная настройка через go-методы\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Дата\",\n",
        "    yaxis_title=\"Значение\",\n",
        "    legend_title=\"Категория\",\n",
        "    font=dict(family=\"Arial\", size=12)\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "#### 1.4. Экспорт и Сохранение Интерактивных и Статических Изображений\n",
        "\n",
        "Plotly предлагает два ключевых способа экспорта:\n",
        "\n",
        "- **Интерактивный HTML**: метод `fig.write_html(\"plot.html\")` сохраняет график в автономный HTML-файл, содержащий весь необходимый JavaScript (Plotly.js). Такой файл можно открыть в любом браузере, делиться им по email или встраивать в веб-страницы. Вся интерактивность (зум, панорамирование, тултипы) сохраняется.\n",
        "- **Статическое изображение**: метод `fig.write_image(\"plot.png\")` (или `.svg`, `.pdf`) генерирует растровое или векторное изображение для печати или вставки в презентации. Для этого требуется библиотека **Kaleido**, которая обеспечивает высококачественный рендеринг без зависимости от браузера.\n",
        "\n",
        "Эта гибкость делает Plotly универсальным решением: от интерактивных дашбордов до публикаций в научных журналах.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел II. Продвинутые Техники Визуализации с Plotly\n",
        "\n",
        "#### 2.1. Трёхмерная Визуализация: Scatter3D и Surface Plots\n",
        "\n",
        "Plotly предоставляет мощные инструменты для работы с трёхмерными данными, включая полный контроль над камерой, освещением и проекциями.\n",
        "\n",
        "**Scatter3D** (`go.Scatter3d`) отображает точки в пространстве, определяемом координатами X, Y, Z. Каждый маркер может быть окрашен, изменён по размеру или форме в зависимости от дополнительных переменных, что позволяет визуализировать до **пяти измерений** одновременно. Это особенно полезно при анализе многомерных наборов данных, таких как Iris или результатов моделирования.\n",
        "\n",
        "**Surface Plots** (`go.Surface`) предназначены для отображения функций вида Z = f(X, Y). Входные данные должны быть представлены в виде двумерных массивов, где каждый элемент `Z[i,j]` соответствует высоте над точкой `(X[i], Y[j])`. Plotly позволяет настраивать **контуры**: отображать их на самой поверхности или проецировать на плоскости XZ и YZ, что значительно улучшает восприятие формы.\n",
        "\n",
        "Полный контроль над 3D-сценой осуществляется через `layout.scene`. Ключевые параметры:\n",
        "\n",
        "- `camera.eye` — позиция камеры (вектор);\n",
        "- `aspectmode=\"manual\"` + `aspectratio` — соотношение масштабов по осям (важно для избежания искажений);\n",
        "- `xaxis.nticks` — количество тиков на оси.\n",
        "\n",
        "Пример 3D-поверхности с контурами:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Генерация сетки\n",
        "x = np.linspace(-5, 5, 50)\n",
        "y = np.linspace(-5, 5, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "fig = go.Figure(data=go.Surface(\n",
        "    x=X, y=Y, z=Z,\n",
        "    contours={\n",
        "        \"z\": {\"show\": True, \"start\": -1, \"end\": 1, \"size\": 0.1},\n",
        "        \"x\": {\"show\": True, \"start\": -5, \"end\": 5, \"size\": 1},\n",
        "        \"y\": {\"show\": True, \"start\": -5, \"end\": 5, \"size\": 1}\n",
        "    }\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        aspectmode=\"manual\",\n",
        "        aspectratio=dict(x=1, y=1, z=0.5)\n",
        "    ),\n",
        "    title=\"3D-поверхность с контурами\"\n",
        ")\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "Такой подход позволяет не просто отобразить данные, но и подчеркнуть их структуру — например, сделать вертикальные колебания более заметными за счёт сжатия оси Z.\n",
        "\n",
        "#### 2.2. Геопространственный Анализ: Choropleth, Scattermapbox и Стили Карт\n",
        "\n",
        "Plotly поддерживает два подхода к картографии:\n",
        "\n",
        "- **Контурные карты** (`layout.geo`) — работают без интернета, но имеют ограниченную детализацию;\n",
        "- **Плиточные карты** (`Mapbox` / `Maplibre`) — используют внешние тайловые сервисы для отображения улиц, зданий и рельефа.\n",
        "\n",
        "Начиная с версии 5.24, Plotly рекомендует использовать **Maplibre-based следы** (`go.Scattermap`, `go.Choroplethmap`), которые не требуют токена и могут работать с открытыми тайловыми сервисами (например, Stadia Maps). В отличие от них, следы на основе **Mapbox** требуют регистрации и токена доступа, что создаёт зависимости при развёртывании в корпоративных средах.\n",
        "\n",
        "Одной из самых мощных возможностей является **композитная картография** — наложение нескольких слоёв. Например, можно отобразить:\n",
        "\n",
        "1. **Choroplethmapbox** — для раскраски регионов (например, областей по среднему доходу), используя GeoJSON-файл с границами;\n",
        "2. **Scattermapbox** — для отображения точек (например, местоположений торговых точек), наложенных поверх регионов.\n",
        "\n",
        "Особая сложность возникает при создании **легенды для размеров маркеров** в пузырьковых картах. Plotly не генерирует автоматическую легенду размеров, поэтому её приходится строить вручную: для каждого уникального размера создаётся отдельный «невидимый» след с соответствующим именем и размером, который отображается только в легенде.\n",
        "\n",
        "#### 2.3. Динамическая Визуализация: Анимации\n",
        "\n",
        "Plotly Express упрощает создание анимаций с помощью параметров `animation_frame` и `animation_group`. Первый определяет переменную, по которой строятся кадры (например, год), второй — объекты, которые следует отслеживать (например, страна).\n",
        "\n",
        "Ключевое методологическое требование: **фиксировать диапазоны осей**. Если этого не сделать, масштаб будет автоматически подстраиваться под данные каждого кадра, что разрушит визуальную непрерывность и сделает сравнение во времени некорректным. Например, в «гонке по странам» (race bar chart) ось значений должна охватывать диапазон от **глобального минимума до глобального максимума** во всём временном интервале.\n",
        "\n",
        "Пример анимированного scatter plot:\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"gdp_per_capita\",\n",
        "    y=\"life_expectancy\",\n",
        "    size=\"population\",\n",
        "    color=\"continent\",\n",
        "    hover_name=\"country\",\n",
        "    animation_frame=\"year\",\n",
        "    animation_group=\"country\",\n",
        "    range_x=[0, 80000],\n",
        "    range_y=[20, 100],\n",
        "    title=\"Изменение здоровья и богатства стран с течением времени\"\n",
        ")\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "Здесь фиксированные `range_x` и `range_y` гарантируют, что движение точек отражает **реальные изменения**, а не артефакты масштабирования.\n",
        "\n"
      ],
      "metadata": {
        "id": "82Vvu-kdTRnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Раздел III. Основы Dash: Построение Реактивного Веб-Приложения\n",
        "\n",
        "### 3.1. Введение в Dash: Философия «No JavaScript Required»\n",
        "\n",
        "Dash — это декларативный и реактивный фреймворк с открытым исходным кодом, предназначенный для создания аналитических веб-приложений и дашбордов исключительно на языке Python. Под капотом он использует Flask в качестве бэкенда и комбинирует Plotly.js с React.js на фронтенде, автоматически преобразуя Python-объекты в HTML, CSS и JavaScript. Ключевое преимущество Dash для специалистов по данным — возможность строить полнофункциональные интерактивные веб-интерфейсы без написания ни строчки JavaScript, что значительно снижает порог входа в веб-разработку.\n",
        "\n",
        "### 3.2. Архитектура Приложения Dash: Инициализация и Макет\n",
        "\n",
        "Создание любого приложения Dash начинается с инициализации объекта:\n",
        "\n",
        "```python\n",
        "from dash import Dash, html\n",
        "app = Dash(__name__)\n",
        "```\n",
        "\n",
        "Этот объект инкапсулирует всю логику приложения. Макет интерфейса определяется через свойство `app.layout`, которое представляет собой декларативное дерево компонентов — фактически, описание DOM-структуры будущей веб-страницы. Макет, как правило, статичен при первом рендере, но может динамически изменяться через колбэки: например, при нажатии кнопки в контейнер `html.Div` может быть добавлен новый график или фильтр. Такой подход позволяет избежать прямой модификации `app.layout` и сохраняет предсказуемость реактивной системы.\n",
        "\n",
        "Приложение запускается вызовом:\n",
        "\n",
        "```python\n",
        "app.run_server(debug=True, port=8050)\n",
        "```\n",
        "\n",
        "В производственной среде, однако, встроенный сервер Flask заменяется на WSGI-совместимый сервер, такой как Gunicorn, для обеспечения стабильности и масштабируемости.\n",
        "\n",
        "### 3.3. HTML Компоненты (html) и Компоненты Dash Core (dcc)\n",
        "\n",
        "Интерфейс Dash строится из двух типов компонентов.\n",
        "\n",
        "Компоненты из модуля `dash.html` соответствуют стандартным HTML-тегам: `html.Div`, `html.H1`, `html.P` и т.д. Они используются для создания структуры страницы, заголовков, абзацев и контейнеров.\n",
        "\n",
        "Компоненты из модуля `dash.dcc` (Dash Core Components) предоставляют интерактивные элементы управления, характерные для аналитических дашбордов:\n",
        "\n",
        "- `dcc.Graph` — контейнер для встраивания фигур Plotly с полной поддержкой интерактивности (зум, выделение, тултипы);\n",
        "- `dcc.Dropdown`, `dcc.Slider`, `dcc.RadioItems` — элементы для выбора параметров и фильтрации;\n",
        "- `dcc.Tabs` — для организации многосекционного интерфейса;\n",
        "- `dcc.Location` и `dcc.Link` — для навигации в многостраничных приложениях.\n",
        "\n",
        "Важно помнить: все свойства компонентов должны быть JSON-сериализуемыми (строки, числа, списки, словари), так как они передаются между Python-бэкендом и React-фронтендом через JSON-мост.\n",
        "\n",
        "### 3.4. Стилевое Оформление и UI/UX Основы\n",
        "\n",
        "Стилизация в Dash осуществляется двумя способами:\n",
        "\n",
        "- через аргумент `className`, который связывает компонент с классами из внешней CSS-таблицы (например, `style.css`);\n",
        "- через аргумент `style`, принимающий словарь для inline-стилей.\n",
        "\n",
        "При проектировании аналитических дашбордов следует придерживаться принципов UI/UX:\n",
        "\n",
        "- **Информационная иерархия**: ключевые метрики (KPI) должны быть видны сразу, без прокрутки («above the fold»). Пользователь должен понимать суть дашборда за 5 секунд.\n",
        "- **Контекстуализация**: каждая метрика должна сопровождаться сравнением (например, «+12% к прошлому месяцу»).\n",
        "- **Адаптивность**: макет должен корректно отображаться на мобильных устройствах, с учётом сенсорного ввода.\n",
        "- **Визуальная чистота**: избыток элементов повышает когнитивную нагрузку. Белое пространство и минимализм улучшают читаемость.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел IV. Механизм Реактивности Dash: Колбэки и Управление Потоком\n",
        "\n",
        "Реактивность — сердце Dash. Она реализуется через систему **колбэков**, управляемых декоратором `@app.callback`.\n",
        "\n",
        "### 4.1. Жизненный Цикл Колбэка\n",
        "\n",
        "Колбэк — это обычная функция Python, связывающая входные и выходные свойства компонентов. Когда свойство, указанное как `Input`, изменяется (например, пользователь выбирает значение в выпадающем списке), Dash запускает функцию, передаёт ей текущие значения всех `Input` и `State`, и использует возвращаемые значения для обновления компонентов, указанных в `Output`.\n",
        "\n",
        "Dash строит **граф зависимостей** между компонентами. Если один колбэк обновляет `Output`, который является `Input` для другого колбэка, система гарантирует, что второй колбэк запустится только после того, как первый завершит обновление. Это предотвращает использование устаревших или несогласованных данных.\n",
        "\n",
        "### 4.2. Центральная Концепция: Роль Input, Output и State\n",
        "\n",
        "Понимание различий между `Input`, `Output` и `State` — ключ к стабильности приложения.\n",
        "\n",
        "- **`Output`** — свойство компонента, которое будет обновлено в результате работы колбэка. Оно не вызывает запуск функции.\n",
        "- **`Input`** — свойство, изменение которого **триггерит** выполнение колбэка.\n",
        "- **`State`** — свойство, значение которого **считывается** в момент запуска колбэка, но его изменение **не вызывает** запуск.\n",
        "\n",
        "Разделение `Input` и `State` критически важно для предотвращения циклических зависимостей. Например, если колбэк обновляет `dcc.Store`, а другой колбэк читает его как `State`, цикла не возникает. Если бы `dcc.Store` был `Input`, любое обновление вызывало бы бесконечный цикл.\n",
        "\n",
        "### 4.3. Продвинутые Паттерны Колбэков и Оптимизация Запуска\n",
        "\n",
        "Колбэк может иметь **несколько `Output`**, возвращая кортеж значений. Если обновление определённого `Output` не требуется, можно вернуть специальное значение `dash.no_update`, что предотвращает ненужную передачу данных в браузер.\n",
        "\n",
        "По умолчанию все колбэки запускаются при инициализации приложения. Для повышения производительности и избежания ошибок с динамически создаваемыми компонентами рекомендуется использовать параметр `prevent_initial_call=True`:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('graph', 'figure'),\n",
        "    Input('dropdown', 'value'),\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def update_graph(selected_value):\n",
        "    return create_figure(selected_value)\n",
        "```\n",
        "\n",
        "Это особенно важно для колбэков, выполняющих тяжёлые вычисления или запросы к базе данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел V. Продвинутые Паттерны Dash: Состояние и Производительность\n",
        "\n",
        "### 5.1. Управление Состоянием на Стороне Клиента: Компонент `dcc.Store`\n",
        "\n",
        "Для эффективного управления данными между колбэками используется невидимый компонент `dcc.Store`. Он хранит данные в браузере и позволяет избежать повторных вычислений.\n",
        "\n",
        "Свойство `storage_type` определяет место хранения:\n",
        "\n",
        "- `'memory'` — данные сбрасываются при перезагрузке;\n",
        "- `'session'` — сохраняются до закрытия вкладки;\n",
        "- `'local'` — сохраняются между сессиями.\n",
        "\n",
        "Выбор типа хранения влияет на UX: например, `'session'` позволяет сохранить выбранные фильтры при обновлении страницы. Однако важно помнить об ограничениях: безопасно хранить до 2 МБ данных. Это указывает на то, что Dash не предназначен для передачи больших массивов через браузер — агрегация должна происходить на сервере.\n",
        "\n",
        "### 5.2. Паттерны Использования `dcc.Store`\n",
        "\n",
        "Три ключевых сценария:\n",
        "\n",
        "1. **Кэширование**: результаты дорогих вычислений сохраняются в `Store` и используются другими колбэками.\n",
        "2. **Инициализация**: для запуска колбэка при загрузке страницы используется `modified_timestamp` как `Input`, а данные — как `State`.\n",
        "3. **Разрыв циклов**: один колбэк записывает в `Store` (`Output`), другой читает (`State`), предотвращая петлю.\n",
        "\n",
        "### 5.3. Взаимосвязь Графиков (Linked Brushing)\n",
        "\n",
        "Plotly поддерживает события взаимодействия пользователя, которые можно использовать в Dash для создания связанных визуализаций. Например, выделение точек на scatter plot может фильтровать карту или временной ряд.\n",
        "\n",
        "Это достигается через специальные свойства `dcc.Graph`:\n",
        "\n",
        "- `clickData` — данные по клику;\n",
        "- `selectedData` — точки, выделенные инструментами Lasso или Box Select;\n",
        "- `hoverData` — данные под курсором;\n",
        "- `relayoutData` — изменения масштаба или позиции.\n",
        "\n",
        "Пример linked brushing:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('map', 'figure'),\n",
        "    Input('scatter', 'selectedData')\n",
        ")\n",
        "def update_map(selected_data):\n",
        "    if selected_data is None:\n",
        "        filtered_df = df\n",
        "    else:\n",
        "        indices = [p['pointIndex'] for p in selected_data['points']]\n",
        "        filtered_df = df.iloc[indices]\n",
        "    return px.scatter_mapbox(filtered_df, ...)\n",
        "```\n",
        "\n",
        "Такой подход превращает дашборд из набора изолированных графиков в единый интерактивный аналитический инструмент.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел VI. Архитектура Масштабируемых Дашбордов и Производственное Развертывание\n",
        "\n",
        "### 6.1. Организация Многостраничных Приложений (Multi-Page Apps)\n",
        "\n",
        "Сложные дашборды часто требуют разделения на страницы. В Dash это достигается через комбинацию `dcc.Location` и `dcc.Link`.\n",
        "\n",
        "- `dcc.Location` отражает текущий путь в адресной строке (`pathname`);\n",
        "- `dcc.Link` создаёт переходы без перезагрузки страницы.\n",
        "\n",
        "Колбэк использует `pathname` как `Input` и возвращает соответствующий макет:\n",
        "\n",
        "```python\n",
        "@app.callback(Output('page-content', 'children'), Input('url', 'pathname'))\n",
        "def display_page(pathname):\n",
        "    if pathname == '/analytics':\n",
        "        return analytics_layout\n",
        "    elif pathname == '/reporting':\n",
        "        return reporting_layout\n",
        "    return home_layout\n",
        "```\n",
        "\n",
        "Такой подход реализует архитектуру Single Page Application (SPA) на чистом Python.\n",
        "\n",
        "### 6.2. Создание Дашбордов в Реальном Времени с `dcc.Interval`\n",
        "\n",
        "Для мониторинговых приложений используется компонент `dcc.Interval`, который с заданной периодичностью увеличивает свойство `n_intervals`. Это свойство служит `Input` для колбэка, обновляющего данные.\n",
        "\n",
        "Чтобы дать пользователю контроль над частотой обновления, можно связать `dcc.Interval` с `dcc.Slider`:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('interval-component', 'interval'),\n",
        "    Input('frequency-slider', 'value')\n",
        ")\n",
        "def update_interval(seconds):\n",
        "    return seconds * 1000  # перевод в миллисекунды\n",
        "```\n",
        "\n",
        "Это позволяет балансировать между актуальностью данных и нагрузкой на систему.\n",
        "\n",
        "### 6.3. Развертывание Приложений Dash (Gunicorn и Heroku)\n",
        "\n",
        "В производственной среде встроенный сервер Flask заменяется на WSGI-сервер, такой как **Gunicorn**. Для развёртывания на Heroku требуется:\n",
        "\n",
        "- файл `requirements.txt` со списком зависимостей;\n",
        "- файл `Procfile` со строкой:  \n",
        "  `web: gunicorn app:server`\n",
        "\n",
        "Здесь `app` — имя Python-файла (например, `app.py`), а `server` — переменная `app.server`, которую Dash предоставляет как внутренний Flask-сервер.\n",
        "\n",
        "Развертывание осуществляется через Git:\n",
        "\n",
        "```bash\n",
        "git push heroku main\n",
        "```\n",
        "\n",
        "Этот процесс подчёркивает, что, несмотря на простоту для пользователя, Dash — полноценный веб-фреймворк, требующий стандартных DevOps-практик.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Plotly и Dash образуют мощный, сквозной стек для создания профессиональных аналитических приложений на Python. Plotly обеспечивает глубокую, методологически обоснованную визуализацию — от 3D-поверхностей с контролем ракурса до многослойных геокарт, где регионы и точки анализируются совместно. Dash превращает эти визуализации в реактивные веб-приложения, где каждое действие пользователя мгновенно отражается на всех связанных элементах.\n",
        "\n",
        "Архитектура Dash, основанная на декларативном макете и строгом разделении `Input`/`State`/`Output`, обеспечивает стабильность и предсказуемость даже в сложных, многокомпонентных дашбордах. Использование `dcc.Store` для управления состоянием, `prevent_initial_call` для оптимизации и `dcc.Interval` для потоковых данных — это не просто технические приёмы, а методологические практики, обеспечивающие производительность и удобство.\n",
        "\n",
        "Наконец, возможность развёртывания через стандартные веб-инструменты (Gunicorn, Heroku) подтверждает зрелость экосистемы: специалист по данным может не только создать, но и доставить до пользователя полноценное веб-приложение, не выходя из привычной среды Python. В совокупности, Plotly и Dash демократизируют создание интерактивной аналитики, делая её доступной для всех, кто владеет языком данных."
      ],
      "metadata": {
        "id": "wcH89FfOUyS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZcnCE0q9nzfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zCx6tEUYmuvr"
      }
    }
  ]
}