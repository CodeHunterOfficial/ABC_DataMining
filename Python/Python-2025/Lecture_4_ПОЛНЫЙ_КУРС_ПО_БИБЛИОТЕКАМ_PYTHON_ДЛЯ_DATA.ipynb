{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMMwVHMzE29I40Jc5rzC0qD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/Python/Python-2025/Lecture_4_%D0%9F%D0%9E%D0%9B%D0%9D%D0%AB%D0%99_%D0%9A%D0%A3%D0%A0%D0%A1_%D0%9F%D0%9E_%D0%91%D0%98%D0%91%D0%9B%D0%98%D0%9E%D0%A2%D0%95%D0%9A%D0%90%D0%9C_PYTHON_%D0%94%D0%9B%D0%AF_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 1: NUMPY — ФУНДАМЕНТ НАУЧНЫХ ВЫЧИСЛЕНИЙ\n",
        "\n",
        "## РАЗДЕЛ I. Введение в ndarray и архитектуру NumPy\n",
        "\n",
        "### 1.1. Роль NumPy в экосистеме Python\n",
        "\n",
        "**NumPy** (*Numerical Python*) является краеугольным камнем современной экосистемы научных вычислений на языке Python. Эта библиотека де-факто стала стандартом для эффективных численных операций и служит основой для большинства инструментов в области анализа данных, машинного обучения и научной визуализации.\n",
        "\n",
        "Фундаментальное значение NumPy заключается в его способности преодолевать ограничения интерпретируемого языка Python. За счёт высокооптимизированных вычислительных ядер, написанных на C и Fortran, NumPy предоставляет пользователю простой и элегантный синтаксис для выполнения сложных математических операций, обеспечивая при этом производительность, сопоставимую с низкоуровневыми языками. Эта эффективность критически важна при работе с большими объёмами данных.\n",
        "\n",
        "Области применения NumPy чрезвычайно широки — от академических исследований до промышленного анализа. Например, NumPy сыграл ключевую роль в обработке данных коллаборации LIGO, что привело к подтверждению существования гравитационных волн. В машинном обучении NumPy лежит в основе реализаций таких библиотек, как XGBoost и LightGBM, а также является основой для визуализационных инструментов, включая Matplotlib, Seaborn и Plotly. Вместе с SciPy NumPy формирует обязательный набор инструментов для любого исследователя или разработчика, работающего с числовыми данными.\n",
        "\n",
        "> **Пример: простое сложение массивов — сравнение с Python-списками**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Стандартный список Python\n",
        "python_list = list(range(1000000))\n",
        "%timeit [x + 1 for x in python_list]  # медленно: интерпретируемый цикл\n",
        "\n",
        "# Массив NumPy\n",
        "numpy_array = np.arange(1000000)\n",
        "%timeit numpy_array + 1  # быстро: векторизованная операция\n",
        "```\n",
        "\n",
        "> *Пояснение:* В этом примере демонстрируется разница в производительности между векторизованной операцией над `ndarray` и циклом по обычному списку. Время выполнения векторизованной операции может быть в десятки или сотни раз меньше.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Основы структуры `ndarray`\n",
        "\n",
        "Центральным объектом NumPy является **`ndarray`** (*N-dimensional array*) — контейнер для хранения **гомогенных** данных (все элементы одного типа) в непрерывном или почти непрерывном блоке памяти. Это фундаментальное отличие от стандартных списков Python, которые являются гетерогенными и хранят ссылки на объекты, что значительно снижает эффективность при численных вычислениях.\n",
        "\n",
        "#### Гомогенность и `dtype` (тип данных)\n",
        "\n",
        "Ключевой характеристикой массива является его **тип данных** (`dtype`), который определяет, как элементы интерпретируются и хранятся в памяти. Например, `np.int32` обозначает 32-битное целое, а `np.float64` — 64-битное число с плавающей точкой.\n",
        "\n",
        "Явное управление `dtype` позволяет контролировать потребление памяти и избегать численных ошибок. Например, если сохранить значение `128` в массив типа `np.int8`, диапазон которого ограничен значениями от –128 до 127, результат будет неверным — произойдёт переполнение, и значение «обрежется» до –128. В научных и инженерных расчётах подобные ошибки недопустимы.\n",
        "\n",
        "При операциях между массивами разных типов NumPy автоматически применяет **правила продвижения типов** (*type promotion*), выбирая общий тип, способный вместить все исходные значения без потерь. Например, сложение массивов типов `np.uint32` и `np.int32` приведёт к массиву типа `np.int64`, который безопасно охватывает диапазон обоих исходных типов.\n",
        "\n",
        "> **Пример: контроль типа данных и последствия переполнения**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Опасный пример с переполнением\n",
        "arr_int8 = np.array([127], dtype=np.int8)\n",
        "print(arr_int8 + 1)  # Вывод: [-128] — переполнение!\n",
        "\n",
        "# Безопасный пример с автоматическим продвижением типа\n",
        "arr_uint32 = np.array([1000], dtype=np.uint32)\n",
        "arr_int32 = np.array([-500], dtype=np.int32)\n",
        "result = arr_uint32 + arr_int32\n",
        "print(result, result.dtype)  # Вывод: [500] dtype('int64')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Первый пример иллюстрирует, как переполнение может привести к некорректным результатам. Второй — как NumPy автоматически выбирает безопасный тип при смешанных операциях.\n",
        "\n",
        "#### Архитектурные преимущества\n",
        "\n",
        "Гомогенность и непрерывное хранение позволяют NumPy использовать **C- или Fortran-континуальный порядок** размещения данных в памяти. Это критически важно для эффективной передачи блоков данных в низкоуровневые библиотеки, такие как **BLAS** и **LAPACK**, которые реализуют высокооптимизированные линейные алгебраические операции. Благодаря этому достигается **экспоненциальный выигрыш в скорости** по сравнению с операциями над стандартными списками.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. Сравнение производительности: векторизация\n",
        "\n",
        "**Векторизация** — ключевой принцип высокой производительности в NumPy. Вместо того чтобы писать явные циклы `for` в Python, которые медленны из-за интерпретируемой природы языка, операции применяются сразу ко всему массиву через вызов оптимизированных функций на C или Fortran.\n",
        "\n",
        "На практике это означает, что **арифметические, логические и многие другие операции автоматически распространяются на все элементы массива**. Если же разработчик по неопытности оставляет цикл Python внутри критического участка кода, этот участок неизбежно становится **«узким местом»**, замедляя всю программу.\n",
        "\n",
        "> **Пример: векторизованная функция против цикла**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 10, 1000000)\n",
        "\n",
        "# Невекторизованный (медленный) подход\n",
        "def slow_sin(x):\n",
        "    return np.array([np.sin(val) for val in x])\n",
        "\n",
        "# Векторизованный (быстрый) подход\n",
        "def fast_sin(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "%timeit slow_sin(x)  # медленно\n",
        "%timeit fast_sin(x)  # быстро\n",
        "```\n",
        "\n",
        "> *Пояснение:* Векторизованный вызов `np.sin(x)` выполняется напрямую в C, без итераций в Python. Это делает его значительно быстрее даже для простых функций.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ II. Создание и инициализация массивов\n",
        "\n",
        "### 2.1. Создание массивов из последовательностей (`np.array`)\n",
        "\n",
        "Самый прямой способ создать массив — преобразовать стандартные Python-структуры, такие как списки или кортежи, с помощью функции `np.array(object, dtype=None)`.\n",
        "\n",
        "Уровень вложенности последовательности определяет размерность массива: одномерный список создаёт вектор, список списков — матрицу, и так далее. В научных задачах **рекомендуется явно указывать `dtype`**, особенно если требуется контролировать точность или потребление памяти.\n",
        "\n",
        "> **Пример: создание массивов разной размерности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# 1D-массив\n",
        "vector = np.array([1, 2, 3])\n",
        "\n",
        "# 2D-массив\n",
        "matrix = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "\n",
        "# Явное указание типа\n",
        "int_array = np.array([1, 2, 3], dtype=np.int64)\n",
        "float_array = np.array([1, 2, 3], dtype=np.float32)\n",
        "\n",
        "print(\"vector:\", vector)\n",
        "print(\"matrix:\\n\", matrix)\n",
        "print(\"int_array dtype:\", int_array.dtype)\n",
        "print(\"float_array dtype:\", float_array.dtype)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Явное задание типа помогает избежать неожиданных преобразований и экономит память, например, при использовании `float32` вместо `float64`, если задача допускает снижение точности.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Создание массивов с фиксированными значениями\n",
        "\n",
        "Для инициализации вычислительных пространств или создания «заготовок» под результаты используются специализированные функции. `np.zeros(shape, dtype=float64)` создаёт массив, заполненный нулями, `np.ones(shape, dtype=float64)` — единицами, а `np.full(shape, fill_value, dtype=None)` — произвольным значением.\n",
        "\n",
        "Эти функции особенно полезны при настройке начальных условий в численных методах или выделении памяти под промежуточные результаты, когда известна итоговая форма данных, но сами значения ещё не вычислены.\n",
        "\n",
        "> **Пример: инициализация массивов**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "zeros_2d = np.zeros((3, 4))\n",
        "ones_1d = np.ones(5, dtype=np.int32)\n",
        "custom = np.full((2, 2), 7.5)\n",
        "\n",
        "print(\"zeros_2d:\\n\", zeros_2d)\n",
        "print(\"ones_1d:\", ones_1d)\n",
        "print(\"custom:\\n\", custom)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такие массивы часто используются в алгоритмах, где нужно «собирать» результаты постепенно — например, при построении матрицы корреляций или накоплении градиентов в итеративных методах оптимизации.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. Создание регулярных последовательностей: `arange` и `linspace`\n",
        "\n",
        "Для генерации одномерных числовых последовательностей NumPy предоставляет две основные функции.\n",
        "\n",
        "**`np.arange(start, stop, step)`** создаёт последовательность с фиксированным шагом и является аналогом встроенной функции `range`, но возвращает `ndarray`. Однако её **не рекомендуется использовать с дробным шагом** из-за накопления ошибок округления, присущих арифметике с плавающей точкой.\n",
        "\n",
        "**`np.linspace(start, stop, num)`** создаёт **ровно `num` точек**, равномерно распределённых между `start` и `stop`, включая обе границы. Эта функция предпочтительна при построении численных сеток, дискретизации функций и любых задачах, где важен точный контроль над количеством и расположением точек.\n",
        "\n",
        "> **Пример: сравнение `arange` и `linspace`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# arange: риск неточности при float-шаге\n",
        "arr1 = np.arange(0, 1, 0.1)\n",
        "print(\"arange:\", arr1)\n",
        "\n",
        "# linspace: гарантированное количество точек\n",
        "arr2 = np.linspace(0, 1, 10)\n",
        "print(\"linspace:\", arr2)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `linspace` более надёжен при работе с вещественными числами, особенно в задачах, требующих точного контроля над границами и количеством точек, таких как построение графиков или решение дифференциальных уравнений.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Воспроизводимая генерация случайных чисел (RNG)\n",
        "\n",
        "Начиная с версии **1.17**, NumPy использует современный API генерации псевдослучайных чисел через объекты **`Generator`**. Этот подход основан на более быстрых и статистически надёжных алгоритмах, таких как **PCG64**, по сравнению со старым классом `RandomState`.\n",
        "\n",
        "#### Управление воспроизводимостью\n",
        "\n",
        "Для воспроизводимости экспериментов и симуляций необходимо инициализировать генератор с фиксированным **зерном** (*seed*):\n",
        "\n",
        "```python\n",
        "rng = np.random.default_rng(seed=42)\n",
        "random_array = rng.random((2, 3))  # массив 2×3 из [0, 1)\n",
        "```\n",
        "\n",
        "#### Роль `SeedSequence`\n",
        "\n",
        "Внутри `Generator` использует **`SeedSequence`** — механизм, который «перемешивает» входное зерно в надёжное начальное состояние. Это позволяет избегать проблем с «плохими» зёрнами, создавать **независимые подпотоки случайности** через метод `.spawn()` и безопасно использовать генерацию в распределённых вычислениях.\n",
        "\n",
        "#### Векторизованные случайные операции\n",
        "\n",
        "`Generator` поддерживает не только базовые распределения, но и **векторизованные операции**, такие как `rng.permuted(x, axis=1)`, которая перемешивает срезы массива вдоль указанной оси, или `rng.shuffle(x)`, которая перемешивает массив *in-place*.\n",
        "\n",
        "> **Пример: генерация и перестановка**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "# Генерация случайных чисел\n",
        "data = rng.normal(loc=0.0, scale=1.0, size=(3, 4))\n",
        "print(\"Исходные данные:\\n\", data)\n",
        "\n",
        "# Перемешивание по строкам\n",
        "shuffled = rng.permuted(data, axis=1)\n",
        "print(\"После перемешивания по строкам:\\n\", shuffled)\n",
        "\n",
        "# In-place перемешивание (по умолчанию — по первой оси)\n",
        "copy_data = data.copy()\n",
        "rng.shuffle(copy_data)\n",
        "print(\"In-place shuffle (по строкам):\\n\", copy_data)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такие функции широко применяются в статистике — например, при бутстреппинге, кросс-валидации или генерации случайных разбиений данных для обучения моделей машинного обучения."
      ],
      "metadata": {
        "id": "x9mLyuvqebLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## РАЗДЕЛ III. Манипуляции формой и осями\n",
        "\n",
        "### 3.1. Форма и описание массива\n",
        "\n",
        "Каждый массив в NumPy характеризуется набором неизменяемых атрибутов, которые полностью описывают его структуру и содержимое. Ключевыми из них являются: `.shape` — кортеж, задающий количество элементов вдоль каждой оси (например, `(3, 4)` означает 3 строки и 4 столбца); `.ndim` — целое число, указывающее ранг массива (количество измерений); `.size` — общее число элементов, равное произведению всех компонент `.shape`; и `.dtype` — тип данных элементов, такой как `int64` или `float32`.\n",
        "\n",
        "Эти атрибуты доступны только для чтения и фиксированы для данного объекта `ndarray`. Любое изменение формы требует создания нового представления или копии данных, но не модификации исходного объекта.\n",
        "\n",
        "> **Пример: основные атрибуты массива**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12).reshape(3, 4)\n",
        "print(\"Массив a:\\n\", a)\n",
        "print(\"shape:\", a.shape)      # (3, 4)\n",
        "print(\"ndim:\", a.ndim)        # 2\n",
        "print(\"size:\", a.size)        # 12\n",
        "print(\"dtype:\", a.dtype)      # int64 (на большинстве систем)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти атрибуты являются первым шагом в диагностике и понимании структуры любого числового массива.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Изменение формы массива (`reshape` и `ravel`)\n",
        "\n",
        "NumPy позволяет эффективно **переинтерпретировать** данные в памяти без их физического перемещения, при условии, что общее количество элементов сохраняется. Это достигается за счёт изменения метаданных о форме и порядке размещения.\n",
        "\n",
        "Функция `np.reshape(a, newshape)` возвращает массив с новой формой. Один из размеров может быть задан как `-1`, что позволяет NumPy автоматически вычислить его на основе `a.size`. Параметр `order` управляет порядком: `'C'` (построчный, по умолчанию) или `'F'` (постолбцовый), что критично при работе с данными, поступающими из Fortran-кода или внешних библиотек.\n",
        "\n",
        "> **Пример: reshape с автоматическим размером**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12)               # shape: (12,)\n",
        "b = a.reshape(3, -1)            # shape: (3, 4)\n",
        "c = a.reshape(-1, 2, 2)         # shape: (3, 2, 2)\n",
        "\n",
        "print(\"Исходный массив:\", a)\n",
        "print(\"После reshape(3, -1):\\n\", b)\n",
        "print(\"После reshape(-1, 2, 2):\\n\", c)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `-1` устраняет необходимость ручного расчёта размеров и снижает вероятность ошибок.\n",
        "\n",
        "Для преобразования многомерного массива в одномерный существуют две функции. `np.ravel(a)` возвращает **представление (view)**, если это возможно, не копируя данные и обеспечивая высокую производительность. В отличие от него, `a.flatten()` всегда создаёт **новую копию** данных, что гарантирует независимость от исходного массива, но увеличивает потребление памяти и время выполнения.\n",
        "\n",
        "> **Пример: разница между ravel и flatten**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "flat_view = np.ravel(a)\n",
        "flat_copy = a.flatten()\n",
        "\n",
        "# Изменяем view — исходный массив тоже изменится\n",
        "flat_view[0] = 999\n",
        "print(\"После изменения view:\\n\", a)        # [[999, 2], [3, 4]]\n",
        "\n",
        "# Копия не влияет на оригинал\n",
        "flat_copy[0] = 0\n",
        "print(\"После изменения копии:\\n\", a)       # всё ещё [[999, 2], [3, 4]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* В вычислительно интенсивных задачах стоит отдавать предпочтение `ravel()`, чтобы избежать ненужного копирования памяти.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Управление осями (транспонирование и пермутация)\n",
        "\n",
        "Изменение порядка осей — частая операция при подготовке данных для матричных операций, нейросетевых архитектур или визуализации. Для 2D-массивов классическое матричное транспонирование выполняется через атрибут `.T`. Для массивов произвольного ранга используется функция `np.transpose(a)`, которая по умолчанию инвертирует порядок всех осей, но может принимать явный кортеж `axes`, задающий новую перестановку.\n",
        "\n",
        "Обе операции возвращают **представление**, если структура памяти позволяет, что делает их крайне эффективными.\n",
        "\n",
        "> **Пример: транспонирование**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.random.rand(2, 3, 4)  # shape: (2, 3, 4)\n",
        "b = a.T                      # shape: (4, 3, 2)\n",
        "c = np.transpose(a, axes=(2, 0, 1))  # явный порядок: (4, 2, 3)\n",
        "\n",
        "print(\"Исходная форма:\", a.shape)\n",
        "print(\"После .T:\", b.shape)\n",
        "print(\"После transpose(2,0,1):\", c.shape)\n",
        "```\n",
        "\n",
        "Для более гибкого перемещения отдельных осей применяется функция `np.moveaxis(a, source, destination)`. Она позволяет переместить одну или несколько осей в новые позиции, сохранив относительный порядок остальных. Это особенно полезно при работе с тензорами, например, при преобразовании формата изображений из `(batch, height, width, channels)` в `(batch, channels, height, width)` для совместимости с фреймворками глубокого обучения.\n",
        "\n",
        "> **Пример: moveaxis**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Тензор: (batch, height, width, channels) → хотим (batch, channels, height, width)\n",
        "x = np.random.rand(10, 64, 64, 3)\n",
        "x_moved = np.moveaxis(x, source=3, destination=1)  # перемещаем ось 3 на позицию 1\n",
        "print(\"Новая форма:\", x_moved.shape)  # (10, 3, 64, 64)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `moveaxis` делает код читаемее и безопаснее, чем ручное перечисление всех осей в `transpose`.\n",
        "\n",
        "Ещё один важный приём — добавление новой оси размером 1 с помощью `np.newaxis` (псевдоним для `None`). Это ключевой механизм для подготовки массивов к бродкастингу. Например, вектор формы `(n,)` можно превратить в столбец `(n, 1)` или строку `(1, n)`, что позволяет выполнять операции, иначе запрещённые из-за несовместимости форм.\n",
        "\n",
        "> **Пример: превращение вектора в столбец**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(4)           # shape: (4,)\n",
        "b = a[:, np.newaxis]       # shape: (4, 1)\n",
        "c = a[np.newaxis, :]       # shape: (1, 4)\n",
        "\n",
        "print(\"Исходный:\", a.shape)\n",
        "print(\"Столбец:\", b.shape)\n",
        "print(\"Строка:\", c.shape)\n",
        "\n",
        "# Теперь можно, например, вычесть вектор из каждой строки матрицы\n",
        "matrix = np.random.rand(4, 5)\n",
        "result = matrix - b  # broadcasting: (4,5) - (4,1) → (4,5)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без добавления оси такие операции были бы невозможны, что подчёркивает центральную роль `np.newaxis` в векторизованных вычислениях.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ IV. Доступ к элементам: индексация и маскирование\n",
        "\n",
        "NumPy предоставляет несколько мощных и взаимодополняющих механизмов доступа к данным, каждый из которых имеет свои особенности с точки зрения производительности, гибкости и поведения в памяти.\n",
        "\n",
        "### 4.1. Базовая индексация и срезы\n",
        "\n",
        "Базовая индексация включает доступ к отдельным элементам и создание срезов с использованием целых чисел и стандартного синтаксиса срезов (`start:stop:step`). Эта форма индексации всегда возвращает **представление (view)**, то есть новый объект массива, который разделяет память с исходным. Это делает операцию чрезвычайно быстрой, но требует осторожности: любое изменение среза напрямую влияет на исходный массив.\n",
        "\n",
        "> **Пример: срез как view**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "sub = a[0, :]  # первая строка — view\n",
        "sub[0] = 999\n",
        "print(\"Изменённый массив:\\n\", a)  # [[999, 2, 3], [4, 5, 6]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Если изоляция данных необходима, следует явно вызывать метод `.copy()`.\n",
        "\n",
        "### 4.2. Булева индексация (маскирование)\n",
        "\n",
        "Булева индексация использует массив логических значений (`True`/`False`) той же формы, что и исходный массив, для выбора элементов, где маска равна `True`. Этот механизм является основным инструментом для фильтрации данных по произвольному условию, замены значений или удаления пропущенных (NaN) или некорректных наблюдений.\n",
        "\n",
        "В отличие от базовой индексации, булево маскирование **всегда возвращает копию** данных в виде **одномерного массива**, независимо от того, к каким осям применяется маска.\n",
        "\n",
        "> **Пример: фильтрация и замена**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2, np.nan, 3])\n",
        "\n",
        "# Замена отрицательных чисел на 0\n",
        "x[x < 0] = 0\n",
        "print(\"После замены отрицательных:\", x)\n",
        "\n",
        "# Удаление NaN (возвращает копию!)\n",
        "clean_x = x[~np.isnan(x)]\n",
        "print(\"Без NaN:\", clean_x)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Булево маскирование является основой для условной обработки данных в векторизованном стиле.\n",
        "\n",
        "### 4.3. Продвинутая (fancy) целочисленная индексация\n",
        "\n",
        "Продвинутая индексация использует массивы или списки целых чисел для выбора произвольных, не обязательно смежных или упорядоченных элементов. Элементы могут повторяться, а их порядок в результате будет точно соответствовать порядку индексов.\n",
        "\n",
        "При передаче массивов индексов для нескольких осей они **согласуются (broadcastятся)** и комбинируются поэлементно. Например, если передать два массива длины 3 для строк и столбцов, будет выбрано ровно 3 элемента — на пересечении `(row[0], col[0])`, `(row[1], col[1])` и так далее.\n",
        "\n",
        "Этот тип индексации **всегда создаёт копию** данных и возвращает массив, форма которого определяется результатом broadcasting индексных массивов.\n",
        "\n",
        "> **Пример: выбор конкретных позиций**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(12).reshape(3, 4)\n",
        "print(\"Массив a:\\n\", a)\n",
        "\n",
        "# Выбор элементов (0,1), (1,2), (2,3)\n",
        "rows = np.array([0, 1, 2])\n",
        "cols = np.array([1, 2, 3])\n",
        "selected = a[rows, cols]\n",
        "print(\"Выбранные элементы:\", selected)  # [1, 6, 11]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Результат представляет собой одномерный массив, даже если исходный массив был многомерным, что важно учитывать при проектировании алгоритмов.\n",
        "\n",
        "### 4.4. Комбинированная индексация: функция `np.ix_`\n",
        "\n",
        "Прямая передача двух отдельных массивов индексов для строк и столбцов не даёт полной подматрицы, а выбирает только диагональные пары. Для получения **всех комбинаций** индексов — то есть подматрицы на пересечении заданных строк и столбцов — используется функция `np.ix_`.\n",
        "\n",
        "Эта функция преобразует одномерные массивы индексов в совместимые формы: первый массив превращается в столбец `(n, 1)`, второй — в строку `(1, m)`. Это позволяет механизму бродкастинга создать полную двумерную сетку индексов.\n",
        "\n",
        "> **Пример: выбор подматрицы с помощью `np.ix_`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[ 0,  1,  2],\n",
        "              [ 3,  4,  5],\n",
        "              [ 6,  7,  8],\n",
        "              [ 9, 10, 11]])\n",
        "\n",
        "# Строки с чётной суммой\n",
        "rows_mask = (x.sum(axis=1) % 2 == 0)  # [True, False, True, False]\n",
        "# Столбцы 0 и 2\n",
        "cols = np.array([0, 2])\n",
        "\n",
        "# Правильный способ: создать полную подматрицу\n",
        "subset = x[np.ix_(rows_mask, cols)]\n",
        "print(\"Подматрица на пересечении:\\n\", subset)\n",
        "# Результат:\n",
        "# [[0  2]\n",
        "#  [6  8]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.ix_` является незаменимым инструментом для сложной фильтрации по нескольким измерениям и обеспечивает полный контроль над структурой результата.\n",
        "\n",
        "---\n",
        "\n",
        "> **Примечание:** Эта часть завершает вводный обзор ключевых возможностей `ndarray`. В следующем модуле будут рассмотрены **математические функции**, **агрегации**, **broadcasting** и **производительность** в NumPy."
      ],
      "metadata": {
        "id": "TMDG0VuWec0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## РАЗДЕЛ V. Векторизованные операции и бродкастинг\n",
        "\n",
        "### 5.1. Универсальные функции (UFuncs)\n",
        "\n",
        "**Универсальные функции** (*universal functions*, или **UFuncs**) составляют основу векторизованных вычислений в NumPy. Это функции, которые выполняют **поэлементные операции** над массивами с высокой скоростью, поскольку их внутренние циклы реализованы на C и не зависят от интерпретируемой природы Python. К этому классу относятся арифметические операции (`np.add`, `np.multiply`), элементарные математические функции (`np.sin`, `np.exp`, `np.log`) и логические операторы (`np.greater`, `np.equal`, `np.logical_and`). Все UFuncs автоматически применяют правила бродкастинга, что позволяет им корректно работать с массивами разной, но совместимой формы, обеспечивая при этом максимальную производительность и экономию памяти.\n",
        "\n",
        "> **Пример: UFunc в действии**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "# Поэлементное умножение через UFunc\n",
        "result = np.multiply(a, b)  # эквивалентно a * b\n",
        "print(result)  # [4 10 18]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Благодаря UFuncs операции над массивами выглядят как обычные арифметические выражения, но выполняются на уровне C — без циклов Python.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Теория бродкастинга (Broadcasting Theory)\n",
        "\n",
        "**Бродкастинг** — это мощный механизм, позволяющий NumPy выполнять арифметические операции над массивами **разной формы**, не копируя данные физически. Вместо дублирования памяти он логически «растягивает» меньший массив при доступе к элементам. Это критически важно для эффективного использования памяти и производительности, особенно при работе с большими тензорами.\n",
        "\n",
        "Совместимость форм определяется строгими правилами. Сравнение начинается с последней (самой правой) оси и движется влево. Для каждой пары осей допускаются два случая: либо их размеры равны, либо один из них равен единице. Если один массив имеет меньше осей, он виртуально дополняется осями размером 1 слева. Если ни одно из условий не выполняется для хотя бы одной пары осей, возникает исключение `ValueError: operands could not be broadcast together`.\n",
        "\n",
        "> **Пример: проверка совместимости**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Формы: (3, 4) и (4,) → совместимы: (3,4) vs (1,4) → (3,4)\n",
        "A = np.ones((3, 4))\n",
        "b = np.array([1, 2, 3, 4])\n",
        "C = A + b  # OK\n",
        "\n",
        "# Формы: (2, 3) и (3, 2) → НЕсовместимы: 3 ≠ 2 и ни одно ≠ 1\n",
        "try:\n",
        "    D = np.ones((2, 3)) + np.ones((3, 2))\n",
        "except ValueError as e:\n",
        "    print(\"Ошибка:\", e)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Бродкастинг — это не копирование, а **логическая «растяжка»** данных при доступе к памяти.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Практические примеры бродкастинга\n",
        "\n",
        "Наиболее простой случай — операция массива со скаляром. Скаляр «виртуально растягивается» до формы массива, и операция применяется поэлементно. Более сложный и часто встречающийся сценарий — добавление одномерного вектора к каждой строке двумерной матрицы. Если длина вектора совпадает с числом столбцов матрицы, он автоматически добавляется к **каждой строке** без необходимости явного цикла или копирования.\n",
        "\n",
        "Для создания **всех возможных комбинаций** между двумя векторами используется приём с `np.newaxis`. Преобразуя один вектор в столбец `(n, 1)`, а другой оставляя строкой `(m,)`, можно построить двумерную матрицу результатов `(n, m)`, что эквивалентно внешнему произведению, но применимо к любой бинарной операции.\n",
        "\n",
        "> **Пример: внешняя сумма**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "a = np.array([0, 10, 20])    # (3,)\n",
        "b = np.array([1, 2])         # (2,)\n",
        "\n",
        "# Превращаем a в столбец: (3, 1)\n",
        "outer_sum = a[:, np.newaxis] + b  # (3,1) + (2,) → (3,2)\n",
        "print(\"Внешняя сумма:\\n\", outer_sum)\n",
        "# [[ 1  2]\n",
        "#  [11 12]\n",
        "#  [21 22]]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой приём часто используется для построения сеток значений, вычисления попарных расстояний или ядерных функций.\n",
        "\n",
        "Ключевое преимущество бродкастинга — **экономия памяти**. Например, при умножении изображения формы `(256, 256, 3)` на скаляр NumPy не создаёт копию объёмом в сотни мегабайт, а выполняет операцию на лету, что делает его незаменимым в задачах компьютерного зрения и обработки сигналов.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Условные операции: `np.where()`\n",
        "\n",
        "Функция `np.where(condition, x, y)` представляет собой **векторизованный аналог конструкции `if-else`**. Для каждого элемента результирующего массива выбирается значение из `x` или `y` в зависимости от соответствующего логического условия. Эта функция возвращает новый массив, что делает её поведение предсказуемым и безопасным по сравнению с in-place операциями.\n",
        "\n",
        "> **Пример: замена значений по условию**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "# Заменить отрицательные на 0, положительные — на 1, нули оставить\n",
        "result = np.where(x < 0, 0, np.where(x > 0, 1, x))\n",
        "print(result)  # [0 0 0 1 1]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.where` особенно полезен в сложных условиях, где требуется несколько уровней ветвления, и позволяет избежать цепочек булевых масок.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ VI. Математика и статистика массивов\n",
        "\n",
        "NumPy предоставляет богатый набор **векторизованных агрегационных функций**, которые могут применяться ко всему массиву или вдоль заданной оси. Эти функции реализованы на C и обеспечивают высокую производительность даже для больших наборов данных.\n",
        "\n",
        "### 6.1. Статистические агрегации\n",
        "\n",
        "К базовым статистическим функциям относятся `np.sum`, `np.mean`, `np.std`, `np.min` и `np.max`. Все они поддерживают параметр `axis`, который определяет, **вдоль какой оси «схлопывается»** массив. Например, при `axis=0` операция выполняется по строкам (результат — вектор по столбцам), а при `axis=1` — по столбцам (результат — вектор по строкам).\n",
        "\n",
        "> **Пример: агрегация по осям**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "\n",
        "print(\"Сумма по всему массиву:\", np.sum(A))        # 21\n",
        "print(\"Сумма по строкам (axis=1):\", np.sum(A, axis=1))  # [6, 15]\n",
        "print(\"Сумма по столбцам (axis=0):\", np.sum(A, axis=0)) # [5, 7, 9]\n",
        "```\n",
        "\n",
        "> *Пояснение:* Параметр `axis` является ключевым для многомерного анализа и часто используется в предобработке данных для машинного обучения.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Кумулятивные операции: `np.cumsum`\n",
        "\n",
        "Функция `np.cumsum(a, axis=None)` возвращает массив с **накопленными суммами**. Важно понимать, что из-за особенностей машинной арифметики с плавающей точкой, последний элемент результата `np.cumsum(a)[-1]` **может не совпадать** с `np.sum(a)`. Причина в том, что `np.sum` использует **оптимизированные алгоритмы суммирования** (например, pairwise summation), которые минимизируют ошибку округления, тогда как `cumsum` выполняет строгую последовательную аккумуляцию, накапливая ошибку на каждом шаге.\n",
        "\n",
        "> **Пример: разница в точности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Массив с очень малыми и большими числами\n",
        "x = np.array([1e10, 1, -1e10, 1])\n",
        "\n",
        "total_sum = np.sum(x)\n",
        "cumsum_last = np.cumsum(x)[-1]\n",
        "\n",
        "print(\"np.sum(x):\", total_sum)           # 2.0\n",
        "print(\"cumsum(x)[-1]:\", cumsum_last)     # 0.0 — потеря точности!\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для итоговых сумм предпочтительнее `np.sum`; `cumsum` следует использовать только если требуется вся последовательность накопленных значений.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. Вычисление квантилей: `np.percentile` и `np.quantile`\n",
        "\n",
        "Функция `np.percentile(a, q, axis=None, method='linear')` вычисляет **перцентили** (квантили, умноженные на 100). Значение `q=50` соответствует медиане, `q=25` и `q=75` — первому и третьему квартилям. Параметр `method` позволяет выбрать алгоритм интерполяции между соседними точками данных. Доступны такие варианты, как `'linear'` (по умолчанию), `'lower'`, `'higher'`, `'midpoint'` и `'inverted_cdf'`, последний из которых соответствует классическому статистическому определению квантиля.\n",
        "\n",
        "> **Пример: медиана и квартили**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "median = np.percentile(data, 50)\n",
        "q1 = np.percentile(data, 25)\n",
        "q3 = np.percentile(data, 75)\n",
        "\n",
        "print(\"Медиана:\", median)  # 5.0\n",
        "print(\"Q1, Q3:\", q1, q3)   # 3.0, 7.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Выбор метода должен соответствовать статистической методологии исследования — особенно при сравнении результатов с другими пакетами, такими как R или pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.4. Управление агрегацией по осям: `axis` и `keepdims`\n",
        "\n",
        "При выполнении агрегации по оси соответствующее измерение **удаляется** из результата. Это может вызвать трудности при последующих операциях, например, при вычитании среднего значения из исходного массива, поскольку формы перестают быть совместимыми. Решение — использование параметра `keepdims=True`, который **сохраняет оси размером 1** в результирующем массиве, делая его совместимым с исходным для бродкастинга.\n",
        "\n",
        "> **Пример: стандартизация данных**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Исходные данные: 10 наблюдений, 5 признаков\n",
        "X = np.random.rand(10, 5)\n",
        "\n",
        "# Без keepdims: среднее — вектор (5,)\n",
        "mean_bad = np.mean(X, axis=0)\n",
        "# X - mean_bad → работает благодаря бродкастингу, но не всегда очевидно\n",
        "\n",
        "# С keepdims: среднее — матрица (1, 5)\n",
        "mean_good = np.mean(X, axis=0, keepdims=True)\n",
        "std_good = np.std(X, axis=0, keepdims=True)\n",
        "\n",
        "# Стандартизация: каждая строка центрируется и масштабируется\n",
        "X_standardized = (X - mean_good) / std_good\n",
        "\n",
        "print(\"Форма X:\", X.shape)               # (10, 5)\n",
        "print(\"Форма mean_good:\", mean_good.shape)  # (1, 5)\n",
        "print(\"Форма X_standardized:\", X_standardized.shape)  # (10, 5)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `keepdims=True` делает код **более явным, устойчивым к ошибкам** и совместимым с последующими бродкастинг-операциями. Это особенно важно в машинном обучении, где центрирование и масштабирование — стандартные этапы предобработки.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:** Векторизованные операции, UFuncs и бродкастинг — это три кита, на которых стоит эффективная работа с данными в NumPy. Понимание этих механизмов позволяет писать код, который не только короче и читабельнее, но и **на порядки быстрее** и **экономичнее по памяти**, чем эквивалент, написанный с использованием циклов Python."
      ],
      "metadata": {
        "id": "aHuYl7A9ehWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## РАЗДЕЛ VII. Линейная алгебра (подмодуль `numpy.linalg`)\n",
        "\n",
        "Подмодуль **`numpy.linalg`** предоставляет доступ к **высокооптимизированным реализациям** стандартных операций линейной алгебры, основанным на промышленных библиотеках **BLAS** и **LAPACK**. Эти функции гарантируют как скорость, так и численную надёжность при работе с матрицами.\n",
        "\n",
        "### 7.1. Матричные произведения\n",
        "\n",
        "NumPy предлагает два основных способа выполнения матричного умножения, различающихся семантикой и областью применения. Функция **`np.dot(A, B)`** является универсальной: для одномерных массивов она возвращает скалярное произведение, для двумерных — выполняет классическое матричное умножение, а для массивов более высокого ранга — суммирует по последней оси первого аргумента и предпоследней оси второго.\n",
        "\n",
        "В отличие от неё, оператор **`A @ B`** или функция **`np.matmul(A, B)`** предназначены строго для матричного умножения. Они не поддерживают скалярное произведение одномерных векторов (выбрасывая исключение `ValueError`), но корректно обрабатывают **«стеки» матриц**, например, тензоры форм `(N, M, K)` и `(N, K, L)`, результатом умножения которых будет тензор `(N, M, L)`. Этот подход строже следует правилам линейной алгебры и делает намерения кода более явными.\n",
        "\n",
        "> **Пример: сравнение `dot` и `matmul`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Скалярное произведение\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "print(\"np.dot(a, b):\", np.dot(a, b))  # 32\n",
        "\n",
        "try:\n",
        "    print(\"a @ b:\", a @ b)  # Ошибка: 1D @ 1D не поддерживается\n",
        "except ValueError as e:\n",
        "    print(\"Ошибка @ с 1D:\", e)\n",
        "\n",
        "# Матричное умножение\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "print(\"A @ B:\\n\", A @ B)\n",
        "\n",
        "# Стеки матриц\n",
        "C = np.random.rand(2, 3, 4)  # 2 матрицы 3×4\n",
        "D = np.random.rand(2, 4, 5)  # 2 матрицы 4×5\n",
        "E = C @ D  # результат: (2, 3, 5)\n",
        "print(\"Форма результата стека:\", E.shape)\n",
        "```\n",
        "\n",
        "> *Пояснение:* В современном коде **предпочтителен оператор `@`**, так как он делает намерения разработчика более явными и безопасными.\n",
        "\n",
        "### 7.2. Решение систем линейных уравнений (СЛУ)\n",
        "\n",
        "Для решения системы вида **`A x = B`**, где `A` — квадратная матрица, используется функция `np.linalg.solve(A, B)`. Она требует, чтобы матрица `A` была **квадратной** и **несингулярной** (полного ранга). Если система **переопределена** (уравнений больше, чем неизвестных) или **недоопределена**, следует использовать **метод наименьших квадратов** через функцию `np.linalg.lstsq(A, B, rcond=None)`.\n",
        "\n",
        "> **Проверка решения:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[3, 1], [1, 2]])\n",
        "B = np.array([9, 8])\n",
        "x = np.linalg.solve(A, B)\n",
        "\n",
        "print(\"Решение x:\", x)\n",
        "print(\"Проверка A @ x ≈ B:\", np.allclose(A @ x, B))  # True\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.allclose` учитывает погрешности машинной арифметики и корректно сравнивает результаты с плавающей точкой.\n",
        "\n",
        "### 7.3. Обратная матрица и численная стабильность\n",
        "\n",
        "Функция `np.linalg.inv(A)` вычисляет обратную матрицу **`A⁻¹`**. Однако **прямое обращение — плохая практика** в большинстве приложений. Если матрица **плохо обусловлена** (близка к сингулярной), результат будет **числово неточным**. Число обусловленности, вычисляемое как `np.linalg.cond(A) = σ_max / σ_min`, количественно оценивает эту чувствительность: чем больше значение, тем выше риск ошибки.\n",
        "\n",
        "Рекомендуется вместо выражения `x = inv(A) @ B` всегда использовать **`x = solve(A, B)`** — это не только быстрее, но и значительно стабильнее с точки зрения численной математики.\n",
        "\n",
        "> **Пример: сравнение точности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Плохо обусловленная матрица Гильберта\n",
        "A = np.array([[1, 1/2], [1/2, 1/3]], dtype=np.float64)\n",
        "B = np.array([1, 1])\n",
        "\n",
        "# Плохой способ\n",
        "x_bad = np.linalg.inv(A) @ B\n",
        "\n",
        "# Хороший способ\n",
        "x_good = np.linalg.solve(A, B)\n",
        "\n",
        "true_x = np.array([4, -6])  # точное решение\n",
        "print(\"Ошибка через inv:\", np.linalg.norm(x_bad - true_x))\n",
        "print(\"Ошибка через solve:\", np.linalg.norm(x_good - true_x))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Даже на малых матрицах разница может быть значимой. В реальных задачах (машинное обучение, физика) предпочтение `solve` критично.\n",
        "\n",
        "### 7.4. Сингулярное разложение (SVD)\n",
        "\n",
        "**Сингулярное разложение (SVD)** — одна из самых мощных техник линейной алгебры. Любая матрица **`A`** может быть разложена как:\n",
        "\\[\n",
        "A = U \\cdot S \\cdot V^H\n",
        "\\]\n",
        "где `U` — левые сингулярные векторы, `S` — диагональная матрица сингулярных значений (в NumPy — вектор `s`), а `V^H` — сопряжённо-транспонированные правые сингулярные векторы.\n",
        "\n",
        "Функция `U, s, Vh = np.linalg.svd(A, full_matrices=False)` возвращает **усечённое SVD**, что экономит память и используется в задачах **PCA**, сжатия данных и регуляризации. Все функции `linalg`, включая `svd`, `solve` и `inv`, поддерживают работу с **N-мерными массивами**, применяя операции к последним двум осям — что идеально для обработки батчей в машинном обучении.\n",
        "\n",
        "> **Пример: реконструкция и PCA-подобное сжатие**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "A = np.random.rand(5, 4)\n",
        "U, s, Vh = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "# Реконструкция\n",
        "A_rec = U @ np.diag(s) @ Vh\n",
        "print(\"Ошибка реконструкции:\", np.linalg.norm(A - A_rec))  # ~1e-15\n",
        "\n",
        "# Сжатие: оставить только 2 главных компоненты\n",
        "k = 2\n",
        "A_approx = U[:, :k] @ np.diag(s[:k]) @ Vh[:k, :]\n",
        "print(\"Сжатая форма:\", A_approx.shape)  # (5, 4), но ранг ≈ 2\n",
        "```\n",
        "\n",
        "> *Пояснение:* SVD лежит в основе **метода главных компонент (PCA)**, рекомендательных систем и решения некорректных СЛУ.\n",
        "\n",
        "---\n",
        "\n",
        "## РАЗДЕЛ VIII. Производительность, продвинутые методы и практическое применение\n",
        "\n",
        "### 8.1. Продвинутая векторизация: конвенция Эйнштейна (`np.einsum`)\n",
        "\n",
        "Для сложных тензорных операций, которые неудобно выражать через `@` или `dot`, NumPy предоставляет **`np.einsum`** — реализацию **конвенции суммирования Эйнштейна**. Синтаксис функции задаётся строкой вида `'индексы_входов->индексы_выхода'`, что делает код читаемым и близким к математической записи.\n",
        "\n",
        "Например, матричное умножение записывается как `'ij,jk->ik'`, скалярное произведение — как `'i,i->'`, а извлечение диагонали — как `'ii->i'`. Преимущества `np.einsum` многообразны: высокая читаемость, гибкость в выражении сложных свёрток и перестановок, а также возможность автоматической оптимизации порядка операций через параметр `optimize=True`, что особенно важно для больших тензоров. Эта функция лежит в основе тензорных операций в современных фреймворках, таких как TensorFlow и PyTorch.\n",
        "\n",
        "> **Пример: ускорение через оптимизацию**\n",
        "\n",
        "```python\n",
        "X = np.random.rand(100, 50, 20)\n",
        "Y = np.random.rand(50, 20, 80)\n",
        "\n",
        "# Без оптимизации — медленно\n",
        "%timeit np.einsum('ijk,jkl->il', X, Y)\n",
        "\n",
        "# С оптимизацией — может быть в разы быстрее\n",
        "%timeit np.einsum('ijk,jkl->il', X, Y, optimize=True)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `np.einsum` — ключевой инструмент в библиотеках вроде **TensorFlow**, **PyTorch** (через `torch.einsum`) и **JAX**.\n",
        "\n",
        "### 8.2. Производительность: бенчмаркинг и профилирование\n",
        "\n",
        "Векторизованный код на NumPy **на порядки быстрее** циклов Python. Однако «островки» не-векторизованного кода легко становятся **узкими местами**. Для поиска и устранения таких проблем рекомендуется использовать `%timeit` для измерения времени выполнения отдельных участков и профилировщики, такие как `cProfile` или `line_profiler`. Основной стратегией оптимизации остаётся минимизация явных циклов `for` в пользу UFuncs, `np.where` и `np.einsum`.\n",
        "\n",
        "> **Пример: векторизация vs цикл**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(1000000)\n",
        "\n",
        "# Медленно\n",
        "def slow_log(x):\n",
        "    return np.array([np.log(val) if val > 0 else 0 for val in x])\n",
        "\n",
        "# Быстро\n",
        "def fast_log(x):\n",
        "    out = np.zeros_like(x)\n",
        "    mask = x > 0\n",
        "    out[mask] = np.log(x[mask])\n",
        "    return out\n",
        "\n",
        "%timeit slow_log(x)  # ~100 ms\n",
        "%timeit fast_log(x)  # ~1 ms\n",
        "```\n",
        "\n",
        "> *Пояснение:* Разница в 100 раз — типична для перехода от Python-циклов к векторизации.\n",
        "\n",
        "### 8.3. Практика 1: обработка изображений\n",
        "\n",
        "Изображение — это **3D-массив** формы `(высота, ширина, каналы)`. NumPy позволяет выполнять базовые операции **без сторонних библиотек**. Например, нормализацию к диапазону `[0, 1]`, геометрические преобразования (поворот, отражение) и центрирование по каналам можно реализовать с помощью стандартных функций и бродкастинга.\n",
        "\n",
        "> **Пример: нормализация и геометрические преобразования**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Имитация цветного изображения 100×100×3\n",
        "img = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "# Нормализация к [0, 1]\n",
        "img_norm = img.astype(np.float32) / 255.0\n",
        "\n",
        "# Поворот на 90° по часовой стрелке\n",
        "img_rot = np.rot90(img, k=-1)\n",
        "\n",
        "# Отражение по горизонтали\n",
        "img_flip = np.fliplr(img)\n",
        "\n",
        "# Удаление среднего по каналам\n",
        "mean_per_channel = img_norm.mean(axis=(0, 1), keepdims=True)\n",
        "img_centered = img_norm - mean_per_channel\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для сложных операций (размытие, градиенты, морфология) используется **`scipy.ndimage`**, но **NumPy — основа** всех этих преобразований.\n",
        "\n",
        "### 8.4. Практика 2: фильтрация сигналов и симуляции\n",
        "\n",
        "Временные ряды и физические модели представляют собой **одно- или двумерные массивы**, которые идеально подходят для NumPy. Например, скользящее среднее можно эффективно вычислить через кумулятивную сумму, избегая циклов. В симуляциях динамических систем все обновления состояний (позиций, скоростей) должны выполняться векторизованно, что обеспечивает высокую производительность и читаемость кода.\n",
        "\n",
        "> **Пример: скользящее среднее через кумулятивную сумму**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def moving_average(x, window):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (cumsum[window:] - cumsum[:-window]) / window\n",
        "\n",
        "signal = np.random.randn(1000)\n",
        "smoothed = moving_average(signal, window=50)\n",
        "```\n",
        "\n",
        "> **Симуляции:**  \n",
        "> Используйте `Generator` для воспроизводимости, а все обновления — через векторизованные операции:\n",
        "\n",
        "```python\n",
        "rng = np.random.default_rng(42)\n",
        "positions = rng.normal(size=(100, 2))  # 100 частиц в 2D\n",
        "velocities = np.zeros_like(positions)\n",
        "\n",
        "for _ in range(1000):\n",
        "    forces = -positions  # упрощённая сила (пружина)\n",
        "    velocities += forces * 0.01\n",
        "    positions += velocities * 0.01\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой подход лежит в основе **численного интегрирования**, **машинного обучения с подкреплением**, **моделирования погоды** и др.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**NumPy — это не просто библиотека, а философия эффективных научных вычислений.** Его архитектура строится на трёх китах:\n",
        "\n",
        "1. **`ndarray`** — гомогенный, непрерывный контейнер, позволяющий использовать оптимизированные ядра C/Fortran.\n",
        "2. **Векторизация и бродкастинг** — механизм, устраняющий циклы Python и экономящий память.\n",
        "3. **Численно устойчивые алгоритмы** в `numpy.linalg` и `numpy.random` — основа для надёжных расчётов.\n",
        "\n",
        "Однако эффективность требует **осознанного подхода**:\n",
        "- Предпочитайте **`view` над `copy`** (`ravel` вместо `flatten`).\n",
        "- Используйте **`keepdims=True`** при агрегации для корректного бродкастинга.\n",
        "- Избегайте **явного обращения матриц** — выбирайте `solve`.\n",
        "- Применяйте **`np.newaxis`** для подготовки к бродкастингу.\n",
        "- Используйте **`np.einsum`** для сложных тензорных операций.\n",
        "\n",
        "С освоением NumPy вы получаете **единый, мощный и стандартизированный язык** для работы с данными — язык, на котором говорят **SciPy**, **pandas**, **scikit-learn**, **Matplotlib**, и даже **PyTorch** и **TensorFlow** (через совместимость с `ndarray`).\n",
        "\n",
        "> Таким образом, овладение NumPy — это не первый шаг в data science, а **фундамент всего здания современных вычислений на Python**."
      ],
      "metadata": {
        "id": "xtlNUalxhZcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 2: Библиотека Pandas — Комплексный анализ и обработка табличных данных\n",
        "\n",
        "Библиотека **Pandas** является краеугольным камнем современной экосистемы обработки данных на языке Python. Она предоставляет высокопроизводительные, удобные в использовании структуры данных и инструменты для анализа, делая Python мощным инструментом для работы с табличными данными — на уровне таких систем, как **R** или **SQL**.\n",
        "\n",
        "В рамках этого модуля рассматриваются ключевые концепции и практики работы с Pandas, необходимые для построения надёжных **ETL-конвейеров** (Extract, Transform, Load): от загрузки и создания структур данных до очистки, трансформации и сохранения результатов.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Фундаментальные структуры данных Pandas\n",
        "\n",
        "Pandas строится на трёх основных структурах: **`Series`**, **`DataFrame`** и **`Index`**. Понимание их взаимосвязи и внутренних механизмов — особенно **принципа выравнивания данных (Data Alignment)** — критически важно для эффективной и предсказуемой работы с библиотекой.\n",
        "\n",
        "### I.1. Обзор: `Series`, `DataFrame`, `Index`\n",
        "\n",
        "#### **`Series` — одномерный индексированный массив**\n",
        "\n",
        "Объект `Series` представляет собой одномерный массив с **явно заданными метками** (индексом). Его можно рассматривать как **индексированный аналог NumPy-массива** или как **высокопроизводительный словарь с фиксированным порядком ключей**.\n",
        "\n",
        "> **Пример: создание Series**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ser = pd.Series([10, 20, 30], index=['a', 'b', 'c'], name='values')\n",
        "print(\"Series:\\n\", ser)\n",
        "# Вывод:\n",
        "# a    10\n",
        "# b    20\n",
        "# c    30\n",
        "# Name: values, dtype: int64\n",
        "```\n",
        "\n",
        "> *Пояснение:* В отличие от обычного словаря, `Series` поддерживает векторизованные операции и интеграцию с `DataFrame`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`DataFrame` — двумерная таблица с метками**\n",
        "\n",
        "`DataFrame` — основная структура данных в Pandas. Это двумерная, изменяемая таблица с **метками строк (индекс)** и **метками столбцов (колонки)**. Концептуально `DataFrame` можно представить как **словарь из `Series`**, где каждый столбец — это отдельный `Series`, а все они разделяют общий индекс.\n",
        "\n",
        "> **Пример: `DataFrame` из `Series`**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "ser = pd.Series([0, 1, 2], index=['a', 'b', 'c'], name='ser_data')\n",
        "df_from_ser = pd.DataFrame(ser)\n",
        "\n",
        "print(\"DataFrame из Series:\\n\", df_from_ser)\n",
        "# Вывод:\n",
        "#    ser_data\n",
        "# a         0\n",
        "# b         1\n",
        "# c         2\n",
        "```\n",
        "\n",
        "> *Пояснение:* Имя `Series` автоматически становится именем столбца. Индекс сохраняется.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Принцип выравнивания данных (Data Alignment)**\n",
        "\n",
        "Ключевое отличие Pandas от NumPy — **автоматическое выравнивание по меткам** при выполнении операций. Арифметические и логические операции выполняются **по совпадающим строкам и столбцам**, а отсутствующие метки заполняются `NaN`.\n",
        "\n",
        "Это гарантирует, что вы всегда работаете с **соответствующими элементами**, независимо от порядка или полноты данных.\n",
        "\n",
        "> **Пример: выравнивание при сложении**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
        "s2 = pd.Series([10, 20], index=['b', 'c'])\n",
        "\n",
        "result = s1 + s2\n",
        "print(\"Результат сложения с выравниванием:\\n\", result)\n",
        "# Вывод:\n",
        "# a    NaN\n",
        "# b    12.0\n",
        "# c    23.0\n",
        "# dtype: float64\n",
        "```\n",
        "\n",
        "> *Пояснение:* Элемент `'a'` отсутствует в `s2`, поэтому результат — `NaN`. Это предотвращает ошибки, характерные для «слепых» операций над массивами без меток.\n",
        "\n",
        "---\n",
        "\n",
        "### I.2. Атрибуты объектов: `shape`, `dtypes`, `index`, `columns`\n",
        "\n",
        "Эти атрибуты предоставляют метаданные, необходимые для анализа структуры и качества данных.\n",
        "\n",
        "- **`.shape`** — кортеж вида `(число строк, число столбцов)`, совпадает с соглашением NumPy.\n",
        "- **`.dtypes`** — `Series`, показывающий тип данных каждого столбца. Тип `object` часто указывает на строки или смешанные типы — это сигнал к проверке данных.\n",
        "- **`.index`** — метки строк (`Index`-объект).\n",
        "- **`.columns`** — метки столбцов (`Index`-объект).\n",
        "\n",
        "> **Пример: доступ к атрибутам**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'ID': [101, 102, 103, 104, 105],\n",
        "    'Value': [10.5, 20.1, 30.7, 40.2, 50.8],\n",
        "    'Category': ['A', 'B', 'A', 'C', 'B']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data, index=['r1', 'r2', 'r3', 'r4', 'r5'])\n",
        "\n",
        "print(\"Форма (shape):\", df.shape)  # (5, 3)\n",
        "print(\"\\nТипы данных (dtypes):\\n\", df.dtypes)\n",
        "# ID           int64\n",
        "# Value      float64\n",
        "# Category    object\n",
        "\n",
        "print(\"\\nИндекс строк:\", df.index)\n",
        "# Index(['r1', 'r2', 'r3', 'r4', 'r5'], dtype='object')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Анализ `.dtypes` помогает выявить неоптимальное хранение (например, числа как `object`) и спланировать преобразования.\n",
        "\n",
        "---\n",
        "\n",
        "### I.3. Создание объектов из различных источников\n",
        "\n",
        "Pandas поддерживает гибкое создание структур из Python-объектов.\n",
        "\n",
        "#### 1. Из словаря списков\n",
        "\n",
        "Наиболее распространённый способ: ключи → имена столбцов, значения → данные.\n",
        "\n",
        "```python\n",
        "dict_data = {\n",
        "    'City': ['Moscow', 'Kazan', 'Saint Petersburg'],\n",
        "    'Population': [12600000, 1270000, 5400000]\n",
        "}\n",
        "df_dict = pd.DataFrame(dict_data)\n",
        "print(\"Из словаря списков:\\n\", df_dict)\n",
        "```\n",
        "\n",
        "#### 2. Из словаря `Series` (демонстрация выравнивания)\n",
        "\n",
        "```python\n",
        "data_series = {\n",
        "    'Col_A': pd.Series([10, 20], index=['x', 'y']),\n",
        "    'Col_B': pd.Series([100, 200, 300], index=['y', 'z', 'x'])\n",
        "}\n",
        "df_aligned = pd.DataFrame(data_series)\n",
        "print(\"Выравнивание Series:\\n\", df_aligned)\n",
        "# Вывод:\n",
        "#    Col_A  Col_B\n",
        "# x   10.0  300.0\n",
        "# y   20.0  100.0\n",
        "# z    NaN  200.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Pandas автоматически объединил все уникальные метки (`x`, `y`, `z`) и вставил `NaN` там, где данных нет.\n",
        "\n",
        "#### 3. Из NumPy-массива\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "numpy_array = np.array([[1, 2, 3],\n",
        "                        [4, 5, 6],\n",
        "                        [7, 8, 9]])\n",
        "\n",
        "df_numpy = pd.DataFrame(\n",
        "    numpy_array,\n",
        "    index=['r1', 'r2', 'r3'],\n",
        "    columns=['c1', 'c2', 'c3']\n",
        ")\n",
        "print(\"Из NumPy-массива:\\n\", df_numpy)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без явного указания `index` и `columns` будут использованы целочисленные метки по умолчанию.\n",
        "\n",
        "---\n",
        "\n",
        "## II. Операции ввода-вывода (I/O) и оптимизация загрузки\n",
        "\n",
        "Эффективность анализа данных начинается с **быстрой и корректной загрузки**. Pandas предлагает мощные инструменты для работы с CSV, Excel, JSON, Parquet и другими форматами.\n",
        "\n",
        "### II.1. Загрузка данных: `pd.read_csv()`\n",
        "\n",
        "Функция `pd.read_csv()` — основной инструмент для чтения табличных данных. Она поддерживает локальные файлы, URL (`http`, `s3`, `gs`), а также потоки (`StringIO`).\n",
        "\n",
        "#### Ключевые параметры для оптимизации:\n",
        "\n",
        "| Параметр | Назначение | Зачем это важно |\n",
        "|--------|-----------|----------------|\n",
        "| `dtype` | Явное указание типов данных | Предотвращает ошибки угадывания (`object` вместо `int`), экономит память и ускоряет операции |\n",
        "| `usecols` | Загрузка только нужных столбцов | Снижает потребление памяти и время парсинга в разы |\n",
        "| `index_col` | Назначение столбца(ов) как индекса | Упрощает последующий анализ и фильтрацию |\n",
        "| `parse_dates` + `date_format` | Парсинг дат с явным форматом | Ускоряет обработку временных меток и избегает неоднозначности |\n",
        "\n",
        "> **Пример: оптимизированная загрузка**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Предположим, файл содержит: Timestamp (строка), Cost (целое), Description (текст), Region (категория)\n",
        "file_path = 'data/large_data.csv'\n",
        "\n",
        "data_types = {\n",
        "    'Cost': 'int32',          # меньше памяти, чем int64\n",
        "    'Region': 'category'      # идеально для строк с небольшим числом уникальных значений\n",
        "}\n",
        "\n",
        "df_optimized = pd.read_csv(\n",
        "    file_path,\n",
        "    usecols=['Timestamp', 'Cost', 'Region'],  # только нужное\n",
        "    dtype=data_types,\n",
        "    parse_dates=['Timestamp'],                # преобразуем в datetime\n",
        "    index_col='Timestamp'                     # делаем индексом\n",
        ")\n",
        "\n",
        "print(\"Типы после загрузки:\\n\", df_optimized.dtypes)\n",
        "# Timestamp    datetime64[ns]\n",
        "# Cost                  int32\n",
        "# Region             category\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `'category'` для `Region` может сократить потребление памяти в 10–100 раз по сравнению с `'object'`.\n",
        "\n",
        "---\n",
        "\n",
        "### II.2. Сохранение данных: `DataFrame.to_csv()`\n",
        "\n",
        "Эта функция завершает этап **Load** в ETL-процессе.\n",
        "\n",
        "#### Ключевые параметры:\n",
        "\n",
        "- **`index=False`** — не сохранять индекс (если он просто `0,1,2,...`).\n",
        "- **`compression='gzip'`** — сжимать «на лету» (поддержка `infer` по расширению: `.csv.gz`).\n",
        "- **`date_format='%Y-%m-%d'`** — контролировать формат дат при экспорте.\n",
        "\n",
        "> **Пример: сохранение с оптимизацией**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Создаём данные с датами\n",
        "dates = pd.date_range('2023-01-01', periods=5)\n",
        "df_result = pd.DataFrame({'Sales': [100, 150, 200, 180, 220]}, index=dates)\n",
        "df_result.index.name = 'Date'\n",
        "\n",
        "# Сохраняем в сжатый CSV\n",
        "df_result.to_csv(\n",
        "    'results/sales_summary.csv.gz',\n",
        "    index=True,               # сохраняем даты как индекс\n",
        "    date_format='%Y-%m-%d',\n",
        "    compression='gzip'\n",
        ")\n",
        "\n",
        "print(\"Данные сохранены в сжатый файл: results/sales_summary.csv.gz\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Сжатие особенно важно при работе с большими объёмами данных — файлы могут быть в 3–10 раз меньше.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas превращает неструктурированные и полуобработанные данные в **аналитически пригодные структуры**, обеспечивая надёжность через выравнивание, гибкость через метки и производительность через интеграцию с NumPy. Освоение базовых структур и оптимизированного I/O — первый шаг к построению промышленных конвейеров обработки данных.\n"
      ],
      "metadata": {
        "id": "-Vtjc_dPiDTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## III. Индексирование и селекция данных\n",
        "\n",
        "Эффективная выборка и модификация данных — основа любого преобразования в Pandas. Библиотека предоставляет **специализированные аксессоры**, оптимизированные под разные сценарии: от массовой фильтрации до сверхбыстрого доступа к отдельным ячейкам.\n",
        "\n",
        "### III.1. Выборка по меткам и позициям: `loc` и `iloc`\n",
        "\n",
        "#### `loc` — индексирование по **меткам**\n",
        "\n",
        "`df.loc[rows, cols]` выбирает данные **исключительно по именам строк и столбцов**.  \n",
        "\n",
        "**Важная особенность:** при использовании срезов (`:`) с метками **конечная метка включается** в результат (инклюзивное срезание).\n",
        "\n",
        "> **Пример: `loc` с инклюзивным срезом**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    np.arange(12).reshape(3, 4),\n",
        "    index=['r_a', 'r_b', 'r_c'],\n",
        "    columns=['c1', 'c2', 'c3', 'c4']\n",
        ")\n",
        "\n",
        "# Выбираем строки от 'r_a' до 'r_c' (включительно) и столбцы от 'c2' до 'c4' (включительно)\n",
        "result_loc = df.loc['r_a':'r_c', 'c2':'c4']\n",
        "print(\"Селекция через .loc (инклюзивно):\\n\", result_loc)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "     c2  c3  c4\n",
        "r_a   1   2   3\n",
        "r_b   5   6   7\n",
        "r_c   9  10  11\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такое поведение интуитивно для аналитиков: «от A до C» обычно включает и C.\n",
        "\n",
        "---\n",
        "\n",
        "#### `iloc` — индексирование по **целочисленным позициям**\n",
        "\n",
        "`df.iloc[rows, cols]` работает **только с целыми индексами** (0, 1, 2, ...), как в NumPy.  \n",
        "\n",
        "**Семантика срезов стандартная:** конечный индекс **не включается** (эксклюзивное срезание).\n",
        "\n",
        "> **Пример: `iloc` с эксклюзивным срезом**\n",
        "\n",
        "```python\n",
        "# Те же данные\n",
        "result_iloc = df.iloc[0:2, 1:3]  # строки 0–1, столбцы 1–2\n",
        "print(\"\\nСелекция через .iloc (эксклюзивно):\\n\", result_iloc)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "     c2  c3\n",
        "r_a   1   2\n",
        "r_b   5   6\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это важно помнить при переходе от меток к позициям — границы ведут себя по-разному.\n",
        "\n",
        "---\n",
        "\n",
        "### III.2. Оптимизированный скалярный доступ: `at` и `iat`\n",
        "\n",
        "Для **чтения или записи одного значения** используйте `at` (по метке) и `iat` (по позиции). Они **значительно быстрее**, чем `loc`/`iloc`, так как не создают промежуточных объектов.\n",
        "\n",
        "> **Пример: высокоскоростной доступ к ячейкам**\n",
        "\n",
        "```python\n",
        "# Изменяем значение по метке\n",
        "df.at['r_b', 'c3'] = 999\n",
        "value_at = df.at['r_b', 'c3']\n",
        "\n",
        "# Изменяем значение по позиции: строка 2 ('r_c'), столбец 3 ('c4')\n",
        "df.iat[2, 3] = 1000\n",
        "value_iat = df.iat[2, 3]\n",
        "\n",
        "print(f\"\\nЗначение в [r_b, c3] после .at: {value_at}\")\n",
        "print(f\"Значение в [r_c, c4] после .iat: {value_iat}\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти методы критичны в редких случаях, когда без итерации не обойтись (например, в сложной логике с зависимыми условиями). Однако **векторизация всегда предпочтительнее**.\n",
        "\n",
        "---\n",
        "\n",
        "### III.3. Булево индексирование и логические операторы\n",
        "\n",
        "Фильтрация по условиям выполняется через **булевы маски**. При объединении условий **обязательно используйте побитовые операторы**:\n",
        "\n",
        "- `&` вместо `and`\n",
        "- `|` вместо `or`\n",
        "- `~` вместо `not`\n",
        "\n",
        "> **Пример: сложная фильтрация**\n",
        "\n",
        "```python\n",
        "data_bool = {\n",
        "    'Age': [25, 35, 45, 65],\n",
        "    'Salary': [35000, 75000, 85000, 95000]\n",
        "}\n",
        "df_cond = pd.DataFrame(data_bool)\n",
        "\n",
        "# Условие: Возраст > 30 ИЛИ (Зарплата < 80000 И возраст ≤ 60)\n",
        "mask = (df_cond['Age'] > 30) | ((df_cond['Salary'] < 80000) & ~(df_cond['Age'] > 60))\n",
        "\n",
        "filtered = df_cond.loc[mask]\n",
        "print(\"\\nФильтрация через булеву маску:\\n\", filtered)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Age  Salary\n",
        "1   35   75000\n",
        "2   45   85000\n",
        "```\n",
        "\n",
        "> *Пояснение:* Скобки **обязательны** из-за приоритета операторов: `&` имеет более высокий приоритет, чем `|`.\n",
        "\n",
        "---\n",
        "\n",
        "### III.4. Высокопроизводительный запрос: метод `query()`\n",
        "\n",
        "Метод `df.query('условие')` позволяет писать фильтры в виде **строковых выражений**, используя синтаксис, близкий к SQL.\n",
        "\n",
        "#### Преимущества:\n",
        "- Использует библиотеку **NumExpr**, которая вычисляет выражения в C — без участия интерпретатора Python.\n",
        "- **Значительно быстрее** при работе с большими DataFrame (обычно > 50 000 строк).\n",
        "- Поддерживает **внешние переменные** через `@`.\n",
        "\n",
        "#### Синтаксис:\n",
        "- Столбцы с пробелами: `` `Column Name` ``\n",
        "- Внешние переменные: `@var_name`\n",
        "\n",
        "> **Пример: использование `query()`**\n",
        "\n",
        "```python\n",
        "target_age = 30\n",
        "target_salary = 90000\n",
        "\n",
        "# Фильтрация с внешними переменными\n",
        "result_query = df_cond.query('Age > @target_age and Salary < @target_salary')\n",
        "print(\"\\nФильтрация через .query():\\n\", result_query)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Age  Salary\n",
        "1   35   75000\n",
        "2   45   85000\n",
        "```\n",
        "\n",
        "> *Пояснение:* Для небольших данных `query()` может быть **медленнее** из-за накладных расходов на парсинг строки. Используйте его осознанно.\n",
        "\n",
        "---\n",
        "\n",
        "#### Сравнительная таблица методов доступа\n",
        "\n",
        "| Метод      | Основа          | Возвращаемое значение     | Скорость (скаляр) | Основное применение |\n",
        "|-----------|------------------|----------------------------|-------------------|----------------------|\n",
        "| `.loc`    | Метка            | Series / DataFrame / Scalar| Умеренная         | Фильтрация по именам, диапазоны (включая конец) |\n",
        "| `.iloc`   | Позиция          | Series / DataFrame / Scalar| Умеренная         | Селекция по индексу (исключая конец) |\n",
        "| `.at`     | Метка            | **Скаляр**                 | **Высокая**       | Быстрое чтение/запись одной ячейки |\n",
        "| `.iat`    | Позиция          | **Скаляр**                 | **Высокая**       | То же по позиции |\n",
        "| `.query()`| Строка-выражение | DataFrame                  | Высокая (на больших данных) | Читаемые, сложные фильтры |\n",
        "\n",
        "---\n",
        "\n",
        "## IV. Методы очистки и подготовки данных\n",
        "\n",
        "Работа с пропущенными значениями — обязательный этап ETL. В Pandas отсутствующие данные обозначаются как **`np.nan`** (или `pd.NA` для новых nullable-типов). Стратегии обработки делятся на три категории.\n",
        "\n",
        "### IV.1. Теоретические основы обработки пропусков\n",
        "\n",
        "1. **Удаление (`dropna`)** — простой, но радикальный метод. Оправдан при малой доле пропусков.\n",
        "2. **Вменение (`fillna`)** — замена на константу, среднее, медиану. Сохраняет объём данных.\n",
        "3. **Интерполяция (`interpolate`)** — оценка пропусков на основе соседей. Идеальна для временных рядов.\n",
        "\n",
        "> **Выбор стратегии зависит от:**\n",
        "> - природы данных,\n",
        "> - доли пропусков,\n",
        "> - наличия структуры (например, временной упорядоченности).\n",
        "\n",
        "---\n",
        "\n",
        "### IV.2. Идентификация и удаление пропусков\n",
        "\n",
        "- **`isna()` / `notna()`** — возвращают булеву маску.\n",
        "- **`dropna()`** — удаляет строки/столбцы с пропусками.\n",
        "\n",
        "Параметры:\n",
        "- `axis=0` — удалять строки (по умолчанию), `axis=1` — столбцы.\n",
        "- `how='any'` — удалить при **любом** `NaN`; `how='all'` — только если **все** значения `NaN`.\n",
        "\n",
        "> **Пример: работа с пропусками**\n",
        "\n",
        "```python\n",
        "df_na = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [5, np.nan, np.nan, 8],\n",
        "    'C': [np.nan, np.nan, np.nan, np.nan]\n",
        "})\n",
        "\n",
        "print(\"Пропуски по столбцам:\\n\", df_na.isna().sum())\n",
        "# A: 1, B: 2, C: 4\n",
        "\n",
        "# Удалить строки с хотя бы одним NaN\n",
        "print(\"\\nПосле dropna(axis=0, how='any'):\\n\", df_na.dropna())\n",
        "\n",
        "# Удалить столбцы с хотя бы одним NaN\n",
        "print(\"\\nПосле dropna(axis=1, how='any'):\\n\", df_na.dropna(axis=1))\n",
        "# Пустой DataFrame — все столбцы содержат NaN\n",
        "```\n",
        "\n",
        "> *Пояснение:* Столбец `C` полностью пуст — его часто удаляют на этапе предварительного анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### IV.3. Заполнение пропусков: `fillna()`\n",
        "\n",
        "#### 1. Скалярные значения и статистика\n",
        "\n",
        "```python\n",
        "# Восстановим исходный df_na\n",
        "df_na = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [5, np.nan, np.nan, 8]\n",
        "})\n",
        "\n",
        "# Замена средним\n",
        "df_na['A'] = df_na['A'].fillna(df_na['A'].mean())\n",
        "\n",
        "# Замена константой\n",
        "df_na = df_na.fillna(0)\n",
        "print(\"После fillna:\\n\", df_na)\n",
        "```\n",
        "\n",
        "#### 2. Пропагация значений (`ffill`, `bfill`)\n",
        "\n",
        "- `method='ffill'` — заполнить предыдущим значением (forward fill).\n",
        "- `method='bfill'` — заполнить следующим значением (backward fill).\n",
        "- `limit` — ограничить количество заполняемых подряд пропусков.\n",
        "\n",
        "> **Пример: пропагация**\n",
        "\n",
        "```python\n",
        "ser = pd.Series([1.0, np.nan, np.nan, 5.0, np.nan, 7.0])\n",
        "\n",
        "# Ffill с лимитом — заполнит только один пропуск\n",
        "ffill_limited = ser.fillna(method='ffill', limit=1)\n",
        "print(\"Ffill (limit=1):\\n\", ffill_limited)\n",
        "# [1.0, 1.0, nan, 5.0, 5.0, 7.0]\n",
        "\n",
        "# Bfill — заполняет в обратном направлении\n",
        "bfill_full = ser.fillna(method='bfill')\n",
        "print(\"\\nBfill:\\n\", bfill_full)\n",
        "# [1.0, 5.0, 5.0, 5.0, 7.0, 7.0]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `limit` предотвращает «размазывание» значения на слишком большой промежуток.\n",
        "\n",
        "---\n",
        "\n",
        "### IV.4. Интерполяция данных: `interpolate()`\n",
        "\n",
        "Интерполяция **оценивает пропущенные значения**, основываясь на соседях. По умолчанию — **линейная**.\n",
        "\n",
        "> **Пример: линейная интерполяция**\n",
        "\n",
        "```python\n",
        "ser_interp = pd.Series([0, 10, np.nan, np.nan, 40, 50])\n",
        "result = ser_interp.interpolate(method='linear')\n",
        "print(\"Линейная интерполяция:\\n\", result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "0     0.0\n",
        "1    10.0\n",
        "2    20.0\n",
        "3    30.0\n",
        "4    40.0\n",
        "5    50.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Особенно полезно для **временных рядов**, **геоданных**, **датчиков**, где данные упорядочены и гладки.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas предоставляет **полный арсенал инструментов** для точечного и массового доступа к данным, а также для гибкой обработки пропусков. Осознанное использование `loc`/`iloc`, `at`/`iat`, `query`, `fillna` и `interpolate` позволяет строить **надёжные, читаемые и производительные** конвейеры очистки и трансформации данных.\n"
      ],
      "metadata": {
        "id": "UPm9PAamjFW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## V. Векторизация, применение пользовательских функций и цепочки вызовов\n",
        "\n",
        "Основа **высокопроизводительного кода в Pandas** — **векторизация**: делегирование вычислений оптимизированным ядрам C/NumPy вместо медленных циклов Python. Однако при необходимости применения пользовательской логики важно выбирать правильный инструмент — от простого `map` до декларативного `assign` и оптимизированного `eval`.\n",
        "\n",
        "---\n",
        "\n",
        "### V.1. Векторизация и применение функций: `map`, `apply`, `applymap`\n",
        "\n",
        "Эти методы различаются по гибкости, производительности и области применения.\n",
        "\n",
        "> **Важно:** все они **медленнее чисто векторизованных операций** (`+`, `np.log`, `str.upper` и др.). Используйте их только тогда, когда векторизация невозможна.\n",
        "\n",
        "#### V.1.1. `Series.map()`\n",
        "\n",
        "Применяется **только к `Series`**, работает поэлементно.  \n",
        "**Идеален для:**\n",
        "- замены значений по словарю (например, кодирование категорий),\n",
        "- простых преобразований через лямбда-функции.\n",
        "\n",
        "> **Ограничение:** не поддерживает передачу дополнительных аргументов.\n",
        "\n",
        "#### V.1.2. `Series.apply()`\n",
        "\n",
        "Более гибкий, чем `map`.  \n",
        "**Позволяет:**\n",
        "- передавать `args` и `kwargs`,\n",
        "- возвращать сложные объекты (например, `Series` → `DataFrame`).\n",
        "\n",
        "#### V.1.3. `DataFrame.apply()`\n",
        "\n",
        "Применяет функцию **к строкам (`axis=1`) или столбцам (`axis=0`)**.\n",
        "\n",
        "- **`axis=0`** (по умолчанию): функция получает каждый **столбец** как `Series` → полезно для статистики.\n",
        "- **`axis=1`**: функция получает каждую **строку** как `Series` → полезно для вычисления признаков из нескольких столбцов.\n",
        "\n",
        "> **Пример: `map` и `apply(axis=1)`**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_func = pd.DataFrame({\n",
        "    'Status_Code': [1, 2, 3, 1],\n",
        "    'Height': [170, 165, 180, 175],   # в см\n",
        "    'Weight': [70, 60, 90, 80]        # в кг\n",
        "})\n",
        "\n",
        "# 1. Замена кодов статусов на метки\n",
        "status_map = {1: 'Active', 2: 'Inactive', 3: 'Pending'}\n",
        "df_func['Status'] = df_func['Status_Code'].map(status_map)\n",
        "\n",
        "# 2. Расчёт BMI построчно\n",
        "def calculate_bmi(row):\n",
        "    height_m = row['Height'] / 100.0\n",
        "    return row['Weight'] / (height_m ** 2)\n",
        "\n",
        "df_func['BMI'] = df_func.apply(calculate_bmi, axis=1)\n",
        "\n",
        "print(\"DataFrame после map() и apply(axis=1):\\n\", df_func)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Status_Code  Height  Weight    Status        BMI\n",
        "0            1     170      70    Active  24.221474\n",
        "1            2     165      60  Inactive  22.038567\n",
        "2            3     180      90   Pending  27.777778\n",
        "3            1     175      80    Active  26.122449\n",
        "```\n",
        "\n",
        "> *Пояснение:* `map` использован для **кодирования**, `apply(axis=1)` — для **расчёта составного признака**. Оба метода создают **новые столбцы**, не изменяя исходные данные.\n",
        "\n",
        "---\n",
        "\n",
        "### V.2. Создание новых признаков: `assign()` и `eval()`\n",
        "\n",
        "#### V.2.1. `DataFrame.assign()` — декларативное создание столбцов\n",
        "\n",
        "Метод `assign()` возвращает **новый DataFrame** с добавленными столбцами. Его главное преимущество — **интеграция в цепочки вызовов** (method chaining), что улучшает читаемость и функциональность кода.\n",
        "\n",
        "```python\n",
        "df_new = df_func.assign(\n",
        "    BMI_rounded = lambda x: x['BMI'].round(1),\n",
        "    Is_Overweight = lambda x: x['BMI'] > 25\n",
        ")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Использование `lambda` позволяет ссылаться на **уже существующие столбцы** в том же вызове `assign`.\n",
        "\n",
        "#### V.2.2. `pandas.eval()` — высокоскоростные вычисления через строку\n",
        "\n",
        "Функция `pd.eval()` использует движок **NumExpr** для выполнения арифметических и логических выражений **в C-слое**, минимизируя overhead Python.\n",
        "\n",
        "> **Когда использовать?**  \n",
        "> Только для **очень больших DataFrame** (обычно > 100 000 строк). Для малых данных накладные расходы на парсинг строки перевешивают выгоду.\n",
        "\n",
        "> **Пример: `eval` с множественными выражениями**\n",
        "\n",
        "```python\n",
        "N = 500_000\n",
        "df_large = pd.DataFrame(\n",
        "    np.random.randint(1, 100, size=(N, 3)),\n",
        "    columns=['A', 'B', 'C']\n",
        ")\n",
        "\n",
        "# Два новых признака за один вызов\n",
        "df_large.eval(\n",
        "    \"D = (A + B) / C; E = A * B - C\",\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "print(\"После eval (первые 3 строки):\\n\", df_large.head(3))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Разделение выражений точкой с запятой позволяет выполнить **несколько операций без промежуточных копий** — это ключ к максимальной производительности.\n",
        "\n",
        "---\n",
        "\n",
        "### V.3. Построение конвейеров: `pipe()`\n",
        "\n",
        "Метод `pipe()` позволяет строить **читаемые, последовательные конвейеры**, где результат предыдущей функции передаётся как первый аргумент в следующую.\n",
        "\n",
        "> **Преимущества:**\n",
        "> - Избегает вложенности: `f(g(h(df)))` → `df.pipe(h).pipe(g).pipe(f)`\n",
        "> - Поддерживает любые пользовательские функции\n",
        "> - Упрощает отладку и тестирование\n",
        "\n",
        "> **Пример: ETL-конвейер через `pipe()`**\n",
        "\n",
        "```python\n",
        "def filter_high_value(df, threshold):\n",
        "    return df[df['Sales'] > threshold]\n",
        "\n",
        "def add_bonus(df, rate):\n",
        "    return df.assign(Bonus=df['Sales'] * rate)\n",
        "\n",
        "data_pipe = pd.DataFrame({\n",
        "    'Agent': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'Sales': [800, 1200, 950, 1500],\n",
        "    'Region': ['North', 'South', 'North', 'South']\n",
        "})\n",
        "\n",
        "df_transformed = (\n",
        "    data_pipe\n",
        "    .pipe(filter_high_value, threshold=1000)\n",
        "    .pipe(add_bonus, rate=0.1)\n",
        "    .sort_values('Sales', ascending=False)\n",
        ")\n",
        "\n",
        "print(\"Результат конвейера через .pipe():\\n\", df_transformed)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "    Agent  Sales Region   Bonus\n",
        "3   Diana   1500  South   150.0\n",
        "1     Bob   1200  South   120.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* `pipe()` — это **синтаксический сахар**, не дающий прироста скорости. Производительность зависит от **векторизации внутри функций**.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Группировка, агрегация и реструктуризация\n",
        "\n",
        "Эти инструменты позволяют выполнять **сводный анализ**, обогащать данные и приводить их к нужному формату для моделирования или визуализации.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.1. Принцип Split-Apply-Combine: объект `GroupBy`\n",
        "\n",
        "Pandas реализует классическую парадигму:\n",
        "\n",
        "1. **Split** — разбиение на группы по одному или нескольким ключам.\n",
        "2. **Apply** — применение функции к каждой группе (агрегация, трансформация, фильтрация).\n",
        "3. **Combine** — сбор результатов в единый объект.\n",
        "\n",
        "> **Создание группы:** `df.groupby('column')` или `df.groupby(['col1', 'col2'])`\n",
        "\n",
        "---\n",
        "\n",
        "### VI.2. Агрегация данных: `agg()`\n",
        "\n",
        "Метод `agg()` (или `aggregate()`) вычисляет сводную статистику **по группам** и **возвращает результат меньшей размерности**.\n",
        "\n",
        "> **Пример: множественная агрегация по столбцам**\n",
        "\n",
        "```python\n",
        "df_group = pd.DataFrame({\n",
        "    'Department': ['HR', 'IT', 'IT', 'Sales', 'HR', 'IT'],\n",
        "    'Salary': [45000, 70000, 80000, 75000, 50000, 70000],\n",
        "    'Experience': [2, 5, 10, 6, 3, 7]\n",
        "})\n",
        "\n",
        "summary = df_group.groupby('Department').agg({\n",
        "    'Salary': ['mean', 'median'],\n",
        "    'Experience': ['sum', 'max']\n",
        "})\n",
        "\n",
        "print(\"Множественная агрегация:\\n\", summary)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "           Salary            Experience      \n",
        "             mean   median        sum max\n",
        "Department                                 \n",
        "HR        47500.0   47500.0          5   3\n",
        "IT        73333.3   70000.0         22  10\n",
        "Sales     75000.0   75000.0          6   6\n",
        "```\n",
        "\n",
        "> *Пояснение:* Результат имеет **MultiIndex в столбцах**, что позволяет точно идентифицировать каждую агрегацию.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.3. Трансформация данных: `transform()`\n",
        "\n",
        "`transform()` применяет функцию к группе, но **возвращает объект той же формы и индекса**, что и исходный DataFrame.\n",
        "\n",
        "> **Сценарий:** добавление групповой статистики к каждой строке **без слияния**.\n",
        "\n",
        "> **Пример: нормализация внутри групп**\n",
        "\n",
        "```python\n",
        "# Z-оценка зарплаты внутри отдела\n",
        "df_group['Salary_Z'] = df_group.groupby('Department')['Salary'].transform(\n",
        "    lambda x: (x - x.mean()) / x.std(ddof=1)\n",
        ")\n",
        "\n",
        "print(\"Трансформация (Z-оценка):\\n\", df_group[['Department', 'Salary', 'Salary_Z']])\n",
        "```\n",
        "\n",
        "> *Пояснение:* `transform` — это «невидимая сила» ETL: он обогащает данные, сохраняя их структуру.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.4. Фильтрация групп: `filter()`\n",
        "\n",
        "Метод `filter()` удаляет **целые группы**, если они не удовлетворяют условию.\n",
        "\n",
        "> **Условие:** функция должна возвращать **одно булево значение** на группу.\n",
        "\n",
        "> **Пример: оставить только отделы с >2 сотрудниками**\n",
        "\n",
        "```python\n",
        "df_filtered = df_group.groupby('Department').filter(lambda x: len(x) > 2)\n",
        "print(\"После фильтрации групп:\\n\", df_filtered['Department'].unique())  # ['HR' 'IT']\n",
        "```\n",
        "\n",
        "> *Пояснение:* Отдел `Sales` (1 сотрудник) исключён.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.5. Объединение таблиц\n",
        "\n",
        "#### VI.5.1. `pd.merge()` — реляционные соединения\n",
        "\n",
        "Аналог **SQL JOIN**. Ключевые параметры:\n",
        "- `on` — столбец-ключ,\n",
        "- `how` — тип соединения (`inner`, `left`, `right`, `outer`).\n",
        "\n",
        "> **Пример: LEFT и INNER JOIN**\n",
        "\n",
        "```python\n",
        "df_left = pd.DataFrame({\n",
        "    'Key': ['K0', 'K1', 'K2', 'K3'],\n",
        "    'A': ['A0', 'A1', 'A2', 'A3']\n",
        "})\n",
        "\n",
        "df_right = pd.DataFrame({\n",
        "    'Key': ['K1', 'K3', 'K4', 'K5'],\n",
        "    'B': ['B1', 'B3', 'B4', 'B5']\n",
        "})\n",
        "\n",
        "left_join = pd.merge(df_left, df_right, on='Key', how='left')\n",
        "inner_join = pd.merge(df_left, df_right, on='Key', how='inner')\n",
        "\n",
        "print(\"LEFT JOIN:\\n\", left_join)\n",
        "print(\"\\nINNER JOIN:\\n\", inner_join)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `left` сохраняет все строки из левой таблицы, `inner` — только совпадающие.\n",
        "\n",
        "#### VI.5.2. `pd.concat()` — конкатенация\n",
        "\n",
        "«Склеивает» объекты вдоль оси:\n",
        "- `axis=0` — вертикально (добавление строк),\n",
        "- `axis=1` — горизонтально (добавление столбцов).\n",
        "\n",
        "> **Параметр `join`:**\n",
        "> - `'outer'` (по умолчанию) — объединение индексов,\n",
        "> - `'inner'` — пересечение.\n",
        "\n",
        "---\n",
        "\n",
        "### VI.6. Изменение формы данных\n",
        "\n",
        "#### Wide vs. Long Format\n",
        "\n",
        "- **Wide**: одна строка = одно наблюдение, переменные — отдельные столбцы.\n",
        "- **Long**: одна строка = одно измерение, переменные и значения — в двух столбцах.\n",
        "\n",
        "> **Long предпочтителен** для `groupby`, `seaborn`, `plotly`.\n",
        "\n",
        "#### `melt()` — Wide → Long\n",
        "\n",
        "```python\n",
        "df_wide = pd.DataFrame({\n",
        "    'ID': [1, 2],\n",
        "    'Score_Math': [90, 85],\n",
        "    'Score_Science': [88, 92]\n",
        "})\n",
        "\n",
        "df_long = df_wide.melt(\n",
        "    id_vars='ID',\n",
        "    var_name='Subject',\n",
        "    value_name='Score'\n",
        ")\n",
        "print(\"Wide → Long:\\n\", df_long)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   ID        Subject  Score\n",
        "0   1     Score_Math     90\n",
        "1   2     Score_Math     85\n",
        "2   1  Score_Science     88\n",
        "3   2  Score_Science     92\n",
        "```\n",
        "\n",
        "#### `pivot_table()` — Long → Wide\n",
        "\n",
        "```python\n",
        "df_pivot = df_long.pivot_table(\n",
        "    index='ID',\n",
        "    columns='Subject',\n",
        "    values='Score',\n",
        "    aggfunc='mean'  # обработка дубликатов\n",
        ")\n",
        "print(\"Long → Wide:\\n\", df_pivot)\n",
        "```\n",
        "\n",
        "#### `stack()` / `unstack()` — работа с MultiIndex\n",
        "\n",
        "- `unstack()`: переносит уровень индекса в столбцы.\n",
        "- `stack()`: обратная операция.\n",
        "\n",
        "> **Пример: unstack**\n",
        "\n",
        "```python\n",
        "index = pd.MultiIndex.from_tuples(\n",
        "    [('X', 'a'), ('X', 'b'), ('Y', 'a'), ('Y', 'b')],\n",
        "    names=['Level1', 'Level2']\n",
        ")\n",
        "df_multi = pd.DataFrame({'Data': [1, 2, 3, 4]}, index=index)\n",
        "\n",
        "df_unstacked = df_multi.unstack('Level2')\n",
        "print(\"Unstacked:\\n\", df_unstacked)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Pandas предоставляет мощный и гибкий инструментарий для **анализа, трансформации и реструктуризации** данных. Освоение `groupby`, `merge`, `melt`, `assign` и `pipe` позволяет строить **промышленные ETL-конвейеры**, сочетающие читаемость, производительность и надёжность.\n"
      ],
      "metadata": {
        "id": "Ui7lqQuLj8UL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## VII. Работа с временными рядами и категориальными данными\n",
        "\n",
        "Работа с **временными рядами** и оптимизация памяти через **категориальные типы** — ключевые навыки для анализа больших и структурированных данных. Pandas предоставляет специализированные инструменты, превращающие эти задачи из «проблем» в «возможности».\n",
        "\n",
        "---\n",
        "\n",
        "### VII.1. Обработка временных данных: `pd.to_datetime()`\n",
        "\n",
        "Функция `pd.to_datetime()` преобразует строки, целые числа (Unix timestamp) или другие представления в объекты типа **`datetime64[ns]`** — основу всей временной аналитики в Pandas.\n",
        "\n",
        "> **Критически важно:** при работе с большими файлами всегда указывайте **`format`** явно. Это отключает медленный механизм «угадывания» формата и ускоряет парсинг в **10–100 раз**.\n",
        "\n",
        "> **Пример: безопасная конвертация дат**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "date_strings = ['2023/10/25', '2023/10/26', '2023/10/27']\n",
        "ser_dates = pd.Series(date_strings)\n",
        "\n",
        "# Явное указание формата — быстро и надёжно\n",
        "dates_dti = pd.to_datetime(ser_dates, format='%Y/%m/%d')\n",
        "print(\"Конвертация в DatetimeIndex:\\n\", dates_dti)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без `format` Pandas пытается проанализировать каждую строку — это недопустимо при загрузке миллионов записей.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.2. Передискретизация (Resampling): `DataFrame.resample()`\n",
        "\n",
        "Метод `resample()` изменяет **частоту временного ряда**, требуя **`DatetimeIndex`** в качестве индекса.\n",
        "\n",
        "- **Downsampling** (понижение частоты): `D → M` → требует **агрегации** (`sum`, `mean`).\n",
        "- **Upsampling** (повышение частоты): `M → D` → требует **заполнения** (`fillna`, `interpolate`).\n",
        "\n",
        "> **Пример: ежедневные данные → еженедельная сумма**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Ежедневный временной ряд\n",
        "daily_index = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n",
        "ts_daily = pd.Series(np.random.randint(10, 50, size=10), index=daily_index)\n",
        "\n",
        "# Downsample: сумма за неделю\n",
        "weekly_sum = ts_daily.resample('W').sum()\n",
        "print(\"\\nЕженедельная сумма:\\n\", weekly_sum)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `resample` возвращает объект `Resampler`, к которому можно применять любые агрегационные функции.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.3. Оконные функции: `DataFrame.rolling()`\n",
        "\n",
        "Метод `rolling(window=N)` создаёт **скользящее окно** фиксированного размера для вычисления локальных статистик.\n",
        "\n",
        "> **Пример: 3-дневное скользящее среднее**\n",
        "\n",
        "```python\n",
        "values = [10, 20, 30, 40, 50]\n",
        "df_ts = pd.DataFrame({'Value': values})\n",
        "\n",
        "# Скользящее среднее (первые 2 значения — NaN)\n",
        "df_ts['Rolling_Mean'] = df_ts['Value'].rolling(window=3).mean()\n",
        "print(\"\\nСкользящее среднее:\\n\", df_ts)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "   Value  Rolling_Mean\n",
        "0     10           NaN\n",
        "1     20           NaN\n",
        "2     30          20.0\n",
        "3     40          30.0\n",
        "4     50          40.0\n",
        "```\n",
        "\n",
        "> *Пояснение:* Скользящие окна — основа технического анализа, сглаживания шума и выявления трендов.\n",
        "\n",
        "---\n",
        "\n",
        "### VII.4. Оптимизация памяти: тип данных `category`\n",
        "\n",
        "Столбцы с **низкой кардинальностью** (мало уникальных значений) — идеальные кандидаты на конвертацию в `category`.\n",
        "\n",
        "> **Как это работает?**  \n",
        "> Вместо хранения строк «HR», «IT», «HR»... Pandas хранит:\n",
        "> - словарь: `{0: 'HR', 1: 'IT'}`,\n",
        "> - массив целых: `[0, 1, 0, ...]`.\n",
        "\n",
        "> **Эффект:** сокращение памяти **в 10–20 раз**.\n",
        "\n",
        "> **Пример: сравнение потребления памяти**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Создаём 100 000 строк с 3 категориями\n",
        "np.random.seed(42)\n",
        "df_mem = pd.DataFrame({\n",
        "    'Category': np.random.choice(['HR', 'IT', 'Sales'], size=100_000),\n",
        "    'Values': np.random.randint(1, 100, size=100_000)\n",
        "})\n",
        "\n",
        "print(\"Память до (object):\")\n",
        "print(df_mem.memory_usage(deep=True) // 1024)  # в КБ\n",
        "\n",
        "# Конвертация в category\n",
        "df_mem['Category'] = df_mem['Category'].astype('category')\n",
        "\n",
        "print(\"\\nПамять после (category):\")\n",
        "print(df_mem.memory_usage(deep=True) // 1024)\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это один из самых простых и эффективных способов **масштабировать Pandas** на большие данные.\n",
        "\n",
        "---\n",
        "\n",
        "## VIII. Комплексная аналитика и оптимизация производительности\n",
        "\n",
        "Профессиональный анализ требует **интеграции всех инструментов** в единый, производительный и читаемый конвейер.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.1. Пример ETL-конвейера на основе Pandas\n",
        "\n",
        "> **Сценарий:** анализ журнала продаж — от загрузки до агрегации.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. EXTRACTION ---\n",
        "df_raw = pd.DataFrame({\n",
        "    'Order_ID': [101, 102, 103, 104, 105],\n",
        "    'Date_Str': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n",
        "    'Amount': [100.5, np.nan, 250.0, 150.0, 300.5],\n",
        "    'Client_Type': ['New', 'Old', 'New', 'Old', 'New']\n",
        "})\n",
        "\n",
        "# Преобразуем даты\n",
        "df_raw['Date'] = pd.to_datetime(df_raw['Date_Str'], format='%Y-%m-%d')\n",
        "df_raw.drop(columns=['Date_Str'], inplace=True)\n",
        "\n",
        "# --- 2. TRANSFORMATION ---\n",
        "# 2.1. Заполнение пропусков\n",
        "df_raw['Amount'].fillna(df_raw['Amount'].median(), inplace=True)\n",
        "\n",
        "# 2.2. Извлечение признаков из даты\n",
        "df_raw['DayOfWeek'] = df_raw['Date'].dt.dayofweek  # 0=Пн, 6=Вс\n",
        "\n",
        "# 2.3. Агрегация: продажи по дням\n",
        "daily_sales = df_raw.groupby('Date').agg(Total_Amount=('Amount', 'sum'))\n",
        "\n",
        "# 2.4. Обогащение: добавляем дневную сумму к каждой строке\n",
        "df_raw['Daily_Total'] = df_raw.groupby('Date')['Amount'].transform('sum')\n",
        "\n",
        "# --- 3. LOADING ---\n",
        "df_raw.to_csv('results/sales_etl_output.csv', index=False)\n",
        "print(\"ETL завершён. Первые 3 строки:\\n\", df_raw.head(3))\n",
        "```\n",
        "\n",
        "> *Пояснение:* Конвейер демонстрирует **полный цикл**: загрузка → очистка → feature engineering → агрегация → сохранение.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.2. Продвинутая разработка признаков (Feature Engineering)\n",
        "\n",
        "#### 1. Извлечение из дат\n",
        "```python\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5\n",
        "```\n",
        "\n",
        "#### 2. Бинаризация (Binning)\n",
        "```python\n",
        "bins = [0, 100, 500, 1000, np.inf]\n",
        "labels = ['Low', 'Medium', 'High', 'Very High']\n",
        "df['Amount_Category'] = pd.cut(df['Amount'], bins=bins, labels=labels)\n",
        "```\n",
        "\n",
        "#### 3. One-Hot Encoding\n",
        "```python\n",
        "df_encoded = pd.get_dummies(df, columns=['Client_Type'], prefix='Type')\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эти методы критичны для **подготовки данных к машинному обучению**.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.3. Ускорение пользовательских функций: Numba\n",
        "\n",
        "Когда векторизация невозможна, **Numba** компилирует Python-код в машинные инструкции.\n",
        "\n",
        "> **Пример: ускорение поэлементной функции**\n",
        "\n",
        "```python\n",
        "import numba\n",
        "import numpy as np\n",
        "\n",
        "@numba.vectorize\n",
        "def custom_transform(x):\n",
        "    return np.sqrt(x) if x > 0 else 0.0\n",
        "\n",
        "# Применяем к NumPy-массиву\n",
        "series = pd.Series(np.random.rand(1_000_000) * 1000)\n",
        "result = custom_transform(series.to_numpy())  # возвращает ndarray\n",
        "```\n",
        "\n",
        "> *Пояснение:* Numba работает **только с NumPy-массивами**, не с Pandas-объектами. Передавайте `.to_numpy()`.\n",
        "\n",
        "---\n",
        "\n",
        "### VIII.4. Параллельные вычисления: Dask и Swifter\n",
        "\n",
        "- **Dask**: разбивает DataFrame на **partitions**, распределяет вычисления по ядрам/кластеру. API похож на Pandas, но с отложенным выполнением.\n",
        "- **Swifter**: автоматически выбирает между **векторизацией** и **параллелизацией**:\n",
        "  ```python\n",
        "  df['new_col'] = df['col'].swifter.apply(complex_function)\n",
        "  ```\n",
        "\n",
        "> *Пояснение:* Используйте Dask/Swifter, когда данные **не помещаются в память** или когда `apply` слишком медлен.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Pandas — это методология, а не просто библиотека.** Её мощь раскрывается через **стратегический выбор инструментов**:\n",
        "\n",
        "| Уровень оптимизации             | Инструменты                                  | Цель                              |\n",
        "|-------------------------------|---------------------------------------------|-----------------------------------|\n",
        "| **I/O**                       | `dtype`, `usecols`, `parse_dates`           | Быстрая и безопасная загрузка     |\n",
        "| **Память**                    | `category`, `pd.Int64`                      | Снижение потребления RAM          |\n",
        "| **Массовые вычисления**       | `eval()`, `query()`, векторизованные UFuncs | Высокая скорость на больших данных|\n",
        "| **Пользовательская логика**   | `Numba`, `Cython`                           | Ускорение нетривиальных функций   |\n",
        "| **Архитектура кода**          | `pipe()`, `assign()`, `loc`                 | Читаемость, модульность, надёжность |\n",
        "\n",
        "Таким образом, Pandas предоставляет **полную экосистему** для построения промышленных конвейеров обработки данных — от первичной очистки до подготовки данных для машинного обучения. Освоение его принципов позволяет не просто «работать с таблицами», а **строить масштабируемые, воспроизводимые и производительные аналитические системы**.\n",
        "\n"
      ],
      "metadata": {
        "id": "2VkX1-FPkkv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# МОДУЛЬ 3: Библиотека Polars — Архитектура высокопроизводительной обработки данных\n",
        "\n",
        "**Polars** — это не просто ещё одна библиотека для работы с табличными данными, а **архитектурная эволюция** аналитических вычислений в Python. В отличие от традиционных инструментов, Polars переносит вычислительную нагрузку за пределы интерпретатора Python, опираясь на современные стандарты памяти и системное программирование. Это позволяет ему достигать **порядков прироста в скорости** и **линейного масштабирования** на многоядерных системах, особенно при работе с большими объёмами данных.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Архитектурные принципы Polars: Фундамент производительности\n",
        "\n",
        "Производительность Polars не является результатом «улучшенной обёртки» вокруг Pandas, а обусловлена **глубинной перестройкой стека**: от языка реализации до модели хранения данных.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1. Двуединый языковой стек: Rust и Apache Arrow\n",
        "\n",
        "#### 1.1.1. Ядро на Rust: скорость без компромиссов\n",
        "\n",
        "Polars полностью написан на **Rust** — языке системного программирования, сочетающем:\n",
        "\n",
        "- **Безопасность памяти** без сборщика мусора,\n",
        "- **Высокую производительность** за счёт компиляции в нативный код,\n",
        "- **Нативную многопоточность** без блокировок.\n",
        "\n",
        "Ключевое преимущество: **обход Global Interpreter Lock (GIL)** Python. В то время как Pandas-операции часто выполняются в одном потоке, ядро Polars **автоматически распределяет работу по всем ядрам CPU**, используя пулы потоков и SIMD-векторизацию. Это обеспечивает **линейное ускорение** при увеличении числа ядер — особенно при агрегациях, фильтрации и трансформациях.\n",
        "\n",
        "#### 1.1.2. Apache Arrow: колоночная память как стандарт\n",
        "\n",
        "Внутреннее представление данных в Polars строится на **Apache Arrow** — отраслевом стандарте для **колоночного хранения данных в оперативной памяти**.\n",
        "\n",
        "> **Почему это важно?**  \n",
        "> Большинство аналитических операций (например, `groupby`, `filter`, `sum`) работают **со столбцами**, а не со строками. Колоночный формат:\n",
        "> - минимизирует промахи кэша CPU,\n",
        "> - позволяет загружать в процессор только нужные данные,\n",
        "> - исключает накладные расходы на упаковку/распаковку объектов.\n",
        "\n",
        "В отличие от Pandas (где `object`-столбцы хранят ссылки на Python-объекты), Arrow хранит **непрерывные блоки однотипных данных**, что делает доступ к ним чрезвычайно быстрым.\n",
        "\n",
        "> **Zero-Copy Interoperability**  \n",
        "> Поскольку Polars строго следует спецификации Arrow, он может **обмениваться данными без копирования** с другими Arrow-совместимыми системами (Apache Spark, DuckDB, PyArrow, Vaex), просто передавая указатели на буферы памяти.\n",
        "\n",
        "> **Важно:** Polars использует **собственную реализацию буферов и вычислений на Rust**, а не обёртку вокруг PyArrow. Это даёт полный контроль над оптимизациями и избегает внешних зависимостей.\n",
        "\n",
        "---\n",
        "\n",
        "### Сравнение архитектур: Polars vs Pandas\n",
        "\n",
        "| Критерий                     | Polars                                      | Pandas (стандартный стек)                |\n",
        "|-----------------------------|---------------------------------------------|------------------------------------------|\n",
        "| **Язык ядра**               | Rust (компилируемый, безопасный)            | Python / C (NumPy)                       |\n",
        "| **Модель памяти**           | Apache Arrow (колоночная, непрерывная)      | NumPy (часто строковая или блочная)      |\n",
        "| **Параллелизм**             | Нативная многопоточность + SIMD             | Преимущественно однопоточный (GIL)       |\n",
        "| **Модель выполнения**       | Eager **и** Lazy (с оптимизатором запросов) | Только Eager                             |\n",
        "| **Типизация**               | Строгая (тип выводится из выражений)        | Гибкая (неявные приведения, `object`)    |\n",
        "| **Стратегия копирования**   | Минимизация (Zero-Copy при совместимости)   | Частые копии (особенно при `copy()` и `fillna`) |\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Параллелизм и распределение нагрузки\n",
        "\n",
        "#### 1.2.1. «Embarrassingly Parallel» по дизайну\n",
        "\n",
        "Polars изначально спроектирован как **лёгкораспараллеливаемая система** (*Embarrassingly Parallel*). Это означает:\n",
        "\n",
        "- Рабочая нагрузка **автоматически делится** между всеми доступными ядрами.\n",
        "- Независимые выражения (например, два новых столбца в `select`) вычисляются **параллельно**.\n",
        "- При `group_by().agg()` каждая группа может обрабатываться **отдельным потоком**.\n",
        "\n",
        "> **Пример:**  \n",
        "> Запрос вида  \n",
        "> ```python\n",
        "> df.select([\n",
        ">     pl.col(\"A\").mean(),\n",
        ">     pl.col(\"B\").std()\n",
        "> ])\n",
        "> ```  \n",
        "> будет выполнен в **двух потоках одновременно**, без участия пользователя.\n",
        "\n",
        "#### 1.2.2. Совместимость с Python multiprocessing\n",
        "\n",
        "Внутренняя многопоточность на Rust накладывает **ограничения на использование `multiprocessing` в Python**:\n",
        "\n",
        "- В Unix-системах метод `fork` (по умолчанию) **копирует состояние всех потоков Rust**, что может привести к **нестабильности**.\n",
        "- **Рекомендация:** при использовании `multiprocessing` всегда устанавливайте контекст `spawn` или `forkserver`:\n",
        "  ```python\n",
        "  import multiprocessing as mp\n",
        "  if __name__ == \"__main__\":\n",
        "      mp.set_start_method(\"spawn\")  # или \"forkserver\"\n",
        "      # ... запуск процессов\n",
        "  ```\n",
        "\n",
        "> *Пояснение:* Это не недостаток, а признак того, что Polars — **независимый вычислительный движок**, а не «тонкая обёртка».\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Модели выполнения запросов: Eager vs Lazy\n",
        "\n",
        "Polars поддерживает два режима работы: **Eager** (немедленное выполнение) и **Lazy** (отложенное с оптимизацией). **Lazy API — предпочтительный подход** для производительного анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1. Концепции Eager и Lazy\n",
        "\n",
        "#### 2.1.1. Eager API — как в Pandas\n",
        "\n",
        "Каждая операция выполняется **сразу**, результат возвращается как `DataFrame`.\n",
        "\n",
        "```python\n",
        "import polars as pl\n",
        "\n",
        "# Eager: данные загружаются немедленно\n",
        "df = pl.read_csv(\"data.csv\")\n",
        "filtered = df.filter(pl.col(\"value\") > 100)\n",
        "```\n",
        "\n",
        "> **Плюсы:** простота, интерактивность.  \n",
        "> **Минусы:** нет глобальной оптимизации, возможны избыточные вычисления.\n",
        "\n",
        "#### 2.1.2. Lazy API — сила оптимизатора\n",
        "\n",
        "Операции строят **логический план запроса** (`LogicalPlan`), но **не выполняются** до вызова `.collect()`.\n",
        "\n",
        "```python\n",
        "# Lazy: создаётся план, данные не загружаются\n",
        "lf = pl.scan_csv(\"large_data.csv\")\n",
        "result = (\n",
        "    lf\n",
        "    .filter(pl.col(\"value\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"price\").mean())\n",
        "    .collect()  # ← запуск выполнения\n",
        ")\n",
        "```\n",
        "\n",
        "> **Преимущество:** движок видит **весь запрос целиком** и может его **оптимизировать глобально**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Оптимизатор запросов Polars\n",
        "\n",
        "Оптимизатор преобразует `LogicalPlan` в **физический план выполнения**, применяя мощные правила:\n",
        "\n",
        "#### Predicate Pushdown (проталкивание фильтров)\n",
        "\n",
        "Фильтры применяются **на этапе чтения данных**, а не после загрузки.  \n",
        "**Результат:** чтение **только релевантных строк** из файла → меньше I/O, меньше памяти.\n",
        "\n",
        "> **До оптимизации:**  \n",
        "> `read → group_by → filter`  \n",
        "> **После:**  \n",
        "> `read + filter → group_by`\n",
        "\n",
        "#### Projection Pushdown (проталкивание проекций)\n",
        "\n",
        "Загружаются **только нужные столбцы**.  \n",
        "Если в запросе используются 3 из 50 столбцов — читаются только эти 3.\n",
        "\n",
        "#### Другие ключевые оптимизации\n",
        "\n",
        "- **Join Ordering** — перестановка соединений для минимизации промежуточных размеров и предотвращения OOM.\n",
        "- **Common Subplan Elimination** — кэширование повторяющихся подзапросов.\n",
        "- **Expression Simplification** — свёртка констант, упрощение логики.\n",
        "- **Type Coercion** — приведение типов к минимально достаточным (например, `Int32` вместо `Int64`).\n",
        "\n",
        "> **Практическое значение:**  \n",
        "> Пользователю **не нужно** вручную оптимизировать порядок операций (например, «фильтровать до сортировки»). Polars делает это **автоматически**.\n",
        "\n",
        "---\n",
        "\n",
        "### Ключевые оптимизации Polars Query Optimizer\n",
        "\n",
        "| Оптимизация                  | Принцип работы                                      | Влияние                              |\n",
        "|-----------------------------|-----------------------------------------------------|--------------------------------------|\n",
        "| **Predicate Pushdown**      | Фильтрация на уровне источника данных               | ↓ I/O, ↓ RAM, ↑ скорость             |\n",
        "| **Projection Pushdown**     | Загрузка только используемых столбцов               | ↓ Потребление памяти                 |\n",
        "| **Join Ordering**           | Выбор порядка JOIN для минимизации промежуточных данных | ↓ Риск OOM, ↑ стабильность         |\n",
        "| **Common Subplan Elimination** | Повторное использование вычисленных подвыражений  | ↓ Избыточные вычисления              |\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение раздела:**  \n",
        "> Polars переосмысливает обработку данных, заменяя «интерпретируемую» модель Pandas на **компилируемый, колоночный, многопоточный движок**. Его архитектура — ответ на вызовы современной аналитики: **скорость**, **масштабируемость** и **эффективность памяти**. Освоение Lazy API и понимание оптимизаций — ключ к раскрытию всего потенциала библиотеки.\n",
        "\n"
      ],
      "metadata": {
        "id": "jG65vH4wlAg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Система выражений (DSL) Polars\n",
        "\n",
        "Сердце Polars — это **декларативный предметно-ориентированный язык (DSL)** на основе объектов типа `pl.Expr`. Выражения не просто трансформируют данные — они описывают **логический план вычислений**, который движок Polars анализирует, оптимизирует и выполняет параллельно в Rust.\n",
        "\n",
        "### 3.1. Выражения как строительные блоки\n",
        "\n",
        "Каждое выражение:\n",
        "- принимает **столбец (`Series`)** на входе,\n",
        "- возвращает **новый столбец** на выходе,\n",
        "- является **компонуемым**: можно строить цепочки без промежуточных копий.\n",
        "\n",
        "> **Пример: цепочка трансформаций**\n",
        "\n",
        "```python\n",
        "import polars as pl\n",
        "\n",
        "expr = (\n",
        "    pl.col(\"Revenue\")\n",
        "    .mul(100)                  # Умножить на 100\n",
        "    .log()                     # Натуральный логарифм\n",
        "    .alias(\"Scaled_Log_Revenue\")  # Переименовать\n",
        ")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Эта конструкция **не вычисляется сразу**. Она становится частью логического плана, который будет оптимизирован и выполнен позже.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Сложные агрегации в `group_by().agg()`\n",
        "\n",
        "Выражения позволяют встраивать **фильтрацию, сортировку и даже подзапросы** непосредственно в агрегацию — без пользовательских функций (UDF).\n",
        "\n",
        "> **Пример: условная агрегация по группе**\n",
        "\n",
        "```python\n",
        "df = pl.DataFrame({\n",
        "    \"ID\": [1, 1, 2, 2],\n",
        "    \"Val\": [10, 15, 18, 20],\n",
        "    \"Flag\": [5, 12, 10, 18]\n",
        "})\n",
        "\n",
        "result = df.group_by(\"ID\").agg(\n",
        "    pl.col(\"Val\")\n",
        "    .filter(pl.col(\"Flag\") > pl.mean(\"Flag\"))  # Только где Flag > среднего в группе\n",
        "    .max()\n",
        "    .alias(\"Conditional_Max\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (2, 2)\n",
        "┌─────┬─────────────────┐\n",
        "│ ID  ┆ Conditional_Max │\n",
        "│ --- ┆ ---             │\n",
        "│ i64 ┆ i64             │\n",
        "╞═════╪═════════════════╡\n",
        "│ 1   ┆ 15              │\n",
        "│ 2   ┆ 20              │\n",
        "└─────┴─────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* В первой группе среднее `Flag = 8.5`. Только вторая строка (`Flag=12`) удовлетворяет условию, поэтому максимум `Val = 15`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Условная логика: `pl.when().then().otherwise()`\n",
        "\n",
        "Это **SIMD-оптимизированная**, безветвёвая реализация условий — аналог `CASE WHEN` в SQL или `np.where` в NumPy, но **значительно быстрее**.\n",
        "\n",
        "> **Пример: условное среднее**\n",
        "\n",
        "```python\n",
        "df_cond = pl.DataFrame({\n",
        "    \"age\": [25, 35, 45, 55],\n",
        "    \"height\": [170, 175, 180, 165]\n",
        "})\n",
        "\n",
        "cutoff = 30\n",
        "result = df_cond.select(\n",
        "    pl.when(pl.col(\"age\") < cutoff)\n",
        "    .then(pl.lit(1.0))\n",
        "    .otherwise(pl.col(\"height\"))\n",
        "    .mean()\n",
        "    .alias(\"Whenthen_Mean\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (1, 1)\n",
        "┌────────────────┐\n",
        "│ Whenthen_Mean  │\n",
        "│ ---            │\n",
        "│ f64            │\n",
        "╞════════════════╡\n",
        "│ 173.333333     │  # (1 + 175 + 180 + 165) / 4\n",
        "└────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Благодаря **branchless-коду** и **SIMD**, эта операция выполняется на порядки быстрее, чем аналог с циклами или медленными UDF.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Продвинутые методы обработки данных\n",
        "\n",
        "### 4.1. Оконные функции: `.over()`\n",
        "\n",
        "Метод `over()` реализует **оконные функции**, аналогичные `OVER (PARTITION BY ...)` в SQL. Он позволяет вычислять агрегаты **внутри групп**, не сворачивая DataFrame.\n",
        "\n",
        "> **Пример: скользящее среднее по региону**\n",
        "\n",
        "```python\n",
        "data = pl.DataFrame({\n",
        "    \"region\": [\"North\", \"North\", \"South\", \"South\", \"South\"],\n",
        "    \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
        "    \"sales\": [100, 150, 200, 250, 300]\n",
        "}).with_columns(\n",
        "    pl.col(\"date\").str.to_date(\"%Y-%m-%d\")  # Преобразуем в Date\n",
        ").sort([\"region\", \"date\"])\n",
        "\n",
        "# Добавляем среднее по региону ко всем строкам\n",
        "result = data.with_columns(\n",
        "    pl.col(\"sales\").mean().over(\"region\").alias(\"Avg_Sales_By_Region\")\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (5, 4)\n",
        "┌────────┬────────────┬───────┬─────────────────────────┐\n",
        "│ region ┆ date       ┆ sales ┆ Avg_Sales_By_Region     │\n",
        "│ ---    ┆ ---        ┆ ---   ┆ ---                     │\n",
        "│ str    ┆ date       ┆ i64   ┆ f64                     │\n",
        "╞════════╪════════════╪═══════╪═════════════════════════╡\n",
        "│ North  ┆ 2023-01-01 ┆ 100   ┆ 125.0                   │\n",
        "│ North  ┆ 2023-01-02 ┆ 150   ┆ 125.0                   │\n",
        "│ South  ┆ 2023-01-01 ┆ 200   ┆ 250.0                   │\n",
        "│ South  ┆ 2023-01-02 ┆ 250   ┆ 250.0                   │\n",
        "│ South  ┆ 2023-01-03 ┆ 300   ┆ 250.0                   │\n",
        "└────────┴────────────┴───────┴─────────────────────────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Каждая строка «видит» среднее своей группы, но сохраняет свою позицию.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Обработка временных рядов: Asof Join\n",
        "\n",
        "`join_asof` — **специализированное соединение для временных рядов**, где точные совпадения времени не требуются.\n",
        "\n",
        "> **Параметры:**\n",
        "> - `strategy`: `'backward'` (по умолчанию), `'forward'`, `'nearest'`,\n",
        "> - `tolerance`: максимальное допустимое отклонение (например, `'1h'`, `'5min'`).\n",
        "\n",
        "> **Пример: присоединение курсов к транзакциям**\n",
        "\n",
        "```python\n",
        "df_transactions = pl.DataFrame({\n",
        "    \"tx_time\": [\n",
        "        pl.datetime(2023, 1, 1, 10, 0),\n",
        "        pl.datetime(2023, 1, 1, 10, 30)\n",
        "    ],\n",
        "    \"amount\": [1000, 1500]\n",
        "})\n",
        "\n",
        "df_fx_rates = pl.DataFrame({\n",
        "    \"fx_time\": [\n",
        "        pl.datetime(2023, 1, 1, 9, 50),   # Ближайший до 10:00\n",
        "        pl.datetime(2023, 1, 1, 10, 15)   # Ближайший до 10:30\n",
        "    ],\n",
        "    \"rate\": [1.1, 1.2]\n",
        "})\n",
        "\n",
        "result = df_transactions.join_asof(\n",
        "    df_fx_rates,\n",
        "    left_on=\"tx_time\",\n",
        "    right_on=\"fx_time\",\n",
        "    strategy=\"backward\",\n",
        "    tolerance=\"1h\"\n",
        ")\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "shape: (2, 3)\n",
        "┌─────────────────────┬────────┬──────┐\n",
        "│ tx_time             ┆ amount ┆ rate │\n",
        "│ ---                 ┆ ---    ┆ ---  │\n",
        "│ datetime[μs]        ┆ i64    ┆ f64  │\n",
        "╞═════════════════════╪════════╪══════╡\n",
        "│ 2023-01-01 10:00:00 ┆ 1000   ┆ 1.1  │\n",
        "│ 2023-01-01 10:30:00 ┆ 1500   ┆ 1.2  │\n",
        "└─────────────────────┴────────┴──────┘\n",
        "```\n",
        "\n",
        "> *Пояснение:* Это стандартный паттерн в финтехе, IoT и лог-аналитике.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Производительность, память и интеграция\n",
        "\n",
        "### 5.1. Оптимизация типов данных\n",
        "\n",
        "Полный контроль над типами — ключ к эффективному использованию памяти.\n",
        "\n",
        "| Исходный тип               | Рекомендуемый тип Polars      | Эффект                          |\n",
        "|---------------------------|-------------------------------|----------------------------------|\n",
        "| Строки с низкой кардинальностью | `pl.Categorical` или `pl.Enum` | ↓ память в 10–100×, ↑ скорость сравнения |\n",
        "| `float64`                 | `pl.Float32`                  | ↓ память в 2×                   |\n",
        "| `int64` (малый диапазон)  | `pl.Int32`, `pl.Int16`        | ↓ память, ↑ кэш-эффективность   |\n",
        "\n",
        "> **Пример: загрузка с оптимизацией типов**\n",
        "\n",
        "```python\n",
        "df = pl.read_csv(\n",
        "    \"large_file.csv\",\n",
        "    dtypes={\n",
        "        \"user_id\": pl.Int32,\n",
        "        \"category\": pl.Categorical,\n",
        "        \"price\": pl.Float32\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Streaming API: обработка данных «вне памяти»\n",
        "\n",
        "Для датасетов **больше RAM** используйте **Streaming API**:\n",
        "\n",
        "```python\n",
        "# LazyFrame\n",
        "lf = pl.scan_csv(\"huge_file.csv\")\n",
        "\n",
        "# Обработка потоками\n",
        "result = (\n",
        "    lf\n",
        "    .filter(pl.col(\"value\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"price\").mean())\n",
        "    .collect(streaming=True)  # ← ключевой параметр\n",
        ")\n",
        "```\n",
        "\n",
        "> **Как это работает?**  \n",
        "> Polars разбивает запрос на этапы и обрабатывает данные **пакетами**, никогда не загружая всё в память. Если операция не поддерживает streaming (например, глобальная сортировка), Polars автоматически откатывается к in-memory режиму.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Интеграция с ML: `to_numpy()` и Zero-Copy\n",
        "\n",
        "Для передачи данных в `scikit-learn` или `PyTorch`:\n",
        "\n",
        "```python\n",
        "X = df.select(pl.col([\"feature1\", \"feature2\"])).to_numpy()\n",
        "```\n",
        "\n",
        "> **Zero-Copy достигается, если:**\n",
        "> - все столбцы — одного числового типа (`Float32` или `Int64`),\n",
        "> - нет пропущенных значений (`null`),\n",
        "> - данные хранятся в одном блоке (chunk),\n",
        "> - порядок памяти — колоночный (Fortran-style, по умолчанию в Polars).\n",
        "\n",
        "Если условия не выполнены, Polars **автоматически выполнит копирование и приведение типов** — но это будет чётко и безопасно.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Сравнение производительности: Polars vs Pandas\n",
        "\n",
        "Независимые бенчмарки (включая [**db-benchmark**](https://h2oai.github.io/db-benchmark/)) демонстрируют:\n",
        "\n",
        "| Операция                | Преимущество Polars       | Типичный прирост скорости |\n",
        "|------------------------|----------------------------|----------------------------|\n",
        "| Чтение + фильтрация    | Predicate Pushdown + параллелизм | **3–4×**                  |\n",
        "| `group_by().agg()`     | Параллельная агрегация по группам | **4–5×**                  |\n",
        "| `join`                 | Join Ordering + Arrow      | **до 14×**                |\n",
        "| Условная логика        | SIMD + branchless          | **2–10×** (в зависимости от сложности) |\n",
        "\n",
        "> **Пояснение:** Выигрыш особенно заметен **на данных > 1 млн строк**, где накладные расходы Pandas (GIL, копирование, отсутствие глобальной оптимизации) становятся критичными.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Polars — это архитектурный прорыв** в обработке табличных данных. Он не пытается «ускорить Pandas», а предлагает **новую парадигму**:\n",
        "\n",
        "- **Вычисления** — в компилируемом, многопоточном ядре на **Rust**,\n",
        "- **Память** — в эффективном колоночном формате **Apache Arrow**,\n",
        "- **Оптимизация** — через **Lazy API** и **логический план запроса**,\n",
        "- **Выразительность** — через **DSL на основе выражений**.\n",
        "\n",
        "Эти принципы позволяют Polars:\n",
        "- обрабатывать **миллионы и миллиарды строк** на одном компьютере,\n",
        "- выполнять **сложные аналитические запросы** без написания UDF,\n",
        "- масштабироваться **линейно с числом ядер**,\n",
        "- интегрироваться с **экосистемой Arrow** без копирования данных.\n",
        "\n",
        "Таким образом, Polars не просто альтернатива Pandas — это **следующее поколение фреймворка для аналитики данных**, объединяющее скорость системного программирования, выразительность DSL и удобство Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "TY2jtxOpmstR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 4. Dask: Архитектура, методология и масштабирование вычислений в экосистеме Python\n",
        "\n",
        "## Введение: Масштабирование PyData и архитектурный вызов\n",
        "\n",
        "Экосистема Scientific Python — с её столпами **NumPy** и **Pandas** — давно стала де-факто стандартом для анализа данных и научных вычислений. Эти библиотеки достигают высокой производительности за счёт **векторизованных операций**, реализованных на C/C++, и оптимизированных под работу **в оперативной памяти (in-memory)**.\n",
        "\n",
        "Однако на практике данные часто:\n",
        "- **превышают объём RAM** (out-of-core),\n",
        "- требуют **интенсивных CPU-вычислений**, которые не могут быть ускорены одним ядром.\n",
        "\n",
        "В этих сценариях традиционный стек PyData сталкивается с фундаментальными ограничениями — в первую очередь, из-за **Global Interpreter Lock (GIL)** в CPython, который блокирует истинный параллелизм на уровне потоков.\n",
        "\n",
        "**Dask** был создан как гибкая платформа параллельных вычислений, которая **расширяет**, а не заменяет, экосистему PyData. Он состоит из двух компонентов:\n",
        "\n",
        "1. **Низкоуровневого планировщика задач**, управляющего исполнением графа вычислений,\n",
        "2. **Высокоуровневых коллекций** (`Dask Array`, `Dask DataFrame`, `Dask Bag`), которые имитируют интерфейсы NumPy, Pandas и итераторов Python.\n",
        "\n",
        "Ключевое преимущество Dask — **масштабируемость вниз и вверх**:\n",
        "- **Вниз**: запуск на ноутбуке для обработки 100 ГБ данных с диска,\n",
        "- **Вверх**: распределённый кластер с тысячами ядер для обработки петабайтов.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Фундаментальные принципы архитектуры Dask\n",
        "\n",
        "### 1.1. Ленивые вычисления (Lazy Evaluation)\n",
        "\n",
        "Все операции в Dask **ленивы**: вызов метода не выполняет расчёты, а лишь **строит граф задач**. Фактическое выполнение запускается только при вызове терминальных методов:\n",
        "\n",
        "- `.compute()` — возвращает итоговый результат в памяти (например, как `pandas.DataFrame`),\n",
        "- `.persist()` — сохраняет промежуточные результаты в распределённой памяти (полезно для интерактивных сессий).\n",
        "\n",
        "Этот подход позволяет Dask **анализировать весь план вычислений целиком** и применять оптимизации: удалять избыточные операции, минимизировать передачу данных, выбирать порядок выполнения.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Графы задач (Task Graphs)\n",
        "\n",
        "Все алгоритмы в Dask кодируются как **ориентированные ациклические графы (DAG)**:\n",
        "\n",
        "- **Узлы** — функции или операции,\n",
        "- **Рёбра** — зависимости между результатами.\n",
        "\n",
        "Этот граф служит **универсальным промежуточным представлением (IR)** для всех коллекций.\n",
        "\n",
        "> **Пример: построение графа с `dask.delayed`**\n",
        "\n",
        "```python\n",
        "import dask\n",
        "\n",
        "@dask.delayed\n",
        "def calculate_mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "# Создаём отложенные объекты\n",
        "data1 = [1, 2, 3, 4]\n",
        "data2 = [10, 20, 30]\n",
        "\n",
        "mean1 = calculate_mean(data1)\n",
        "mean2 = calculate_mean(data2)\n",
        "\n",
        "# Складываем результаты\n",
        "final_sum = dask.delayed(lambda x, y: x + y)(mean1, mean2)\n",
        "\n",
        "# Вычисление запускается здесь\n",
        "result = final_sum.compute()\n",
        "print(\"Результат:\", result)  # 20.5\n",
        "```\n",
        "\n",
        "> *Пояснение:* Каждый вызов отложенной функции добавляет узел в граф. Dask выполняет их **параллельно**, если зависимости позволяют.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. Динамический планировщик (Scheduler)\n",
        "\n",
        "Планировщик — «мозг» Dask. Он:\n",
        "- распределяет задачи по исполнителям (workers),\n",
        "- управляет зависимостями,\n",
        "- оптимизирует **локальность данных** (стремится выполнять задачи там, где уже находятся входные данные).\n",
        "\n",
        "#### Типы планировщиков:\n",
        "\n",
        "| Тип | Описание | Использование |\n",
        "|-----|----------|---------------|\n",
        "| **Single-machine** | Локальный пул потоков/процессов | Быстрый старт, небольшие данные |\n",
        "| **Distributed** | Полноценный кластер (даже на одной машине через `LocalCluster`) | Масштабирование, дашборд, отказоустойчивость |\n",
        "\n",
        "> **Дашборд (Dashboard)** — одно из главных преимуществ распределённого планировщика: визуализация графа, загрузки CPU, объёма памяти, сетевого трафика в реальном времени.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.4. Накладные расходы и гранулярность задач\n",
        "\n",
        "Dask спроектирован с минимальными накладными расходами (~1 мс на задачу), но **это не бесплатно**. Если задача выполняется < 100 мс, накладные расходы на планирование и передачу данных могут **перевесить выгоду от параллелизма**.\n",
        "\n",
        "> **Рекомендация:**  \n",
        "> Размер чанка (chunk/partition) должен быть таким, чтобы **время выполнения одной задачи ≥ 100 мс**.\n",
        "\n",
        "Неправильный выбор гранулярности — частая ошибка новичков, приводящая к **деградации производительности** по сравнению с Pandas.\n",
        "\n",
        "---\n",
        "\n",
        "### Сравнение высокоуровневых коллекций Dask\n",
        "\n",
        "| Коллекция | Основа | Принцип | Использование |\n",
        "|----------|--------|--------|----------------|\n",
        "| **Dask Array** | `numpy.ndarray` | Блочная (chunked) структура | Научные расчёты, многомерные данные, линейная алгебра |\n",
        "| **Dask DataFrame** | `pandas.DataFrame` | Разбиение по строкам (partitioning) | ETL, агрегация, обработка больших таблиц |\n",
        "| **Dask Bag** | Python-итераторы | Параллельные коллекции объектов | Логи, JSON, неструктурированные данные |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dask DataFrame: параллельная обработка табличных данных\n",
        "\n",
        "### 2.1. Архитектура и секционирование\n",
        "\n",
        "`Dask DataFrame` — это **коллекция обычных `pandas.DataFrame`**, разбитых по строкам. Каждый «кусок» (partition) обрабатывается независимо.\n",
        "\n",
        "- **Преимущество**: может обрабатывать **100 ГБ на ноутбуке** или **100 ТБ на кластере**.\n",
        "- **Ограничение**: операции, требующие **перетасовки (shuffle)**, становятся медленными из-за межпроцессной коммуникации.\n",
        "\n",
        "> **Совет:** если вы часто фильтруете или группируете по определённому столбцу, **разбивайте данные по этому столбцу заранее** (например, при сохранении в Parquet).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Совместимость с Pandas и ленивое исполнение\n",
        "\n",
        "API `Dask DataFrame` **почти идентичен Pandas**:\n",
        "\n",
        "```python\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Ленивая загрузка (данные не читаются!)\n",
        "df = dd.read_parquet(\"data/*.parquet\")\n",
        "\n",
        "# Ленивые трансформации\n",
        "filtered = df[df.value > 0]\n",
        "aggregated = filtered.groupby(\"category\").value.mean()\n",
        "\n",
        "# Фактическое вычисление\n",
        "result = aggregated.compute()  # → pandas.Series\n",
        "```\n",
        "\n",
        "> **Важно:** `compute()` возвращает **обычный Pandas-объект**. Если результат не помещается в память — используйте `.to_parquet()` или другие методы сохранения без материализации.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. Производительность: что работает быстро, а что — нет\n",
        "\n",
        "- ✅ **Быстро**: `groupby().sum()`, `groupby().mean()` — **декомпозируемые агрегации** (MapReduce).\n",
        "- ❌ **Медленно**: `groupby().apply(custom_func)` — требует **shuffle**, так как все строки одной группы должны быть на одном worker'е.\n",
        "\n",
        "> **Пример: избегайте `apply`, если можно**\n",
        "\n",
        "```python\n",
        "# ПЛОХО: медленно из-за shuffle\n",
        "df.groupby(\"user\").apply(lambda x: fit_model(x))\n",
        "\n",
        "# ХОРОШО: используйте встроенные агрегаты или перепишите логику через map_partitions\n",
        "```\n",
        "\n",
        "> *Пояснение:* Dask не может оптимизировать произвольные функции. Старайтесь оставаться в рамках векторизованных операций.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Dask Array: масштабирование многомерных массивов\n",
        "\n",
        "### 3.1. Архитектура и чанкинг\n",
        "\n",
        "`Dask Array` — это блочная структура поверх `numpy.ndarray`. Данные разбиваются на **чанки** — небольшие NumPy-массивы, которые помещаются в память одного worker'а.\n",
        "\n",
        "> **Как выбрать размер чанка?**\n",
        "> - Слишком мал → много накладных расходов,\n",
        "> - Слишком велик → не помещается в память,\n",
        "> - **Идея**: 10–100 МБ на чанк, время выполнения ≥ 100 мс.\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "\n",
        "# Создаём массив 10 000×10 000, разбитый на чанки 1000×1000\n",
        "x = da.random.random((10_000, 10_000), chunks=(1000, 1000))\n",
        "y = x + x.T  # Ленивые операции\n",
        "result = y.sum().compute()  # Фактическое вычисление\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Совместимость с NumPy\n",
        "\n",
        "Dask Array поддерживает **большую часть API NumPy**:\n",
        "- Универсальные функции (`sin`, `log`, `+`, `*`) — применяются к каждому чанку,\n",
        "- Редукции (`sum`, `mean`) — координируются между чанками,\n",
        "- Линейная алгебра (`dot`, `svd`) — реализована с учётом распределённости.\n",
        "\n",
        "> **Важно:** не все функции NumPy доступны. Если операция требует глобального контекста (например, `np.argsort`), Dask либо выдаст ошибку, либо предложит альтернативу.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Пользовательские функции: `map_blocks`\n",
        "\n",
        "Для внедрения собственной логики используется `map_blocks`:\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "import numpy as np\n",
        "\n",
        "x = da.arange(1000, chunks=100)\n",
        "\n",
        "def block_max(block):\n",
        "    return np.array([block.max()])  # 100 элементов → 1\n",
        "\n",
        "# Указываем форму выходного чанка\n",
        "result = x.map_blocks(block_max, chunks=(1,), dtype=x.dtype)\n",
        "\n",
        "print(result.compute())  # [99, 199, 299, ..., 999]\n",
        "```\n",
        "\n",
        "> *Пояснение:* `map_blocks` — точка расширения для высокопроизводительных кернелов (Numba, Cython), которые можно масштабировать на весь массив.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Dask Bag: параллелизм для неструктурированных данных\n",
        "\n",
        "### 4.1. Архитектура и использование\n",
        "\n",
        "`Dask Bag` — коллекция **произвольных Python-объектов** (словари, строки, JSON). Используется на ранних этапах ETL:\n",
        "\n",
        "```python\n",
        "import dask.bag as db\n",
        "import json\n",
        "\n",
        "# Чтение и параллельная обработка логов\n",
        "bag = db.read_text(\"logs/*.json.gz\").map(json.loads)\n",
        "\n",
        "# Пайплайн: фильтрация → извлечение → агрегация\n",
        "top_jobs = (\n",
        "    bag\n",
        "    .filter(lambda r: r.get(\"age\", 0) > 30)\n",
        "    .map(lambda r: r[\"job\"])\n",
        "    .frequencies()\n",
        "    .topk(5, key=lambda x: x[1])\n",
        "    .compute()\n",
        ")\n",
        "```\n",
        "\n",
        "> **Преимущество:** гибкость для «грязных» данных без схемы.\n",
        "\n",
        "> **Недостаток:** высокие накладные расходы на **сериализацию** (каждый объект pickle'ится при передаче между процессами).\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Методология: Bag — только на входе\n",
        "\n",
        "**Рекомендация:** используйте `Dask Bag` **только для первоначальной очистки и парсинга**. Как только данные становятся структурированными — **конвертируйте в `Dask DataFrame`**:\n",
        "\n",
        "```python\n",
        "# После парсинга JSON\n",
        "df = bag.to_dataframe()  # или bag.to_delayed() → обработка → dd.from_delayed()\n",
        "```\n",
        "\n",
        "Так вы получите преимущества **типизированной памяти**, **векторизации** и **оптимизированного планировщика**.\n",
        "\n"
      ],
      "metadata": {
        "id": "XExFt3xhmv9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Запуск и управление вычислениями: планирование и диагностика\n",
        "\n",
        "### 5.1. Модель «Клиент–Планировщик–Работник»\n",
        "\n",
        "Распределённый планировщик Dask (`Dask Distributed`) использует классическую трёхзвенную архитектуру, применимую как на локальной машине, так и в кластере:\n",
        "\n",
        "1. **Клиент (Client)** — интерфейс пользователя. Отправляет граф задач планировщику и получает результаты.\n",
        "2. **Планировщик (Scheduler)** — центральный узел. Управляет зависимостями, распределяет задачи, отслеживает состояние работников и локальность данных.\n",
        "3. **Работник (Worker)** — исполнитель. Выполняет задачи, хранит промежуточные результаты в памяти и может обмениваться данными с другими работниками напрямую (по указанию планировщика), минимизируя задержки.\n",
        "\n",
        "Эта архитектура обеспечивает **масштабируемость**, **отказоустойчивость** и **гибкое управление ресурсами**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Локальный запуск с `LocalCluster`\n",
        "\n",
        "Даже при работе на одном компьютере рекомендуется использовать **распределённый режим** через `LocalCluster` — он предоставляет доступ к **асинхронному API** и, что особенно важно, к **диагностическому дашборду**.\n",
        "\n",
        "> **Пример: инициализация локального кластера**\n",
        "\n",
        "```python\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "# Запускает планировщик и несколько worker-процессов\n",
        "cluster = LocalCluster(\n",
        "    n_workers=4,        # количество процессов\n",
        "    threads_per_worker=2,\n",
        "    memory_limit=\"2GB\"  # лимит памяти на worker\n",
        ")\n",
        "\n",
        "# Подключаем клиент\n",
        "client = Client(cluster)\n",
        "\n",
        "# URL дашборда выводится автоматически (например, http://127.0.0.1:8787)\n",
        "print(\"Дашборд:\", client.dashboard_link)\n",
        "```\n",
        "\n",
        "> *Пояснение:* `LocalCluster` использует **процессы** (а не потоки), чтобы обойти GIL и обеспечить истинный параллелизм даже на одном ядре.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Интерактивная диагностика: Dask Dashboard\n",
        "\n",
        "Дашборд — **главный инструмент профилирования** в Dask. Он построен на Bokeh и предоставляет в реальном времени:\n",
        "\n",
        "#### **Task Stream**\n",
        "- Каждый прямоугольник — задача на одном потоке.\n",
        "- **Цвета** — тип операции (`read-parquet`, `groupby-sum` и т.д.).\n",
        "- **Красные полосы** — передача данных между работниками (**shuffle**). Много красного = проблема с чанкингом или избыточной коммуникацией.\n",
        "- **Белые промежутки** — простой потока = несбалансированная нагрузка или блокировки.\n",
        "\n",
        "#### **Memory Usage**\n",
        "- **Синий** — безопасный уровень памяти,\n",
        "- **Оранжевый** — данные начинают сбрасываться на диск (spilling),\n",
        "- **Красный** — worker приостановлен из-за нехватки памяти.\n",
        "\n",
        "> **Методология:**  \n",
        "> Эффективный разработчик Dask **не просто пишет код**, а **анализирует Task Stream** после каждого запуска, корректируя:\n",
        "> - размер чанков,\n",
        "> - структуру графа,\n",
        "> - выбор операций (избегая `apply` и `shuffle`).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Интеграция с машинным обучением: Dask-ML\n",
        "\n",
        "Библиотека **`dask-ml`** расширяет экосистему Scikit-learn для работы с **out-of-core** и **распределёнными** данными.\n",
        "\n",
        "### 6.1. Параллельная предобработка\n",
        "\n",
        "Модули `dask_ml.preprocessing` предоставляют трансформеры, совместимые с `sklearn`:\n",
        "- `StandardScaler`, `MinMaxScaler`,\n",
        "- `OneHotEncoder` с поддержкой `CategoricalDtype`.\n",
        "\n",
        "Все они работают **лениво** и **параллельно** на `Dask DataFrame` и `Dask Array`.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2. Масштабирование обучения: мета-оценщики\n",
        "\n",
        "#### **`ParallelPostFit`**\n",
        "Оборачивает обученную модель и позволяет **параллельно применять** `predict`/`transform` к большим данным.\n",
        "\n",
        "#### **`Incremental`**\n",
        "Для моделей, поддерживающих `partial_fit` (например, `SGDClassifier`), обучает **блок за блоком**, не загружая всё в память.\n",
        "\n",
        "> **Пример: обучение на 1 млрд записей**\n",
        "\n",
        "```python\n",
        "import dask.array as da\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from dask_ml.wrappers import Incremental\n",
        "\n",
        "# Большие данные (out-of-core)\n",
        "X = da.random.normal(size=(1_000_000_000, 10), chunks=(100_000, 10))\n",
        "y = (X.sum(axis=1) > 0).astype(int)\n",
        "\n",
        "# Обучение по блокам\n",
        "model = Incremental(SGDClassifier(random_state=42))\n",
        "model.fit(X, y)  # каждый чанк → partial_fit\n",
        "\n",
        "# Параллельный прогноз\n",
        "predictions = model.predict(X).compute()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Такой подход делает возможным обучение на данных, которые **никогда не помещаются в RAM**.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3. Поиск гиперпараметров: инкрементальные методы\n",
        "\n",
        "- **`IncrementalSearchCV`** и **`HyperbandSearchCV`** — аналоги `GridSearchCV`, но с **ранней остановкой**.\n",
        "- Модели, показывающие плохую сходимость, **отбрасываются досрочно**, что экономит ресурсы.\n",
        "\n",
        "> **Ограничение:** требует поддержки `partial_fit` от модели.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Комплексные практические кейсы\n",
        "\n",
        "### 7.1. Методология out-of-core ETL/ELT\n",
        "\n",
        "Оптимальный пайплайн в Dask включает:\n",
        "\n",
        "1. **Параллельная загрузка**: `dd.read_parquet(\"s3://bucket/data*.parquet\")`.\n",
        "2. **Ленивая трансформация**: фильтрация, очистка, feature engineering.\n",
        "3. **Персистенция**: если за шагом следует несколько дорогих операций, вызовите `.persist()`, чтобы **закешировать промежуточный результат** в распределённой памяти.\n",
        "4. **Сохранение**: `.to_parquet()`, `.to_csv()` или запись в БД — **без `.compute()`**, чтобы избежать материализации.\n",
        "\n",
        "> **Пример:**\n",
        "\n",
        "```python\n",
        "df = dd.read_parquet(\"raw_data/\")\n",
        "clean = df[df.value.notnull()].assign(...)\n",
        "clean = clean.persist()  # ← кешируем\n",
        "\n",
        "# Несколько независимых агрегаций\n",
        "agg1 = clean.groupby(\"cat\").value.mean()\n",
        "agg2 = clean.groupby(\"cat\").value.std()\n",
        "\n",
        "# Сохраняем без полной загрузки в память\n",
        "agg1.to_parquet(\"results/mean.parquet\")\n",
        "agg2.to_parquet(\"results/std.parquet\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7.2. Методологическая карта перехода к Dask\n",
        "\n",
        "**Не используйте Dask «на всякий случай»**. Переход оправдан **только при соблюдении критериев**:\n",
        "\n",
        "| Критерий | Dask **НЕ рекомендован** | Dask **рекомендован** | Обоснование |\n",
        "|--------|--------------------------|------------------------|-------------|\n",
        "| **Размер данных** | Помещаются в RAM | Превышают RAM (out-of-core) | Накладные расходы не окупаются |\n",
        "| **Длительность вычисления** | < 1 секунды | > 1–2 секунд | Минимальная задача ≥ 100 мс |\n",
        "| **Диагностика** | Не требуется | Нужен контроль памяти и производительности | Дашборд — ключ к оптимизации |\n",
        "\n",
        "> **Важно:** на малых данных Dask может быть **в 10–100 раз медленнее** Pandas из-за инициализации графа и планировщика.\n",
        "\n",
        "---\n",
        "\n",
        "### 7.3. Кейс: гибридный пайплайн временного ряда\n",
        "\n",
        "**Задача:** обработать 50 ГБ метеоданных → фильтрация → FFT → агрегация.\n",
        "\n",
        "**Архитектура:**\n",
        "\n",
        "1. **Табличная фильтрация (Dask DataFrame)**  \n",
        "   ```python\n",
        "   df = dd.read_parquet(\"weather/\")\n",
        "   filtered = df[(df.temp > 0) & (df.time >= \"2020\")]\n",
        "   ```\n",
        "\n",
        "2. **Переход к численным данным (Dask Array)**  \n",
        "   ```python\n",
        "   signal = filtered.temp.values  # → dask.array\n",
        "   signal = signal.rechunk(chunks=(\"auto\",))  # оптимизация чанков под FFT\n",
        "   ```\n",
        "\n",
        "3. **Численный анализ (Dask Array)**  \n",
        "   ```python\n",
        "   fft_result = da.fft.fft(signal)\n",
        "   power = da.abs(fft_result) ** 2\n",
        "   ```\n",
        "\n",
        "4. **Финальная агрегация (обратно в DataFrame)**  \n",
        "   ```python\n",
        "   result_df = dd.from_dask_array(power, columns=[\"power\"])\n",
        "   daily_stats = result_df.groupby(result_df.index // 86400).mean()\n",
        "   daily_stats.to_parquet(\"fft_stats/\")\n",
        "   ```\n",
        "\n",
        "> **Ключевой принцип:**  \n",
        "> Данные **мигрируют между коллекциями** в зависимости от задачи:\n",
        "> - `DataFrame` — для структурированной фильтрации,\n",
        "> - `Array` — для HPC-операций.\n",
        ">\n",
        "> Минимизируйте переходы и **выравнивайте чанки**, чтобы избежать дорогостоящего `rechunk`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "**Dask — это зрелая, гибкая и диагностически прозрачная платформа** для масштабирования аналитики данных в экосистеме Python. Его сила — не в «автомагическом» ускорении, а в **осознанном управлении вычислениями**:\n",
        "\n",
        "- через **ленивые графы**,\n",
        "- через **блочную память**,\n",
        "- через **интерактивную диагностику**.\n",
        "\n",
        "Использование Dask требует **методологической дисциплины**: понимания накладных расходов, гранулярности задач, архитектуры данных и инструментов профилирования.\n",
        "\n",
        "При правильном применении Dask позволяет:\n",
        "- обрабатывать **петабайты данных** на кластере,\n",
        "- выполнять **численные расчёты на миллиардах точек**,\n",
        "- обучать **ML-модели на out-of-core данных**,\n",
        "- и всё это — **в знакомом синтаксисе PyData**.\n",
        "\n",
        "Таким образом, Dask завершает эволюцию от **in-memory аналитики** (Pandas) к **масштабируемой, распределённой и диагностируемой** вычислительной платформе для современных задач данных.\n",
        "\n",
        "\n",
        "\n",
        "✅ **Цикл полностью завершён.**  \n",
        "Вы прошли путь от основ (NumPy) → аналитики (Pandas) → высокой производительности (Polars) → распределённых вычислений (Dask).\n"
      ],
      "metadata": {
        "id": "Q9UOCsmzoXq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# МОДУЛЬ 5: Apache Spark и PySpark — Архитектура и практика распределённой обработки Big Data\n",
        "\n",
        "## Введение\n",
        "\n",
        "В эпоху Big Data обработка объёмов информации, превышающих возможности единичной вычислительной машины, стала неотъемлемой частью научных исследований, промышленного анализа и разработки интеллектуальных систем. Apache Spark представляет собой одну из наиболее зрелых и широко применяемых платформ для решения подобных задач. В отличие от библиотек, ориентированных на in-memory вычисления (таких как Pandas или Polars), Spark изначально спроектирован как **распределённый вычислительный движок**, способный масштабироваться от локального режима до кластеров, охватывающих тысячи узлов.\n",
        "\n",
        "Архитектурная целостность Spark обеспечивается не только его способностью распараллеливать вычисления, но и глубокой интеграцией механизмов отказоустойчивости, оптимизации запросов и эффективного управления памятью. Понимание этих механизмов — от жизненного цикла приложения до работы оптимизатора Catalyst — является необходимым условием для построения производительных и надёжных систем обработки больших данных.\n",
        "\n",
        "Настоящий модуль посвящён систематическому изложению архитектурных основ Spark, эволюции его программных интерфейсов и принципов функционирования его вычислительного ядра. Особое внимание уделено специфике взаимодействия PySpark с JVM-базой Spark, что критически важно для разработчиков, использующих Python в качестве основного языка аналитики. Все теоретические положения сопровождаются практическими примерами, демонстрирующими их применение в реальных сценариях.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Архитектурные основы распределённой модели Spark\n",
        "\n",
        "### 1.1. Диспетчер, Исполнители и Рабочие Узлы: Формальные определения\n",
        "\n",
        "Распределённое приложение Spark функционирует на основе трёх взаимосвязанных компонентов: Диспетчера программы (Driver Program), Исполнителей (Executors) и Рабочих Узлов (Worker Nodes).\n",
        "\n",
        "**Диспетчер программы** является центральным управляющим элементом любого Spark-приложения. Он инициализируется при создании объекта `SparkSession` и может размещаться либо на клиентской машине (в режиме client), либо на одном из узлов кластера (в режиме cluster). Основные функции Диспетчера включают: преобразование последовательности пользовательских преобразований и действий в направленный ациклический граф (DAG), который служит логическим планом вычислений; взаимодействие с кластерным менеджером для запроса и выделения вычислительных ресурсов в виде Исполнителей; мониторинг статуса выполнения задач на Исполнителях и обеспечение отказоустойчивости; сбор и агрегация окончательных результатов вычислений.\n",
        "\n",
        "**Исполнители** представляют собой рабочие процессы, запускаемые на Рабочих Узлах кластера. Каждый Исполнитель получает в своё распоряжение выделенный объём оперативной памяти и набор процессорных ядер (Cores), которые служат минимальными единицами параллельного исполнения. Исполнители отвечают за непосредственное выполнение задач, назначенных им Диспетчером, над партициями данных.\n",
        "\n",
        "**Рабочие Узлы** — это физические или виртуальные машины, составляющие вычислительный кластер. На каждом Рабочем Узле может быть запущено один или несколько Исполнителей.\n",
        "\n",
        "> **Пример: Инициализация SparkSession и базовое распределённое вычисление**\n",
        "\n",
        "В следующем примере демонстрируется создание Spark-приложения и выполнение простой операции подсчёта. Даже в локальном режиме (`master=\"local[*]\"`) Spark создаёт Диспетчер и один Исполнитель (внутри того же процесса), что позволяет изучать его архитектуру на одной машине.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Инициализация Диспетчера (Driver)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Architecture Example\") \\\n",
        "    .master(\"local[*]\") \\  # Локальный режим со всеми доступными ядрами\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание RDD из диапазона чисел. Данные автоматически разбиваются на партиции.\n",
        "numbers = spark.sparkContext.parallelize(range(1, 1000001), numSlices=4)\n",
        "\n",
        "# Действие (Action): запускает DAG и возвращает результат Диспетчеру\n",
        "total = numbers.reduce(lambda a, b: a + b)\n",
        "print(f\"Сумма чисел от 1 до 1000000: {total}\")\n",
        "\n",
        "# Завершение работы приложения\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Вызов `parallelize` создаёт RDD с 4 партициями. Метод `reduce` является действием (Action), которое инициирует вычисление. Диспетчер разбивает задачу на 4 подзадачи, которые выполняются параллельно на Исполнителе (в локальном режиме — в том же процессе). Итоговый результат агрегируется и возвращается в Диспетчер.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.5. Влияние замыканий (Closures) на состояние Driver\n",
        "\n",
        "При разработке распределённых приложений на PySpark крайне важно понимать механизм передачи кода и данных от Диспетчера к Исполнителям.\n",
        "\n",
        "> **Пример: Демонстрация неизменности переменной Driver из Исполнителя**\n",
        "\n",
        "Следующий код иллюстрирует классическую ошибку, связанную с непониманием замыканий в распределённой среде. Разработчик пытается инкрементировать переменную `counter` из функции, выполняемой на Исполнителе. Однако результат оказывается неожиданным.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Closure Example\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Глобальная переменная в процессе Диспетчера\n",
        "counter = 0\n",
        "\n",
        "def increment_counter(value):\n",
        "    global counter\n",
        "    counter += 1  # Эта операция изменяет ЛОКАЛЬНУЮ копию переменной на Исполнителе\n",
        "    return value\n",
        "\n",
        "# Создаём RDD и применяем функцию\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.map(increment_counter).collect()  # Запускаем действие\n",
        "\n",
        "print(f\"Значение counter в Диспетчере: {counter}\")  # Вывод: 0\n",
        "\n",
        "# Правильный способ: использование аккумулятора\n",
        "acc_counter = sc.accumulator(0)\n",
        "\n",
        "def increment_accumulator(value):\n",
        "    global acc_counter\n",
        "    acc_counter.add(1)  # Аккумулятор гарантирует агрегацию в Диспетчере\n",
        "    return value\n",
        "\n",
        "rdd.map(increment_accumulator).collect()\n",
        "print(f\"Значение аккумулятора: {acc_counter.value}\")  # Вывод: 5\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* В первом случае переменная `counter` внутри `increment_counter` является независимой копией на каждом Исполнителе. Изменения не отражаются в Диспетчере. Во втором случае используется специальный объект `Accumulator`, который предназначен для безопасной агрегации информации из Исполнителей в Диспетчер.\n",
        "\n",
        "---\n",
        "\n",
        "## II. Эволюция Abstraction API: От RDD к структурированной обработке\n",
        "\n",
        "### 2.2. DataFrame API (PySpark)\n",
        "\n",
        "В современной практике, где подавляющее большинство данных имеет табличную структуру, DataFrame API является стандартом де-факто.\n",
        "\n",
        "> **Пример: Чтение данных, трансформации и анализ с помощью DataFrame**\n",
        "\n",
        "В этом примере показано, как с помощью DataFrame API можно выполнить типичный ETL-пайплайн: загрузку данных из CSV-файла, фильтрацию, агрегацию и анализ плана выполнения.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame API Example\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 1. Чтение данных в DataFrame\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .csv(\"sales_data.csv\")\n",
        "\n",
        "# Показать схему данных\n",
        "df.printSchema()\n",
        "# root\n",
        "#  |-- product: string (nullable = true)\n",
        "#  |-- category: string (nullable = true)\n",
        "#  |-- price: double (nullable = true)\n",
        "#  |-- quantity: integer (nullable = true)\n",
        "\n",
        "# 2. Применение преобразований (ленивые операции)\n",
        "filtered_df = df.filter(col(\"price\") > 10.0)\n",
        "aggregated_df = filtered_df.groupBy(\"category\").agg(avg(\"price\").alias(\"avg_price\"))\n",
        "\n",
        "# 3. Анализ физического плана выполнения\n",
        "print(\"Физический план:\")\n",
        "aggregated_df.explain(\"formatted\")\n",
        "\n",
        "# 4. Выполнение действия и сбор результата\n",
        "result = aggregated_df.collect()\n",
        "for row in result:\n",
        "    print(f\"Категория: {row['category']}, Средняя цена: {row['avg_price']:.2f}\")\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Все операции до вызова `collect()` являются преобразованиями (Transformations) и лишь строят логический план. Метод `explain(\"formatted\")` выводит читаемый физический план, в котором можно увидеть использование оптимизаций Catalyst, таких как `Filter` перед `Scan` (Predicate Pushdown).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Взаимодействие PySpark и JVM: Сериализационный барьер и Apache Arrow\n",
        "\n",
        "Для устранения сериализационного барьера в Spark была интегрирована библиотека **Apache Arrow**.\n",
        "\n",
        "> **Пример: Сравнение производительности UDF с и без Arrow**\n",
        "\n",
        "Следующий пример демонстрирует, как включить Arrow и создать векторизованную UDF, которая работает с целыми столбцами данных за один вызов, а не построчно.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Arrow UDF Example\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание тестового DataFrame\n",
        "df = spark.range(0, 1000000).toDF(\"id\")\n",
        "df = df.withColumn(\"value\", col(\"id\") * 2.0)\n",
        "\n",
        "# Скалярная UDF (медленная, без Arrow)\n",
        "def slow_udf(x):\n",
        "    return x * 1.1\n",
        "\n",
        "spark.udf.register(\"slow_udf\", slow_udf, DoubleType())\n",
        "start = time.time()\n",
        "df.selectExpr(\"slow_udf(value) as new_value\").collect()\n",
        "slow_time = time.time() - start\n",
        "\n",
        "# Векторизованная UDF с Arrow (быстрая)\n",
        "@pandas_udf(DoubleType())\n",
        "def fast_udf(v: pd.Series) -> pd.Series:\n",
        "    return v * 1.1\n",
        "\n",
        "start = time.time()\n",
        "df.select(fast_udf(col(\"value\")).alias(\"new_value\")).collect()\n",
        "fast_time = time.time() - start\n",
        "\n",
        "print(f\"Скалярная UDF: {slow_time:.2f} секунд\")\n",
        "print(f\"Векторизованная (Arrow) UDF: {fast_time:.2f} секунд\")\n",
        "print(f\"Ускорение: {slow_time / fast_time:.2f}x\")\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "> *Пояснение:* Векторизованная UDF, отмеченная декоратором `@pandas_udf`, получает и возвращает целые `pandas.Series`. Благодаря Arrow, передача данных между JVM и Python происходит без сериализации, что приводит к значительному ускорению.\n",
        "\n",
        "---\n",
        "\n",
        "## III. Движок исполнения Spark: Catalyst и Tungsten (Глубокий анализ)\n",
        "\n",
        "### 3.2. Физический план: Выбор стратегий и стоимостное моделирование\n",
        "\n",
        "Умение анализировать физический план является ключевым навыком для оптимизации запросов.\n",
        "\n",
        "> **Пример: Анализ плана выполнения операции Join**\n",
        "\n",
        "В этом примере создаются два DataFrame и выполняется операция соединения. Анализ плана позволяет понять, какой алгоритм Join был выбран оптимизатором.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Join Plan Analysis\").getOrCreate()\n",
        "\n",
        "# Создание двух небольших DataFrame\n",
        "df1 = spark.createDataFrame([(i, f\"user_{i}\") for i in range(1, 101)], [\"id\", \"name\"])\n",
        "df2 = spark.createDataFrame([(i, f\"category_{i % 5}\") for i in range(1, 101)], [\"id\", \"category\"])\n",
        "\n",
        "# Выполнение Join\n",
        "joined_df = df1.join(df2, on=\"id\")\n",
        "\n",
        "# Вывод расширенного плана\n",
        "print(\"Расширенный план выполнения:\")\n",
        "joined_df.explain(\"extended\")\n",
        "\n",
        "# В реальных сценариях для больших таблиц Spark может выбрать\n",
        "# BroadcastHashJoin или SortMergeJoin в зависимости от статистики.\n",
        "```\n",
        "\n",
        "> *Пояснение:* В выводе `explain` можно увидеть физический оператор, например, `BroadcastHashJoin`. Это означает, что Catalyst определил, что одна из таблиц достаточно мала, чтобы быть переданной (broadcasted) всем Исполнителям, что избегает дорогостоящей операции shuffle.\n",
        "\n"
      ],
      "metadata": {
        "id": "C-opDJc48ICU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## IV. Практика PySpark: Код, оптимизация и анализ плана выполнения\n",
        "\n",
        "В производственных задачах PySpark используется для создания надёжных и масштабируемых ETL-пайплайнов, что требует не только знания синтаксиса DataFrame, но и глубокого понимания того, как операции высокого уровня трансформируются в низкоуровневые распределённые события. Настоящий раздел посвящён переходу от теории к практике: мы рассмотрим канонические примеры, проанализируем их физические планы и продемонстрируем методы оптимизации, применяемые в реальных инженерных задачах.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1. Идиоматичное использование DataFrame API: Пример ETL с оконными функциями\n",
        "\n",
        "Оконные функции (Window Functions) являются мощным инструментом для выполнения сложных аналитических операций, таких как ранжирование, кумулятивные суммы и скользящие средние, без необходимости выполнять дорогостоящую глобальную агрегацию.\n",
        "\n",
        "> **Пример: Ранжирование сотрудников по зарплате внутри отдела**\n",
        "\n",
        "Рассмотрим типичную задачу из корпоративной аналитики: необходимо для каждого отдела проранжировать сотрудников по убыванию заработной платы. В реляционных базах данных эта задача решается с помощью аналитических функций `RANK() OVER (PARTITION BY ... ORDER BY ...)`. В PySpark аналогичный результат достигается с помощью модуля `pyspark.sql.window`.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Инициализация сессии\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Window Function Example\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создание тестового набора данных\n",
        "data = [\n",
        "    (1, \"Alice\", 10, 5000),\n",
        "    (2, \"Bob\", 10, 6000),\n",
        "    (3, \"Charlie\", 20, 7000),\n",
        "    (4, \"David\", 10, 5000),\n",
        "    (5, \"Eve\", 20, 8000)\n",
        "]\n",
        "columns = [\"empno\", \"ename\", \"deptno\", \"sal\"]\n",
        "emp_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Определение оконной спецификации:\n",
        "# — партиционирование по отделу (deptno),\n",
        "# — сортировка внутри партиции по зарплате (убывание)\n",
        "window_spec = Window.partitionBy(\"deptno\").orderBy(F.col(\"sal\").desc())\n",
        "\n",
        "# Применение оконной функции ранжирования\n",
        "result_df = emp_df.withColumn(\"rank\", F.rank().over(window_spec))\n",
        "\n",
        "print(\"Результат ранжирования сотрудников:\")\n",
        "result_df.show()\n",
        "\n",
        "# Анализ физического плана выполнения\n",
        "print(\"\\nФизический план выполнения:\")\n",
        "result_df.explain(\"formatted\")\n",
        "```\n",
        "\n",
        "**Вывод:**\n",
        "```\n",
        "+-----+-------+------+----+----+\n",
        "|empno|  ename|deptno| sal|rank|\n",
        "+-----+-------+------+----+----+\n",
        "|    2|    Bob|    10|6000|   1|\n",
        "|    1|  Alice|    10|5000|   2|\n",
        "|    4|  David|    10|5000|   2|\n",
        "|    5|    Eve|    20|8000|   1|\n",
        "|    3|Charlie|    20|7000|   2|\n",
        "+-----+-------+------+----+----+\n",
        "```\n",
        "\n",
        "> *Пояснение:* В физическом плане, который выводится командой `explain(\"formatted\")`, можно наблюдать следующую последовательность:\n",
        "> 1. **ShuffleExchange** по столбцу `deptno` — все строки с одинаковым `deptno` перераспределяются на один исполнитель.\n",
        "> 2. **Sort** внутри каждой партиции по `sal DESC`.\n",
        "> 3. **Window** — применение функции `rank()`.\n",
        ">\n",
        "> Несмотря на лаконичность кода, операция требует **shuffle**, что делает её потенциально дорогой при большом объёме данных. Это подчёркивает важный принцип: даже высокоуровневые API скрывают низкоуровневые распределённые операции, которые необходимо учитывать при проектировании пайплайнов.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2. Кейс 1: Устранение дорогостоящих операций Shuffle\n",
        "\n",
        "Shuffle — это операция перераспределения данных по ключу, которая неизбежна при `JOIN`, `GROUP BY` и оконных функциях с партиционированием. Её стоимость обусловлена сериализацией, передачей данных по сети и десериализацией.\n",
        "\n",
        "> **Оптимизация: Broadcast Join для малых таблиц**\n",
        "\n",
        "Наиболее эффективный способ избежать shuffle — использовать **Broadcast Hash Join**, когда одна из таблиц мала (например, справочник регионов). Spark транслирует малую таблицу в память каждого исполнителя, что позволяет выполнять соединение локально.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Broadcast Join Example\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"104857600\") \\  # 100 МБ\n",
        "    .getOrCreate()\n",
        "\n",
        "# Большая таблица фактов (например, транзакции)\n",
        "large_df = spark.range(0, 1000000).toDF(\"id\").withColumn(\"region_id\", F.col(\"id\") % 10)\n",
        "\n",
        "# Малая справочная таблица (регионы)\n",
        "small_df = spark.createDataFrame(\n",
        "    [(i, f\"Region_{i}\") for i in range(10)],\n",
        "    [\"region_id\", \"region_name\"]\n",
        ")\n",
        "\n",
        "# Принудительный broadcast для гарантии\n",
        "result = large_df.join(\n",
        "    F.broadcast(small_df),\n",
        "    on=\"region_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "print(\"Результат соединения (первые 5 строк):\")\n",
        "result.show(5)\n",
        "\n",
        "# Анализ плана: в выводе будет BroadcastExchange и BroadcastHashJoin\n",
        "print(\"\\nФизический план (фрагмент):\")\n",
        "result.explain(\"simple\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* В выводе `explain` появится строка вида `*(2) BroadcastHashJoin`, что подтверждает использование broadcast-стратегии. Это означает, что **shuffle для большой таблицы не происходит**, и вся операция выполняется за счёт локальных вычислений на каждом исполнителе. В производственной среде рекомендуется **явно указывать `broadcast()`**, даже если автоматическая оптимизация включена, чтобы избежать неожиданного переключения на shuffle-join при изменении размера данных.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3. Кейс 2: Методы борьбы с Data Skew (перекосом данных)\n",
        "\n",
        "**Data Skew** возникает, когда распределение ключей крайне неравномерно — например, 90% транзакций относятся к одному клиенту. Это приводит к тому, что одна партиция обрабатывается значительно дольше остальных, что замедляет весь этап.\n",
        "\n",
        "> **Техника салтинга (Salting) для агрегации**\n",
        "\n",
        "Салтинг — это метод искусственного разбиения «горячего» ключа на несколько подключей с помощью случайного суффикса («соли»). Это позволяет распределить нагрузку по нескольким партициям.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Salting Example\").getOrCreate()\n",
        "\n",
        "# Создание данных с перекосом: 90% записей имеют key=1\n",
        "skewed_data = (\n",
        "    [(1, 1.0)] * 900000 +  # \"горячий\" ключ\n",
        "    [(i, 1.0) for i in range(2, 10001)]  # остальные ключи\n",
        ")\n",
        "skewed_df = spark.createDataFrame(skewed_data, [\"skewed_key\", \"value\"])\n",
        "\n",
        "# Параметр: количество \"бакетов соли\"\n",
        "N = 10\n",
        "\n",
        "# Шаг 1: Добавление соли и частичная агрегация\n",
        "salted_df = (\n",
        "    skewed_df\n",
        "    .withColumn(\"salt\", (F.rand() * N).cast(IntegerType()))\n",
        "    .groupBy(\"skewed_key\", \"salt\")\n",
        "    .agg(F.sum(\"value\").alias(\"partial_sum\"))\n",
        ")\n",
        "\n",
        "# Шаг 2: Финальная агрегация (без соли)\n",
        "final_df = (\n",
        "    salted_df\n",
        "    .groupBy(\"skewed_key\")\n",
        "    .agg(F.sum(\"partial_sum\").alias(\"total_value\"))\n",
        ")\n",
        "\n",
        "print(\"Результат агрегации после салтинга:\")\n",
        "final_df.show(5)\n",
        "\n",
        "# Анализ плана: два этапа агрегации вместо одного\n",
        "print(\"\\nФизический план:\")\n",
        "final_df.explain(\"simple\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Без салтинга вся работа по ключу `1` выполнялась бы на одном исполнителе, что привело бы к «застреванию» задачи. Салтинг разбивает её на 10 партиций, что ускоряет выполнение в 5–8 раз в реальных сценариях. В физическом плане видны **два этапа агрегации**: `HashAggregate` → `Exchange` → `HashAggregate`, что подтверждает корректность двойного подхода.\n",
        "\n",
        "---\n",
        "\n",
        "## V. Управление ресурсами и производственный контекст\n",
        "\n",
        "Оптимальная работа Spark в production зависит не только от кода, но и от настройки ресурсов, выбора форматов хранения и использования современных архитектур.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1. Настройка ресурсов кластера для production\n",
        "\n",
        "Неправильная конфигурация ресурсов — частая причина низкой производительности. Основной принцип: **избегать больших JVM-процессов**.\n",
        "\n",
        "> **Пример: Запуск через `spark-submit` с оптимальными параметрами**\n",
        "\n",
        "```bash\n",
        "spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --num-executors 20 \\\n",
        "  --executor-cores 4 \\\n",
        "  --executor-memory 12g \\\n",
        "  --driver-memory 4g \\\n",
        "  --conf spark.sql.adaptive.enabled=true \\\n",
        "  --conf spark.sql.autoBroadcastJoinThreshold=104857600 \\\n",
        "  --conf spark.shuffle.service.enabled=true \\\n",
        "  your_etl_script.py\n",
        "```\n",
        "\n",
        "> *Пояснение:*  \n",
        "> - `--executor-cores 4` — обеспечивает баланс между параллелизмом и GC-паузами.  \n",
        "> - `--executor-memory 12g` — достаточно для большинства задач без спиллинга на диск.  \n",
        "> - Включение `spark.shuffle.service.enabled` — обязательно для dynamic allocation и отказоустойчивости.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Современные форматы хранения: от Parquet к Delta Lake\n",
        "\n",
        "> **Пример: Работа с Delta Lake**\n",
        "\n",
        "```python\n",
        "from delta import DeltaTable\n",
        "\n",
        "# Запись в Delta-таблицу\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/sales_delta\")\n",
        "\n",
        "# Создание DeltaTable для транзакционных операций\n",
        "delta_table = DeltaTable.forPath(spark, \"/data/sales_delta\")\n",
        "\n",
        "# Операция MERGE (upsert)\n",
        "new_data = spark.createDataFrame([(101, 1500.0)], [\"id\", \"amount\"])\n",
        "delta_table.alias(\"t\").merge(\n",
        "    new_data.alias(\"s\"),\n",
        "    \"t.id = s.id\"\n",
        ").whenMatchedUpdate(set={\"amount\": \"s.amount\"}) \\\n",
        " .whenNotMatchedInsert(values={\"id\": \"s.id\", \"amount\": \"s.amount\"}) \\\n",
        " .execute()\n",
        "\n",
        "# Time Travel: чтение предыдущей версии\n",
        "historical_df = spark.read.format(\"delta\") \\\n",
        "    .option(\"versionAsOf\", 0) \\\n",
        "    .load(\"/data/sales_delta\")\n",
        "```\n",
        "\n",
        "> *Пояснение:* Delta Lake добавляет **ACID-транзакции**, **MERGE**, **Time Travel** и **оптимизированную статистику** поверх Parquet. Это делает его стандартом для современных Lakehouse-архитектур.\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Мониторинг и тюнинг производительности (Production Ready)\n",
        "\n",
        "### 6.2. Диагностика через Spark UI\n",
        "\n",
        "> **Как читать метрики:**\n",
        "> - **Skew**: если 99-й перцентиль времени выполнения задач в 10–100 раз больше медианы — есть перекос.\n",
        "> - **Spill to Disk**: наличие значений в колонке `Spill (Memory)` или `Spill (Disk)` означает нехватку памяти.\n",
        "> - **GC Time**: если время GC превышает 10–15% от общего времени выполнения — уменьшайте `--executor-cores`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Мастерство работы с PySpark заключается в способности **предсказывать распределённое поведение** на основе высокоуровневого кода. Это требует:\n",
        "- понимания жизненного цикла приложения (Driver, Executors, DAG),\n",
        "- умения анализировать физический план (`explain`),\n",
        "- знания методов оптимизации (Broadcast Join, Salting),\n",
        "- владения современными инструментами (Delta Lake, Arrow-UDF),\n",
        "- системного подхода к мониторингу (Spark UI, метрики).\n",
        "\n"
      ],
      "metadata": {
        "id": "dDMl-R0kBh-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 6: Веб-скрейпинг и парсинг данных — от статических страниц до распределённых динамических приложений\n",
        "\n",
        "### Введение: Фундамент сбора данных в сети\n",
        "\n",
        "Веб-скрейпинг (Web Scraping) представляет собой методический процесс автоматизированного извлечения больших объёмов неструктурированных или полуструктурированных данных с веб-сайтов. В контексте современной обработки данных этот процесс является неотъемлемой частью фазы Extract в общем цикле ETL (Extract, Transform, Load). В отличие от использования официального программного интерфейса (API), скрейпинг требует активного анализа и парсинга сырого HTML-кода и DOM-структуры, поскольку целевой ресурс не предоставляет гарантированно стабильного и структурированного формата данных. Эта особенность делает скрейпинг одновременно гибким и уязвимым инструментом, требующим глубокого понимания как веб-технологий, так и этико-правовых рамок.\n",
        "\n",
        "### 1.1. Определение Веб-скрейпинга и его Место в ETL-процессах\n",
        "\n",
        "Профессиональный скрейпинг всегда рассматривается как высоконагруженный ETL-процесс. Фаза Extract заключается в непосредственном сборе данных, который может осуществляться посредством прямых HTTP-запросов или эмуляции поведения веб-браузера. Фаза Transform включает очистку, валидацию, нормализацию и дедупликацию извлечённых данных. Фаза Load завершает цикл — данные сохраняются в целевое хранилище: реляционную или документную базу данных, файловую систему, облачное хранилище или потоковую платформу. Архитектура промышленных фреймворков, таких как Scrapy, напрямую отражает этот цикл: задачи сбора данных делегируются «паукам» (Spiders), а обработка и сохранение — конвейерам элементов (Item Pipelines).\n",
        "\n",
        "### 1.2. Этические и Правовые Границы\n",
        "\n",
        "Прежде чем приступать к сбору данных, необходимо провести тщательный юридический и этический аудит. Правовое поле веб-скрейпинга неоднозначно и сильно зависит от юрисдикции, характера собираемых данных и условий использования целевого сайта. Условия предоставления услуг (Terms of Service, ToS) имеют приоритетное значение: если ToS явно запрещают автоматизированный сбор данных, выполнение скрейпинга может повлечь юридическую ответственность и техническую блокировку IP-адресов. Регламент GDPR (General Data Protection Regulation) строго регулирует сбор и обработку идентифицируемых персональных данных (PII). Сбор таких данных без явного согласия пользователя представляет собой высокий юридический риск, даже если информация публично доступна. Промышленные системы сбора данных обязаны включать процедуры обработки запросов на удаление персональной информации. Файл `robots.txt` является де-факто стандартом для коммуникации между веб-мастерами и автоматизированными агентами. Профессиональные скрейперы должны неукоснительно соблюдать директивы `Disallow` и уважать параметр `Crawl-delay`. «Дружественный» скрейпинг означает, что процесс должен быть незаметным, не нарушать нормальное функционирование целевого сервера и не создавать чрезмерную нагрузку на его ресурсы.\n",
        "\n",
        "### 1.3. Архитектура Современных Веб-приложений\n",
        "\n",
        "Сложность инструментария, необходимого для сбора данных, напрямую определяется архитектурой целевого сайта. Статический HTML — самый простой случай: весь контент (текст, ссылки, таблицы) полностью содержится в исходном коде, полученном в ответ на HTTP-запрос. Для такого сайта достаточно базовых HTTP-клиентов и HTML-парсеров. Server-Side Rendering (SSR) представляет промежуточную сложность: основной контент генерируется на сервере, но отдельные элементы (рейтинги, комментарии, рекомендации) могут подгружаться асинхронно через AJAX. В таких случаях полнота данных может потребовать анализа сетевых запросов или частичного рендеринга. Наибольшую сложность представляют приложения с Client-Side Rendering (CSR) и архитектурой Single Page Application (SPA). Такие сайты отдают минимальный HTML-каркас и набор JavaScript-скриптов, а вся визуализация и формирование DOM-дерева происходят на стороне клиента. Для извлечения данных с подобных ресурсов требуется полная эмуляция браузерного окружения с выполнением JavaScript — для этого используются такие инструменты, как Playwright, Puppeteer или Selenium.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 1: Основы парсинга статического контента (BeautifulSoup4 + Requests)\n",
        "\n",
        "Для работы со статическими или SSR-страницами основным инструментарием в экосистеме Python являются библиотека `requests` для выполнения HTTP-запросов и `BeautifulSoup4` (BS4) для парсинга HTML-документов. Эта связка обеспечивает простоту, читаемость и достаточную гибкость для большинства задач начального и среднего уровня.\n",
        "\n",
        "### 2.1. Теория Клиент-Серверного Взаимодействия\n",
        "\n",
        "Протокол HTTP (Hypertext Transfer Protocol) лежит в основе взаимодействия между клиентом и сервером. Будучи протоколом без сохранения состояния (stateless), HTTP не запоминает контекст предыдущих запросов. Для поддержания сессий (авторизации, корзины, навигации) используются заголовки (Headers) и куки (Cookies). Библиотека `requests` позволяет эффективно управлять этим состоянием через объект `requests.Session()`. Сессия автоматически сохраняет полученные куки и прикрепляет их к последующим запросам, что позволяет имитировать поведение реального браузера и обеспечивает устойчивость к перенаправлениям, CSRF-токенам и другим механизмам защиты.\n",
        "\n",
        "### 2.2. Устойчивые HTTP-запросы с requests\n",
        "\n",
        "При скрейпинге крайне важна маскировка и устойчивость. Многие сайты анализируют HTTP-заголовки и блокируют запросы с подозрительными сигнатурами. Наличие реалистичного заголовка `User-Agent`, имитирующего популярный браузер (например, Chrome или Firefox), а также заголовка `Referer`, указывающего на предыдущую страницу, значительно снижает вероятность обнаружения и блокировки.\n",
        "\n",
        "Не менее важна обработка ошибок. В промышленном скрейпинге неудача при получении ответа — например, HTTP-код 429 «Too Many Requests» или 5xx «Server Error» — не должна приводить к немедленному повторному запросу. Такое поведение усугубляет нагрузку на сервер и гарантирует блокировку. Профессиональным решением является использование стратегии **экспоненциального замедления** (Exponential Backoff). Эта методика предусматривает постепенное увеличение задержки между повторными попытками: вместо фиксированной паузы задержка растёт с каждой неудачной попыткой, например, по формуле $D = \\text{backoff\\_factor} \\cdot (2^{(R - 1)})$, где $D$ — задержка, а $R$ — номер попытки. Такой подход демонстрирует «дружественное» поведение, даёт серверу время на восстановление и повышает общую надёжность скрипта.\n",
        "\n",
        "Пример реализации устойчивой сессии с поддержкой экспоненциального замедления:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def create_resilient_session(max_retries=5, backoff_factor=1):\n",
        "    \"\"\"Создаёт сессию requests с логикой повторных попыток и экспоненциальным замедлением.\"\"\"\n",
        "    retry_strategy = Retry(\n",
        "        total=max_retries,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        backoff_factor=backoff_factor,\n",
        "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session = requests.Session()\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    return session\n",
        "\n",
        "# Пример использования\n",
        "session = create_resilient_session()\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "try:\n",
        "    response = session.get('https://example.com/data', headers=headers, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    print(\"Успешно получен статус:\", response.status_code)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Критическая ошибка после всех попыток: {e}\")\n",
        "```\n",
        "\n",
        "Этот код создаёт HTTP-сессию, которая автоматически повторяет запросы при временных ошибках, постепенно увеличивая паузу между попытками. Такая практика является стандартом для production-скриптов.\n",
        "\n",
        "### 2.3. Построение DOM-Дерева и Навигация (BeautifulSoup)\n",
        "\n",
        "После получения HTML-контента необходимо преобразовать его из плоского текста в иерархическую структуру — дерево объектов (DOM-дерево). Библиотека BeautifulSoup4 (BS4) является наиболее популярным инструментом для этой задачи благодаря своей толерантности к невалидному HTML и интуитивно понятному API. BS4 позволяет легко находить элементы по тегам, атрибутам, текстовому содержимому и CSS-селекторам.\n",
        "\n",
        "Однако важно учитывать производительность. BS4 сама по себе является обёрткой над внешними парсерами. Наиболее эффективный выбор — использовать `lxml` в качестве бэкенда. Библиотека `lxml` основана на высокоскоростных C-библиотеках `libxml2` и `libxslt`, что делает её значительно быстрее встроенного `html.parser`, особенно при обработке больших объёмов данных. Кроме того, `lxml` поддерживает мощный язык запросов XPath, который позволяет точно навигировать по сложным и глубоко вложенным структурам. Сама BS4 не поддерживает XPath напрямую, но при использовании `lxml` как парсера можно комбинировать подходы или перейти к более продвинутым библиотекам, таким как `parsel` (используется в Scrapy).\n",
        "\n",
        "Пример парсинга с использованием BS4 и `lxml`:\n",
        "\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://example-news-site.com\"\n",
        "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0...\"})\n",
        "soup = BeautifulSoup(response.content, 'lxml')  # Используем lxml для скорости\n",
        "\n",
        "# Извлечение заголовков статей с помощью CSS-селектора\n",
        "headlines = [h.get_text(strip=True) for h in soup.select('h2.article-title')]\n",
        "print(\"Найдено заголовков:\", len(headlines))\n",
        "```\n",
        "\n",
        "В этом примере используется CSS-селектор `h2.article-title` для точного извлечения заголовков, а `lxml` обеспечивает быструю обработку даже при большом объёме HTML.\n",
        "\n",
        "### 2.4. Практика: Парсинг и Обход Пагинации\n",
        "\n",
        "Обход пагинации — одна из самых распространённых задач при сборе данных. Сайты реализуют пагинацию по-разному: через параметры URL (например, `?page=2`), смещение (`?offset=20`), или динамическую подгрузку по клику на кнопку «Далее». В случае статической пагинации процесс сводится к циклическому формированию URL и извлечению данных с каждой страницы.\n",
        "\n",
        "Ключевые шаги: сначала анализируется структура URL или HTML-кнопки перехода, затем реализуется цикл, который последовательно запрашивает каждую страницу. Важно соблюдать «дружественные» практики: добавлять задержки между запросами, использовать устойчивую сессию и обрабатывать возможные ошибки.\n",
        "\n",
        "Пример обхода пагинации:\n",
        "\n",
        "```python\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def scrape_paginated_site(base_url, total_pages):\n",
        "    all_data = []\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\"User-Agent\": \"Mozilla/5.0...\"})\n",
        "    \n",
        "    for page_num in range(1, total_pages + 1):\n",
        "        url = f\"{base_url}?page={page_num}\"\n",
        "        try:\n",
        "            response = session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            \n",
        "            # Извлечение элементов с помощью CSS-селектора\n",
        "            items = soup.select('div.product-item h3')\n",
        "            for item in items:\n",
        "                title = item.get_text(strip=True)\n",
        "                all_data.append(title)\n",
        "                \n",
        "            print(f\"Обработана страница {page_num}\")\n",
        "            time.sleep(1.5)  # Уважительная задержка\n",
        "            \n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Ошибка на странице {page_num}: {e}\")\n",
        "            break\n",
        "            \n",
        "    return all_data\n",
        "\n",
        "# Запуск сбора\n",
        "data = scrape_paginated_site(\"https://example-store.com/products\", total_pages=10)\n",
        "print(f\"Всего собрано элементов: {len(data)}\")\n",
        "```\n",
        "\n",
        "Этот код демонстрирует полный цикл: инициализация сессии, постраничный запрос, извлечение данных и уважительная задержка. Такой подход легко масштабируется и адаптируется под различные схемы пагинации.\n",
        "\n",
        "### 2.5. Резюме Части 1\n",
        "\n",
        "Инструментарий `requests` и `BeautifulSoup4` (предпочтительно с парсером `lxml`) идеально подходит для быстрого прототипирования, сбора данных с небольших статических или SSR-сайтов, а также для первичного анализа структуры веб-ресурсов. Его преимущества — простота, читаемость кода и низкий порог входа. Однако у этого подхода есть чёткие границы применимости: он является синхронным, не масштабируется для высоконагруженных задач и совершенно неспособен обрабатывать контент, генерируемый JavaScript. При переходе к промышленным объёмам, динамическим SPA или требованию высокой пропускной способности необходимо переходить к асинхронным фреймворкам, таким как Scrapy, или к решениям с полной эмуляцией браузера.\n"
      ],
      "metadata": {
        "id": "sNG32N6rFAhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Часть 2: Промышленный фреймворк для скрейпинга (Scrapy)\n",
        "\n",
        "Scrapy — это мощный, полнофункциональный фреймворк для веб-скрейпинга, написанный на Python. Он представляет собой промышленный стандарт для высоконагруженного сбора данных с сайтов, не требующих выполнения JavaScript. Благодаря своей архитектуре, Scrapy обеспечивает не только высокую производительность, но и чёткую структуру для всего ETL-цикла — от извлечения до загрузки.\n",
        "\n",
        "### 3.1. Архитектура Scrapy: Асинхронное Ядро\n",
        "\n",
        "Ключевое архитектурное преимущество Scrapy заключается в использовании асинхронного фреймворка Twisted в качестве основы. Этот подход позволяет эффективно управлять тысячами одновременных сетевых операций в одном потоке, избегая блокировок и достигая высокой скорости обхода. В отличие от синхронных решений (например, `requests`), Scrapy не ждёт завершения каждого запроса, а продолжает обрабатывать другие задачи, что делает его особенно эффективным при работе с большим числом URL.\n",
        "\n",
        "Архитектура Scrapy состоит из нескольких взаимосвязанных компонентов. Ядро (Scrapy Engine) выступает главным контроллером, координирующим обмен данными между всеми частями системы. Планировщик (Scheduler) отвечает за управление очередью запросов: он получает их от пауков, упорядочивает по приоритету и гарантирует, что одна и та же страница не будет запрошена дважды благодаря встроенному механизму дедупликации. Менеджер загрузок (Downloader) выполняет асинхронные HTTP-запросы и возвращает HTML-ответы. Пауки (Spiders) содержат логику обхода сайта и извлечения данных. Item Pipelines отвечают за обработку и сохранение структурированных данных, а Downloader Middleware позволяет вмешиваться в процесс обработки запросов и ответов на низком уровне — например, для ротации прокси или заголовков.\n",
        "\n",
        "### 3.2. Structuring ETL: Items, Pipelines и Middleware\n",
        "\n",
        "В Scrapy данные структурируются с помощью классов `scrapy.Item`. Эти контейнеры похожи на словари, но имеют строго определённые поля (`Field`), что обеспечивает типобезопасность и предсказуемость на всех этапах обработки. Например, можно явно указать, что каждый товар должен иметь `title`, `price`, `url` и `sku`.\n",
        "\n",
        "Пример определения структуры данных:\n",
        "\n",
        "```python\n",
        "# items.py\n",
        "import scrapy\n",
        "\n",
        "class ProductItem(scrapy.Item):\n",
        "    title = scrapy.Field()\n",
        "    price = scrapy.Field()\n",
        "    url = scrapy.Field()\n",
        "    sku = scrapy.Field()  # Артикул для дедупликации\n",
        "```\n",
        "\n",
        "Item Pipelines — это последовательность классов, через которые проходят все извлечённые элементы. Каждый этап конвейера выполняет определённую задачу, соответствующую фазам Transform и Load ETL-цикла. На первом этапе данные могут быть очищены от HTML-тегов, преобразованы в числовые типы или проверены на наличие обязательных полей. Невалидные элементы можно отбрасывать с помощью исключения `DropItem`. На следующем этапе реализуется дедупликация — например, по уникальному `sku` или `url`, чтобы избежать дублирования в хранилище. Наконец, данные сохраняются в выбранный формат: JSON Lines, CSV, или напрямую в базу данных с использованием SQLAlchemy или другого ORM.\n",
        "\n",
        "Downloader Middleware — это мощный механизм для настройки поведения запросов. Он позволяет реализовать ротацию заголовков `User-Agent`, автоматическую смену прокси-серверов и гибкую логику повторных попыток. Scrapy включает встроенный `RetryMiddleware`, который автоматически повторяет запросы при временных ошибках (например, HTTP 500, 503, 504). Количество попыток и коды ошибок настраиваются через параметры `RETRY_TIMES` и `RETRY_HTTP_CODES`. Неудачные запросы возвращаются в очередь с пониженным приоритетом, что обеспечивает устойчивость к временным сбоям сети или сервера.\n",
        "\n",
        "### 3.3. Создание Паука (Spider) и Обход\n",
        "\n",
        "Паук (Spider) — это сердце любого Scrapy-проекта. Он определяет, с каких URL начинать обход, как извлекать данные и как переходить по ссылкам. Scrapy использует библиотеку `parsel` (основанную на `lxml`) для навигации по HTML с помощью CSS-селекторов и XPath.\n",
        "\n",
        "Пример паука для каталога товаров:\n",
        "\n",
        "```python\n",
        "# spiders/product_spider.py\n",
        "import scrapy\n",
        "from myproject.items import ProductItem\n",
        "\n",
        "class ProductSpider(scrapy.Spider):\n",
        "    name = \"product_spider\"\n",
        "    start_urls = ['https://example-store.com/catalog']\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Извлечение карточек товаров\n",
        "        for card in response.css('div.product-card'):\n",
        "            item = ProductItem()\n",
        "            item['title'] = card.css('h2.title::text').get()\n",
        "            item['price'] = card.css('span.price::text').re_first(r'(\\d+)')\n",
        "            item['url'] = response.urljoin(card.css('a::attr(href)').get())\n",
        "            yield item\n",
        "\n",
        "        # Переход на следующую страницу пагинации\n",
        "        next_page = response.css('a.pagination-next::attr(href)').get()\n",
        "        if next_page:\n",
        "            yield response.follow(next_page, self.parse)\n",
        "```\n",
        "\n",
        "Этот код демонстрирует три ключевых паттерна Scrapy: извлечение данных с текущей страницы, генерация структурированного элемента и рекурсивный обход по ссылкам. Все запросы обрабатываются асинхронно, а дубликаты URL автоматически фильтруются планировщиком.\n",
        "\n",
        "### 3.4. Масштабирование и Распределённый Скрейпинг\n",
        "\n",
        "При сборе данных в промышленных масштабах (миллионы и миллиарды страниц) одного сервера недостаточно. Для горизонтального масштабирования используется расширение **Scrapy-Redis**. Оно заменяет локальный планировщик Scrapy на распределённый, использующий Redis в качестве централизованного хранилища состояния.\n",
        "\n",
        "В такой архитектуре все рабочие узлы (воркеры) подключаются к одному экземпляру Redis. Запросы, сгенерированные любым пауком, помещаются в общую очередь, откуда их может взять любой свободный узел. Глобальный механизм дедупликации на основе отпечатков (fingerprints) хранится в Redis, что гарантирует, что одна и та же страница не будет обработана дважды, даже если генерация запроса происходила на разных машинах. При сбое одного из воркеров его задачи автоматически перераспределяются, что обеспечивает отказоустойчивость.\n",
        "\n",
        "Для обхода сайтов с динамическим контентом Scrapy можно интегрировать с инструментами рендеринга JavaScript. Например, **Scrapy-Playwright** или **Scrapy-Splash** работают как специальные Downloader Middleware: они перехватывают запросы, требующие выполнения JavaScript, отправляют их во внешний браузерный движок, а затем возвращают полностью отрендеренный HTML в паук для обычного парсинга. Это позволяет сохранить преимущества асинхронной архитектуры Scrapy даже при работе с SPA.\n",
        "\n",
        "Для команд, не желающих заниматься инфраструктурой, существуют облачные платформы, такие как **Zyte** (ранее Scrapy Cloud). Они предоставляют управляемую среду для деплоя, мониторинга и автоматического масштабирования пауков, а также включают встроенные инструменты для обхода антибот-систем, ротации прокси и решения CAPTCHA.\n",
        "\n",
        "### 3.5. Резюме Части 2\n",
        "\n",
        "Scrapy — это зрелый, масштабируемый фреймворк, идеально подходящий для крупномасштабного сбора данных со статических и SSR-сайтов. Его архитектура обеспечивает высокую производительность, а модульная структура — чёткое разделение ответственности между этапами ETL. Однако он не предназначен для сайтов, где основной контент полностью генерируется JavaScript на клиенте. В таких случаях требуется полная эмуляция браузера, что выводит нас за пределы возможностей классического Scrapy.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 3: Автоматизация браузера для сложного JavaScript (Selenium)\n",
        "\n",
        "Когда сайт построен по архитектуре Single Page Application (SPA), традиционный HTTP-скрейпинг теряет смысл: сервер возвращает только пустой HTML-каркас и JavaScript-файлы, а весь контент формируется в браузере. Для извлечения данных с таких ресурсов требуется эмуляция поведения реального пользователя — именно эту задачу решает **Selenium WebDriver**.\n",
        "\n",
        "### 4.1. Теория: Принципы работы WebDriver\n",
        "\n",
        "Selenium использует протокол W3C WebDriver для взаимодействия между кодом на Python и физическим браузером (например, Chrome или Firefox). Архитектура состоит из трёх уровней: скрипт на Python → драйвер браузера (например, `chromedriver`) → сам браузер. Все команды передаются через HTTP-запросы, что делает архитектуру универсальной, но вносит задержки. Важное отличие от статического парсинга: Selenium не просто получает HTML — он запускает полноценный браузер, выполняет весь JavaScript, загружает ресурсы и рендерит DOM, как это сделал бы человек.\n",
        "\n",
        "### 4.2. Практика: Настройка и Управление\n",
        "\n",
        "Работа с Selenium требует установки соответствующего драйвера для выбранного браузера. Для упрощения управления рекомендуется использовать менеджеры драйверов, такие как `webdriver-manager`, которые автоматически скачивают нужную версию.\n",
        "\n",
        "Одной из главных сложностей при работе с динамическим контентом является **синхронизация**. Selenium не может автоматически определить, когда JavaScript завершил рендеринг или когда AJAX-запрос вернул данные. Попытка взаимодействовать с элементом до его появления приведёт к исключению. Для решения этой проблемы используются **явные ожидания** (Explicit Waits) через класс `WebDriverWait`.\n",
        "\n",
        "Пример безопасного ожидания динамического элемента:\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example.com/dynamic-content\")\n",
        "\n",
        "try:\n",
        "    # Ожидание появления элемента с ID 'result' в течение 10 секунд\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.ID, \"result\"))\n",
        "    )\n",
        "    print(\"Данные загружены:\", element.text)\n",
        "except Exception as e:\n",
        "    print(\"Элемент не появился вовремя:\", e)\n",
        "finally:\n",
        "    driver.quit()\n",
        "```\n",
        "\n",
        "Этот подход делает скрипты надёжными: вместо фиксированных задержек (`time.sleep()`) код ждёт именно нужного состояния страницы.\n",
        "\n",
        "### 4.3. Реализация Сложных Сценариев\n",
        "\n",
        "Selenium позволяет эмулировать действия пользователя: кликать по кнопкам, вводить текст в формы, прокручивать страницу и даже загружать файлы. Это критически важно для скрейпинга сайтов с ленивой загрузкой контента или многошаговыми формами.\n",
        "\n",
        "Например, для загрузки скрытых товаров в интернет-магазине можно прокручивать страницу вниз до тех пор, пока не перестанут появляться новые элементы:\n",
        "\n",
        "```python\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "```\n",
        "\n",
        "Для отладки сложных сценариев полезно сохранять скриншоты или HTML-код страницы в момент ошибки — это помогает понять, на каком этапе скрипт отклонился от ожидаемого поведения.\n",
        "\n",
        "\n",
        "### 4.4. Обход CAPTCHA\n",
        "\n",
        "Столкновение с системами CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) — одна из наиболее частых и сложных проблем при автоматизации веб-скрейпинга. Современные реализации, такие как **reCAPTCHA v2/v3** от Google или **hCaptcha**, интегрированы в форму отправки данных и активируются при подозрении на автоматизированное поведение. Хотя полностью обойти CAPTCHA без внешней помощи невозможно, её можно интегрировать в автоматизированный workflow с использованием специализированных сервисов распознавания.\n",
        "\n",
        "Общий сценарий обхода CAPTCHA состоит из нескольких этапов:  \n",
        "— сначала скрипт должен обнаружить наличие CAPTCHA на странице;  \n",
        "— затем извлечь её идентификаторы и параметры (в первую очередь `sitekey`);  \n",
        "— передать эти данные в сторонний сервис решения (например, 2Captcha, Anti-Captcha или CapMonster);  \n",
        "— дождаться получения токена-ответа;  \n",
        "— ввести этот токен в скрытое поле формы;  \n",
        "— и только после этого выполнить отправку.\n",
        "\n",
        "Рассмотрим каждый шаг на примере обхода **reCAPTCHA v2** с использованием Selenium и сервиса **2Captcha**.\n",
        "\n",
        "#### Шаг 1: Обнаружение CAPTCHA\n",
        "\n",
        "Скрипт должен уметь определять, появилась ли CAPTCHA. Для reCAPTCHA это делается через поиск iframe с определённым идентификатором или проверку наличия скрытого поля с атрибутом `data-sitekey`.\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example-form-with-recaptcha.com\")\n",
        "\n",
        "# Ожидание появления reCAPTCHA на странице\n",
        "try:\n",
        "    captcha_iframe = WebDriverWait(driver, 5).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[src*='recaptcha']\"))\n",
        "    )\n",
        "    print(\"Обнаружена reCAPTCHA v2.\")\n",
        "    has_captcha = True\n",
        "except:\n",
        "    has_captcha = False\n",
        "    print(\"CAPTCHA не обнаружена.\")\n",
        "```\n",
        "\n",
        "#### Шаг 2: Извлечение параметров\n",
        "\n",
        "Если CAPTCHA обнаружена, необходимо извлечь её ключ — `sitekey`. Он обычно содержится в атрибуте `data-sitekey` у самого элемента `<div class=\"g-recaptcha\">` или в URL iframe.\n",
        "\n",
        "```python\n",
        "if has_captcha:\n",
        "    # Извлечение sitekey из DOM\n",
        "    recaptcha_div = driver.find_element(By.CSS_SELECTOR, \"div.g-recaptcha\")\n",
        "    sitekey = recaptcha_div.get_attribute(\"data-sitekey\")\n",
        "    page_url = driver.current_url\n",
        "    print(f\"Извлечён sitekey: {sitekey}\")\n",
        "```\n",
        "\n",
        "#### Шаг 3: Отправка задачи на решение\n",
        "\n",
        "Сервисы вроде 2Captcha предоставляют REST API для решения CAPTCHA. Для reCAPTCHA v2 требуется отправить POST-запрос с вашим API-ключом, `sitekey` и URL страницы.\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import time\n",
        "\n",
        "API_KEY = \"ваш_ключ_2captcha\"\n",
        "CAPTCHA_METHOD = \"userrecaptcha\"\n",
        "\n",
        "# Отправка задачи на решение\n",
        "task_resp = requests.post(\"http://2captcha.com/in.php\", data={\n",
        "    'key': API_KEY,\n",
        "    'method': CAPTCHA_METHOD,\n",
        "    'googlekey': sitekey,\n",
        "    'pageurl': page_url,\n",
        "    'json': 1\n",
        "})\n",
        "\n",
        "task_data = task_resp.json()\n",
        "if task_data.get(\"status\") == 1:\n",
        "    captcha_id = task_data[\"request\"]\n",
        "    print(f\"Задача отправлена, ID: {captcha_id}\")\n",
        "else:\n",
        "    raise Exception(f\"Ошибка создания задачи: {task_data.get('request')}\")\n",
        "\n",
        "# Ожидание результата (обычно 10–30 секунд)\n",
        "while True:\n",
        "    time.sleep(5)\n",
        "    result_resp = requests.get(\n",
        "        f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "    )\n",
        "    result_data = result_resp.json()\n",
        "    if result_data.get(\"status\") == 1:\n",
        "        captcha_token = result_data[\"request\"]\n",
        "        print(\"Токен CAPTCHA получен.\")\n",
        "        break\n",
        "    elif result_data[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "        continue\n",
        "    else:\n",
        "        raise Exception(f\"Ошибка получения результата: {result_data['request']}\")\n",
        "```\n",
        "\n",
        "#### Шаг 4: Вставка токена и отправка формы\n",
        "\n",
        "reCAPTCHA v2 ожидает, что токен будет помещён в скрытое поле формы с именем `g-recaptcha-response`. Иногда это поле изначально отсутствует и создаётся динамически — в таком случае его нужно вставить в DOM вручную.\n",
        "\n",
        "```python\n",
        "# Найти или создать скрытое поле для токена\n",
        "try:\n",
        "    token_field = driver.find_element(By.NAME, \"g-recaptcha-response\")\n",
        "except:\n",
        "    # Если поле не существует, создаём его\n",
        "    driver.execute_script(\"\"\"\n",
        "        var response = document.createElement('textarea');\n",
        "        response.name = 'g-recaptcha-response';\n",
        "        response.style.display = 'none';\n",
        "        document.querySelector('form').appendChild(response);\n",
        "    \"\"\")\n",
        "    token_field = driver.find_element(By.NAME, \"g-recaptcha-response\")\n",
        "\n",
        "# Вставить токен\n",
        "driver.execute_script(\"arguments[0].value = arguments[1];\", token_field, captcha_token)\n",
        "\n",
        "# Отправить форму\n",
        "submit_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n",
        "submit_button.click()\n",
        "\n",
        "print(\"Форма отправлена с решённой CAPTCHA.\")\n",
        "```\n",
        "\n",
        "#### Альтернатива: Ручной ввод\n",
        "\n",
        "В исследовательских или низкочастотных сценариях можно приостановить выполнение и запросить у оператора ввод решения вручную. Это особенно полезно при отладке или при работе с дорогостоящими CAPTCHA.\n",
        "\n",
        "```python\n",
        "if has_captcha:\n",
        "    input(\"CAPTCHA обнаружена. Пройдите проверку вручную и нажмите Enter...\")\n",
        "    # После ввода оператором скрипт продолжает выполнение\n",
        "```\n",
        "\n",
        "Такой подход не масштабируем, но исключительно надёжен и не требует финансовых затрат.\n",
        "\n",
        "#### Важные нюансы\n",
        "\n",
        "Стоимость решения одной reCAPTCHA v2 через 2Captcha составляет около \\$0.5–\\$1 за 1000 решений, что делает такой подход экономически оправданным только при высокой ценности данных. Для reCAPTCHA v3, которая не требует визуального взаимодействия, сервисы имитируют поведенческий профиль и возвращают оценку `score` в виде токена. Кроме того, некоторые сайты используют «невидимую» CAPTCHA, которая срабатывает фоново — в таких случаях необходимо эмулировать поведение пользователя (движения мыши, задержки) до отправки формы, иначе токен может быть отклонён.\n",
        "\n",
        "Таким образом, обход CAPTCHA — это не обход в прямом смысле, а **делегирование** задачи распознавания человеку или машинному сервису с последующей интеграцией ответа в автоматизированный процесс. Это требует тщательной обработки ошибок, управления временем ожидания и соблюдения этических и юридических норм при использовании внешних сервисов.\n",
        "\n",
        "\n",
        "### 4.5. Резюме Части 3\n",
        "\n",
        "Selenium — мощный инструмент для работы с динамическими, JavaScript-интенсивными сайтами. Он обеспечивает полную эмуляцию поведения пользователя, что делает его незаменимым для сложных сценариев. Однако его архитектура на основе HTTP-коммуникации с внешним браузером приводит к высокой ресурсоёмкости, низкой скорости и потенциальной нестабильности. Эти недостатки делают Selenium менее подходящим для высокоскоростного, массового скрейпинга, где предпочтение отдаётся более лёгким и управляемым решениям, таким как Playwright или Puppeteer в headless-режиме.\n"
      ],
      "metadata": {
        "id": "_m23d5ZYGPXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Часть 4: Современный подход к браузерной автоматизации (Playwright)\n",
        "\n",
        "Playwright — это современный фреймворк для автоматизации браузеров, разработанный Microsoft и быстро ставший промышленным стандартом для скрейпинга динамических веб-приложений. В отличие от устаревших решений, Playwright устраняет ключевые архитектурные недостатки Selenium и обеспечивает высокую производительность, надёжность и удобство разработки.\n",
        "\n",
        "### 5.1. Теория: Архитектурные Преимущества\n",
        "\n",
        "Фундаментальное отличие Playwright заключается в способе взаимодействия с браузерными движками. В то время как Selenium опирается на HTTP-протокол и промежуточные драйверы (например, `chromedriver`), Playwright устанавливает прямое, постоянное соединение с ядром браузера через WebSocket. Такой подход обеспечивает нативный контроль над Chromium, Firefox и WebKit, минимизируя задержки и накладные расходы на сериализацию запросов. Это не просто архитектурное улучшение — это кардинальное повышение эффективности.\n",
        "\n",
        "Одной из самых значимых инноваций Playwright является механизм **автоматического ожидания** (Auto-Wait). Перед выполнением любого действия — клика, ввода текста, извлечения содержимого — Playwright автоматически проверяет, что целевой элемент присутствует в DOM, видим, стабилен и готов к взаимодействию. Это устраняет необходимость вручную настраивать сложные условия ожидания, как это требуется в Selenium, где разработчик должен явно указывать, на что именно стоит ждать (`visibility_of_element_located`, `element_to_be_clickable` и т.д.). В результате код становится короче, чище и устойчивее к колебаниям времени загрузки страницы.\n",
        "\n",
        "### 5.2. Практика Playwright: Скорость и Эффективность\n",
        "\n",
        "Playwright предоставляет унифицированный и лаконичный API как для синхронного, так и для асинхронного использования. Он поддерживает headless-режим по умолчанию, что делает его идеальным для автоматизированных задач сбора данных.\n",
        "\n",
        "Пример базового скрипта для извлечения динамического контента:\n",
        "\n",
        "```python\n",
        "from playwright.sync_api import sync_playwright\n",
        "\n",
        "def scrape_dynamic_data(url):\n",
        "    with sync_playwright() as p:\n",
        "        browser = p.chromium.launch(headless=True)\n",
        "        page = browser.new_page()\n",
        "        page.goto(url)\n",
        "        \n",
        "        # Автоматическое ожидание появления элемента\n",
        "        page.wait_for_selector(\".dynamic-content\", state=\"visible\")\n",
        "        \n",
        "        # Взаимодействие с элементом\n",
        "        page.click(\"button#load-more\")\n",
        "        \n",
        "        # Извлечение текста из всех элементов с классом .result-item\n",
        "        data = page.locator(\".result-item\").all_text_contents()\n",
        "        \n",
        "        browser.close()\n",
        "        return data\n",
        "```\n",
        "\n",
        "Этот код демонстрирует ключевые преимущества Playwright: отсутствие явных ожиданий, интуитивный синтаксис (`page.click`, `page.locator`) и встроенную поддержку headless-режима. Playwright также позволяет легко эмулировать различные условия — например, мобильные устройства или конкретные геолокации — через создание контекстов с заданными параметрами. Это особенно полезно при сборе данных, которые варьируются в зависимости от региона или типа устройства.\n",
        "\n",
        "### 5.3. Перехват и Модификация Сетевых Запросов (Оптимизация)\n",
        "\n",
        "Одной из самых мощных функций Playwright является возможность перехвата и модификации всего сетевого трафика. Это открывает путь к радикальной оптимизации производительности при работе с SPA-приложениями.\n",
        "\n",
        "При загрузке типичного современного сайта до 80% времени и ресурсов уходит на получение ненужных для скрейпинга ресурсов: изображений, шрифтов, рекламных скриптов, аналитики и метрик. Playwright позволяет блокировать такие запросы на лету, что значительно сокращает время загрузки, потребление памяти и сетевой трафик.\n",
        "\n",
        "Пример блокировки ненужных ресурсов:\n",
        "\n",
        "```python\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def run_scraper_optimized(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch()\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Блокировка изображений и шрифтов\n",
        "        await page.route(\n",
        "            \"**/*.{png,jpg,jpeg,gif,webp,woff,woff2,ttf,eot}\",\n",
        "            lambda route: route.abort()\n",
        "        )\n",
        "\n",
        "        await page.goto(url)\n",
        "        content = await page.locator(\"div.main-content\").inner_text()\n",
        "        await browser.close()\n",
        "        return content\n",
        "```\n",
        "\n",
        "Кроме блокировки, перехват запросов позволяет получать данные напрямую из AJAX-вызовов. Например, если SPA загружает данные через `fetch()` в формате JSON, можно перехватить этот ответ и извлечь структурированные данные, минуя рендеринг DOM и парсинг HTML. Это не только быстрее, но и надёжнее, поскольку структура JSON-ответа обычно стабильнее, чем разметка страницы.\n",
        "\n",
        "### 5.4. Организация Параллельного Выполнения\n",
        "\n",
        "Playwright эффективно использует системные ресурсы, позволяя создавать множество изолированных браузерных контекстов в рамках одного процесса. Каждый контекст представляет собой независимую сессию с собственными куками, локальным хранилищем и настройками. Это обеспечивает высокую степень параллелизации без необходимости запускать отдельный экземпляр браузера на каждый запрос, как это часто делают в Selenium. В результате пропускная способность скрейпера возрастает на порядки.\n",
        "\n",
        "### 5.5. Резюме Части 4\n",
        "\n",
        "Playwright представляет собой современное, архитектурно превосходящее решение для скрейпинга динамических сайтов. Его нативное взаимодействие с браузерными движками, автоматические ожидания, возможность перехвата и фильтрации сетевых запросов, а также поддержка эффективной параллелизации делают его наиболее производительным и надёжным инструментом для работы со сложными SPA. В промышленной практике Playwright постепенно вытесняет Selenium как основной выбор для задач, требующих выполнения JavaScript.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 5: Продвинутые техники обхода ограничений\n",
        "\n",
        "Современные веб-сайты активно защищаются от автоматизированного сбора данных, используя многоуровневые системы обнаружения ботов. Это требует от промышленных скрейперов применения адаптивных, многослойных стратегий защиты и обхода.\n",
        "\n",
        "### 6.1. Теория: Эволюция Систем Защиты\n",
        "\n",
        "Современные решения, такие как Cloudflare, Akamai или PerimeterX, используют комплексные эвристики для идентификации нечеловеческого трафика. Основные методы включают ограничение скорости запросов с одного IP-адреса (Rate Limiting), что приводит к ответам с кодом 429 «Too Many Requests». Более изощрённые системы применяют **фингерпринтинг** — создание уникального «отпечатка» браузера на основе сотен параметров: версии движка, списка плагинов, характеристик Canvas и WebGL, поведения при рендеринге и даже временных задержек в JavaScript. Отдельное внимание уделяется обнаружению признаков автоматизации: во многих браузерных движках, управляемых через WebDriver, устанавливается скрытое свойство `window.webdriver`, которое легко детектируется. Наконец, системы могут анализировать поведение пользователя: неестественно быстрый скроллинг, отсутствие движений мыши, идеально точные клики — всё это может быть признаком бота.\n",
        "\n",
        "### 6.2. Построение Надёжной Системы Ротации\n",
        "\n",
        "Для успешного обхода защит требуется динамическая смена идентификационных данных. Ключевой компонент — это ротация HTTP-заголовков, в первую очередь `User-Agent`. Использование библиотек вроде `fake-useragent` позволяет генерировать реалистичные, постоянно обновляемые строки, имитирующие популярные браузеры и устройства. Ещё важнее — управление IP-адресами. Датацентровые прокси, хотя и дешевы, легко блокируются, так как их IP-адреса принадлежат известным диапазонам центров обработки данных. В то же время резидентские прокси, использующие IP-адреса реальных домашних или мобильных пользователей, значительно эффективнее обходят современные системы защиты. В Scrapy ротация прокси и заголовков реализуется через Downloader Middleware, а в Playwright — через параметры при создании нового контекста (`browser.new_context(proxy=...)`).\n",
        "\n",
        "### 6.3. Маскировка Автоматизации (Stealth Techniques)\n",
        "\n",
        "Даже при использовании резидентских прокси и реалистичных заголовков автоматизированный браузер может выдать себя через специфические JavaScript-свойства. Для решения этой проблемы применяются **stealth-техники** — инъекция скриптов, которые модифицируют или удаляют признаки WebDriver до загрузки целевой страницы. Например, можно переопределить `navigator.webdriver`, подменить WebGL-рендерер, скрыть автоматическое разрешение экрана и многое другое. В экосистеме Playwright существуют специализированные библиотеки, такие как `playwright-stealth`, которые автоматически применяют десятки проверенных модификаций, делая автоматизированный браузер практически неотличимым от обычного.\n",
        "\n",
        "### 6.4. Многоуровневый Алгоритм Реагирования на Блокировки\n",
        "\n",
        "Для обеспечения промышленной устойчивости необходимо внедрить иерархическую систему реагирования на ошибки. На первом уровне — временные сбои (HTTP 429, 5xx) — применяется стратегия экспоненциального замедления и повторная попытка. Если повторный запрос также завершается ошибкой, система переходит ко второму уровню: выполняется ротация заголовков и, при необходимости, смена cookies. На третьем уровне, при устойчивых блокировках (HTTP 403 Forbidden), запрос перенаправляется через новый IP-адрес из пула резидентских прокси. Наконец, если сайт выдаёт CAPTCHA, запрос автоматически передаётся в сторонний сервис решения (например, 2Captcha), и полученный токен вводится в форму через тот же Playwright. Такой многоступенчатый подход позволяет поддерживать высокий уровень успешности даже при работе с наиболее защищёнными ресурсами.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть 6: Инструменты и правовые аспекты промышленного скрейпинга\n",
        "\n",
        "### 7.1. Сравнительный Анализ Производительности Парсеров\n",
        "\n",
        "Выбор парсера напрямую влияет на производительность промышленного скрейпинга. Библиотека `lxml`, основанная на высокоскоростных C-библиотеках `libxml2` и `libxslt`, является безусловным лидером по скорости парсинга HTML и XML. Она поддерживает мощный язык XPath, что делает её незаменимой для навигации по сложным структурам. `Parsel` — это обёртка над `lxml`, используемая в Scrapy для унификации селекторов; она сохраняет всю производительность `lxml`, добавляя поддержку CSS-селекторов. В отличие от этого, `BeautifulSoup4`, хотя и превосходит конкурентов в устойчивости к невалидному HTML, значительно уступает в скорости и рекомендуется только для прототипирования или небольших задач, где важна простота кода, а не пропускная способность.\n",
        "\n",
        "### 7.2. Алгоритм Принятия Решения: Scraping vs Official API\n",
        "\n",
        "Стратегический выбор между использованием официального API и разработкой собственного скрейпера должен основываться на комплексной оценке. Официальный API всегда предпочтителен: он предоставляет структурированные, стабильные данные, минимизирует юридические риски и не требует постоянного сопровождения из-за изменений в DOM-структуре. Скрейпинг оправдан только в ситуациях, когда API отсутствует, непомерно дорог, накладывает жёсткие ограничения на объём или частоту запросов, либо не предоставляет доступ к историческим данным, необходимым для анализа. Таким образом, скрейпинг — это вынужденная мера, а не предпочтительный путь.\n",
        "\n",
        "### 7.3. Юридические Прецеденты и Практика Соблюдения\n",
        "\n",
        "Хотя судебная практика в некоторых юрисдикциях (например, в США по делу *hiQ Labs v. LinkedIn*) подтверждает право на сбор публично доступных данных, это не отменяет обязательств перед условиями предоставления услуг (ToS) и требованиями регуляторов. Соблюдение файла `robots.txt` остаётся минимальным условием «дружественного» скрейпинга. При работе с любыми данными, которые могут быть связаны с физическим лицом — даже если они публичны, как профили в соцсетях — необходимо учитывать требования GDPR. Это включает разработку внутренних процедур на случай получения запроса на удаление персональной информации («право на забвение»).\n",
        "\n",
        "### 7.4. Обзор Управляемых Инструментов\n",
        "\n",
        "Сложность современных систем защиты привела к росту популярности управляемых решений. **Zyte API** (ранее Scrapy Cloud) предлагает не только платформу для деплоя и мониторинга пауков, но и функции обхода блокировок как услугу: автоматическая ротация прокси, решение CAPTCHA, защита от фингерпринтинга — всё это скрыто за простым HTTP-интерфейсом. Это позволяет разработчикам сосредоточиться исключительно на логике извлечения данных. Для менее требовательных задач может подойти **requests-html** — лёгкая библиотека, сочетающая `requests` и `lxml` с ограниченной возможностью рендеринга JavaScript через headless-браузер. Она полезна для случаев, где требуется минимальное выполнение скриптов без перехода к полной архитектуре Playwright или Scrapy.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение: Скрейпинг как Архитектурный Вызов\n",
        "\n",
        "Эффективный и надёжный веб-скрейпинг в Python — это, прежде всего, архитектурная задача. Выбор инструментария должен определяться двумя ключевыми факторами: природой целевого сайта и требуемым масштабом операции.\n",
        "\n",
        "Если сайт состоит из статического или серверно-рендеренного контента и объём данных невелик, оптимальным выбором будет связка `requests` и `BeautifulSoup4` с парсером `lxml`. Для промышленного сбора с таких ресурсов следует использовать **Scrapy** — его асинхронная архитектура, система Item Pipelines и поддержка распределённого выполнения через **Scrapy-Redis** обеспечивают надёжность и масштабируемость. В случае с динамическими SPA-приложениями, где контент формируется на клиенте, требуется полная эмуляция браузера, и здесь **Playwright** становится предпочтительным решением благодаря своей скорости, автоматическим ожиданиям и мощным инструментам оптимизации.\n",
        "\n",
        "Будущее веб-скрейпинга лежит в области высокопроизводительной браузерной автоматизации и интеграции с управляемыми API, которые берут на себя всю сложность обхода постоянно эволюционирующих систем защиты. Инженер данных должен не просто уметь писать парсеры, но и понимать, как устроены современные веб-приложения, как работают системы антибот-защиты и как проектировать устойчивые, этичные и масштабируемые архитектуры сбора данных.\n",
        "\n"
      ],
      "metadata": {
        "id": "L9NKBmWQG3Rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 7: Библиотека Matplotlib — основы построения научной визуализации\n",
        "\n",
        "### Раздел 1: Архитектура Matplotlib и Приоритет OO-Стиля для Научной Визуализации\n",
        "\n",
        "Matplotlib является фундаментальной библиотекой Python для создания статических, анимированных и интерактивных визуализаций данных. Для исследователей, стремящихся к высокой степени контроля, воспроизводимости и стандартизации своих графиков для публикации, необходимо глубокое понимание внутренней архитектуры библиотеки. Это понимание позволяет перейти от быстрого прототипирования к созданию изображений публикационного качества.\n",
        "\n",
        "#### 1.1. Фундаментальная Объектная Иерархия (The Anatomy of a Plot)\n",
        "\n",
        "Архитектура Matplotlib строится на чёткой иерархии объектов, что критически важно для эффективного использования объектно-ориентированного (OO) API. В основе этой иерархии лежат три ключевых компонента: **Figure**, **Axes** и **Artist**.\n",
        "\n",
        "**Figure** представляет собой самый верхний контейнер — «холст», на котором размещаются все элементы визуализации. Он управляет дочерними объектами Axes, а также глобальными элементами, такими как общий заголовок (`fig.suptitle`), легенда на уровне всей фигуры или цветовая шкала (`fig.colorbar`). Типичный способ создания фигуры — вызов `fig = plt.figure()` для пустого холста или использование функции `plt.subplots()`, которая одновременно создаёт Figure и один или несколько объектов Axes.\n",
        "\n",
        "**Axes** — это не отдельная ось, а именно система координат, в которой отображаются данные. Именно на уровне Axes выполняется подавляющее большинство операций: построение линий, гистограмм, облаков точек, добавление подписей и легенд. Один Figure может содержать несколько Axes (например, в случае многопанельных графиков), и каждый из них полностью независим: имеет собственные оси, данные и оформление.\n",
        "\n",
        "**Artist** — это самая общая концепция в архитектуре Matplotlib. Любой видимый элемент на графике — линия, текст, изображение, метка, тик, сам Axes или даже Figure — является объектом Artist. OO-стиль работы с Matplotlib состоит в том, чтобы вызывать методы этих Artist-объектов для точной настройки их внешнего вида.\n",
        "\n",
        "#### 1.2. Сравнение Двух Интерфейсов: Pyplot vs. Object-Oriented (OO) API\n",
        "\n",
        "Matplotlib предоставляет два основных способа взаимодействия с графиками: **Pyplot API** и **Object-Oriented API**.\n",
        "\n",
        "**Pyplot API** (модуль `matplotlib.pyplot`, обычно импортируемый как `plt`) работает через механизм неявного состояния. Функции вроде `plt.plot()`, `plt.title()` или `plt.xlabel()` автоматически создают и управляют текущими объектами Figure и Axes «за кулисами». Пользователь не ссылается на эти объекты напрямую. Такой подход удобен для быстрого интерактивного анализа в Jupyter Notebook или при создании простых графиков в несколько строк кода.\n",
        "\n",
        "**Object-Oriented API** требует явного создания объектов Figure и Axes, например, через `fig, ax = plt.subplots()`. Все последующие действия — построение данных, настройка подписей, добавление легенд — выполняются через вызов методов этих объектов: `ax.plot()`, `ax.set_title()`, `fig.colorbar()`. Этот подход не полагается на глобальное состояние и предоставляет полный контроль над каждым элементом визуализации.\n",
        "\n",
        "Важно понимать, что Pyplot API на самом деле является обёрткой над OO-интерфейсом. Например, вызов `plt.plot(x, y)` эквивалентен последовательности `ax = plt.gca(); ax.plot(x, y)`. Аналогично, `plt.title()` преобразуется в `plt.gca().set_title()`. Таким образом, OO-стиль — это не альтернатива, а основа, на которой построен весь Matplotlib.\n",
        "\n",
        "#### 1.3. Ключевой Вывод для Научной Визуализации: Приоритет OO-Стиля\n",
        "\n",
        "Для создания сложных многопанельных графиков, написания функций, предназначенных для повторного использования в рамках крупного проекта, и обеспечения максимальной воспроизводимости в научных публикациях настоятельно рекомендуется использовать OO-стиль. Явное управление объектами `fig` и `ax` устраняет зависимость от внутреннего «текущего состояния» Matplotlib, которое может вести себя непредсказуемо при создании множества фигур в цикле или в асинхронной среде. OO-стиль обеспечивает чёткость, модульность и предсказуемость кода — качества, без которых невозможно строить надёжные научные pipeline’ы.\n",
        "\n",
        "#### 1.4. Практика: Использование `plt.subplots()` как Вход в OO-Мир\n",
        "\n",
        "Наиболее идиоматическим способом начать работу в OO-стиле является функция `plt.subplots()`. Для одиночного графика она возвращает кортеж `(fig, ax)`, где `fig` — объект Figure, а `ax` — единственный объект Axes. Для многопанельного макета вызов `fig, axs = plt.subplots(nrows=2, ncols=3)` создаёт фигуру и двумерный массив `axs` из шести объектов Axes, каждый из которых можно настраивать независимо.\n",
        "\n",
        "Пример простого графика в OO-стиле:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.sin(x)\n",
        "\n",
        "# Создание фигуры и осей в OO-стиле\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "# Построение данных\n",
        "ax.plot(x, y, color='steelblue', linewidth=2, label='sin(x)')\n",
        "\n",
        "# Настройка элементов графика\n",
        "ax.set_xlabel('Время (с)', fontsize=12)\n",
        "ax.set_ylabel('Амплитуда', fontsize=12)\n",
        "ax.set_title('Гармоническое колебание', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Отображение\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код демонстрирует ключевые принципы OO-стиля: явное создание `fig` и `ax`, вызов методов на `ax` для построения и настройки, и полный контроль над каждым аспектом визуализации.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 2: Базовые Инструменты Научной Визуализации в OO-Стиле\n",
        "\n",
        "Научная визуализация требует инструментов, способных точно представлять зависимости, распределения данных и сопутствующую им неопределённость. Все эти построения в OO-стиле осуществляются через методы, вызываемые на объекте Axes.\n",
        "\n",
        "#### 2.1. Отображение Зависимостей: Линейные и Точечные Графики\n",
        "\n",
        "Линейные и точечные графики — основа для демонстрации взаимосвязей между переменными. Метод `ax.plot()` используется для отображения функциональных зависимостей, временных рядов или любых упорядоченных данных. Он позволяет настраивать цвет (`color`), стиль линии (`linestyle`), ширину (`linewidth`) и маркеры (`marker`), что делает его гибким инструментом для отображения нескольких наборов данных на одном графике.\n",
        "\n",
        "Метод `ax.scatter()` предназначен для визуализации парных распределений. Он особенно полезен при анализе корреляций, выявлении кластеров и обнаружении выбросов. При работе с большими объёмами данных ключевым параметром становится `alpha` — прозрачность точек. Низкое значение `alpha` (например, 0.3) позволяет визуально выделить области с высокой плотностью точек, тогда как перекрывающиеся точки в стандартном режиме (`alpha=1`) создают «тёмные пятна», искажающие восприятие.\n",
        "\n",
        "Пример сравнения линейного и точечного графиков:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Синтетические данные с шумом\n",
        "x = np.linspace(0, 4, 100)\n",
        "y_true = np.exp(-x) * np.cos(2 * np.pi * x)\n",
        "y_obs = y_true + 0.1 * np.random.randn(len(x))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Линейный график\n",
        "ax1.plot(x, y_true, 'k--', label='Истинная модель')\n",
        "ax1.plot(x, y_obs, 'o', color='crimson', markersize=3, alpha=0.6, label='Наблюдения')\n",
        "ax1.set_title('Линейный + точечный (модель vs данные)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Чистый точечный график с прозрачностью\n",
        "ax2.scatter(x, y_obs, c=y_obs, cmap='viridis', alpha=0.6, edgecolors='none')\n",
        "ax2.set_title('Точечный график с прозрачностью')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот пример иллюстрирует, как `alpha` и цветовая кодировка помогают раскрыть структуру данных, которая была бы скрыта при стандартном отображении.\n",
        "\n",
        "#### 2.2. Визуализация Распределений и Статистики\n",
        "\n",
        "Для анализа формы распределения Matplotlib предоставляет несколько взаимодополняющих инструментов. Метод `ax.hist()` строит гистограмму — базовый способ визуализации частотного распределения. Важно выбирать адекватное количество бинов (`bins`), так как слишком мелкое или грубое разбиение может исказить представление о данных.\n",
        "\n",
        "Метод `ax.boxplot()` создаёт «ящик с усами» — компактное изображение, отражающее пять ключевых статистик: минимум, первый квартиль (Q1), медиану, третий квартиль (Q3) и максимум (с исключением выбросов). Box plot идеален для сравнения распределений между группами, но скрывает детали формы распределения.\n",
        "\n",
        "Более информативной альтернативой является **скрипичный график** (`ax.violinplot()`), который отображает ядерную оценку плотности распределения (KDE). Он сохраняет все преимущества box plot, но дополнительно показывает, является ли распределение унимодальным, бимодальным или скошенным. Для научных публикаций, где форма распределения имеет значение (например, при проверке нормальности остатков), скрипичный график часто предпочтительнее.\n",
        "\n",
        "Пример сравнения box plot и violin plot:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация синтетических выборок\n",
        "np.random.seed(42)\n",
        "data_A = np.random.normal(0, 1, 200)\n",
        "data_B = np.concatenate([np.random.normal(-2, 0.8, 100), np.random.normal(2, 0.8, 100)])  # бимодальное\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Box plots\n",
        "ax1.boxplot([data_A, data_B], labels=['Выборка A', 'Выборка B'])\n",
        "ax1.set_title('Box Plot')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Violin plots\n",
        "ax2.violinplot([data_A, data_B], showmedians=True)\n",
        "ax2.set_xticks([1, 2])\n",
        "ax2.set_xticklabels(['Выборка A', 'Выборка B'])\n",
        "ax2.set_title('Violin Plot')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "На этом примере видно, что box plot для выборки B выглядит как стандартный симметричный «ящик», тогда как violin plot явно демонстрирует наличие двух пиков — информацию, критически важную для интерпретации.\n",
        "\n",
        "#### 2.3. Добавление Элементов Ошибки (Error Bars)\n",
        "\n",
        "Научная визуализация неполна без количественной оценки неопределённости. Метод `ax.errorbar()` позволяет отображать погрешности — будь то стандартное отклонение, стандартная ошибка среднего или доверительный интервал. Это обязательный элемент для публикаций в рецензируемых журналах.\n",
        "\n",
        "Элементы ошибок могут быть симметричными (`yerr=0.1`) или асимметричными (`yerr=[[низ, низ], [верх, верх]]`). Кроме того, можно одновременно отображать ошибки по X и по Y (`xerr`, `yerr`), а также настраивать их внешний вид: цвет, ширину штрихов (`capsize`), стиль линий.\n",
        "\n",
        "Пример с элементами ошибок:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array([2.1, 3.9, 6.0, 8.2])\n",
        "yerr = np.array([0.2, 0.3, 0.25, 0.4])  # стандартная ошибка\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.errorbar(x, y, yerr=yerr, fmt='o', color='darkgreen', ecolor='lightgray',\n",
        "            elinewidth=2, capsize=5, markersize=6, label='Измерения ± SE')\n",
        "ax.set_xlabel('Независимая переменная')\n",
        "ax.set_ylabel('Зависимая переменная')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь `fmt='o'` задаёт стиль маркеров, `ecolor` — цвет полос ошибок, а `capsize` добавляет «шапочки» на концы, что улучшает читаемость. Такой график не только передаёт данные, но и честно демонстрирует степень уверенности в них.\n",
        "\n"
      ],
      "metadata": {
        "id": "HQ15P4cSGLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Раздел 3: Тонкая Настройка Axes для Публикационного Качества (OO Customization)\n",
        "\n",
        "Достижение публикационного качества в научной визуализации требует не просто отображения данных, а точной и осознанной настройки каждого визуального элемента. Объектно-ориентированный (OO) стиль Matplotlib предоставляет явные, предсказуемые методы-сеттеры, которые позволяют полностью контролировать заголовки, подписи осей, тики, лимиты и декоративные элементы.\n",
        "\n",
        "#### 3.1. Управление Заголовками и Подписями Осей\n",
        "\n",
        "Для обеспечения ясности и профессионального вида графика необходимо чётко разделять заголовки подграфиков и общие заголовки всей фигуры. Метод `ax.set_title(\"Название графика\")` устанавливает заголовок конкретного объекта Axes, что особенно важно при работе с многопанельными композициями. Аналогично, `ax.set_xlabel(\"Ось X\")` и `ax.set_ylabel(\"Ось Y\")` задают метки осей с полным контролем над их текстом, шрифтом и положением. Если фигура содержит несколько подграфиков, а требуется передать общую тему исследования, используется метод `fig.suptitle(\"Общее название\")`, который размещает заголовок над всеми Axes и не привязан к какому-либо отдельному подграфику.\n",
        "\n",
        "Пример настройки заголовков в многопанельном графике:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "x = np.linspace(0, 5, 100)\n",
        "ax1.plot(x, np.sin(x), label='sin(x)')\n",
        "ax2.plot(x, np.cos(x), label='cos(x)', color='tab:orange')\n",
        "\n",
        "# Заголовки подграфиков\n",
        "ax1.set_title('Синусоида')\n",
        "ax2.set_title('Косинусоида')\n",
        "\n",
        "# Подписи осей\n",
        "ax1.set_xlabel('Угол (рад)')\n",
        "ax1.set_ylabel('Амплитуда')\n",
        "ax2.set_xlabel('Угол (рад)')\n",
        "\n",
        "# Общий заголовок фигуры\n",
        "fig.suptitle('Тригонометрические функции', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код демонстрирует чёткое разделение ответственности: каждый подграфик управляет своими локальными подписями, а фигура — общей темой.\n",
        "\n",
        "#### 3.2. Детальная Работа с Тиками и Лимитами Осей\n",
        "\n",
        "Автоматическая разметка осей, предлагаемая Matplotlib по умолчанию, часто не соответствует требованиям научной публикации. Исследователь должен иметь возможность точно определять, какие значения отображаются на осях и как они подписаны. Методы `ax.set_xlim()` и `ax.set_ylim()` позволяют ограничить отображаемый диапазон данных, исключая выбросы или нерелевантные области и фокусируя внимание читателя на ключевой зоне.\n",
        "\n",
        "Ещё более важна настройка тиков. Методы `ax.set_xticks()` и `ax.set_yticks()` принимают два аргумента: позиции тиков и, опционально, их текстовые метки. Это особенно полезно при работе с физическими величинами, денежными единицами или научной нотацией. Например, можно заменить числовые значения на подписи вида «\\$1.0 M» или «1.2 × 10⁴», что значительно улучшает читаемость.\n",
        "\n",
        "Пример ручной настройки тиков с форматированными метками:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Данные в миллионных единицах\n",
        "years = [2020, 2021, 2022, 2023]\n",
        "revenue = [1.2, 1.8, 2.5, 3.1]  # миллионы долларов\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.plot(years, revenue, marker='o')\n",
        "\n",
        "# Установка тиков по годам и форматированных меток по доходу\n",
        "ax.set_xticks(years)\n",
        "ax.set_yticks([1, 2, 3])\n",
        "ax.set_yticklabels(['\\$1.0 M', '\\$2.0 M', '\\$3.0 M'])\n",
        "\n",
        "ax.set_xlabel('Год')\n",
        "ax.set_ylabel('Выручка')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой подход гарантирует, что оси не только технически точны, но и интерпретируемы без дополнительных пояснений.\n",
        "\n",
        "#### 3.3. Легенды, Сетки и Стилизация\n",
        "\n",
        "Легенда (`ax.legend()`) играет ключевую роль при визуализации нескольких наборов данных на одном графике. В сложных композициях её расположение должно быть тщательно продумано, чтобы избежать перекрытия с данными. Это достигается с помощью параметра `loc` (например, `'upper right'`) или более точного управления через `bbox_to_anchor`, который позволяет размещать легенду в произвольной точке относительно Axes.\n",
        "\n",
        "Сетка (`ax.grid(True)`) облегчает точное считывание значений, особенно на графиках с плотным расположением точек. Однако её стиль должен быть ненавязчивым: рекомендуется использовать прерывистые линии (`linestyle='--'`) и пониженную прозрачность (`alpha=0.5`), чтобы сетка служила фоном, а не отвлекала от данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 4: Создание Сложных Многопанельных Макетов с GridSpec\n",
        "\n",
        "Для научных отчётов часто требуются несимметричные, иерархические композиции, где подграфики имеют разный размер и расположение. Стандартный подход `plt.subplots(nrows, ncols)` ограничен равномерными сетками. Для создания произвольной геометрии используется класс `GridSpec`.\n",
        "\n",
        "#### 4.1. Введение в GridSpec\n",
        "\n",
        "`GridSpec` определяет логическую сетку внутри объекта Figure, а затем позволяет объединять ячейки этой сетки с помощью синтаксиса срезов Python. Это превращает проектирование макета в декларативный процесс, управляемый индексами.\n",
        "\n",
        "Инициализация выполняется как `gs = GridSpec(nrows, ncols, figure=fig)`. После этого объекты Axes создаются вызовом `fig.add_subplot(gs[срез])`. Например, `gs[0, :]` охватывает всю первую строку, а `gs[1:, -1]` — последний столбец, начиная со второй строки. Такой подход особенно мощен при построении составных графиков, где, например, гистограмма распределения по X должна быть размещена под основным точечным графиком, а гистограмма по Y — справа от него.\n",
        "\n",
        "Пример асимметричного макета:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных\n",
        "np.random.seed(0)\n",
        "x = np.random.randn(1000)\n",
        "y = 1.2 * x + np.random.randn(1000) * 0.5\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8), layout=\"constrained\")\n",
        "gs = gridspec.GridSpec(3, 3, figure=fig)\n",
        "\n",
        "# Основной scatter plot (занимает 2x2 в левом верхнем углу)\n",
        "ax_main = fig.add_subplot(gs[:2, :2])\n",
        "ax_main.scatter(x, y, alpha=0.6)\n",
        "ax_main.set_xlabel('X')\n",
        "ax_main.set_ylabel('Y')\n",
        "\n",
        "# Гистограмма по X (внизу)\n",
        "ax_hist_x = fig.add_subplot(gs[2, :2], sharex=ax_main)\n",
        "ax_hist_x.hist(x, bins=30, color='steelblue')\n",
        "ax_hist_x.set_ylabel('Частота')\n",
        "\n",
        "# Гистограмма по Y (справа)\n",
        "ax_hist_y = fig.add_subplot(gs[:2, 2], sharey=ax_main)\n",
        "ax_hist_y.hist(y, bins=30, orientation='horizontal', color='crimson')\n",
        "ax_hist_y.set_xlabel('Частота')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код создаёт классический «scatter plot с маргинальными гистограммами», где все подграфики точно выровнены, а их размеры определяются логикой анализа, а не техническими ограничениями.\n",
        "\n",
        "#### 4.2. Вложенные GridSpec (Nested GridSpec)\n",
        "\n",
        "Для ещё более сложных композиций, где разные области фигуры требуют независимых внутренних сеток, используется вложенная структура GridSpec. Сначала создаётся основной `GridSpec`, затем для выбранной области вызывается метод `subgridspec()`, который определяет дочернюю сетку. Это позволяет, например, разделить фигуру на две колонки, а в каждой — создать свою независимую композицию из нескольких графиков.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 5: Обеспечение Публикационного Качества: Constrained Layout и Сохранение\n",
        "\n",
        "Создание сложного макета часто сопровождается проблемой наложения или обрезания подписей, заголовков и легенд. Решение этой проблемы — использование современного механизма автоматической компоновки.\n",
        "\n",
        "#### 5.1. Современное Решение: Constrained Layout\n",
        "\n",
        "Исторически для этой цели использовалась функция `plt.tight_layout()`, но она имеет ограничения при работе со сложными элементами, такими как цветовые шкалы или многоуровневые легенды. Современный и рекомендуемый подход — **Constrained Layout**. Он активируется при создании фигуры через параметр `layout=\"constrained\"` и использует внутренний решатель для расчёта необходимого пространства под все элементы графика. Constrained Layout полностью совместим с `GridSpec` и вложенными макетами, обеспечивая гармоничную компоновку даже в самых сложных сценариях.\n",
        "\n",
        "#### 5.2. Интеграция Цветовых Шкал (Colorbars)\n",
        "\n",
        "Цветовые шкалы — частый источник проблем в макетировании. При вызове `fig.colorbar(im, ax=ax)` в сочетании с Constrained Layout система автоматически уменьшает размер указанных Axes и выделяет место для шкалы, предотвращая перекрытия. Это особенно важно при сравнении нескольких тепловых карт с общей цветовой шкалой — задача, типичная для научных публикаций в физике, биологии и геоинформатике.\n",
        "\n",
        "#### 5.3. Сохранение Графиков для Публикации: `fig.savefig()`\n",
        "\n",
        "Финальный шаг — экспорт графика в формате, пригодном для публикации. Метод `fig.savefig()` предоставляет ключевые параметры для контроля качества:\n",
        "\n",
        "- **Разрешение (dpi):** Для печати требуется не менее 300 DPI. По умолчанию Matplotlib использует 100 DPI, что недостаточно для журналов.\n",
        "- **Формат файла:** Векторные форматы (PDF, SVG) предпочтительны для академических публикаций, так как они масштабируются без потерь. Растровые форматы (PNG, JPG) используются для веба.\n",
        "- **Обрезка (bbox_inches='tight'):** Этот параметр автоматически удаляет избыточные белые поля, гарантируя, что сохранённый файл содержит только необходимые элементы.\n",
        "\n",
        "Пример экспорта:\n",
        "\n",
        "```python\n",
        "fig.savefig(\n",
        "    'scientific_plot.pdf',\n",
        "    format='pdf',\n",
        "    dpi=300,\n",
        "    bbox_inches='tight',\n",
        "    transparent=False\n",
        ")\n",
        "```\n",
        "\n",
        "Такой файл будет соответствовать требованиям большинства научных журналов и сохранит все детали визуализации при любом масштабе.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Matplotlib предоставляет строгую, иерархическую модель для создания научной визуализации публикационного качества. Ключ к успеху — системный подход, основанный на трёх принципах.\n",
        "\n",
        "Во-первых, **контроль через ОО-стиль**: начинайте с `plt.subplots()` и используйте явные методы `ax.set_*()` для настройки каждого элемента. Это исключает зависимость от глобального состояния и обеспечивает воспроизводимость.\n",
        "\n",
        "Во-вторых, **сложное макетирование через GridSpec**: при необходимости асимметричных или иерархических композиций переходите от простых сеток к срезам `GridSpec`, что позволяет программно определять пространственные отношения между графиками.\n",
        "\n",
        "В-третьих, **гарантия качества через Constrained Layout и правильный экспорт**: активируйте `layout=\"constrained\"` при создании фигуры, чтобы автоматически избежать наложений, и сохраняйте результат в векторном формате с `dpi=300` и `bbox_inches='tight'`.\n",
        "\n",
        "Освоение этих принципов превращает Matplotlib из инструмента быстрого прототипирования в мощную платформу для создания визуализаций, соответствующих самым строгим стандартам научной и технической коммуникации."
      ],
      "metadata": {
        "id": "-SrdkFwqJTse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6: Продвинутые возможности Matplotlib — анимация, 3D-визуализация, аннотации и интеграция\n",
        "\n",
        "### Раздел 1: Анимация данных для демонстрации динамики\n",
        "\n",
        "Статическая визуализация не всегда способна передать эволюцию процесса во времени. Matplotlib предоставляет мощные инструменты для создания анимаций, которые позволяют наглядно демонстрировать динамические системы, сходимость алгоритмов или изменение распределений. Основой анимации служит класс `FuncAnimation` из модуля `matplotlib.animation`.\n",
        "\n",
        "В отличие от построения серии отдельных кадров, `FuncAnimation` оптимизирует рендеринг, обновляя только те части графика, которые изменились. Это достигается за счёт механизма **blitting**, который сохраняет фон и перерисовывает только движущиеся элементы.\n",
        "\n",
        "Пример анимации гармонического осциллятора:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.set_xlim(0, 4 * np.pi)\n",
        "ax.set_ylim(-1.2, 1.2)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "x = np.linspace(0, 4 * np.pi, 200)\n",
        "line, = ax.plot([], [], 'b-', lw=2)\n",
        "point, = ax.plot([], [], 'ro', markersize=8)\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    point.set_data([], [])\n",
        "    return line, point\n",
        "\n",
        "def animate(frame):\n",
        "    t = x[:frame]\n",
        "    y = np.sin(t)\n",
        "    line.set_data(t, y)\n",
        "    if frame > 0:\n",
        "        point.set_data(t[-1], y[-1])\n",
        "    return line, point\n",
        "\n",
        "anim = FuncAnimation(\n",
        "    fig, animate, init_func=init, frames=len(x),\n",
        "    interval=30, blit=True, repeat=False\n",
        ")\n",
        "\n",
        "# Для сохранения: anim.save('oscillation.mp4', fps=30)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код иллюстрирует ключевые компоненты анимации: функцию инициализации (`init`), которая задаёт начальное состояние, и функцию обновления (`animate`), вызываемую для каждого кадра. Анимации особенно ценны в образовательных материалах и при представлении результатов моделирования, где важна временная последовательность событий.\n",
        "\n",
        "### Раздел 2: Трёхмерная визуализация научных данных\n",
        "\n",
        "Для анализа многомерных зависимостей или пространственных структур Matplotlib поддерживает 3D-графику через модуль `mpl_toolkits.mplot3d`. Объект `Axes3D` расширяет стандартный `Axes`, добавляя методы для построения поверхностей, облаков точек и контурных сечений в трёхмерном пространстве.\n",
        "\n",
        "Ключевыми методами являются `plot_surface` для отображения гладких функций двух переменных, `scatter` для визуализации трёхмерных наборов данных и `contour`/`contourf` для построения изолиний на плоскостях. Важно помнить, что 3D-графики в Matplotlib остаются статическими в формате PDF или PNG; интерактивное вращение возможно только в интерактивных средах (Jupyter, Qt).\n",
        "\n",
        "Пример построения поверхности:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Генерация сетки\n",
        "x = np.linspace(-5, 5, 50)\n",
        "y = np.linspace(-5, 5, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "# Построение поверхности с цветовой картой\n",
        "surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.9, edgecolor='none')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=10, label='Амплитуда')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Трёхмерная визуализация требует особой осторожности: перегруженные графики сложно интерпретировать на печати. Рекомендуется использовать прозрачность (`alpha`), упрощённую геометрию и вспомогательные проекции (например, контуры на дне графика).\n",
        "\n",
        "### Раздел 3: Аннотации и произвольные графические примитивы\n",
        "\n",
        "Научная визуализация часто требует выделения ключевых точек, добавления пояснительных стрелок или визуального подчёркивания определённых областей. Matplotlib предоставляет богатый набор инструментов для таких задач.\n",
        "\n",
        "Метод `ax.annotate()` позволяет размещать текст с указателем, направленным на конкретную координату. Это особенно полезно для подписи экстремумов, точек пересечения или выбросов. Для выделения областей используются методы вроде `ax.axhspan()` (горизонтальная полоса), `ax.axvline()` (вертикальная линия) или `ax.fill_between()` (заливка между кривыми).\n",
        "\n",
        "Пример аннотации экстремума:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = x * np.exp(-x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.plot(x, y, 'b-', lw=2)\n",
        "\n",
        "# Найдём максимум\n",
        "x_max = 1.0\n",
        "y_max = x_max * np.exp(-x_max)\n",
        "\n",
        "# Аннотация с изогнутой стрелкой\n",
        "ax.annotate(\n",
        "    f'Максимум\\n({x_max:.1f}, {y_max:.2f})',\n",
        "    xy=(x_max, y_max),\n",
        "    xytext=(4, 0.3),\n",
        "    arrowprops=dict(\n",
        "        arrowstyle='->',\n",
        "        connectionstyle='arc3,rad=0.3',\n",
        "        color='red'\n",
        "    ),\n",
        "    fontsize=11,\n",
        "    ha='center'\n",
        ")\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('f(x) = x·e⁻ˣ')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Дополнительно, через модуль `matplotlib.patches` можно добавлять геометрические фигуры: круги, прямоугольники, эллипсы и многоугольники. Это позволяет строить схематические диаграммы, выделять зоны неопределённости или создавать кастомные визуальные элементы.\n",
        "\n",
        "### Раздел 4: Интеграция с экосистемой Python и кастомизация стиля\n",
        "\n",
        "Хотя Matplotlib предоставляет низкоуровневый контроль, на практике часто используется совместно с библиотеками высокого уровня. Например, метод `.plot()` объектов pandas DataFrame и Series является обёрткой над `ax.plot()`, автоматически использующей индексы как X-координаты и имена столбцов как метки. Это значительно ускоряет анализ временных рядов и табличных данных.\n",
        "\n",
        "Для более сложной статистической визуализации (парные графики, регрессионные полосы, тепловые карты корреляций) исследователи часто прибегают к библиотеке **seaborn**, которая построена поверх Matplotlib и использует те же объекты Figure и Axes. Это означает, что любой график seaborn можно донастроить с помощью ОО-методов Matplotlib, сочетая удобство высокоуровневого API с гибкостью низкоуровневого.\n",
        "\n",
        "Кроме того, Matplotlib поддерживает систему стилей, позволяющую глобально изменять внешний вид всех графиков. Стиль можно активировать через `plt.style.use('seaborn-v0_8')` или загрузить из пользовательского файла `.mplstyle`. Это обеспечивает единообразие визуализации в рамках одного проекта или публикации.\n",
        "\n",
        "### Раздел 5: Поддержка математической нотации и работа с изображениями\n",
        "\n",
        "Для научных публикаций критически важна корректная отрисовка математических формул. Matplotlib встроенно поддерживает подмножество LaTeX через механизм **mathtext**. Достаточно заключить выражение в символы `$...$`, и библиотека отобразит его в соответствии с типографскими правилами математики.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```python\n",
        "ax.set_xlabel(r'Время $t$ (с)')\n",
        "ax.set_ylabel(r'Амплитуда $\\psi(t) = A e^{-\\gamma t} \\sin(\\omega t + \\phi)$')\n",
        "```\n",
        "\n",
        "Для отображения растровых данных (изображений, тепловых карт, спектрограмм) используются методы `ax.imshow()` и `ax.pcolormesh()`. Первый интерпретирует массив как изображение с пиксельной семантикой, второй — как дискретизированную функцию двух переменных. Оба метода поддерживают произвольные цветовые карты (`cmap`), нормализацию значений и добавление цветовых шкал, что делает их незаменимыми в обработке сигналов, компьютерном зрении и физическом моделировании.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение модуля**  \n",
        "> Matplotlib — это не только инструмент для построения статических линейных графиков, но и полноценная платформа для научной визуализации любого уровня сложности. От анимации динамических процессов и трёхмерного моделирования до точной типографики и интеграции с экосистемой Python — библиотека предоставляет исследователю исчерпывающий набор возможностей. Освоение этих продвинутых функций позволяет переходить от простого отображения данных к созданию выразительных, информативных и публикационно-готовых научных иллюстраций."
      ],
      "metadata": {
        "id": "CWbEhsjmJR9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Модуль 8: Статистическая Визуализация Высокого Уровня в Python (Seaborn)\n",
        "\n",
        "### 1. Методологические Основы Seaborn и Принцип «Опрятных Данных»\n",
        "\n",
        "#### 1.1. Роль Seaborn в конвейере EDA (Exploratory Data Analysis)\n",
        "\n",
        "Seaborn представляет собой высокоуровневую библиотеку для статистической визуализации, построенную поверх Matplotlib и тесно интегрированную с экосистемой pandas. В отличие от низкоуровневого Matplotlib, где требуется явное управление осями, метками и элементами графика, Seaborn предоставляет декларативный, ориентированный на данные API. Аналитик задаёт семантические отношения между переменными — например, указывает, что столбец `'region'` должен определять цвет (`hue`), а `'time'` — ось X, — и библиотека автоматически выполняет необходимую агрегацию, трассировку и отрисовку.\n",
        "\n",
        "Этот подход кардинально ускоряет процесс эксплораторного анализа данных (EDA). Вместо того чтобы вручную группировать данные, вычислять средние или строить отдельные кривые для каждой категории, исследователь формулирует гипотезу на языке переменных, и Seaborn мгновенно предоставляет визуальное подтверждение или опровержение. Таким образом, Seaborn заполняет критический пробел между сырыми табличными данными и статистическим пониманием, превращая визуализацию в инструмент прямого познания структуры данных.\n",
        "\n",
        "#### 1.2. Философия Tidy Data: Преимущества длинного формата данных (Long-Form Data)\n",
        "\n",
        "Эффективное использование Seaborn требует, чтобы данные соответствовали принципам «опрятных данных» (Tidy Data), предложенным Хадли Уикэмом. В этом формате каждая переменная занимает отдельный столбец, каждое наблюдение — отдельную строку, а каждое значение — одну ячейку. Такой подход противопоставляется широкому формату (wide-form), где, например, продажи по регионам могут быть разбросаны по разным столбцам (`sales_EU`, `sales_US`, `sales_APAC`).\n",
        "\n",
        "Длинный формат не является просто эстетическим предпочтением — он является архитектурной необходимостью для Seaborn. Ключевые компоненты, такие как `FacetGrid`, полагаются на возможность семантического сопоставления имени переменной с графическим атрибутом. Если уровни категории хранятся в одном столбце (например, `'region'` со значениями `'EU'`, `'US'`, `'APAC'`), система может автоматически создать фасеты по этим уровням. В широком формате такая информация теряется: библиотека не «знает», что три столбца относятся к одной и той же переменной.\n",
        "\n",
        "Преобразование данных в длинний формат легко выполняется с помощью метода `pandas.melt()`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Исходные данные в широком формате\n",
        "wide_df = pd.DataFrame({\n",
        "    'product': ['A', 'B'],\n",
        "    'Q1': [100, 150],\n",
        "    'Q2': [120, 160],\n",
        "    'Q3': [110, 155]\n",
        "})\n",
        "\n",
        "# Преобразование в длинний формат\n",
        "long_df = wide_df.melt(\n",
        "    id_vars='product',\n",
        "    value_vars=['Q1', 'Q2', 'Q3'],\n",
        "    var_name='quarter',\n",
        "    value_name='sales'\n",
        ")\n",
        "\n",
        "print(long_df)\n",
        "```\n",
        "\n",
        "Результат — таблица, где каждая строка представляет одно наблюдение (продажи продукта в квартале), что делает её идеальной для передачи в любую функцию Seaborn.\n",
        "\n",
        "#### 1.3. Эстетические основы: Управление стилями и выбор палитр\n",
        "\n",
        "Seaborn не только статистически, но и визуально улучшает стандартный вывод Matplotlib. Вызов `sns.set_theme()` активирует одну из встроенных тем (`'darkgrid'`, `'whitegrid'`, `'ticks'`), которая настраивает фон, сетку, шрифты и отступы, обеспечивая профессиональный вид «из коробки».\n",
        "\n",
        "Особое внимание в Seaborn уделяется **цветовым палитрам**, поскольку цвет является мощным, но и极易 вводящим в заблуждение каналом передачи информации. Библиотека предоставляет палитры, спроектированные с учётом перцептивной равномерности — то есть равные шаги в данных соответствуют равным шагам в восприятии.\n",
        "\n",
        "Различают три методологических типа палитр:\n",
        "\n",
        "- **Категориальные палитры** (например, `husl`, `Set1`) используют максимально различимые цвета для дискретных групп без внутреннего порядка.\n",
        "- **Последовательные палитры** (например, `Blues`, `viridis`) отображают монотонный градиент от низких к высоким значениям.\n",
        "- **Дивергентные палитры** (например, `vlag`, `coolwarm`) критически важны для данных с центральной точкой (обычно нулём или средним). Они используют контрастные цвета (синий/красный) для обозначения отклонений в противоположных направлениях.\n",
        "\n",
        "Некорректный выбор палитры может искажать интерпретацию. Например, при визуализации матрицы корреляций использование последовательной палитры (`Blues`) скроет знак коэффициентов: отрицательная корреляция будет выглядеть как «менее интенсивная», а не как противоположная по смыслу.\n",
        "\n",
        "Пример корректного выбора палитры для heatmap:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Загрузка примера данных\n",
        "flights = sns.load_dataset(\"flights\")\n",
        "flights_wide = flights.pivot(\"month\", \"year\", \"passengers\")\n",
        "\n",
        "# Использование последовательной палитры для положительных данных\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(flights_wide, cmap=\"YlGnBu\", annot=False, cbar_kws={'label': 'Пассажиры'})\n",
        "plt.title('Пассажиропоток по месяцам и годам')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь `YlGnBu` — последовательная палитра, уместная для неотрицательных данных. Для корреляций следовало бы выбрать `vlag`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Архитектура Сетки: Метод Малых Мультиплов (Small Multiples)\n",
        "\n",
        "#### 2.1. Концептуальное значение малых мультиплов\n",
        "\n",
        "Метод «малых мультиплов» (small multiples) — один из самых мощных приёмов в визуальном анализе многомерных данных. Он предполагает создание серии графиков одинакового типа, где каждый график отображает условный срез данных (например, по региону, году или категории). Поскольку визуальная кодировка остаётся постоянной, зритель может легко сравнивать формы распределений, тренды или отношения между переменными в разных условиях.\n",
        "\n",
        "#### 2.2. Класс FacetGrid и его измерения\n",
        "\n",
        "В Seaborn за реализацию малых мультиплов отвечает класс `FacetGrid`. Он создаёт сетку из объектов Axes, где структура определяется категориальными переменными. Основные измерения:\n",
        "\n",
        "- **`row` и `col`** задают физическое размещение подграфиков в двумерной сетке. Каждый уникальный уровень переменной порождает отдельную строку или столбец.\n",
        "- **`hue`** добавляет третье измерение через цвет: разные категории отображаются разными цветами на одном и том же подграфике.\n",
        "\n",
        "Таким образом, `FacetGrid` позволяет одновременно анализировать до четырёх переменных: две непрерывные (X и Y), одна для фасетирования по сетке и одна для цветового кодирования.\n",
        "\n",
        "#### 2.3. Взаимодействие с высокоуровневыми функциями\n",
        "\n",
        "Seaborn предоставляет два типа функций: **уровня фигуры** (Figure-Level) и **уровня осей** (Axes-Level).\n",
        "\n",
        "Функции уровня фигуры — `relplot()`, `displot()`, `catplot()`, `lmplot()` — автоматически создают `FacetGrid` и применяют к нему соответствующую функцию уровня осей (`scatterplot`, `histplot`, `boxplot`, `regplot`). Они идеальны для быстрого многомерного анализа.\n",
        "\n",
        "Функции уровня осей работают с уже существующим объектом Axes, что даёт полный контроль, но требует ручного управления макетом.\n",
        "\n",
        "Пример использования `relplot` для анализа по категориям:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Анализ зависимости чаевых от счёта, разбитый по полу и курению\n",
        "g = sns.relplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"sex\", col=\"smoker\", row=\"time\",\n",
        "    height=4, aspect=1\n",
        ")\n",
        "g.set_axis_labels(\"Счёт ($)\", \"Чаевые ($)\")\n",
        "g.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот код создаёт сетку 2×2 графиков за одну строку, демонстрируя силу Figure-Level API. Если бы мы использовали `scatterplot`, пришлось бы вручную создавать сетку через `plt.subplots()` и писать цикл для заполнения.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Визуализация и Статистическая Интерпретация Одномерных Распределений\n",
        "\n",
        "Seaborn предлагает три взаимодополняющих метода визуализации одномерных распределений, каждый со своими статистическими достоинствами.\n",
        "\n",
        "#### 3.1. Гистограммы (`histplot`)\n",
        "\n",
        "Гистограмма разбивает диапазон значений на интервалы (бины) и отображает частоту или плотность наблюдений в каждом бине. Главный недостаток — зависимость от выбора количества и ширины бинов. Seaborn смягчает эту проблему, позволяя добавить к гистограмме кривую оценки плотности ядра (`kde=True`), что даёт более плавное представление формы распределения.\n",
        "\n",
        "#### 3.2. Оценка плотности ядра (`kdeplot`)\n",
        "\n",
        "KDE-график строит гладкую оценку функции плотности вероятности, суммируя ядра (обычно гауссовы) вокруг каждой точки данных. Ключевой параметр — **полоса пропускания** (bandwidth): слишком широкая скрывает мультимодальность, слишком узкая — усиливает шум. В Seaborn она настраивается через параметр `bw_adjust`.\n",
        "\n",
        "#### 3.3. Эмпирическая кумулятивная функция распределения (`ecdfplot`)\n",
        "\n",
        "ECDF-график показывает долю наблюдений, не превышающих заданное значение. Его главное преимущество — **отсутствие настраиваемых параметров**. Каждая точка данных отображается напрямую, что делает ECDF объективным инструментом для сравнения распределений. Хотя форма менее интуитивна, чем KDE, она свободна от субъективного сглаживания.\n",
        "\n",
        "Пример сравнения трёх методов:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "body_mass = penguins[\"body_mass_g\"].dropna()\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Гистограмма с KDE\n",
        "sns.histplot(body_mass, kde=True, ax=axs[0])\n",
        "axs[0].set_title('Гистограмма + KDE')\n",
        "\n",
        "# Чистый KDE\n",
        "sns.kdeplot(body_mass, ax=axs[1])\n",
        "axs[1].set_title('KDE')\n",
        "\n",
        "# ECDF\n",
        "sns.ecdfplot(body_mass, ax=axs[2])\n",
        "axs[2].set_title('ECDF')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "В академическом анализе, где важна воспроизводимость и объективность, ECDF следует рассматривать как основу, а гистограммы и KDE — как вспомогательные инструменты для интуитивного понимания.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Анализ Групп и Категориальных Переменных\n",
        "\n",
        "#### 4.1. Сводные распределения: `boxplot` и `violinplot`\n",
        "\n",
        "Boxplot предоставляет компактное резюме распределения: медиана, квартили, whiskers и выбросы. Он устойчив к выбросам и идеален для быстрого сравнения локации и разброса.\n",
        "\n",
        "Violinplot дополняет эту информацию, отображая полную форму распределения через KDE. Он может выявить бимодальность или асимметрию, которые boxplot скрывает.\n",
        "\n",
        "#### 4.2. Точечные представления: `stripplot` и `swarmplot`\n",
        "\n",
        "Swarmplot отображает каждую точку данных, избегая наложения за счёт небольшого смещения вдоль категориальной оси. Это позволяет видеть не только центральную тенденцию, но и плотность, количество и точное расположение наблюдений.\n",
        "\n",
        "#### 4.3. Комбинирование графиков для комплексного анализа\n",
        "\n",
        "Наиболее информативный подход — комбинировать методы. Например, наложить `swarmplot` на `violinplot`:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.violinplot(data=penguins, x=\"species\", y=\"body_mass_g\", inner=None, color=\".8\")\n",
        "sns.swarmplot(data=penguins, x=\"species\", y=\"body_mass_g\", size=4)\n",
        "plt.title('Масса пингвинов по видам')\n",
        "plt.ylabel('Масса (г)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Здесь серый violinplot показывает форму распределения, а точки — фактические наблюдения. Это даёт полную картину: и статистическую, и эмпирическую.\n",
        "\n",
        "Кроме того, важно **упорядочивать категории осмысленно**. Если категории не имеют естественного порядка, их можно сортировать, например, по медиане:\n",
        "\n",
        "```python\n",
        "order = penguins.groupby(\"species\")[\"body_mass_g\"].median().sort_values().index\n",
        "sns.boxplot(data=penguins, x=\"species\", y=\"body_mass_g\", order=order)\n",
        "```\n",
        "\n",
        "Такой подход накладывает на визуализацию статистически обоснованную структуру, что улучшает интерпретацию.\n",
        "\n",
        "---\n",
        "\n",
        "> **Заключение главы**  \n",
        "> Seaborn — это не просто библиотека для «красивых графиков», а инструмент для **статистического мышления через визуализацию**. Его архитектура, основанная на принципах tidy data, малых мультиплов и перцептивно обоснованных палитр, направляет исследователя к методологически корректному анализу. Освоение различий между Figure-Level и Axes-Level функциями, понимание сильных и слабых сторон каждого типа графика распределения, а также умение комбинировать визуальные методы позволяют превратить EDA из рутинной проверки в процесс глубокого познания данных. В руках внимательного аналитика Seaborn становится мостом между сырыми числами и научным выводом."
      ],
      "metadata": {
        "id": "-tDirS3oQCsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 5. Визуализация Отношений и Регрессионный Анализ\n",
        "\n",
        "### 5.1. Реляционные графики (`relplot`)\n",
        "\n",
        "Функция `relplot()` является ключевым инструментом Seaborn для визуализации взаимосвязей между переменными. Как функция уровня фигуры, она автоматически управляет сеткой подграфиков и поддерживает два основных типа графиков: точечные (`kind=\"scatter\"`) и линейные (`kind=\"line\"`). Главное преимущество `relplot` — его мощная семантическая кодировка, позволяющая одновременно отображать до пяти переменных: две позиционные (X и Y), а также цвет (`hue`), форму маркера (`style`), размер (`size`) и условные фасеты (`row`/`col`). Это превращает простой scatter plot в многомерный аналитический инструмент.\n",
        "\n",
        "Пример визуализации сложного отношения в данных о чаевых:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Отображение связи между счётом и чаевыми с учётом пола, курения и времени\n",
        "sns.relplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"sex\", style=\"smoker\", size=\"size\",\n",
        "    sizes=(40, 200),  # диапазон размеров маркеров\n",
        "    alpha=0.7,\n",
        "    height=5, aspect=1.2\n",
        ")\n",
        "plt.title('Многомерный анализ чаевых')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот график позволяет одновременно оценить, как размер счёта влияет на сумму чаевых, и как это соотношение изменяется в зависимости от пола клиента, привычки к курению и количества людей в группе. Такая визуализация служит отправной точкой для формулировки гипотез.\n",
        "\n",
        "### 5.2. Построение регрессионных моделей (`regplot` и `lmplot`)\n",
        "\n",
        "Для визуального представления линейной зависимости Seaborn предоставляет две функции. `regplot()` — это функция уровня осей, которая строит точечный график с наложенной линией регрессии и доверительным интервалом. `lmplot()` — это её обёртка уровня фигуры, интегрированная с `FacetGrid`, что позволяет строить отдельные регрессионные модели для каждого уровня категориальной переменной.\n",
        "\n",
        "Важно подчеркнуть, что Seaborn не предназначен для формального статистического вывода — для оценки коэффициентов, p-значений или критериев качества модели следует использовать библиотеки вроде `statsmodels` или `scikit-learn`. Регрессионные графики в Seaborn служат **визуальным руководством**: они помогают оценить силу, направление и линейность связи, а также выявить потенциальные выбросы или нелинейные паттерны.\n",
        "\n",
        "Пример сравнения регрессий по категориям:\n",
        "\n",
        "```python\n",
        "# Отдельная регрессия для курящих и некурящих\n",
        "sns.lmplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    hue=\"smoker\",\n",
        "    height=5, aspect=1.2\n",
        ")\n",
        "plt.title('Регрессия чаевых по группам курильщиков')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой график сразу показывает, различается ли наклон регрессионной линии между группами — важный признак взаимодействия переменных.\n",
        "\n",
        "### 5.3. Диагностика модели: Применение `residplot()`\n",
        "\n",
        "Визуализация самой регрессии недостаточна — необходимо проверить адекватность модели. Для этого используется `residplot()`, который отображает остатки (разницу между наблюдаемыми и предсказанными значениями) в зависимости от предиктора.\n",
        "\n",
        "Для корректной линейной модели остатки должны быть **случайно рассеяны** вокруг горизонтальной линии `y = 0`. Любая структура — например, парабола, волна или «веер» (расширяющаяся дисперсия) — указывает на нарушение допущений: нелинейность, гетероскедастичность или пропущенную переменную.\n",
        "\n",
        "Seaborn позволяет углубить диагностику, подгоняя нелинейные модели. Например, параметр `order=2` строит квадратичную регрессию, а `lowess=True` добавляет сглаживающую кривую без параметрических допущений:\n",
        "\n",
        "```python\n",
        "# Диагностика линейной модели\n",
        "sns.residplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip\",\n",
        "    lowess=True,  # непараметрическая линия тренда\n",
        "    scatter_kws={'alpha': 0.6}\n",
        ")\n",
        "plt.title('Остатки регрессионной модели')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Если кривая LOWESS явно отклоняется от нуля, это сигнал: линейная модель неадекватна, и следует рассмотреть нелинейные преобразования или добавление полиномиальных признаков.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Методология Отображения Статистической Неопределенности (`errorbar`)\n",
        "\n",
        "Начиная с версии 0.12, Seaborn ввёл унифицированный параметр `errorbar`, который строго разделяет два фундаментально разных типа неопределённости: **неопределённость оценки** и **разброс данных**.\n",
        "\n",
        "### 6.1. Два типа интервалов ошибки\n",
        "\n",
        "**Неопределённость оценки** отражает, насколько точно выборочная статистика (например, среднее) оценивает параметр генеральной совокупности. Этот интервал (доверительный интервал, CI, или стандартная ошибка, SE) **уменьшается с ростом размера выборки**.\n",
        "\n",
        "**Разброс данных** (стандартное отклонение, SD, или процентильный интервал, PI) описывает изменчивость самих наблюдений вокруг центра. Он **не зависит от размера выборки** и отражает дисперсию популяции.\n",
        "\n",
        "Смешивание этих понятий — серьёзная методологическая ошибка. Например, отображение SD вместо CI в графике средних по группам создаёт ложное впечатление, что различия между группами статистически значимы, даже если они нет.\n",
        "\n",
        "### 6.2. Методы построения доверительных интервалов\n",
        "\n",
        "Seaborn поддерживает два подхода к оценке неопределённости:\n",
        "\n",
        "- **Параметрический**: предполагает нормальность данных и использует аналитические формулы (например, `errorbar=(\"se\", 1)` для одной стандартной ошибки).\n",
        "- **Непараметрический (бутстрап)**: многократно ресэмплирует данные с замещением, строит эмпирическое распределение статистики и определяет интервал по процентилям (например, `errorbar=(\"ci\", 95)` для 95% доверительного интервала).\n",
        "\n",
        "Бутстрап особенно ценен при нарушении нормальности, наличии выбросов или сложных статистик, где аналитическая оценка дисперсии затруднена.\n",
        "\n",
        "Сравнение методов на практике:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# График средних с разными типами ошибок\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=(\"ci\", 95), ax=axes[0])\n",
        "axes[0].set_title('95% CI (бутстрап)')\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=\"se\", ax=axes[1])\n",
        "axes[1].set_title('Стандартная ошибка')\n",
        "\n",
        "sns.barplot(data=tips, x=\"day\", y=\"tip\", errorbar=\"sd\", ax=axes[2])\n",
        "axes[2].set_title('Стандартное отклонение')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Первый график отвечает на вопрос: «В каком диапазоне, вероятно, лежит истинное среднее для всей популяции?». Третий — на вопрос: «Насколько сильно варьируются чаевые в группе?».\n",
        "\n",
        "### 6.3. Контроль прозрачности и извлечение данных\n",
        "\n",
        "При построении линейных графиков с доверительными интервалами (`lineplot`, `regplot`) неопределённость отображается в виде заштрихованной области. Её прозрачность регулируется параметром `err_kws={'alpha': 0.3}`. Это особенно важно при наложении нескольких линий, чтобы избежать визуального перегруза.\n",
        "\n",
        "Хотя Seaborn не предоставляет прямого API для извлечения численных значений границ CI, их можно получить из объекта Axes Matplotlib, что позволяет проводить дальнейший анализ или настраивать отображение.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Матричный Анализ: Корреляции и Иерархическая Кластеризация\n",
        "\n",
        "### 7.1. Тепловые карты (`heatmap`)\n",
        "\n",
        "Функция `heatmap()` предназначена для визуализации двумерных матриц, чаще всего — корреляционных. Использование **дивергентной палитры** (например, `vlag` или `coolwarm`) критически важно: она интуитивно разделяет положительные (тёплые цвета) и отрицательные (холодные) корреляции, а нейтральный центр (обычно белый или серый) обозначает отсутствие связи.\n",
        "\n",
        "Для научной публикации рекомендуется включать численные значения через `annot=True` и управлять их форматом через `fmt`:\n",
        "\n",
        "```python\n",
        "# Визуализация корреляционной матрицы\n",
        "numeric_vars = tips.select_dtypes(include=\"number\")\n",
        "corr_matrix = numeric_vars.corr()\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"vlag\",\n",
        "    center=0,\n",
        "    square=True,\n",
        "    cbar_kws={\"label\": \"Коэффициент корреляции Пирсона\"}\n",
        ")\n",
        "plt.title('Матрица корреляций')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Такой график позволяет мгновенно оценить силу и направление всех попарных линейных связей.\n",
        "\n",
        "### 7.2. Иерархическая кластеризация (`clustermap`)\n",
        "\n",
        "Функция `clustermap()` расширяет `heatmap`, добавляя **иерархическую кластеризацию** строк и столбцов. Алгоритм переупорядочивает элементы матрицы на основе их сходства (например, `1 - |ρ|` для корреляций), а вдоль осей отображает дендрограммы, показывающие иерархию объединения кластеров.\n",
        "\n",
        "`clustermap` — инструмент для **генерации гипотез**, а не для окончательного вывода. Выявленные кластеры требуют подтверждения формальными методами. Однако визуальная группировка сильно коррелирующих признаков или схожих наблюдений чрезвычайно полезна на этапе EDA.\n",
        "\n",
        "Важно: в отличие от большинства функций Seaborn, `heatmap` и `clustermap` работают с **широким форматом** (матрицей), хотя `clustermap` поддерживает tidy data через параметр `pivot_kws`.\n",
        "\n",
        "Пример:\n",
        "\n",
        "```python\n",
        "# Кластеризация признаков по корреляции\n",
        "sns.clustermap(\n",
        "    corr_matrix,\n",
        "    cmap=\"vlag\",\n",
        "    center=0,\n",
        "    metric=\"correlation\",  # расстояние = 1 - |корреляция|\n",
        "    method=\"average\",      # метод связывания\n",
        "    figsize=(8, 8)\n",
        ")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Этот график выявляет, например, что `'total_bill'` и `'tip'` образуют один кластер, а `'size'` — другой, что может навести на мысль о латентных факторах (например, «размер группы» vs «щедрость»).\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Комплексное Применение Seaborn: Практические Кейсы EDA\n",
        "\n",
        "### 8.1. Пошаговый рабочий процесс EDA (набор данных `tips`)\n",
        "\n",
        "Эффективный EDA следует структурированному итеративному процессу.\n",
        "\n",
        "**Шаг 1: Унивариантный анализ.**  \n",
        "Изучение распределения ключевых переменных. Например, `sns.histplot(tips[\"total_bill\"], kde=True)` показывает, что распределение счёта скошено вправо, что может потребовать логарифмического преобразования перед регрессионным анализом.\n",
        "\n",
        "**Шаг 2: Создание производных метрик и сравнение групп.**  \n",
        "Аналитик вводит новую переменную — процент чаевых: `tips[\"tip_pct\"] = tips[\"tip\"] / tips[\"total_bill\"]`. Затем сравнивает его по категориям:  \n",
        "```python\n",
        "sns.boxplot(data=tips, x=\"day\", y=\"tip_pct\", hue=\"smoker\")\n",
        "```  \n",
        "Boxplot выявляет, что в выходные дни курящие оставляют меньший процент чаевых, чем некурящие.\n",
        "\n",
        "**Шаг 3: Многомерный анализ отношений.**  \n",
        "Семантический scatterplot позволяет одновременно учесть несколько факторов:  \n",
        "```python\n",
        "sns.scatterplot(\n",
        "    data=tips,\n",
        "    x=\"total_bill\", y=\"tip_pct\",\n",
        "    hue=\"time\", style=\"sex\", size=\"size\"\n",
        ")\n",
        "```  \n",
        "График может показать, что в ужины (dinner) связь между размером счёта и процентом чаевых слабее, чем в обеды.\n",
        "\n",
        "**Шаг 4: Регрессия и корреляция.**  \n",
        "Построение `regplot` и матрицы корреляций завершает обзор, подтверждая или опровергая наблюдаемые тенденции количественно.\n",
        "\n",
        "### 8.2. Заключительные рекомендации по эффективному дизайну\n",
        "\n",
        "Эффективная статистическая визуализация требует методологической дисциплины:\n",
        "\n",
        "- **Чётко указывайте, что означают полосы ошибок** — CI, SE или SD. В подписи к графику пишите: «Планки ошибок: 95% доверительный интервал (бутстрап)».\n",
        "- **Используйте перцептивно корректные палитры**: дивергентные для корреляций, последовательные для положительных величин.\n",
        "- **Избегайте перегрузки**: не используйте одновременно `hue`, `style`, `size`, `row` и `col`, если это не критично для гипотезы.\n",
        "- **Помните о Matplotlib**: для тонкой настройки (аннотации, кастомные тики, LaTeX-формулы) используйте методы `ax.set_*()` после построения графика Seaborn.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Выводы и Методологическое Заключение\n",
        "\n",
        "Seaborn — это не просто инструмент для «красивых картинок», а **методологическая платформа для статистического мышления через визуализацию**. Его архитектура, основанная на принципах tidy data и малых мультиплов, направляет исследователя к структурированному, воспроизводимому анализу.\n",
        "\n",
        "Процесс EDA в Seaborn итеративен: гистограмма генерирует вопрос о распределении, scatterplot — о связи, а `residplot` — о валидности модели. Каждый график — не конечный результат, а **диалог с данными**.\n",
        "\n",
        "Особое внимание библиотека уделяет **статистической честности**. Разделение неопределённости оценки и разброса данных, использование робастных методов вроде бутстрапа, визуальная диагностика моделей — всё это защищает от поспешных выводов.\n",
        "\n",
        "Таким образом, Seaborn служит критически важным мостом между сырыми данными и обоснованным научным выводом. Его декларативный API позволяет исследователю сосредоточиться на содержании анализа, а не на технических деталях отрисовки, делая статистическую визуализацию неотъемлемой частью мышления аналитика."
      ],
      "metadata": {
        "id": "x6VZzGRlSqvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 9. Интерактивная Визуализация и Веб-Дашборды в Python: Методическая Лекция по Plotly и Dash\n",
        "\n",
        "### Раздел I. Архитектурные Основы Plotly: От Данных к Интерактивному Графику\n",
        "\n",
        "#### 1.1. Фигура Plotly как Фундаментальная Структура Данных\n",
        "\n",
        "Центральным элементом визуализации в библиотеке Plotly является объект `plotly.graph_objects.Figure`. Этот объект представляет собой декларативный контейнер, полностью описывающий график: его данные, внешний вид и интерактивные возможности. На самом низком уровне фигура Plotly структурирована как словарь Python, который последовательно транслируется в JSON-схему, интерпретируемую фронтенд-библиотекой **Plotly.js**.\n",
        "\n",
        "Такая архитектура превращает Plotly Python API в генератор строго формализованной спецификации. Любое изменение, внесённое через методы Python, неизбежно отражается в конкретных ключах и значениях этой схемы. Это обеспечивает высокую предсказуемость и воспроизводимость при кастомизации. Для разработчика, стремящегося к точному контролю, понимание иерархии этой структуры — не опция, а необходимость.\n",
        "\n",
        "Объект `Figure` состоит из трёх компонентов:\n",
        "\n",
        "- **`data`** — список следов (`traces`), каждый из которых описывает один набор данных и способ его отображения (точки, столбцы, поверхность и т.д.);\n",
        "- **`layout`** — объект, содержащий все стилистические настройки, не зависящие от данных: заголовки, оси, отступы, легенды, параметры 3D-сцены;\n",
        "- **`frames`** — список кадров, используемых для анимаций, где каждый кадр определяет новое состояние `data` и/или `layout`.\n",
        "\n",
        "#### 1.2. Объектная Модель Plotly: Trace и Layout\n",
        "\n",
        "Для построения и модификации фигур используются два класса объектов: **Trace** и **Layout**.\n",
        "\n",
        "Каждый **след** (`go.Scatter`, `go.Bar`, `go.Surface` и др.) соответствует определённому типу визуализации и инкапсулирует массивы данных (например, `x`, `y`, `z`) и параметры отображения (например, `mode='lines+markers'`). Plotly поддерживает сотни типов следов, включая геопространственные (`Choroplethmapbox`), 3D (`Surface`) и специализированные (`Sankey`, `Sunburst`). После создания фигуру можно динамически изменять: метод `Figure.add_traces()` добавляет новые следы, а `Figure.update_traces()` — массово обновляет свойства существующих (например, меняет цвет всех линий).\n",
        "\n",
        "Объект **`Layout`** управляет глобальным видом графика. Его свойства настраиваются через `Figure.update_layout()`. Для создания многопанельных композиций используется функция `make_subplots()`, которая генерирует предварительно настроенную фигуру с сеткой подграфиков. При добавлении следов в такую фигуру явно указывается их позиция (`row=1, col=2`).\n",
        "\n",
        "Важный методологический нюанс: при построении линейных графиков (например, временных рядов) **данные должны быть отсортированы по оси X**. Если этого не сделать, Plotly соединит точки в порядке их следования в массиве, что может привести к визуально искажённой, «путающейся» линии, не отражающей истинный тренд. Это не ошибка библиотеки, а следствие некорректной подготовки данных.\n",
        "\n",
        "#### 1.3. Сравнение Plotly Express (px) и Graph Objects (go)\n",
        "\n",
        "В Plotly существует два уровня API: высокоуровневый **Plotly Express** (`px`) и низкоуровневый **Graph Objects** (`go`).\n",
        "\n",
        "**Plotly Express** — это декларативный интерфейс, оптимизированный для быстрого создания типовых статистических графиков. Он принимает pandas DataFrame и автоматически назначает цвета, легенды, оси и даже анимации. Например, `px.scatter(df, x=\"income\", y=\"spending\", color=\"region\")` за одну строку строит многоцветный scatter plot. Это идеальный инструмент для разведочного анализа (EDA) и прототипирования.\n",
        "\n",
        "**Graph Objects** предоставляет полный контроль над каждым элементом фигуры. Он требует больше кода, но позволяет создавать сложные, нетиповые композиции: например, 3D-поверхность с наложенными контурами и точками, или интерактивную карту с несколькими слоями. При работе с `go` разработчик явно создаёт каждый след и настраивает каждый параметр макета.\n",
        "\n",
        "Выбор между `px` и `go` — это выбор между скоростью и гибкостью. Часто используется гибридный подход: график создаётся через `px`, а затем детально донастраивается через `fig.update_layout()` и `fig.update_traces()`.\n",
        "\n",
        "Пример гибридного подхода:\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Быстрое создание через px\n",
        "fig = px.line(\n",
        "    data_frame=df_sorted,\n",
        "    x=\"date\", y=\"value\",\n",
        "    color=\"category\",\n",
        "    title=\"Динамика показателей по категориям\"\n",
        ")\n",
        "\n",
        "# Детальная настройка через go-методы\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Дата\",\n",
        "    yaxis_title=\"Значение\",\n",
        "    legend_title=\"Категория\",\n",
        "    font=dict(family=\"Arial\", size=12)\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "#### 1.4. Экспорт и Сохранение Интерактивных и Статических Изображений\n",
        "\n",
        "Plotly предлагает два ключевых способа экспорта:\n",
        "\n",
        "- **Интерактивный HTML**: метод `fig.write_html(\"plot.html\")` сохраняет график в автономный HTML-файл, содержащий весь необходимый JavaScript (Plotly.js). Такой файл можно открыть в любом браузере, делиться им по email или встраивать в веб-страницы. Вся интерактивность (зум, панорамирование, тултипы) сохраняется.\n",
        "- **Статическое изображение**: метод `fig.write_image(\"plot.png\")` (или `.svg`, `.pdf`) генерирует растровое или векторное изображение для печати или вставки в презентации. Для этого требуется библиотека **Kaleido**, которая обеспечивает высококачественный рендеринг без зависимости от браузера.\n",
        "\n",
        "Эта гибкость делает Plotly универсальным решением: от интерактивных дашбордов до публикаций в научных журналах.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел II. Продвинутые Техники Визуализации с Plotly\n",
        "\n",
        "#### 2.1. Трёхмерная Визуализация: Scatter3D и Surface Plots\n",
        "\n",
        "Plotly предоставляет мощные инструменты для работы с трёхмерными данными, включая полный контроль над камерой, освещением и проекциями.\n",
        "\n",
        "**Scatter3D** (`go.Scatter3d`) отображает точки в пространстве, определяемом координатами X, Y, Z. Каждый маркер может быть окрашен, изменён по размеру или форме в зависимости от дополнительных переменных, что позволяет визуализировать до **пяти измерений** одновременно. Это особенно полезно при анализе многомерных наборов данных, таких как Iris или результатов моделирования.\n",
        "\n",
        "**Surface Plots** (`go.Surface`) предназначены для отображения функций вида Z = f(X, Y). Входные данные должны быть представлены в виде двумерных массивов, где каждый элемент `Z[i,j]` соответствует высоте над точкой `(X[i], Y[j])`. Plotly позволяет настраивать **контуры**: отображать их на самой поверхности или проецировать на плоскости XZ и YZ, что значительно улучшает восприятие формы.\n",
        "\n",
        "Полный контроль над 3D-сценой осуществляется через `layout.scene`. Ключевые параметры:\n",
        "\n",
        "- `camera.eye` — позиция камеры (вектор);\n",
        "- `aspectmode=\"manual\"` + `aspectratio` — соотношение масштабов по осям (важно для избежания искажений);\n",
        "- `xaxis.nticks` — количество тиков на оси.\n",
        "\n",
        "Пример 3D-поверхности с контурами:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Генерация сетки\n",
        "x = np.linspace(-5, 5, 50)\n",
        "y = np.linspace(-5, 5, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "fig = go.Figure(data=go.Surface(\n",
        "    x=X, y=Y, z=Z,\n",
        "    contours={\n",
        "        \"z\": {\"show\": True, \"start\": -1, \"end\": 1, \"size\": 0.1},\n",
        "        \"x\": {\"show\": True, \"start\": -5, \"end\": 5, \"size\": 1},\n",
        "        \"y\": {\"show\": True, \"start\": -5, \"end\": 5, \"size\": 1}\n",
        "    }\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    scene=dict(\n",
        "        aspectmode=\"manual\",\n",
        "        aspectratio=dict(x=1, y=1, z=0.5)\n",
        "    ),\n",
        "    title=\"3D-поверхность с контурами\"\n",
        ")\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "Такой подход позволяет не просто отобразить данные, но и подчеркнуть их структуру — например, сделать вертикальные колебания более заметными за счёт сжатия оси Z.\n",
        "\n",
        "#### 2.2. Геопространственный Анализ: Choropleth, Scattermapbox и Стили Карт\n",
        "\n",
        "Plotly поддерживает два подхода к картографии:\n",
        "\n",
        "- **Контурные карты** (`layout.geo`) — работают без интернета, но имеют ограниченную детализацию;\n",
        "- **Плиточные карты** (`Mapbox` / `Maplibre`) — используют внешние тайловые сервисы для отображения улиц, зданий и рельефа.\n",
        "\n",
        "Начиная с версии 5.24, Plotly рекомендует использовать **Maplibre-based следы** (`go.Scattermap`, `go.Choroplethmap`), которые не требуют токена и могут работать с открытыми тайловыми сервисами (например, Stadia Maps). В отличие от них, следы на основе **Mapbox** требуют регистрации и токена доступа, что создаёт зависимости при развёртывании в корпоративных средах.\n",
        "\n",
        "Одной из самых мощных возможностей является **композитная картография** — наложение нескольких слоёв. Например, можно отобразить:\n",
        "\n",
        "1. **Choroplethmapbox** — для раскраски регионов (например, областей по среднему доходу), используя GeoJSON-файл с границами;\n",
        "2. **Scattermapbox** — для отображения точек (например, местоположений торговых точек), наложенных поверх регионов.\n",
        "\n",
        "Особая сложность возникает при создании **легенды для размеров маркеров** в пузырьковых картах. Plotly не генерирует автоматическую легенду размеров, поэтому её приходится строить вручную: для каждого уникального размера создаётся отдельный «невидимый» след с соответствующим именем и размером, который отображается только в легенде.\n",
        "\n",
        "#### 2.3. Динамическая Визуализация: Анимации\n",
        "\n",
        "Plotly Express упрощает создание анимаций с помощью параметров `animation_frame` и `animation_group`. Первый определяет переменную, по которой строятся кадры (например, год), второй — объекты, которые следует отслеживать (например, страна).\n",
        "\n",
        "Ключевое методологическое требование: **фиксировать диапазоны осей**. Если этого не сделать, масштаб будет автоматически подстраиваться под данные каждого кадра, что разрушит визуальную непрерывность и сделает сравнение во времени некорректным. Например, в «гонке по странам» (race bar chart) ось значений должна охватывать диапазон от **глобального минимума до глобального максимума** во всём временном интервале.\n",
        "\n",
        "Пример анимированного scatter plot:\n",
        "\n",
        "```python\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"gdp_per_capita\",\n",
        "    y=\"life_expectancy\",\n",
        "    size=\"population\",\n",
        "    color=\"continent\",\n",
        "    hover_name=\"country\",\n",
        "    animation_frame=\"year\",\n",
        "    animation_group=\"country\",\n",
        "    range_x=[0, 80000],\n",
        "    range_y=[20, 100],\n",
        "    title=\"Изменение здоровья и богатства стран с течением времени\"\n",
        ")\n",
        "fig.show()\n",
        "```\n",
        "\n",
        "Здесь фиксированные `range_x` и `range_y` гарантируют, что движение точек отражает **реальные изменения**, а не артефакты масштабирования.\n",
        "\n"
      ],
      "metadata": {
        "id": "82Vvu-kdTRnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Раздел III. Основы Dash: Построение Реактивного Веб-Приложения\n",
        "\n",
        "### 3.1. Введение в Dash: Философия «No JavaScript Required»\n",
        "\n",
        "Dash — это декларативный и реактивный фреймворк с открытым исходным кодом, предназначенный для создания аналитических веб-приложений и дашбордов исключительно на языке Python. Под капотом он использует Flask в качестве бэкенда и комбинирует Plotly.js с React.js на фронтенде, автоматически преобразуя Python-объекты в HTML, CSS и JavaScript. Ключевое преимущество Dash для специалистов по данным — возможность строить полнофункциональные интерактивные веб-интерфейсы без написания ни строчки JavaScript, что значительно снижает порог входа в веб-разработку.\n",
        "\n",
        "### 3.2. Архитектура Приложения Dash: Инициализация и Макет\n",
        "\n",
        "Создание любого приложения Dash начинается с инициализации объекта:\n",
        "\n",
        "```python\n",
        "from dash import Dash, html\n",
        "app = Dash(__name__)\n",
        "```\n",
        "\n",
        "Этот объект инкапсулирует всю логику приложения. Макет интерфейса определяется через свойство `app.layout`, которое представляет собой декларативное дерево компонентов — фактически, описание DOM-структуры будущей веб-страницы. Макет, как правило, статичен при первом рендере, но может динамически изменяться через колбэки: например, при нажатии кнопки в контейнер `html.Div` может быть добавлен новый график или фильтр. Такой подход позволяет избежать прямой модификации `app.layout` и сохраняет предсказуемость реактивной системы.\n",
        "\n",
        "Приложение запускается вызовом:\n",
        "\n",
        "```python\n",
        "app.run_server(debug=True, port=8050)\n",
        "```\n",
        "\n",
        "В производственной среде, однако, встроенный сервер Flask заменяется на WSGI-совместимый сервер, такой как Gunicorn, для обеспечения стабильности и масштабируемости.\n",
        "\n",
        "### 3.3. HTML Компоненты (html) и Компоненты Dash Core (dcc)\n",
        "\n",
        "Интерфейс Dash строится из двух типов компонентов.\n",
        "\n",
        "Компоненты из модуля `dash.html` соответствуют стандартным HTML-тегам: `html.Div`, `html.H1`, `html.P` и т.д. Они используются для создания структуры страницы, заголовков, абзацев и контейнеров.\n",
        "\n",
        "Компоненты из модуля `dash.dcc` (Dash Core Components) предоставляют интерактивные элементы управления, характерные для аналитических дашбордов:\n",
        "\n",
        "- `dcc.Graph` — контейнер для встраивания фигур Plotly с полной поддержкой интерактивности (зум, выделение, тултипы);\n",
        "- `dcc.Dropdown`, `dcc.Slider`, `dcc.RadioItems` — элементы для выбора параметров и фильтрации;\n",
        "- `dcc.Tabs` — для организации многосекционного интерфейса;\n",
        "- `dcc.Location` и `dcc.Link` — для навигации в многостраничных приложениях.\n",
        "\n",
        "Важно помнить: все свойства компонентов должны быть JSON-сериализуемыми (строки, числа, списки, словари), так как они передаются между Python-бэкендом и React-фронтендом через JSON-мост.\n",
        "\n",
        "### 3.4. Стилевое Оформление и UI/UX Основы\n",
        "\n",
        "Стилизация в Dash осуществляется двумя способами:\n",
        "\n",
        "- через аргумент `className`, который связывает компонент с классами из внешней CSS-таблицы (например, `style.css`);\n",
        "- через аргумент `style`, принимающий словарь для inline-стилей.\n",
        "\n",
        "При проектировании аналитических дашбордов следует придерживаться принципов UI/UX:\n",
        "\n",
        "- **Информационная иерархия**: ключевые метрики (KPI) должны быть видны сразу, без прокрутки («above the fold»). Пользователь должен понимать суть дашборда за 5 секунд.\n",
        "- **Контекстуализация**: каждая метрика должна сопровождаться сравнением (например, «+12% к прошлому месяцу»).\n",
        "- **Адаптивность**: макет должен корректно отображаться на мобильных устройствах, с учётом сенсорного ввода.\n",
        "- **Визуальная чистота**: избыток элементов повышает когнитивную нагрузку. Белое пространство и минимализм улучшают читаемость.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел IV. Механизм Реактивности Dash: Колбэки и Управление Потоком\n",
        "\n",
        "Реактивность — сердце Dash. Она реализуется через систему **колбэков**, управляемых декоратором `@app.callback`.\n",
        "\n",
        "### 4.1. Жизненный Цикл Колбэка\n",
        "\n",
        "Колбэк — это обычная функция Python, связывающая входные и выходные свойства компонентов. Когда свойство, указанное как `Input`, изменяется (например, пользователь выбирает значение в выпадающем списке), Dash запускает функцию, передаёт ей текущие значения всех `Input` и `State`, и использует возвращаемые значения для обновления компонентов, указанных в `Output`.\n",
        "\n",
        "Dash строит **граф зависимостей** между компонентами. Если один колбэк обновляет `Output`, который является `Input` для другого колбэка, система гарантирует, что второй колбэк запустится только после того, как первый завершит обновление. Это предотвращает использование устаревших или несогласованных данных.\n",
        "\n",
        "### 4.2. Центральная Концепция: Роль Input, Output и State\n",
        "\n",
        "Понимание различий между `Input`, `Output` и `State` — ключ к стабильности приложения.\n",
        "\n",
        "- **`Output`** — свойство компонента, которое будет обновлено в результате работы колбэка. Оно не вызывает запуск функции.\n",
        "- **`Input`** — свойство, изменение которого **триггерит** выполнение колбэка.\n",
        "- **`State`** — свойство, значение которого **считывается** в момент запуска колбэка, но его изменение **не вызывает** запуск.\n",
        "\n",
        "Разделение `Input` и `State` критически важно для предотвращения циклических зависимостей. Например, если колбэк обновляет `dcc.Store`, а другой колбэк читает его как `State`, цикла не возникает. Если бы `dcc.Store` был `Input`, любое обновление вызывало бы бесконечный цикл.\n",
        "\n",
        "### 4.3. Продвинутые Паттерны Колбэков и Оптимизация Запуска\n",
        "\n",
        "Колбэк может иметь **несколько `Output`**, возвращая кортеж значений. Если обновление определённого `Output` не требуется, можно вернуть специальное значение `dash.no_update`, что предотвращает ненужную передачу данных в браузер.\n",
        "\n",
        "По умолчанию все колбэки запускаются при инициализации приложения. Для повышения производительности и избежания ошибок с динамически создаваемыми компонентами рекомендуется использовать параметр `prevent_initial_call=True`:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('graph', 'figure'),\n",
        "    Input('dropdown', 'value'),\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def update_graph(selected_value):\n",
        "    return create_figure(selected_value)\n",
        "```\n",
        "\n",
        "Это особенно важно для колбэков, выполняющих тяжёлые вычисления или запросы к базе данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел V. Продвинутые Паттерны Dash: Состояние и Производительность\n",
        "\n",
        "### 5.1. Управление Состоянием на Стороне Клиента: Компонент `dcc.Store`\n",
        "\n",
        "Для эффективного управления данными между колбэками используется невидимый компонент `dcc.Store`. Он хранит данные в браузере и позволяет избежать повторных вычислений.\n",
        "\n",
        "Свойство `storage_type` определяет место хранения:\n",
        "\n",
        "- `'memory'` — данные сбрасываются при перезагрузке;\n",
        "- `'session'` — сохраняются до закрытия вкладки;\n",
        "- `'local'` — сохраняются между сессиями.\n",
        "\n",
        "Выбор типа хранения влияет на UX: например, `'session'` позволяет сохранить выбранные фильтры при обновлении страницы. Однако важно помнить об ограничениях: безопасно хранить до 2 МБ данных. Это указывает на то, что Dash не предназначен для передачи больших массивов через браузер — агрегация должна происходить на сервере.\n",
        "\n",
        "### 5.2. Паттерны Использования `dcc.Store`\n",
        "\n",
        "Три ключевых сценария:\n",
        "\n",
        "1. **Кэширование**: результаты дорогих вычислений сохраняются в `Store` и используются другими колбэками.\n",
        "2. **Инициализация**: для запуска колбэка при загрузке страницы используется `modified_timestamp` как `Input`, а данные — как `State`.\n",
        "3. **Разрыв циклов**: один колбэк записывает в `Store` (`Output`), другой читает (`State`), предотвращая петлю.\n",
        "\n",
        "### 5.3. Взаимосвязь Графиков (Linked Brushing)\n",
        "\n",
        "Plotly поддерживает события взаимодействия пользователя, которые можно использовать в Dash для создания связанных визуализаций. Например, выделение точек на scatter plot может фильтровать карту или временной ряд.\n",
        "\n",
        "Это достигается через специальные свойства `dcc.Graph`:\n",
        "\n",
        "- `clickData` — данные по клику;\n",
        "- `selectedData` — точки, выделенные инструментами Lasso или Box Select;\n",
        "- `hoverData` — данные под курсором;\n",
        "- `relayoutData` — изменения масштаба или позиции.\n",
        "\n",
        "Пример linked brushing:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('map', 'figure'),\n",
        "    Input('scatter', 'selectedData')\n",
        ")\n",
        "def update_map(selected_data):\n",
        "    if selected_data is None:\n",
        "        filtered_df = df\n",
        "    else:\n",
        "        indices = [p['pointIndex'] for p in selected_data['points']]\n",
        "        filtered_df = df.iloc[indices]\n",
        "    return px.scatter_mapbox(filtered_df, ...)\n",
        "```\n",
        "\n",
        "Такой подход превращает дашборд из набора изолированных графиков в единый интерактивный аналитический инструмент.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел VI. Архитектура Масштабируемых Дашбордов и Производственное Развертывание\n",
        "\n",
        "### 6.1. Организация Многостраничных Приложений (Multi-Page Apps)\n",
        "\n",
        "Сложные дашборды часто требуют разделения на страницы. В Dash это достигается через комбинацию `dcc.Location` и `dcc.Link`.\n",
        "\n",
        "- `dcc.Location` отражает текущий путь в адресной строке (`pathname`);\n",
        "- `dcc.Link` создаёт переходы без перезагрузки страницы.\n",
        "\n",
        "Колбэк использует `pathname` как `Input` и возвращает соответствующий макет:\n",
        "\n",
        "```python\n",
        "@app.callback(Output('page-content', 'children'), Input('url', 'pathname'))\n",
        "def display_page(pathname):\n",
        "    if pathname == '/analytics':\n",
        "        return analytics_layout\n",
        "    elif pathname == '/reporting':\n",
        "        return reporting_layout\n",
        "    return home_layout\n",
        "```\n",
        "\n",
        "Такой подход реализует архитектуру Single Page Application (SPA) на чистом Python.\n",
        "\n",
        "### 6.2. Создание Дашбордов в Реальном Времени с `dcc.Interval`\n",
        "\n",
        "Для мониторинговых приложений используется компонент `dcc.Interval`, который с заданной периодичностью увеличивает свойство `n_intervals`. Это свойство служит `Input` для колбэка, обновляющего данные.\n",
        "\n",
        "Чтобы дать пользователю контроль над частотой обновления, можно связать `dcc.Interval` с `dcc.Slider`:\n",
        "\n",
        "```python\n",
        "@app.callback(\n",
        "    Output('interval-component', 'interval'),\n",
        "    Input('frequency-slider', 'value')\n",
        ")\n",
        "def update_interval(seconds):\n",
        "    return seconds * 1000  # перевод в миллисекунды\n",
        "```\n",
        "\n",
        "Это позволяет балансировать между актуальностью данных и нагрузкой на систему.\n",
        "\n",
        "### 6.3. Развертывание Приложений Dash (Gunicorn и Heroku)\n",
        "\n",
        "В производственной среде встроенный сервер Flask заменяется на WSGI-сервер, такой как **Gunicorn**. Для развёртывания на Heroku требуется:\n",
        "\n",
        "- файл `requirements.txt` со списком зависимостей;\n",
        "- файл `Procfile` со строкой:  \n",
        "  `web: gunicorn app:server`\n",
        "\n",
        "Здесь `app` — имя Python-файла (например, `app.py`), а `server` — переменная `app.server`, которую Dash предоставляет как внутренний Flask-сервер.\n",
        "\n",
        "Развертывание осуществляется через Git:\n",
        "\n",
        "```bash\n",
        "git push heroku main\n",
        "```\n",
        "\n",
        "Этот процесс подчёркивает, что, несмотря на простоту для пользователя, Dash — полноценный веб-фреймворк, требующий стандартных DevOps-практик.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Plotly и Dash образуют мощный, сквозной стек для создания профессиональных аналитических приложений на Python. Plotly обеспечивает глубокую, методологически обоснованную визуализацию — от 3D-поверхностей с контролем ракурса до многослойных геокарт, где регионы и точки анализируются совместно. Dash превращает эти визуализации в реактивные веб-приложения, где каждое действие пользователя мгновенно отражается на всех связанных элементах.\n",
        "\n",
        "Архитектура Dash, основанная на декларативном макете и строгом разделении `Input`/`State`/`Output`, обеспечивает стабильность и предсказуемость даже в сложных, многокомпонентных дашбордах. Использование `dcc.Store` для управления состоянием, `prevent_initial_call` для оптимизации и `dcc.Interval` для потоковых данных — это не просто технические приёмы, а методологические практики, обеспечивающие производительность и удобство.\n",
        "\n",
        "Наконец, возможность развёртывания через стандартные веб-инструменты (Gunicorn, Heroku) подтверждает зрелость экосистемы: специалист по данным может не только создать, но и доставить до пользователя полноценное веб-приложение, не выходя из привычной среды Python. В совокупности, Plotly и Dash демократизируют создание интерактивной аналитики, делая её доступной для всех, кто владеет языком данных."
      ],
      "metadata": {
        "id": "wcH89FfOUyS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Модуль 11: SymPy — Символьные вычисления и аналитическая математика\n",
        "\n",
        "### Раздел 1. Фундаментальные Основы Символьных Вычислений в SymPy\n",
        "\n",
        "#### 1.1. Философия SymPy: Символьные Объекты и Строгая Точность\n",
        "\n",
        "SymPy представляет собой полнофункциональную систему компьютерной алгебры (Computer Algebra System, CAS), написанную на языке Python. Её фундаментальное отличие от численных библиотек, таких как NumPy или SciPy, заключается в том, что она оперирует не приближёнными значениями, а абстрактными математическими символами и выражениями, сохраняя их точную алгебраическую форму на протяжении всех манипуляций.\n",
        "\n",
        "Основным строительным блоком в SymPy является символическая переменная, создаваемая с помощью класса `Symbol`. Для удобства работы в интерактивных средах, таких как Jupyter Notebook, рекомендуется вызывать функцию `init_printing()`, которая обеспечивает форматированный вывод математических выражений с использованием LaTeX/MathJax, делая их визуально идентичными записям в научных публикациях.\n",
        "\n",
        "SymPy обеспечивает строгую математическую точность. Все числовые объекты в SymPy наследуются от класса `Number` и его подклассов, включая `Integer` и `Rational`. Это означает, что рациональные числа, например, $2/3$, сохраняются в виде точной дроби, а не в виде приближения с плавающей точкой, что полностью исключает ошибки округления в аналитических вычислениях. Символьные константы, такие как $\\pi$, $e$ (представляется как `E`), мнимая единица $\\mathbf{i}$ (`I`) и бесконечность (`oo`), также обрабатываются символически.\n",
        "\n",
        "Способность сохранять числа в виде точных рациональных дробей или символьных констант является краеугольным камнем философии SymPy. При аналитическом выводе даже минимальное округление может скрыть алгебраическое тождество или нарушить каноническую форму выражения. В тех случаях, когда требуется взаимодействие с внешними численными системами или вывод десятичного приближения, используется метод `.evalf()` или функция `N()`. Эти методы позволяют явно указать необходимую точность — например, до 50 знаков после запятой, что обеспечивает контролируемый и воспроизводимый переход от точной символьной формы к численному приближению.\n",
        "\n",
        "#### 1.2. Внутренняя Структура Выражений: Древовидная Интерпретация\n",
        "\n",
        "В SymPy любое математическое выражение интерпретируется как древовидная структура — иерархия объектов, где каждый узел представляет операцию, а листья — атомарные символы или числа. Эта структура лежит в основе всех алгоритмов компьютерной алгебры и позволяет системе последовательно применять правила преобразования.\n",
        "\n",
        "Каждый символьный объект обладает двумя ключевыми атрибутами: `func` и `args`. Атрибут `func` указывает на класс операции, определяющей тип узла (например, `Add`, `Mul`, `Pow`), а `args` — это кортеж дочерних узлов. Например, в выражении $x \\cdot y$ атрибут `func` будет ссылаться на класс `Mul`, а `args` — на кортеж `(x, y)`.\n",
        "\n",
        "SymPy применяет принцип алгебраической канонизации, стремясь к минимальному набору базовых операций. Так, операция деления $x/y$ не имеет отдельного класса `Div`; вместо этого она интерпретируется как умножение $x$ на $y^{-1}$, то есть как `Mul(x, Pow(y, -1))`. Аналогично, выражение $\\cos(a + b)$ представляется как объект `Cos`, чьим единственным аргументом является операция сложения `Add(a, b)`. Такая унификация упрощает реализацию преобразований и повышает стабильность системы.\n",
        "\n",
        "В контексте отладки или разработки алгоритмов, требующих явного контроля над порядком операций, может потребоваться предотвращение автоматической оценки. Это достигается либо передачей параметра `evaluate=False` при создании выражения, либо использованием класса `UnevaluatedExpr`. Например, выражение `x * UnevaluatedExpr(1/x)` сохранит свою форму и не будет автоматически сокращено до единицы.\n",
        "\n",
        "#### 1.3. Система Допущений (Assumptions System)\n",
        "\n",
        "Система допущений SymPy — это критически важный механизм, позволяющий выполнять алгебраические преобразования, зависящие от области определения переменных. По умолчанию SymPy работает в наиболее общем домене — комплексных числах. Этот консервативный подход означает, что если не заданы явные ограничения, система не будет выполнять упрощения, которые могут быть неверны для произвольного комплексного числа. Например, упрощение $\\sqrt{y^2}$ до $y$ корректно только при условии, что $y$ — неотрицательное вещественное число.\n",
        "\n",
        "Допущения декларируются при создании символа с помощью ключевых слов, таких как `positive=True`, `real=True` или `integer=True`. Например, объявление `y = Symbol('y', positive=True)` позволяет SymPy автоматически упростить $\\sqrt{y^2}$ до $y$. Без этого допущения результат останется в виде $\\sqrt{y^2}$, чтобы сохранить корректность в комплексной плоскости.\n",
        "\n",
        "Система допущений использует трёхзначную нечёткую логику: запросы о свойствах выражения (например, `expr.is_positive`) могут возвращать `True`, `False` или `None`, где `None` означает, что свойство не может быть однозначно определено. Кроме того, система способна к логической инференции: если символ объявлен как `integer=True`, SymPy автоматически выводит, что он также является рациональным (`rational=True`), поскольку любое целое число — рационально. Аналогично, из `positive=True` следует `negative=False`.\n",
        "\n",
        "В прикладном математическом моделировании — особенно в физике и инженерии — переменные, представляющие физические величины (масса, время, длина), всегда являются положительными вещественными числами. Неиспользование допущений в таких задачах заставляет SymPy придерживаться консервативных правил, что может привести к избыточно сложным результатам. Явное декларирование домена не только упрощает аналитические формы, но и гарантирует, что полученное решение соответствует физической реальности.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 2. Задача 1: Аналитическое Упрощение и Канонические Формы Выражений\n",
        "\n",
        "Символьное упрощение — одна из наиболее востребованных и одновременно методологически сложных задач в компьютерной алгебре. Сложность заключается в том, что понятие «простоты» не имеет универсального определения: для одних задач предпочтительна разложенная (раскрытая) форма, для других — факторизованная, а для третьих — тригонометрическое тождество.\n",
        "\n",
        "#### 2.1. Методологическое Различие: Эвристика vs. Гарантированный Алгоритм\n",
        "\n",
        "SymPy предлагает два подхода к упрощению.\n",
        "\n",
        "Функция `simplify()` — это универсальный эвристический решатель, который пытается применить множество специализированных алгоритмов (тригонометрических, полиномиальных, для специальных функций) и выбирает результат, который, по её мнению, является «наиболее простым». Несмотря на свою мощь, `simplify()` не гарантирует достижения желаемой формы и может быть неэффективной из-за попытки применить широкий спектр преобразований.\n",
        "\n",
        "Поэтому для надёжного аналитического вывода предпочтение отдается **специализированным функциям**, которые гарантируют приведение выражения к определённой алгебраической канонической форме. Такой подход обеспечивает предсказуемость результата, что критически важно при подготовке формул для экспорта, автоматической генерации кода или дальнейшего алгоритмического анализа.\n",
        "\n",
        "#### 2.2. Полиномиальная и Рациональная Алгебра\n",
        "\n",
        "Для работы с полиномами и рациональными функциями SymPy предоставляет набор гарантированных алгоритмов:\n",
        "\n",
        "- **Факторизация (`factor`)** разлагает полином с рациональными коэффициентами на неприводимые множители. Это полезно для нахождения корней или анализа полюсов рациональной функции. Например, `factor(x**2 - 1)` даёт $(x - 1)(x + 1)$.\n",
        "- **Раскрытие (`expand`)** приводит выражение к форме суммы, раскрывая все произведения и степени. Это каноническая форма для многих алгебраических операций: `expand((x + 1)**2)` возвращает $x^2 + 2x + 1$.\n",
        "- **Сбор членов (`collect`)** организует полином по степеням заданной переменной, группируя коэффициенты.\n",
        "- **Общий знаменатель (`together`)** объединяет сумму рациональных функций в одну дробь $P(x)/Q(x)$.\n",
        "- **Разложение на простейшие дроби (`apart`)** выполняет классическое разложение рациональной функции на элементарные слагаемые, что незаменимо в интегрировании и теории управления.\n",
        "\n",
        "#### 2.3. Специализированные Преобразования\n",
        "\n",
        "Для других классов функций используются соответствующие алгоритмы:\n",
        "\n",
        "- **Тригонометрия (`trigsimp`)** применяет тригонометрические тождества. Классический пример: `trigsimp(sin(x)**2 + cos(x)**2)` преобразуется в единицу.\n",
        "- **Степени (`powsimp`)** упрощает выражения, содержащие степени с одинаковыми основаниями или показателями.\n",
        "- **Специальные функции**: SymPy содержит алгоритмы для упрощения выражений с гамма-функцией, дзета-функцией и другими специальными математическими объектами.\n",
        "\n",
        "#### 2.4. Кейс-стади (Инженерия): Аналитическое Упрощение Коэффициентов\n",
        "\n",
        "В инженерии и физике при выводе уравнений движения или передаточных функций часто возникают сложные рациональные выражения, требующие упрощения. Рассмотрим пример:\n",
        "\n",
        "$$E(x) = \\frac{x^3 + x^2 - x - 1}{x^2 + 2x + 1}$$\n",
        "\n",
        "Для получения чистой и вычислительно эффективной формы, пригодной для кодогенерации или дальнейшего анализа, предпочтительны гарантированные методы, а не эвристический `simplify()`.\n",
        "\n",
        "```python\n",
        "from sympy import symbols, factor\n",
        "\n",
        "x = symbols('x')\n",
        "expr = (x**3 + x**2 - x - 1) / (x**2 + 2*x + 1)\n",
        "\n",
        "# Факторизация числителя и знаменателя\n",
        "num = factor(x**3 + x**2 - x - 1)  # (x - 1)*(x + 1)**2\n",
        "den = factor(x**2 + 2*x + 1)       # (x + 1)**2\n",
        "\n",
        "# Сокращение\n",
        "simplified = num / den  # x - 1\n",
        "```\n",
        "\n",
        "Преимущество специализированных функций, таких как `factor()`, заключается в том, что они обеспечивают вывод в предсказуемой канонической форме. При разработке систем, где символьные формулы экспортируются для численного расчёта (например, в C или Fortran), требуется, чтобы выражения были минимальны с точки зрения вычислительной сложности. Специализированные алгоритмы гарантируют получение наиболее эффективной алгебраической формы, чего нельзя сказать об эвристическом подходе.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 3. Задача 2: Решение Алгебраических и Трансцендентных Уравнений\n",
        "\n",
        "Аналитическое решение уравнений — ключевая задача символьных вычислений, позволяющая находить точные корни или параметрические зависимости.\n",
        "\n",
        "#### 3.1. Современный Алгоритмический Подход: `solveset`\n",
        "\n",
        "Исторически SymPy использовал функцию `solve()`, но из-за её неконсистентного интерфейса и неспособности чётко различать типы решений (отсутствие, конечное или бесконечное множество) был разработан новый, методологически строгий интерфейс — `solveset()`.\n",
        "\n",
        "Функция `solveset(equation, variable, domain=S.Complexes)` возвращает строгий математический объект типа `Set`, что позволяет однозначно представлять различные типы решений: `EmptySet` (нет решений), `FiniteSet` (конечное число корней) или `ImageSet` (бесконечное множество, например, для уравнения $\\sin(x) = 0$).\n",
        "\n",
        "Использование `solveset()` требует явного задания области определения. По умолчанию это комплексные числа, но для прикладных задач в физике или экономике чаще всего используется `domain=S.Reals`.\n",
        "\n",
        "#### 3.2. Решение Систем Уравнений\n",
        "\n",
        "Для систем уравнений SymPy предлагает специализированные решатели:\n",
        "\n",
        "- **`linsolve`** применяет матричные методы для надёжного решения линейных систем.\n",
        "- **`nonlinsolve`** предназначен для систем нелинейных или полиномиальных уравнений. Результат возвращается в виде множества кортежей, где каждый кортеж соответствует полному решению по всем переменным.\n",
        "\n",
        "Например, решение системы $a^2 + a = 0$ и $a - b = 0$ с помощью `nonlinsolve` даёт множество $\\{(-1, -1), (0, 0)\\}$.\n",
        "\n",
        "#### 3.3. Кейс-стади (Экономика): Аналитическое Выведение Оптимального Выбора\n",
        "\n",
        "В экономическом моделировании символьные вычисления незаменимы для вывода параметрических формул, описывающих равновесие. Рассмотрим классическую задачу потребительского выбора в модели Кобба-Дугласа: максимизация полезности $U(x_0, x_1) = x_0^\\alpha x_1^{1-\\alpha}$ при бюджетном ограничении $p_0 x_0 + p_1 x_1 = I$.\n",
        "\n",
        "После применения метода множителей Лагранжа получается система нелинейных уравнений — условий первого порядка (FOCs). Эти уравнения передаются в `nonlinsolve`, чтобы найти оптимальные объёмы спроса $x_0^*$ и $x_1^*$ как функции параметров $I, p_0, p_1, \\alpha$.\n",
        "\n",
        "```python\n",
        "from sympy import symbols, nonlinsolve, Eq\n",
        "\n",
        "x0, x1, p0, p1, I, alpha, L = symbols('x0 x1 p0 p1 I alpha Lambda')\n",
        "\n",
        "# Условия первого порядка\n",
        "eq1 = Eq(alpha * x0**(alpha - 1) * x1**(1 - alpha), L * p0)\n",
        "eq2 = Eq((1 - alpha) * x0**alpha * x1**(-alpha), L * p1)\n",
        "eq3 = Eq(p0 * x0 + p1 * x1, I)\n",
        "\n",
        "# Решение системы\n",
        "solution = nonlinsolve([eq1, eq2, eq3], [x0, x1, L])\n",
        "# Аналитический результат:\n",
        "# x0* = I * alpha / p0\n",
        "# x1* = I * (1 - alpha) / p1\n",
        "```\n",
        "\n",
        "Получение точной параметрической формулы спроса — центральный элемент теоретического анализа. В отличие от численного подхода, символьное решение позволяет доказывать фундаментальные теоремы (например, о гомогенности функций спроса) и создаёт основу для эффективных численных реализаций, где градиенты и производные уже выведены аналитически.\n",
        ""
      ],
      "metadata": {
        "id": "ZcnCE0q9nzfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Раздел 4. Задача 3: Символьный Анализ Функций (Дифференциальное Исчисление)\n",
        "\n",
        "Символьное дифференциальное исчисление является основой для анализа изменения функций, линеаризации нелинейных моделей, вычисления градиентов и построения уравнений движения в физике и инженерии.\n",
        "\n",
        "#### 4.1. Дифференцирование: Оператор vs. Результат\n",
        "\n",
        "SymPy обеспечивает гибкость в работе с производными, предоставляя как немедленное вычисление, так и символическое представление оператора. Функция `diff(expr, var)` немедленно вычисляет производную выражения `expr` по переменной `var`. Она поддерживает частные производные, в том числе повторное дифференцирование по одной или нескольким переменным.\n",
        "\n",
        "В тех случаях, когда требуется сохранить структуру дифференциального оператора без немедленного вычисления — например, для построения сложных уравнений в теоретической физике — используется класс `Derivative(expr, var)`. Этот объект представляет собой символическую запись оператора $\\frac{d}{dx}f(x)$. Фактическое вычисление производной выполняется вызовом метода `.doit()` на экземпляре `Derivative`.\n",
        "\n",
        "#### 4.2. Векторное Исчисление\n",
        "\n",
        "Модуль `sympy.vector` реализует оператор Набла ($\\nabla$) через класс `Del()`, который не привязан к конкретной системе координат. Это позволяет символьно вычислять ключевые характеристики скалярных и векторных полей — градиент, дивергенцию и ротор.\n",
        "\n",
        "Градиент скалярного поля создаётся как применение `Del()` к полю, что возвращает выражение с невычисленными операторами `Derivative`. Для получения конкретного результата необходимо вызвать `.doit()`. Аналогично вычисляются дивергенция (`delop.dot(vector_field)`) и ротор (`delop.cross(vector_field)`), что делает SymPy мощным инструментом для аналитической электродинамики, гидродинамики и теории упругости.\n",
        "\n",
        "```python\n",
        "from sympy.vector import CoordSys3D, Del\n",
        "\n",
        "C = CoordSys3D('C')\n",
        "delop = Del()\n",
        "# Скалярное поле\n",
        "scalar_field = C.x * C.y * C.z\n",
        "\n",
        "# Символический градиент\n",
        "gradient_field = delop(scalar_field)\n",
        "# Фактическое вычисление\n",
        "result = gradient_field.doit()\n",
        "# Результат: C.y*C.z*C.i + C.x*C.z*C.j + C.x*C.y*C.k\n",
        "```\n",
        "\n",
        "#### 4.3. Аппроксимация Функций: Ряды Тейлора\n",
        "\n",
        "Символьное вычисление ряда Тейлора — фундаментальный инструмент для локальной аппроксимации функций. Функция `series(formula, variable, center_point, degree)` возвращает разложение вокруг заданной точки. Вывод включает остаточный член вида $O((x - a)^n)$, который символически обозначает все члены более высокого порядка.\n",
        "\n",
        "Для практических целей — например, линеаризации уравнений — необходим чистый полином без остатка. Это достигается вызовом метода `.removeO()`, который удаляет символический остаток и оставляет лишь полиномиальную часть разложения. Такой подход лежит в основе методов возмущений, линеаризации нелинейных систем и анализа устойчивости.\n",
        "\n",
        "#### 4.4. Кейс-стади (Инженерия): Анализ Устойчивости и Линеаризация\n",
        "\n",
        "В динамических системах — будь то механические конструкции, экономические модели или системы управления — часто требуется линеаризовать нелинейные уравнения движения вокруг точки равновесия.\n",
        "\n",
        "Процесс включает три шага. Сначала находится точка равновесия $\\mathbf{x}_e$, где вектор скорости $\\mathbf{f}(\\mathbf{x}_e) = \\mathbf{0}$. Затем вычисляется матрица Якоби $\\mathbf{J} = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$ — это делается символьно с помощью метода `.jacobian()` для объекта `Matrix`. Наконец, анализируются собственные значения $\\lambda_i$ матрицы $\\mathbf{J}$, вычисленные через `.eigenvals()`. Если вещественная часть всех $\\lambda_i$ отрицательна, система устойчива в окрестности $\\mathbf{x}_e$.\n",
        "\n",
        "SymPy позволяет выполнить весь этот процесс в символьной форме. Результат — параметрические выражения для собственных значений — показывает, как устойчивость зависит от физических параметров модели (масс, коэффициентов трения, жёсткости). Это невозможно в рамках чисто численного подхода и делает SymPy незаменимым в теоретическом анализе.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 5. Задача 4: Интегральное Исчисление и Дифференциальные Уравнения\n",
        "\n",
        "#### 5.1. Символьное Интегрирование: Точность и Пределы\n",
        "\n",
        "Функция `integrate()` служит основным инструментом для вычисления первообразных и определённых интегралов. Для неопределённого интеграла достаточно передать выражение и переменную, например, `integrate(cos(x), x)`. Важно отметить, что SymPy **не добавляет константу интегрирования** $C$ автоматически — её необходимо учитывать вручную или использовать `dsolve()` для задач, где константы критичны.\n",
        "\n",
        "Определённый интеграл вычисляется передачей кортежа `(переменная, нижний_предел, верхний_предел)`. SymPy поддерживает символическую бесконечность `oo`, что позволяет вычислять несобственные интегралы, такие как $\\int_0^{\\infty} e^{-x} dx = 1$. Также возможно многократное интегрирование — например, вычисление двойных или тройных интегралов через передачу нескольких кортежей с пределами.\n",
        "\n",
        "#### 5.2. Алгоритм Риша и Неэлементарные Функции\n",
        "\n",
        "SymPy использует детерминированный **алгоритм Риша** для интегрирования элементарных функций. Этот алгоритм обладает уникальным свойством: если первообразная существует в классе элементарных функций, она будет найдена; если нет — алгоритм доказывает неэлементарность интеграла.\n",
        "\n",
        "Например, интеграл $\\int e^{-x^2} dx$ не может быть выражен через элементарные функции, и SymPy оставит его в виде объекта `NonElementaryIntegral` или просто не вычислит, в зависимости от контекста. Ограничение алгоритма Риша — его неприменимость к специальным функциям (Бесселя, гипергеометрическим и др.), которые часто встречаются в физике. В таких случаях требуется расширение базы функций или переход к численным методам.\n",
        "\n",
        "#### 5.3. Решение Обыкновенных Дифференциальных Уравнений (ОДУ)\n",
        "\n",
        "Ключевым инструментом для аналитического решения ОДУ является функция `dsolve()`. Она принимает уравнение (в виде `Eq` или выражения, равного нулю) и искомую функцию. Результат возвращается как объект `Eq`, поскольку решения часто оказываются неявными.\n",
        "\n",
        "Функция `dsolve()` автоматически вводит произвольные константы интегрирования ($C_1, C_2, \\dots$), количество которых соответствует порядку уравнения. Для нахождения частного решения можно передать начальные или краевые условия через параметр `ics`. Например, `dsolve(eq, y, ics={y.subs(t, 0): 1})` задаёт условие $y(0) = 1$.\n",
        "\n",
        "#### 5.4. Кейс-стади (Физика): Аналитическое Решение Динамического Уравнения\n",
        "\n",
        "Рассмотрим классическую задачу радиоактивного распада, описываемую ОДУ первого порядка:\n",
        "$$\n",
        "\\frac{dy(t)}{dt} = - \\lambda y(t)\n",
        "$$\n",
        "где $y(t)$ — количество вещества, $\\lambda$ — константа распада.\n",
        "\n",
        "```python\n",
        "import sympy as sym\n",
        "t, l = sym.symbols('t lambda')\n",
        "y = sym.Function('y')(t)\n",
        "expr = sym.Eq(y.diff(t), -l * y)\n",
        "\n",
        "solution = sym.dsolve(expr, y)\n",
        "# Результат: y(t) = C1*exp(-l*t)\n",
        "```\n",
        "\n",
        "Это решение играет двойную роль. Во-первых, оно даёт точную замкнутую формулу для анализа поведения системы. Во-вторых, оно служит **аналитическим эталоном** для верификации численных методов. Даже если полная модель нелинейна и не поддаётся аналитическому решению, её упрощённая линейная версия может быть решена в SymPy и использована для проверки корректности численного интегратора.\n",
        "\n",
        "---\n",
        "\n",
        "### Раздел 6. Применение: Сквозное Символическое Моделирование\n",
        "\n",
        "SymPy выходит за рамки отдельных математических операций, обеспечивая поддержку полного цикла аналитического моделирования в прикладных науках.\n",
        "\n",
        "#### 6.1. Аналитический Вывод Уравнений Движения (Физика/Механика)\n",
        "\n",
        "Модуль `sympy.physics.mechanics` позволяет формализовать задачи классической механики. Одним из самых мощных подходов является **метод Лагранжа**: пользователь задаёт кинетическую ($T$) и потенциальную ($U$) энергии, SymPy формирует лагранжиан $L = T - U$ и автоматически генерирует уравнения движения через класс `LagrangesMethod`. Это устраняет многочасовую рутинную алгебру и исключает ошибки при выводе сложных ОДУ. Полученные уравнения могут быть либо решены аналитически, либо преобразованы в численные функции для симуляции.\n",
        "\n",
        "#### 6.2. Символическая Статистика и Вероятность\n",
        "\n",
        "Модуль `sympy.stats` позволяет работать с вероятностными распределениями в аналитической форме. Например, для равномерного распределения $X \\sim U(90, 100)$ функция `E(X)` возвращает точное значение $95$, а не оценку по выборке. Также возможно символьное вычисление плотности вероятности, кумулятивной функции распределения и вероятностей событий вида $P(X < a)$. Это особенно ценно в теоретической статистике и при выводе распределений оценок.\n",
        "\n",
        "#### 6.3. Вычислительная Алгебра и Линейные Системы\n",
        "\n",
        "SymPy предоставляет полный набор инструментов для работы с **символьными матрицами**. Можно вычислять определители, обратные матрицы, собственные значения и векторы, а также приводить матрицы к каноническим формам, таким как жорданова. Это критически важно в теории управления (анализ устойчивости), механике (модальный анализ) и квантовой физике (диагонализация гамильтонианов). Результат — параметрические формулы, показывающие, как свойства системы зависят от её параметров.\n",
        "\n",
        "#### 6.4. Переход к Численным Вычислениям и Визуализация\n",
        "\n",
        "Завершающий этап — **конвертация** символьных выражений в эффективный численный код. Функция `lambdify` преобразует выражение SymPy в быструю Python-функцию (с опциональной поддержкой NumPy или SciPy). Также возможен экспорт в C, Fortran или Julia для интеграции в высокопроизводительные симуляции.\n",
        "\n",
        "Кроме того, SymPy поддерживает **аналитическую визуализацию**: модуль `plot` позволяет строить 2D- и 3D-графики символьных функций, `plot_complex` — отображать комплексные функции методом цветового кодирования (domain coloring), а `plot_vector` — визуализировать векторные поля. Это делает возможным не только вычисление, но и непосредственное восприятие аналитических результатов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Раздел 7. Задача 5: Интегральные Преобразования и Асимптотический Анализ\n",
        "\n",
        "Интегральные преобразования представляют собой мощный аналитический аппарат для решения дифференциальных уравнений, анализа сигналов и изучения поведения функций в предельных режимах. В отличие от локальных методов, таких как ряды Тейлора, интегральные преобразования работают с функцией на всей её области определения, конвертируя дифференциальные операции в алгебраические. SymPy предоставляет символьные реализации ключевых преобразований, что позволяет получать точные аналитические решения и избегать ошибок, связанных с численной аппроксимацией.\n",
        "\n",
        "#### 7.1. Преобразование Лапласа: Решение Линейных ОДУ с Начальными Условиями\n",
        "\n",
        "Преобразование Лапласа является стандартным инструментом в теории управления, электротехнике и механике для анализа линейных динамических систем. Оно переводит функцию времени $f(t)$ в функцию комплексной переменной $F(s)$ по формуле:\n",
        "\\[\n",
        "F(s) = \\mathcal{L}\\{f(t)\\} = \\int_0^\\infty f(t) e^{-st}  dt.\n",
        "\\]\n",
        "Ключевое преимущество заключается в том, что **дифференцирование во временной области** превращается в **умножение на $s$** в частотной:\n",
        "\\[\n",
        "\\mathcal{L}\\{f'(t)\\} = s F(s) - f(0).\n",
        "\\]\n",
        "Это позволяет преобразовать линейное ОДУ с постоянными коэффициентами в алгебраическое уравнение относительно $F(s)$, решить его и затем применить обратное преобразование.\n",
        "\n",
        "SymPy реализует этот процесс через функции `laplace_transform` и `inverse_laplace_transform`. Они корректно обрабатывают начальные условия и возвращают результат в аналитической форме.\n",
        "\n",
        "> **Пример: колебательная система под воздействием ступенчатого сигнала**\n",
        "\n",
        "Рассмотрим уравнение вынужденных колебаний без затухания:\n",
        "\\[\n",
        "\\frac{d^2 y}{dt^2} + \\omega^2 y = u(t), \\quad y(0) = 0, \\quad y'(0) = 0,\n",
        "\\]\n",
        "где $u(t)$ — функция Хевисайда (ступенька).\n",
        "\n",
        "```python\n",
        "import sympy as sym\n",
        "t, s, w = sym.symbols('t s omega', positive=True)\n",
        "y = sym.Function('y')\n",
        "\n",
        "# Уравнение в символьной форме\n",
        "ode = sym.Eq(y(t).diff(t, t) + w**2 * y(t), sym.Heaviside(t))\n",
        "\n",
        "# Прямое преобразование Лапласа\n",
        "Y_s = sym.laplace_transform(ode.lhs, t, s)[0] - sym.laplace_transform(ode.rhs, t, s)[0]\n",
        "# После учёта начальных условий: Y_s = s**2 * Y(s) + w**2 * Y(s) - 1/s\n",
        "\n",
        "# Решение для Y(s)\n",
        "Y_s_solution = 1 / (s * (s**2 + w**2))\n",
        "\n",
        "# Обратное преобразование\n",
        "y_t = sym.inverse_laplace_transform(Y_s_solution, s, t)\n",
        "# Результат: y(t) = (1 - cos(omega*t)) / omega**2\n",
        "```\n",
        "\n",
        "Такой подход не только даёт точное решение, но и позволяет анализировать **передачу сигнала** через систему в частотной области, что является основой для построения диаграмм Боде и анализа устойчивости.\n",
        "\n",
        "#### 7.2. Преобразование Фурье: Анализ Частотного Спектра\n",
        "\n",
        "Преобразование Фурье служит для разложения функции на гармонические компоненты и широко применяется в обработке сигналов, квантовой механике и теории вероятностей. SymPy поддерживает несколько соглашений о нормировке, но по умолчанию использует физическое определение:\n",
        "\\[\n",
        "\\mathcal{F}\\{f(x)\\} = \\int_{-\\infty}^{\\infty} f(x) e^{-i k x}  dx.\n",
        "\\]\n",
        "\n",
        "Функции `fourier_transform` и `inverse_fourier_transform` позволяют точно вычислять спектры даже для обобщённых функций, таких как дельта-функция Дирака $\\delta(x)$ или гауссиан $e^{-x^2}$. Например, преобразование Фурье от гауссиана является гауссианом, что является фундаментальным свойством в теории неопределённости.\n",
        "\n",
        "> **Пример: спектр прямоугольного импульса**\n",
        "\n",
        "```python\n",
        "x, k = sym.symbols('x k', real=True)\n",
        "rect_pulse = sym.Piecewise((1, sym.Abs(x) < 1), (0, True))\n",
        "\n",
        "# Преобразование Фурье\n",
        "F_k = sym.fourier_transform(rect_pulse, x, k)\n",
        "# Результат: 2*sin(k)/k (функция sinc)\n",
        "```\n",
        "\n",
        "Этот результат напрямую связывает ширину импульса во временной области с шириной его спектра — ключевой принцип в теории связи.\n",
        "\n",
        "#### 7.3. Асимптотический Анализ: Поведение Функций в Беспредельных Режимах\n",
        "\n",
        "Помимо локальной аппроксимации (ряд Тейлора), часто требуется понять, как ведёт себя функция при $x \\to \\infty$ или $x \\to 0^+$. Для этого используется **асимптотическое разложение**, которое может включать не только степени, но и логарифмические члены или экспоненциально малые слагаемые.\n",
        "\n",
        "SymPy предоставляет функцию `asymptotic_series`, но на практике чаще используется метод `.asymptotic_expand()` или комбинация `limit` и `series` с указанием направления (`dir='+'` или `dir='-'`).\n",
        "\n",
        "> **Пример: асимптотика интегрального синуса**\n",
        "\n",
        "Интегральный синус $\\text{Si}(x) = \\int_0^x \\frac{\\sin t}{t} dt$ при $x \\to \\infty$ стремится к $\\pi/2$. Асимптотическое разложение показывает, как именно происходит это приближение:\n",
        "\n",
        "```python\n",
        "x = sym.symbols('x', positive=True)\n",
        "Si = sym.Si(x)\n",
        "asympt = Si.asymptotic_expand(x, n=3)  # до 3-го порядка\n",
        "# Результат: pi/2 - cos(x)/x - sin(x)/x**2 + O(1/x**3)\n",
        "```\n",
        "\n",
        "Такое разложение критически важно в физике рассеяния, где необходимо учитывать осциллирующие поправки к предельному значению.\n",
        "\n",
        "#### 7.4. Связь с Дифференциальными Уравнениями и Специальными Функциями\n",
        "\n",
        "Интегральные преобразования неразрывно связаны со специальными функциями. Например, решение уравнения Бесселя или гипергеометрического уравнения часто выражается через функции, чьи интегральные представления являются каноническими. SymPy позволяет не только вычислять преобразования, но и **распознавать** специальные функции в результатах, что обеспечивает согласованность с теоретическими справочниками.\n",
        "\n",
        "Более того, преобразования предоставляют **альтернативный путь к решению ОДУ**, особенно в случае сингулярных коэффициентов или краевых задач на бесконечности, где методы конечных разностей или прямое применение `dsolve` могут оказаться неэффективными.\n",
        "\n",
        "---\n",
        "\n",
        "> **Методологическое значение**  \n",
        "> Интегральные преобразования и асимптотический анализ завершают картину символьного моделирования, переходя от **локального** (дифференцирование, ряды Тейлора) к **глобальному** описанию поведения систем. SymPy, предоставляя точные реализации этих методов, позволяет исследователю не просто получить численный ответ, но и понять **физическую или математическую структуру** решения — его частотный состав, устойчивость, асимптотику и связь с фундаментальными функциями математической физики.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "SymPy является незаменимым инструментом в арсенале математического моделирования, обеспечивая строгую аналитическую базу, недоступную в чисто численных пакетах. Его методологическая сила основана на трёх ключевых принципах.\n",
        "\n",
        "Во-первых, **строгая алгебраическая точность**: использование символьных объектов, рациональных чисел и системы допущений гарантирует, что все выводы корректны в заданной области определения и свободны от ошибок округления.\n",
        "\n",
        "Во-вторых, **древовидная структура и канонизация**: интерпретация выражений как алгебраических деревьев позволяет применять гарантированные алгоритмы — такие как факторизация, алгоритм Риша или символьное дифференцирование, — которые приводят к предсказуемым, эффективным и математически корректным формам.\n",
        "\n",
        "В-третьих, **сквозная аналитика**: SymPy поддерживает полный цикл моделирования — от вывода уравнений движения в механике и градиентов в оптимизации, до решения дифференциальных уравнений и анализа устойчивости. Он выступает в роли **аналитического процессора**, который берёт на себя сложнейшие этапы формального вывода, после чего передаёт точные, верифицированные формулы численной машине.\n",
        "\n",
        "Таким образом, SymPy не просто дополняет численные библиотеки — он создаёт над ними **надёжный теоретический фундамент**, повышая достоверность, воспроизводимость и глубину современного научного и инженерного моделирования."
      ],
      "metadata": {
        "id": "5fSSgCWqecfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 10: SciPy — Научные и Инженерные Вычисления\n",
        "\n",
        "SciPy является фундаментальным компонентом экосистемы научных вычислений на языке Python, предлагая обширную коллекцию высокоуровневых алгоритмов, предназначенных для решения сложных математических, инженерных и научных задач. Этот модуль служит мостом между эффективными структурами данных NumPy и специализированными, проверенными временем численными процедурами, охватывающими оптимизацию, интеграцию, линейную алгебру, обработку сигналов и статистический анализ. Использование SciPy позволяет инженерам и исследователям формулировать сложные вычислительные задачи в виде высокоуровневых Python-команд, полагаясь при этом на скорость и надежность низкоуровневых языков программирования, таких как C и Fortran.\n",
        "\n",
        "Это первая часть в цикле материалов, посвящённых ключевым Python-библиотекам для научных и инженерных вычислений. В ней рассматриваются архитектурные особенности SciPy, его интеграция с другими компонентами экосистемы, а также подробный разбор наиболее важных подмодулей и алгоритмов с акцентом на численную устойчивость и практическое применение.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Фундаментальная Архитектура и Интеграция SciPy\n",
        "\n",
        "### 1.1. Место SciPy в экосистеме Python\n",
        "\n",
        "SciPy функционирует как библиотека, которая значительно расширяет возможности NumPy, предоставляя специализированные подпрограммы, работающие непосредственно с массивами NumPy. Его субмодули — такие как `scipy.optimize`, `scipy.integrate` и `scipy.linalg` — содержат высокоэффективные алгоритмы, необходимые для моделирования и анализа данных. Разделение функций между NumPy (базовая работа с массивами, элементарная линейная алгебра) и SciPy (продвинутые численные методы) обеспечивает модульность и чистоту архитектуры.\n",
        "\n",
        "Эта интеграция позволяет использовать Python для сквозных научных рабочих процессов: от загрузки и манипуляции данными (NumPy/Pandas) до сложного численного анализа (SciPy) и визуализации (Matplotlib). Такой стек обеспечивает полную независимость от закрытых коммерческих систем (например, MATLAB), сохраняя при этом высокую производительность и гибкость.\n",
        "\n",
        "**Пример: Простая интеграция NumPy и SciPy**\n",
        "\n",
        "Предположим, мы хотим решить систему линейных уравнений \\(A\\mathbf{x} = \\mathbf{b}\\), используя массивы NumPy в качестве входных данных и функции линейной алгебры из SciPy для решения:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.linalg import solve\n",
        "\n",
        "# Определяем коэффициенты системы\n",
        "A = np.array([[3, 2], [1, -1]], dtype=float)\n",
        "b = np.array([1, 4], dtype=float)\n",
        "\n",
        "# Решаем систему\n",
        "x = solve(A, b)\n",
        "print(\"Решение:\", x)\n",
        "```\n",
        "\n",
        "*Пояснение:* В этом примере массивы `A` и `b` создаются с помощью NumPy, а функция `solve` из `scipy.linalg` выполняет численно стабильное решение системы без явного вычисления обратной матрицы. SciPy автоматически выбирает оптимальный метод (обычно LU-разложение) в зависимости от структуры матрицы.\n",
        "\n",
        "*После выполнения:* Подобная комбинация демонстрирует элегантность и выразительность научного стека Python: пользователь формулирует задачу на естественном языке программирования, а низкоуровневые оптимизации остаются «под капотом».\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Высокопроизводительная Основа: Наследие Fortran и C\n",
        "\n",
        "Производительность SciPy, особенно в таких критически важных областях, как линейная алгебра и быстрое преобразование Фурье (БПФ), обеспечивается за счёт обёрток над высокооптимизированными библиотеками, написанными на C и Fortran.\n",
        "\n",
        "#### Использование BLAS/LAPACK\n",
        "\n",
        "Функции линейной алгебры в SciPy и NumPy зависят от BLAS (*Basic Linear Algebra Subprograms*) и LAPACK (*Linear Algebra Package*). Эти библиотеки предоставляют эффективные низкоуровневые реализации стандартных алгоритмов. При установке пакетов SciPy и NumPy (например, через `pip` или `conda`) автоматически обнаруживается и выбирается наиболее производительная доступная реализация BLAS/LAPACK — такая как Intel MKL, OpenBLAS или Accelerate (на macOS).\n",
        "\n",
        "Эти оптимизированные реализации активно используют многопоточность и специализированные векторные инструкции процессора (например, AVX-512), что критически важно для скорости вычислений при работе с большими массивами. Порядок поиска и выбора библиотеки определяется конфигурацией сборки, что позволяет достичь максимальной производительности в целевой среде исполнения.\n",
        "\n",
        "**Проверка используемой BLAS-реализации**\n",
        "\n",
        "```python\n",
        "import scipy\n",
        "print(scipy.show_config())\n",
        "```\n",
        "\n",
        "*Пояснение:* Выполнение этой команды выводит информацию о том, какие библиотеки BLAS/LAPACK были обнаружены при компиляции SciPy. Это особенно полезно при диагностике производительности на серверах или кластерах.\n",
        "\n",
        "#### Специализированные Fortran-библиотеки\n",
        "\n",
        "Значительная часть надёжности и точности SciPy основана на обёртках десятилетиями проверенных библиотек Fortran 77, включая:\n",
        "\n",
        "- **QUADPACK** — численное интегрирование;\n",
        "- **FITPACK** — сплайн-интерполяция;\n",
        "- **ODEPACK** — решение обыкновенных дифференциальных уравнений (ОДУ);\n",
        "- **MINPACK** — оптимизация и решение нелинейных систем.\n",
        "\n",
        "Использование этих библиотек гарантирует, что лежащие в основе численные алгоритмы являются стабильными, хорошо изученными и прошедшими многолетнюю проверку в реальных научных и инженерных приложениях.\n",
        "\n",
        "Однако такой архитектурный выбор несёт в себе инженерный компромисс. Высокая производительность и точность SciPy достигаются за счёт сложности сопровождения кода: устаревший Fortran 77 трудно поддерживать, тестировать и компилировать для новых аппаратных платформ (например, Windows on ARM или macOS с архитектурой Apple Silicon) или сред выполнения (таких как Pyodide/WebAssembly).\n",
        "\n",
        "Это обстоятельство указывает на то, что устойчивость и скорость SciPy являются результатом сложного архитектурного баланса между использованием надёжного, проверенного численного ядра и растущей сложностью поддержания его совместимости с современной вычислительной инфраструктурой.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Численная Линейная Алгебра (`scipy.linalg`): Стабильность и Декомпозиции\n",
        "\n",
        "Модуль `scipy.linalg` предлагает высокоуровневые функции для решения систем уравнений, нахождения собственных значений и выполнения матричных декомпозиций. В отличие от `numpy.linalg`, он предоставляет более полный набор алгоритмов и более надёжные реализации, особенно для плохо обусловленных задач.\n",
        "\n",
        "### 2.1. Концепция Обусловленности Матриц\n",
        "\n",
        "Численная стабильность при решении систем линейных уравнений \\(A\\mathbf{x} = \\mathbf{b}\\) напрямую зависит от числа обусловленности матрицы \\(A\\), обозначаемого \\(\\kappa(A)\\).\n",
        "\n",
        "**Математическое определение.** Число обусловленности определяется как  \n",
        "\\[\n",
        "\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|\n",
        "\\]  \n",
        "в некоторой матричной норме (обычно используют спектральную или 2-норму). Матрица считается **хорошо обусловленной**, если \\(\\kappa(A)\\) мало (близко к 1), что означает, что малые изменения во входных данных (матрице \\(A\\) или векторе \\(\\mathbf{b}\\)) приводят лишь к малым изменениям в решении \\(\\mathbf{x}\\).\n",
        "\n",
        "**Интерпретация.** Если \\(\\kappa(A)\\) велико (например, \\(\\gg 10^3\\)), матрица считается **плохо обусловленной** (*ill-conditioned*). В таком случае даже незначительные ошибки округления или шум во входных данных могут вызвать катастрофически большие ошибки в вычисленном решении. Плохая обусловленность часто возникает, когда матрица близка к сингулярной или имеет почти нулевые сингулярные значения.\n",
        "\n",
        "**Пример: Оценка числа обусловленности**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.linalg import svdvals, cond\n",
        "\n",
        "# Создаём плохо обусловленную матрицу (например, матрицу Гильберта)\n",
        "def hilbert_matrix(n):\n",
        "    return np.array([[1.0 / (i + j + 1) for j in range(n)] for i in range(n)])\n",
        "\n",
        "A = hilbert_matrix(6)\n",
        "kappa = cond(A)\n",
        "print(f\"Число обусловленности κ(A) = {kappa:.2e}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Матрица Гильберта — классический пример плохо обусловленной матрицы. При \\(n=6\\) её число обусловленности уже превышает \\(10^7\\), что делает решение системы крайне нестабильным в арифметике с плавающей точкой.\n",
        "\n",
        "*После выполнения:* Такие оценки позволяют заранее диагностировать потенциальные проблемы численной точности и выбирать более робастные методы решения (например, SVD с регуляризацией).\n",
        "\n",
        "### 2.2. Основные Декомпозиции и Их Численная Роль\n",
        "\n",
        "Численные декомпозиции позволяют решать линейные задачи более эффективно и стабильно, чем прямое обращение матрицы.\n",
        "\n",
        "- **LU-разложение** (\\(PA = LU\\)).  \n",
        "  Этот метод факторизует квадратную матрицу \\(A\\) на матрицу перестановок \\(P\\), нижнюю треугольную матрицу \\(L\\) (с единичной диагональю) и верхнюю треугольную матрицу \\(U\\). Включение матрицы перестановок \\(P\\) не является просто организационным моментом — она реализует стратегию частичного выбора ведущего элемента (*partial pivoting*), что критически важно для обеспечения численной стабильности разложения в условиях ограниченной точности вычислений.\n",
        "\n",
        "- **Разложение Холецкого** (\\(A = L L^T\\)).  \n",
        "  Является специализированным и наиболее быстрым разложением для решения систем линейных уравнений. Оно применимо только к матрицам, которые являются симметричными и положительно определёнными. Когда это условие выполняется, разложение Холецкого примерно в два раза эффективнее LU-разложения. Оно широко используется в методах Монте-Карло, нелинейной оптимизации и фильтрах Калмана.\n",
        "\n",
        "- **Сингулярное разложение** (SVD, \\(A = U \\Sigma V^T\\)).  \n",
        "  SVD является наиболее робастным инструментом в линейной алгебре, применимым к матрицам любого размера. Его высочайшая численная стабильность делает его незаменимым при работе с плохо обусловленными или сингулярными системами. Диагональная матрица \\(\\Sigma\\) содержит сингулярные значения, которые являются квадратными корнями из собственных значений \\(A^T A\\). Сингулярные значения напрямую связаны с числом обусловленности:  \n",
        "  \\[\n",
        "  \\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\n",
        "  \\]  \n",
        "  где \\(\\sigma_{\\max}\\) и \\(\\sigma_{\\min}\\) — наибольшее и наименьшее сингулярные значения соответственно.\n",
        "\n",
        "  Для обеспечения робастности в инженерных задачах, особенно когда матрица \\(A\\) получена из зашумлённых измерений, необходимо включать SVD в рабочий процесс. Это позволяет не только диагностировать \\(\\kappa(A)\\), но и стабилизировать решение через усечённое SVD или регуляризацию Тихонова.\n",
        "\n",
        "**Пример: Решение плохо обусловленной системы через SVD**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.linalg import svd\n",
        "\n",
        "# Используем ту же матрицу Гильберта\n",
        "A = hilbert_matrix(6)\n",
        "b = np.ones(6)\n",
        "\n",
        "# Обычное решение (нестабильно)\n",
        "x_bad = np.linalg.solve(A, b)\n",
        "\n",
        "# Решение через SVD с отсечением малых сингулярных значений\n",
        "U, s, Vt = svd(A)\n",
        "# Отсекаем сингулярные значения меньше 1e-10\n",
        "s_inv = np.array([1/si if si > 1e-10 else 0 for si in s])\n",
        "x_good = Vt.T @ (s_inv * (U.T @ b))\n",
        "\n",
        "print(\"Норма разности решений:\", np.linalg.norm(x_bad - x_good))\n",
        "```\n",
        "\n",
        "*Пояснение:* Усечённое SVD игнорирует компоненты, соответствующие малым сингулярным значениям, которые усиливают шум. Это — простейшая форма регуляризации.\n",
        "\n",
        "**Таблица 2.1: Сравнение основных методов разложения матриц (`scipy.linalg`)**\n",
        "\n",
        "| Метод | Требования к матрице | Численная роль | Вычислительная эффективность |\n",
        "|-------|----------------------|----------------|-------------------------------|\n",
        "| LU (\\(PA = LU\\)) | Квадратная | Решение общих систем | Хорошая стабильность за счёт \\(P\\), универсальное применение |\n",
        "| Холецкого (\\(LL^T\\)) | Симметричная, положительно определённая | Монте-Карло, оптимизация | В ~2 раза быстрее LU (если применимо) |\n",
        "| SVD (\\(U \\Sigma V^T\\)) | Произвольная (\\(m \\times n\\)) | Диагностика \\(\\kappa(A)\\), псевдоинверсия | Наивысшая стабильность; основной диагностический инструмент |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Интегрирование и Дифференциальные Уравнения (`scipy.integrate`)\n",
        "\n",
        "Модуль `scipy.integrate` предоставляет инструменты как для численной квадратуры (интегрирование функций), так и для решения обыкновенных дифференциальных уравнений (ОДУ).\n",
        "\n",
        "### 3.1. Численная Квадратура (`scipy.integrate.quad`)\n",
        "\n",
        "Функция `quad` предназначена для интегрирования функции одной переменной и является обёрткой над проверенной Fortran-библиотекой **QUADPACK**.\n",
        "\n",
        "**Алгоритмическая база.** `quad` использует методы адаптивной квадратуры, часто основанные на модифицированном методе Кленшоу–Кертиса. Ключевая особенность — адаптивное разбиение интервала: алгоритм итеративно делит интервал интегрирования, концентрируя вычислительные ресурсы (т.е. добавляя больше узлов) в тех областях, где подынтегральная функция имеет высокую вариацию или сингулярности. Этот механизм направлен на минимизацию локальной ошибки и достижение заданного допуска.\n",
        "\n",
        "**Практическое ограничение.** Несмотря на свою робастность, адаптивная квадратура может столкнуться с трудностями. Если функция содержит узкую, но важную особенность (например, острый пик или узкий гауссиан), а интегрирование выполняется на чрезвычайно широком конечном интервале, адаптивная процедура может «пропустить» эту область. В результате алгоритм может ложно оценить ошибку как низкую, давая неточный результат.\n",
        "\n",
        "**Пример: Интегрирование функции с узким пиком**\n",
        "\n",
        "```python\n",
        "from scipy.integrate import quad\n",
        "import numpy as np\n",
        "\n",
        "def narrow_peak(x):\n",
        "    return np.exp(-((x - 1000)**2) / (2 * 0.01**2))\n",
        "\n",
        "# Попытка \"грубого\" интегрирования на широком интервале\n",
        "I1, err1 = quad(narrow_peak, -10000, 10000)\n",
        "print(f\"Широкий интервал: I = {I1:.3e}, оценка ошибки = {err1:.3e}\")\n",
        "\n",
        "# Интегрирование в окрестности пика\n",
        "I2, err2 = quad(narrow_peak, 999, 1001)\n",
        "print(f\"Узкий интервал: I = {I2:.3e}, оценка ошибки = {err2:.3e}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* В первом случае `quad` \"не замечает\" пика и возвращает почти нулевой результат с завышенной уверенностью. Во втором случае мы явно указываем интересующую область — и получаем корректное значение.\n",
        "\n",
        "### 3.2. Решение Задач с Начальными Значениями ОДУ (`scipy.integrate.solve_ivp`)\n",
        "\n",
        "`solve_ivp` — современный и рекомендуемый интерфейс для решения систем ОДУ с начальными условиями:  \n",
        "\\[\n",
        "\\dot{\\mathbf{y}} = \\mathbf{f}(t, \\mathbf{y}), \\quad \\mathbf{y}(t_0) = \\mathbf{y}_0\n",
        "\\]\n",
        "\n",
        "#### Явные методы (Explicit RK)\n",
        "\n",
        "По умолчанию используется метод `'RK45'` — явный метод Рунге–Кутты 5(4)-го порядка. Он быстр и эффективен для **нежёстких** систем. Однако явные методы имеют ограниченную область устойчивости и требуют очень малого временного шага \\(h\\), если система жёсткая.\n",
        "\n",
        "#### Проблема жёсткости (Stiffness)\n",
        "\n",
        "Жёсткость возникает в системах ОДУ, где присутствуют процессы с сильно различающимися временными масштабами (например, в химической кинетике или реакциях горения). При применении явных методов к жёстким системам шаг интегрирования вынужденно уменьшается до значения, определяемого самым быстрым (часто несущественным) процессом, что делает вычисления неэффективными.\n",
        "\n",
        "#### Неявные методы (Implicit Solvers)\n",
        "\n",
        "Для жёстких систем необходимо использовать неявные методы, такие как `'BDF'` (формулы обратного дифференцирования) или `'Radau'`. Эти методы обладают свойством **A-устойчивости**, что позволяет им использовать большие шаги, оставаясь стабильными даже при больших отрицательных собственных значениях якобиана.\n",
        "\n",
        "**Пример: Сравнение решателей на жёсткой системе**\n",
        "\n",
        "```python\n",
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def stiff_system(t, y):\n",
        "    return [-1000 * (y[0] - np.sin(t)) + np.cos(t)]\n",
        "\n",
        "y0 = [0.0]\n",
        "t_span = (0, 1)\n",
        "\n",
        "# Используем BDF для жёсткой системы\n",
        "sol_bdf = solve_ivp(stiff_system, t_span, y0, method='BDF', rtol=1e-6)\n",
        "\n",
        "# Попробуем RK45 (он будет крайне медленным или не сойдётся)\n",
        "try:\n",
        "    sol_rk = solve_ivp(stiff_system, t_span, y0, method='RK45', rtol=1e-6, max_step=1e-4)\n",
        "    print(\"RK45 завершился успешно.\")\n",
        "except Exception as e:\n",
        "    print(\"RK45 не справился:\", e)\n",
        "\n",
        "plt.plot(sol_bdf.t, sol_bdf.y[0], 'b-', label='BDF (жёсткий)')\n",
        "plt.xlabel('t'); plt.ylabel('y(t)'); plt.legend(); plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Явный метод `'RK45'` либо не сходится, либо требует тысяч шагов, тогда как `'BDF'` даёт точное решение за несколько десятков шагов.\n",
        "\n",
        "#### Контроль допусков\n",
        "\n",
        "`solve_ivp` использует адаптивное управление шагом на основе относительных (`rtol`) и абсолютных (`atol`) допусков. Значение `rtol=1e-3` по умолчанию часто недостаточно для научных расчётов. Для высокоточных задач рекомендуется устанавливать `rtol=1e-6` и `atol=1e-9` или даже строже.\n",
        "\n",
        "**Таблица 3.1: Выбор метода для решения ОДУ (`solve_ivp`)**\n",
        "\n",
        "| Метод | Класс | Устойчивость | Применение | Компромисс |\n",
        "|-------|-------|--------------|------------|------------|\n",
        "| `'RK45'` | Явный РК 5(4) | Ограниченная | Нежёсткие системы | Быстрый шаг, но нестабилен при жёсткости |\n",
        "| `'BDF'` | Неявный | A-устойчивость | Жёсткие системы | Более дорогой шаг, но устойчив при больших \\(h\\) |\n",
        "| `'LSODA'` | Гибридный (Адамс/BDF) | Автоматическое переключение | Универсальный выбор | Надёжность, но менее гибкий интерфейс |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Численная Оптимизация (`scipy.optimize`)\n",
        "\n",
        "Модуль `scipy.optimize` предоставляет инструменты для минимизации скалярных функций, нахождения корней уравнений и подгонки кривых.\n",
        "\n",
        "### 4.1. Локальная и Глобальная Оптимизация\n",
        "\n",
        "Основная функция `minimize` предоставляет единый интерфейс для локальной минимизации многомерных скалярных функций.\n",
        "\n",
        "#### Локальные методы\n",
        "\n",
        "1. **BFGS** — квази-Ньютоновский градиентный метод. Использует информацию о градиенте (аналитическом или численном) и эффективно аппроксимирует обратную матрицу Гессе. Требует гладкости целевой функции.\n",
        "2. **Nelder–Mead** — бесградиентный симплекс-метод. Устойчив к шуму и разрывам, но сходится медленнее.\n",
        "\n",
        "#### Глобальная оптимизация\n",
        "\n",
        "Методы глобальной оптимизации, такие как **Differential Evolution** (`differential_evolution`), предназначены для поиска глобального минимума в мультимодальных ландшафтах. Это стохастический алгоритм, не требующий градиента, который эволюционирует популяцию кандидатов.\n",
        "\n",
        "**Компромисс.** Глобальные методы надёжнее, но требуют на порядки больше вычислений. Эффективная стратегия — сначала выполнить грубый глобальный поиск, затем уточнить результат локальным методом.\n",
        "\n",
        "**Пример: Комбинированный подход**\n",
        "\n",
        "```python\n",
        "from scipy.optimize import differential_evolution, minimize\n",
        "import numpy as np\n",
        "\n",
        "def multimodal_func(x):\n",
        "    return np.sin(x[0]) * np.cos(x[1]) + 0.1 * (x[0]**2 + x[1]**2)\n",
        "\n",
        "bounds = [(-5, 5), (-5, 5)]\n",
        "\n",
        "# Шаг 1: глобальный поиск\n",
        "result_de = differential_evolution(multimodal_func, bounds, seed=42)\n",
        "print(\"Глобальный минимум (DE):\", result_de.x)\n",
        "\n",
        "# Шаг 2: локальное уточнение\n",
        "result_local = minimize(multimodal_func, result_de.x, method='BFGS')\n",
        "print(\"Локальный минимум (BFGS):\", result_local.x)\n",
        "print(\"Значение функции:\", result_local.fun)\n",
        "```\n",
        "\n",
        "*Пояснение:* Такой подход сочетает робастность глобального поиска с высокой скоростью сходимости градиентных методов.\n",
        "\n",
        "### 4.2. Нелинейный Метод Наименьших Квадратов — `curve_fit`\n",
        "\n",
        "Функция `curve_fit` подгоняет параметрическую модель \\(f(x; \\theta)\\) к экспериментальным данным, минимизируя сумму квадратов остатков.\n",
        "\n",
        "```python\n",
        "from scipy.optimize import curve_fit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def model(x, a, b, c):\n",
        "    return a * np.exp(-b * x) + c\n",
        "\n",
        "# Синтетические данные с шумом\n",
        "x_data = np.linspace(0, 4, 50)\n",
        "y_true = model(x_data, 2.5, 1.3, 0.5)\n",
        "y_data = y_true + 0.2 * np.random.normal(size=x_data.size)\n",
        "\n",
        "# Подгонка\n",
        "popt, pcov = curve_fit(model, x_data, y_data)\n",
        "perr = np.sqrt(np.diag(pcov))  # стандартные ошибки\n",
        "\n",
        "print(\"Оценённые параметры:\", popt)\n",
        "print(\"Стандартные ошибки:\", perr)\n",
        "\n",
        "# Визуализация\n",
        "plt.scatter(x_data, y_data, label='Данные')\n",
        "plt.plot(x_data, model(x_data, *popt), 'r-', label='Подгонка')\n",
        "plt.legend(); plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* `pcov` — ковариационная матрица параметров. Её диагональные элементы позволяют оценить доверительные интервалы, что критично для научной интерпретации.\n",
        "\n",
        "### 4.3. Линейное Программирование (`linprog`)\n",
        "\n",
        "Функция `linprog` решает задачи линейного программирования: минимизацию линейной функции при линейных ограничениях.\n",
        "\n",
        "**Стандартная форма.** Задача должна быть приведена к виду:  \n",
        "\\[\n",
        "\\min c^T x \\quad \\text{при} \\quad A_{ub} x \\leq b_{ub},\\ A_{eq} x = b_{eq},\\ x \\geq 0\n",
        "\\]\n",
        "\n",
        "Максимизация \\(z\\) преобразуется в минимизацию \\(-z\\), неравенства \\(\\geq\\) — в \\(\\leq\\) умножением на \\(-1\\).\n",
        "\n",
        "**Пример: Транспортная задача**\n",
        "\n",
        "```python\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "# Минимизация стоимости: c = [2, 3, 1, 4]\n",
        "c = [2, 3, 1, 4]\n",
        "\n",
        "# Ограничения по запасам и потребностям\n",
        "A_eq = [\n",
        "    [1, 1, 0, 0],  # Сумма из склада 1\n",
        "    [0, 0, 1, 1],  # Сумма из склада 2\n",
        "    [1, 0, 1, 0],  # Сумма на магазин A\n",
        "    [0, 1, 0, 1]   # Сумма на магазин B\n",
        "]\n",
        "b_eq = [10, 15, 12, 13]  # Запасы и спрос\n",
        "\n",
        "res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=(0, None), method='highs')\n",
        "print(\"Оптимальное решение:\", res.x)\n",
        "print(\"Минимальная стоимость:\", res.fun)\n",
        "```\n",
        "\n",
        "*Пояснение:* `linprog` с методом `'highs'` (новый, высокопроизводительный решатель) эффективно справляется даже со сложными задачами оптимизации ресурсов.\n"
      ],
      "metadata": {
        "id": "9bhVuNA7gTqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично! Ниже — полностью отредактированная, дополненная и стилистически выровненная версия разделов **5–10** вашего текста. Я:\n",
        "\n",
        "- Убрал все посторонние цифры и опечатки (вроде «точек данных.28» → «точек данных»);\n",
        "- Расширил недостаточно раскрытые идеи;\n",
        "- Добавил **практические примеры кода** с пояснениями до и после каждого;\n",
        "- Сохранил академический, но доступный тон, соответствующий первой части;\n",
        "- Убедился, что структура логична, а переходы между разделами плавные.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Интерполяция (`scipy.interpolate`): Избегая Фантомов\n",
        "\n",
        "Интерполяция — это процесс построения функции, которая проходит через заданный набор дискретных точек данных. Модуль `scipy.interpolate` предлагает широкий спектр методов, от простой линейной интерполяции до многомерных сплайнов, позволяя выбирать стратегию, адекватную характеру данных и требованиям к гладкости результата.\n",
        "\n",
        "### 5.1. Феномен Рунге и Необходимость Сплайнов\n",
        "\n",
        "При использовании глобальных полиномов высокой степени для интерполяции большого числа узлов возникает так называемый **феномен Рунге**. Он проявляется в виде неконтролируемых осцилляций, особенно вблизи границ интервала, что приводит к значительным отклонениям от истинного поведения функции, даже если полином точно проходит через все заданные точки.\n",
        "\n",
        "Этот эффект демонстрирует принципиальное ограничение глобальных аппроксимаций: добавление новых точек может ухудшить поведение интерполянта в уже хорошо описанных областях.\n",
        "\n",
        "**Сплайн-интерполяция** решает эту проблему, заменяя единый полином высокой степени на **кусочно-полиномиальные функции низкой степени** (обычно кубические). Узлы интерполяции (*knots*) служат точками соединения этих полиномиальных сегментов. При этом обеспечивается непрерывность не только самой функции, но и её первой и второй производных — так называемая \\(C^2\\)-гладкость.\n",
        "\n",
        "Такой подход даёт локальный контроль: изменение одной точки влияет только на соседние сегменты, а не на всю интерполяционную кривую. Это критически важно при работе с экспериментальными данными, где требуется не просто «провести кривую», а построить физически осмысленное представление процесса, допускающее последующее дифференцирование или интегрирование.\n",
        "\n",
        "**Пример: Феномен Рунге vs. Кубический сплайн**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Истинная функция и узлы интерполяции\n",
        "def runge(x):\n",
        "    return 1 / (1 + 25 * x**2)\n",
        "\n",
        "x_nodes = np.linspace(-1, 1, 11)\n",
        "y_nodes = runge(x_nodes)\n",
        "\n",
        "# Глобальный полином (через NumPy для демонстрации)\n",
        "coeffs = np.polyfit(x_nodes, y_nodes, deg=10)\n",
        "poly_interp = np.poly1d(coeffs)\n",
        "\n",
        "# Кубический сплайн\n",
        "spline = interp1d(x_nodes, y_nodes, kind='cubic')\n",
        "\n",
        "# Точки для построения графика\n",
        "x_plot = np.linspace(-1, 1, 400)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_plot, runge(x_plot), 'k--', label='Истинная функция')\n",
        "plt.plot(x_plot, poly_interp(x_plot), 'r-', label='Полином 10-й степени (Runge)')\n",
        "plt.plot(x_plot, spline(x_plot), 'b-', label='Кубический сплайн')\n",
        "plt.scatter(x_nodes, y_nodes, c='k', zorder=5)\n",
        "plt.legend(); plt.grid(); plt.title('Феномен Рунге и сплайн-интерполяция')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Глобальный полином демонстрирует сильные осцилляции у краёв интервала, в то время как кубический сплайн остаётся близким к истинной функции на всём отрезке.\n",
        "\n",
        "*После выполнения:* Этот пример иллюстрирует, почему в научной практике предпочтение отдаётся сплайнам: они обеспечивают устойчивость, гладкость и естественность формы, не внося артефактов, отсутствующих в исходных данных.\n",
        "\n",
        "### 5.2. Инструменты SciPy на Базе FITPACK\n",
        "\n",
        "Многие высококачественные сплайн-интерполяторы в SciPy основаны на обёртках над старой, но надёжной Fortran-библиотекой **FITPACK**, разработанной Полом Дирксеном.\n",
        "\n",
        "- **1D-интерполяция** (`interp1d`) — удобный класс для быстрого создания интерполирующей функции. Поддерживает методы `'linear'`, `'nearest'`, `'cubic'` и `'quadratic'`.  \n",
        "- **Сплайны напрямую** — классы `UnivariateSpline`, `InterpolatedUnivariateSpline` и процедурные функции вроде `splrep`/`splev` дают более тонкий контроль: например, можно задать степень сглаживания при наличии шума.  \n",
        "- **Многомерная интерполяция** — для нерегулярных данных (точки в произвольных местах) используется `griddata`, основанная на триангуляции Делоне; для данных на регулярной сетке — `RegularGridInterpolator`, которая позволяет эффективно интерполировать в 2D, 3D и выше.\n",
        "\n",
        "Использование кубических сплайнов в науке — это не просто заполнение пробелов в данных; это **методический выбор**, направленный на построение физически осмысленного, гладкого представления процесса. Поскольку сплайны гарантируют непрерывность первой и второй производной, они критически важны для тех областей анализа, где последующее численное дифференцирование или интегрирование по интерполированной кривой должно быть точным и устойчивым.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Обработка Сигналов (`scipy.signal`)\n",
        "\n",
        "Модуль `scipy.signal` предоставляет инструменты для анализа частотного спектра, свёртки и, в особенности, для проектирования и применения цифровых фильтров — ключевых операций в обработке временных рядов, биомедицинских сигналов, астрофизики и многих других областях.\n",
        "\n",
        "### 6.1. FIR против IIR: Математика и Компромиссы\n",
        "\n",
        "Цифровые фильтры классифицируются по характеру их импульсной характеристики.\n",
        "\n",
        "- **FIR** (*Finite Impulse Response*) — фильтры с конечной импульсной характеристикой. Выход зависит только от текущих и прошлых входных значений. Импульсная характеристика обнуляется за конечное время.  \n",
        "  **Преимущество**: могут обеспечивать **строго линейную фазовую характеристику**, что означает одинаковую задержку для всех частотных компонент. Это критично, когда форма сигнала должна сохраняться (например, в нейрофизиологии или аудиообработке).\n",
        "\n",
        "- **IIR** (*Infinite Impulse Response*) — рекурсивные фильтры, где выход зависит как от входа, так и от предыдущих выходов.  \n",
        "  **Преимущество**: значительно более **вычислительно эффективны** — достигают той же частотной избирательности при гораздо меньшем порядке фильтра. Классические проекты (Баттерворта, Чебышева, Эллиптические) аппроксимируют идеальный «прямоугольный» частотный отклик.  \n",
        "  **Недостаток**: их фазовая характеристика, как правило, **нелинейна**, что приводит к искажению формы сигнала.\n",
        "\n",
        "**Пример: Сравнение БИХ и КИХ фильтров**\n",
        "\n",
        "```python\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fs = 1000  # частота дискретизации\n",
        "nyq = 0.5 * fs\n",
        "low = 50 / nyq\n",
        "high = 150 / nyq\n",
        "\n",
        "# IIR: фильтр Баттерворта 5-го порядка\n",
        "b_iir, a_iir = signal.butter(5, [low, high], btype='band')\n",
        "\n",
        "# FIR: фильтр с окном Кайзера (длина 101)\n",
        "b_fir = signal.firwin(101, [low, high], pass_zero=False, window='kaiser', beta=8.6)\n",
        "\n",
        "# АЧХ и ФЧХ\n",
        "w_iir, h_iir = signal.freqz(b_iir, a_iir, fs=fs)\n",
        "w_fir, h_fir = signal.freqz(b_fir, worN=len(w_iir), fs=fs)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(w_iir, 20 * np.log10(abs(h_iir)), 'r', label='IIR (Баттерворт)')\n",
        "plt.plot(w_fir, 20 * np.log10(abs(h_fir)), 'b', label='FIR (Кайзер)')\n",
        "plt.title('АЧХ'); plt.xlabel('Частота (Гц)'); plt.ylabel('Усиление (дБ)'); plt.grid(); plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(w_iir, np.unwrap(np.angle(h_iir)), 'r', label='IIR')\n",
        "plt.plot(w_fir, np.unwrap(np.angle(h_fir)), 'b', label='FIR')\n",
        "plt.title('ФЧХ'); plt.xlabel('Частота (Гц)'); plt.ylabel('Фаза (рад)'); plt.grid(); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* FIR-фильтр (синий) демонстрирует линейную ФЧХ (прямая линия), тогда как IIR (красный) — сильно искривлённую.\n",
        "\n",
        "### 6.2. Решение Проблемы Нелинейной Фазы\n",
        "\n",
        "В инженерных задачах, где важна неискажённая форма сигнала, но требуется высокая эффективность IIR-фильтров (например, при анализе больших массивов данных), используется функция **`scipy.signal.filtfilt`**.\n",
        "\n",
        "Эта функция предназначена для **офлайн-обработки**, когда весь сигнал доступен заранее. `filtfilt` применяет IIR-фильтр дважды: сначала в прямом направлении, затем — в обратном. В результате фазовые искажения компенсируются, и общий фазовый сдвиг становится **нулевым**.\n",
        "\n",
        "**Пример: Устранение фазового сдвига с помощью `filtfilt`**\n",
        "\n",
        "```python\n",
        "t = np.linspace(0, 1, 1000)\n",
        "x = np.sin(2 * np.pi * 10 * t) + np.sin(2 * np.pi * 20 * t)  # Два тона\n",
        "x_noisy = x + 0.5 * np.random.randn(len(t))\n",
        "\n",
        "# Применяем IIR-фильтр обычным способом\n",
        "x_filtered = signal.lfilter(b_iir, a_iir, x_noisy)\n",
        "\n",
        "# И с filtfilt\n",
        "x_filtfilt = signal.filtfilt(b_iir, a_iir, x_noisy)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(t[:200], x_noisy[:200], 'k:', alpha=0.5, label='Шумный сигнал')\n",
        "plt.plot(t[:200], x_filtered[:200], 'r-', label='lfilter (с фазовым сдвигом)')\n",
        "plt.plot(t[:200], x_filtfilt[:200], 'b-', label='filtfilt (нулевая фаза)')\n",
        "plt.legend(); plt.grid(); plt.title('Сравнение lfilter и filtfilt')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Обычный `lfilter` сдвигает пики сигнала во времени, в то время как `filtfilt` сохраняет их положение. Это делает IIR-фильтры **практически универсальными** для аналитических задач, где причинность не требуется.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Статистический Анализ (`scipy.stats`)\n",
        "\n",
        "Модуль `scipy.stats` предоставляет мощный и унифицированный интерфейс для работы с вероятностными распределениями и статистическими тестами.\n",
        "\n",
        "### 7.1. Модели Распределений\n",
        "\n",
        "SciPy включает более 100 непрерывных и 20 дискретных распределений. Все они имеют **единый API**, что упрощает сравнение и подбор моделей:\n",
        "\n",
        "- `.pdf()` / `.pmf()` — плотность вероятности / функция массы;\n",
        "- `.cdf()` — функция распределения;\n",
        "- `.ppf()` — обратная функция распределения (квантили);\n",
        "- `.rvs()` — генерация случайных чисел;\n",
        "- `.fit()` — оценка параметров методом максимального правдоподобия.\n",
        "\n",
        "**Пример: Подбор распределения к данным**\n",
        "\n",
        "```python\n",
        "from scipy.stats import norm, lognorm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Сгенерируем логнормальные данные\n",
        "np.random.seed(42)\n",
        "data = lognorm.rvs(s=0.5, scale=2, size=1000)\n",
        "\n",
        "# Оценим параметры нормального и логнормального распределений\n",
        "params_norm = norm.fit(data)\n",
        "params_lognorm = lognorm.fit(data, floc=0)  # фиксируем сдвиг\n",
        "\n",
        "# Визуализация\n",
        "x = np.linspace(data.min(), data.max(), 200)\n",
        "plt.hist(data, bins=50, density=True, alpha=0.6, label='Данные')\n",
        "plt.plot(x, norm.pdf(x, *params_norm), 'r-', label='Нормальное')\n",
        "plt.plot(x, lognorm.pdf(x, *params_lognorm), 'g-', label='Логнормальное')\n",
        "plt.legend(); plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Метод `.fit()` автоматически оценивает параметры, что позволяет быстро проверять гипотезы о природе данных.\n",
        "\n",
        "### 7.2. Статистические Гипотезы и Робастность\n",
        "\n",
        "Классические тесты (например, t-тест) предполагают **нормальность** и **равенство дисперсий**. При нарушении этих условий выводы становятся ненадёжными.\n",
        "\n",
        "SciPy предлагает **робастные альтернативы**:\n",
        "\n",
        "1. **Поправка Уэлча** — через `equal_var=False` в `ttest_ind`, когда дисперсии различны.\n",
        "2. **Триммированный t-тест** — через параметр `trim` в `ttest_ind`, который отбрасывает экстремальные наблюдения (например, `trim=0.1` удаляет 10% с каждого хвоста).\n",
        "\n",
        "**Пример: Робастный t-тест**\n",
        "\n",
        "```python\n",
        "from scipy.stats import ttest_ind\n",
        "import numpy as np\n",
        "\n",
        "# Две выборки с выбросами\n",
        "np.random.seed(0)\n",
        "a = np.random.normal(0, 1, 100)\n",
        "b = np.random.normal(0.5, 1, 100)\n",
        "a[0] = 100  # выброс\n",
        "\n",
        "# Обычный t-тест\n",
        "t1, p1 = ttest_ind(a, b)\n",
        "\n",
        "# Робастный (триммированный)\n",
        "t2, p2 = ttest_ind(a, b, trim=0.1)\n",
        "\n",
        "print(f\"Обычный t-тест: p = {p1:.3f}\")\n",
        "print(f\"Триммированный: p = {p2:.3f}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Обычный тест может не обнаружить различия из-за выброса, тогда как триммированный остаётся устойчивым.\n",
        "\n",
        "Эти инструменты позволяют принимать **прагматичные решения**, обеспечивая достоверность выводов даже при работе с реальными, зашумлёнными данными.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Специальные Функции и Разреженные Структуры\n",
        "\n",
        "### 8.1. Специальные Функции (`scipy.special`)\n",
        "\n",
        "Модуль `scipy.special` содержит сотни функций, возникающих в физике и инженерии: функции Бесселя, Гамма, интегралы ошибок, эллиптические интегралы и др.\n",
        "\n",
        "**Численная устойчивость.** При больших аргументах стандартные функции (например, `jv` — функция Бесселя первого рода) могут вызывать переполнение или потерю точности. Для этого SciPy предоставляет **масштабированные версии**, такие как `jve`, возвращающие \\(e^{-|z|} J_\\nu(z)\\), что предотвращает переполнение.\n",
        "\n",
        "**Пример: Устойчивое вычисление функции Бесселя**\n",
        "\n",
        "```python\n",
        "from scipy.special import jv, jve\n",
        "import numpy as np\n",
        "\n",
        "z = 1000\n",
        "print(\"jv(0, 1000):\", jv(0, z))        # может быть 0.0 из-за underflow\n",
        "print(\"jve(0, 1000):\", jve(0, z))      # масштабированное значение\n",
        "print(\"Восстановлено:\", jve(0, z) * np.exp(z))  # ≈ jv(0, z), но вычислено устойчиво\n",
        "```\n",
        "\n",
        "### 8.2. Разреженные Матрицы (`scipy.sparse`)\n",
        "\n",
        "Разреженные матрицы — стандарт при решении больших систем уравнений (например, в МКЭ). Хранение только ненулевых элементов экономит память и ускоряет вычисления.\n",
        "\n",
        "**Ключевые форматы:**\n",
        "\n",
        "- **CSR** (*Compressed Sparse Row*) — оптимален для операций по строкам: умножение, сложение, итерационные решатели.\n",
        "- **CSC** (*Compressed Sparse Column*) — оптимален для операций по столбцам: LU-разложение, собственные значения.\n",
        "\n",
        "Выбор формата — **архитектурное решение**, а не техническая деталь. Преобразование между форматами возможно, но требует времени.\n",
        "\n",
        "**Пример: Эффективное решение разреженной системы**\n",
        "\n",
        "```python\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import spsolve\n",
        "import numpy as np\n",
        "\n",
        "# Создаём разреженную матрицу (диагональная + немного шума)\n",
        "n = 10000\n",
        "diagonal = np.ones(n)\n",
        "off_diag = np.full(n-1, 0.01)\n",
        "data = np.concatenate([off_diag, diagonal, off_diag])\n",
        "offsets = [-1, 0, 1]\n",
        "A_sparse = csr_matrix((data, offsets, np.arange(n+1)), shape=(n, n))\n",
        "\n",
        "b = np.random.rand(n)\n",
        "x = spsolve(A_sparse, b)  # Эффективное решение\n",
        "print(\"Решение получено. Норма остатка:\", np.linalg.norm(A_sparse @ x - b))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Сквозной Инженерный Кейс-Стади: Моделирование Жёсткой Реакционной Системы и Оценка Параметров\n",
        "\n",
        "Комплексные научные задачи требуют интеграции нескольких модулей SciPy. Рассмотрим задачу из химической кинетики.\n",
        "\n",
        "### 9.1. Постановка Задачи\n",
        "\n",
        "Система ОДУ описывает концентрации трёх веществ с сильно различающимися временными масштабами (задача Робертсона). Неизвестна константа скорости \\(k\\). Цель — оценить её по зашумлённым измерениям.\n",
        "\n",
        "### 9.2. Фаза 1: Численная Симуляция (`scipy.integrate`)\n",
        "\n",
        "Используем `solve_ivp` с методом `'BDF'` и строгими допусками (`rtol=1e-6`), чтобы гарантировать точность.\n",
        "\n",
        "### 9.3. Фаза 2: Оценка Параметров (`scipy.optimize`)\n",
        "\n",
        "Определяем функцию-обёртку, которая вызывает `solve_ivp` с заданным \\(k\\), и передаём её в `curve_fit`.\n",
        "\n",
        "```python\n",
        "def robertson(t, y, k):\n",
        "    return np.array([\n",
        "        -0.04 * y[0] + 1e4 * y[1] * y[2],\n",
        "        0.04 * y[0] - 1e4 * y[1] * y[2] - k * y[1]**2,\n",
        "        k * y[1]**2\n",
        "    ])\n",
        "\n",
        "def model(t, k):\n",
        "    sol = solve_ivp(robertson, [0, t[-1]], [1, 0, 0], t_eval=t, args=(k,),\n",
        "                    method='BDF', rtol=1e-6, atol=1e-9)\n",
        "    return sol.y[1]  # возвращаем вторую компоненту\n",
        "\n",
        "# Синтетические данные\n",
        "t_data = np.logspace(-2, 6, 50)\n",
        "y_true = model(t_data, k=3e7)\n",
        "y_data = y_true + 0.01 * np.random.randn(len(t_data))\n",
        "\n",
        "# Подгонка\n",
        "k_opt, pcov = curve_fit(model, t_data, y_data, p0=[1e7])\n",
        "print(f\"Оценённая k = {k_opt[0]:.1e}\")\n",
        "```\n",
        "\n",
        "### 9.4. Фаза 3: Анализ и Визуализация\n",
        "\n",
        "- **Неопределённость**: `np.sqrt(np.diag(pcov))` даёт стандартную ошибку.\n",
        "- **Гладкая визуализация**: используем `interp1d` с `kind='cubic'` для построения публикационно-готового графика.\n",
        "\n",
        "\n",
        "## 8.3. Пространственные Структуры и Расстояния (`scipy.spatial`): Геометрия Данных\n",
        "\n",
        "В анализе данных часто требуется не просто обрабатывать числовые значения, но и **понимать взаимное расположение наблюдений в многомерном пространстве признаков**. Расстояния между точками лежат в основе кластеризации, поиска аномалий, рекомендательных систем, снижения размерности и даже оценки качества моделей. Модуль `scipy.spatial` предоставляет эффективные инструменты для работы с геометрией данных, от вычисления метрик до построения иерархических структур.\n",
        "\n",
        "### 8.3.1. Метрики Расстояний: За Пределами Евклида\n",
        "\n",
        "Хотя евклидово расстояние интуитивно понятно, в реальных задачах часто требуются **альтернативные метрики**, лучше отражающие природу данных:\n",
        "\n",
        "- **Манхэттенское расстояние** (\\(L_1\\)) — устойчиво к выбросам, полезно при разреженных данных (например, в NLP).\n",
        "- **Косинусное расстояние** — измеряет угловое сходство, игнорируя длину векторов; идеально для текстов, где важна не частота, а **тематическое направление**.\n",
        "- **Корреляционное расстояние** — основано на коэффициенте Пирсона; полезно при сравнении **временных паттернов** (например, схожесть динамики продаж у двух продуктов).\n",
        "\n",
        "Функции `pdist` (попарные расстояния внутри одного набора) и `cdist` (расстояния между двумя наборами) позволяют эффективно вычислять полные матрицы расстояний без явных циклов.\n",
        "\n",
        "**Пример: Сравнение метрик на текстоподобных данных**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Имитация разреженных TF-IDF векторов (3 документа, 5 признаков)\n",
        "X = np.array([\n",
        "    [0.8, 0.0, 0.2, 0.0, 0.0],  # документ A\n",
        "    [0.0, 0.7, 0.0, 0.3, 0.0],  # документ B\n",
        "    [0.6, 0.0, 0.3, 0.0, 0.1],  # документ A', похожий на A\n",
        "])\n",
        "\n",
        "# Вычисляем матрицы расстояний\n",
        "euclidean = squareform(pdist(X, metric='euclidean'))\n",
        "cosine = squareform(pdist(X, metric='cosine'))\n",
        "manhattan = squareform(pdist(X, metric='cityblock'))\n",
        "\n",
        "# Визуализация\n",
        "metrics = {'Евклидово': euclidean, 'Косинусное': cosine, 'Манхэттен': manhattan}\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
        "for ax, (name, mat) in zip(axes, metrics.items()):\n",
        "    im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)\n",
        "    ax.set_title(name)\n",
        "    ax.set_xticks([0, 1, 2]); ax.set_yticks([0, 1, 2])\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            ax.text(j, i, f\"{mat[i, j]:.2f}\", ha='center', va='center', color='white')\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение:* Косинусное расстояние корректно показывает, что документы A и A' близки (угол мал), тогда как евклидово расстояние преувеличивает разницу из-за различий в «длине» векторов. Это демонстрирует, почему выбор метрики — **существенная часть проектирования признакового пространства**, а не техническая деталь.\n",
        "\n",
        "### 8.3.2. Эффективный Поиск Соседей: KD-деревья\n",
        "\n",
        "При работе с большими наборами данных (десятки или сотни тысяч наблюдений) вычисление полной матрицы расстояний становится **вычислительно неприемлемым** (\\(O(n^2)\\) по времени и памяти). В таких случаях применяются **пространственные индексы**, такие как **KD-дерево** (*k*-dimensional tree).\n",
        "\n",
        "Класс `cKDTree` (оптимизированная C-версия) позволяет находить *k* ближайших соседей или все точки в заданном радиусе за время, близкое к \\(O(\\log n)\\).\n",
        "\n",
        "**Пример: Быстрый поиск похожих клиентов в CRM**\n",
        "\n",
        "```python\n",
        "from scipy.spatial import cKDTree\n",
        "import numpy as np\n",
        "\n",
        "# Условные данные: 50 000 клиентов, 8 числовых признаков\n",
        "np.random.seed(42)\n",
        "customers = np.random.rand(50000, 8)\n",
        "\n",
        "# Строим индекс\n",
        "tree = cKDTree(customers)\n",
        "\n",
        "# Новый клиент (вектор признаков)\n",
        "new_client = np.random.rand(8)\n",
        "\n",
        "# Найти 10 самых похожих клиентов (по евклидову расстоянию)\n",
        "distances, indices = tree.query(new_client, k=10)\n",
        "\n",
        "print(f\"Индексы 10 ближайших клиентов: {indices}\")\n",
        "print(f\"Среднее расстояние: {np.mean(distances):.4f}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Такой подход лежит в основе **рекомендательных систем на основе сходства** (\"пользователям, похожим на вас, понравилось...\"), а также методов **аномалий**: если расстояние до ближайшего соседа аномально велико — объект может быть выбросом.\n",
        "\n",
        "### 8.3.3. Иерархическая Кластеризация: Когда Число Кластеров Неизвестно\n",
        "\n",
        "В отличие от методов вроде K-средних, **иерархическая кластеризация** не требует заранее задавать число кластеров. Она строит **дендрограмму** — древовидную структуру, в которой каждый уровень соответствует определённому порогу объединения кластеров.\n",
        "\n",
        "Процесс начинается с того, что каждая точка — отдельный кластер. Затем на каждом шаге объединяются два **наиболее близких** кластера (стратегия зависит от метода связывания: *single*, *complete*, *average*, *ward*).\n",
        "\n",
        "**Пример: Кластеризация клиентов с визуализацией дендрограммы**\n",
        "\n",
        "```python\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Генерируем данные: 20 точек в 2D (для наглядности)\n",
        "np.random.seed(0)\n",
        "X = np.vstack([\n",
        "    np.random.normal(0, 0.5, (7, 2)),\n",
        "    np.random.normal(3, 0.5, (6, 2)),\n",
        "    np.random.normal([0, 3], 0.5, (7, 2))\n",
        "])\n",
        "\n",
        "# Вычисляем попарные расстояния и строим иерархию (метод Уорда)\n",
        "Z = linkage(X, method='ward')\n",
        "\n",
        "# Строим дендрограмму\n",
        "plt.figure(figsize=(10, 4))\n",
        "dendrogram(Z, color_threshold=4)\n",
        "plt.title('Дендрограмма: иерархическая кластеризация')\n",
        "plt.xlabel('Индекс наблюдения'); plt.ylabel('Расстояние')\n",
        "plt.axhline(y=4, color='r', linestyle='--', label='Порог разреза')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Формируем 3 кластера\n",
        "labels = fcluster(Z, t=4, criterion='distance')\n",
        "print(\"Метки кластеров:\", labels)\n",
        "```\n",
        "\n",
        "*Пояснение:* Дендрограмма позволяет **визуально выбрать оптимальное число кластеров**, анализируя «скачки» в высоте слияния. Метод Уорда (`'ward'`) минимизирует внутрикластерную дисперсию и часто даёт компактные, сферические кластеры — что соответствует интуиции аналитика.\n",
        "\n",
        "*После выполнения:* Такой подход особенно ценен на этапе разведочного анализа данных (EDA), когда исследователь ещё не знает структуры данных, но хочет понять, существуют ли естественные группы.\n",
        "\n",
        "---\n",
        "\n",
        "### Значение для Анализа Данных\n",
        "\n",
        "Модуль `scipy.spatial` превращает абстрактные векторы признаков в **геометрические объекты**, для которых применимы интуитивные понятия «близости», «группировки» и «изоляции». Это не просто вспомогательный инструмент — это **основа геометрического взгляда на данные**, который лежит в сердце большинства методов машинного обучения. Понимание того, как вычисляются расстояния, как строятся индексы и как интерпретируются дендрограммы, позволяет аналитику:\n",
        "\n",
        "- осознанно выбирать метрики сходства;\n",
        "- эффективно работать с большими наборами данных;\n",
        "- визуально обосновывать гипотезы о структуре данных;\n",
        "- строить надёжные системы рекомендаций и обнаружения аномалий.\n",
        "\n",
        "Таким образом, `scipy.spatial` — неотъемлемая часть арсенала современного специалиста по анализу данных.\n",
        "\n",
        "---\n",
        "## 10. Заключение\n",
        "\n",
        "SciPy — это не просто набор функций, а **система численного мышления**, построенная на десятилетиях развития вычислительной математики. Его архитектура позволяет исследователю сосредоточиться на научной сути задачи, не теряя контроля над численной устойчивостью.\n",
        "\n",
        "Ключевые принципы экспертного применения SciPy:\n",
        "\n",
        "1. **Осознанный выбор алгоритма**: не «что работает», а «что стабильно и уместно» — будь то `'BDF'` вместо `'RK45'` для жёстких систем или `'differential_evolution'` для мультимодальных ландшафтов.\n",
        "2. **Контроль точности**: ужесточение `rtol`/`atol`, диагностика обусловленности через SVD, оценка ошибок параметров.\n",
        "3. **Использование архитектурных возможностей**: `filtfilt` для нулевой фазы, CSR/CSC для разреженных систем, масштабированные специальные функции.\n",
        "\n",
        "Сквозные задачи, подобные оценке кинетических параметров, демонстрируют **иерархическую природу численной ошибки**: неточность на уровне ОДУ-решателя разрушает всю последующую оптимизацию. Поэтому мастерство в SciPy — это не только знание API, но и понимание **компромиссов между скоростью, точностью и устойчивостью**.\n",
        ""
      ],
      "metadata": {
        "id": "ykgdYP5vgVKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 12: Scikit-learn — Основы машинного обучения\n",
        "\n",
        "Scikit-learn (sklearn) является краеугольным камнем экосистемы Python для машинного обучения, предоставляя унифицированный, последовательный и методологически строгий интерфейс для реализации классических алгоритмов. Библиотека отличается высококачественными, протестированными реализациями, охватывающими **полный цикл разработки модели**: от предобработки данных и отбора признаков до обучения, оценки, настройки гиперпараметров и развёртывания. Главное преимущество sklearn — не в обилии моделей, а в **строгом соблюдении стандартизированного API**, который делает рабочие процессы модульными, воспроизводимыми и защищёнными от типичных методологических ошибок, таких как утечка данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 1. Фундаментальная Философия Scikit-learn и Единый API\n",
        "\n",
        "### 1.1. Роль и архитектура Scikit-learn\n",
        "\n",
        "В основе архитектуры Scikit-learn лежит базовый класс `BaseEstimator`, от которого наследуются **все** объекты библиотеки — модели, преобразователи, мета-оценщики. Это архитектурное решение обеспечивает единый синтаксис для всех шагов машинного обучения: разработчик может заменить логистическую регрессию на метод опорных векторов, или `StandardScaler` на `QuantileTransformer`, **не меняя структуру основного кода**. Такая унификация превращает машинное обучение из набора разрозненных скриптов в **инженерную дисциплину с воспроизводимыми пайплайнами**.\n",
        "\n",
        "### 1.2. Концепции Estimator и Transformer\n",
        "\n",
        "В Scikit-learn все объекты делятся на две категории в зависимости от их назначения:\n",
        "\n",
        "- **Estimator (Оценщик)** — обучается на данных и делает прогнозы.  \n",
        "  Реализует:  \n",
        "  - `.fit(X, y)` — обучение на признаках `X` и целевой переменной `y` (для регрессии/классификации);  \n",
        "  - `.predict(X)` — генерация прогнозов для новых данных.\n",
        "\n",
        "- **Transformer (Преобразователь)** — модифицирует данные, не используя целевую переменную.  \n",
        "  Реализует:  \n",
        "  - `.fit(X)` — вычисление параметров преобразования (например, среднее и std для стандартизации);  \n",
        "  - `.transform(X)` — применение этих параметров к данным.\n",
        "\n",
        "Некоторые объекты (например, `PCA`) являются **одновременно и Transformer, и Estimator**, так как их можно встраивать в пайплайны и использовать для преобразования, а также оценивать качество через кросс-валидацию.\n",
        "\n",
        "### 1.3. Универсальные методы: `fit()`, `transform()`, `predict()` и их методологическое значение\n",
        "\n",
        "Фундаментальное разделение обязанностей в Scikit-learn отражается в трёх ключевых методах:\n",
        "\n",
        "- `.fit(X, y)` — **единственный** этап, где модель «видит» данные. Здесь она запоминает параметры: веса, пороги, статистики.\n",
        "- `.transform(X)` — применяет **уже выученные** параметры к новым данным.\n",
        "- `.fit_transform(X)` — сокращение для `.fit(X).transform(X)`, **используется только на обучающей выборке**.\n",
        "\n",
        "#### Почему это критически важно?\n",
        "\n",
        "Разделение `fit` и `transform` — не техническое удобство, а **гарантия статистической валидности**. Нарушение этого принципа приводит к **утечке данных** (*data leakage*) — ситуации, когда информация из тестового набора неявно попадает в обучение, что делает оценку производительности **нечестной и оптимистичной**.\n",
        "\n",
        "**Пример: Утечка данных при неправильном масштабировании**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Синтетические данные с сильным разбросом\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(1000, 2)\n",
        "X[:, 0] *= 1000  # первый признак в 1000 раз больше\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
        "\n",
        "# Разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ❌ НЕПРАВИЛЬНО: масштабируем до разделения\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # ИСПОЛЬЗУЕТ ВСЕ ДАННЫЕ!\n",
        "X_train_bad, X_test_bad, _, _ = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ✅ ПРАВИЛЬНО: масштабируем только после разделения\n",
        "scaler = StandardScaler()\n",
        "X_train_good = scaler.fit_transform(X_train)\n",
        "X_test_good = scaler.transform(X_test)  # только transform!\n",
        "\n",
        "# Обучение и оценка\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_bad, y_train)\n",
        "acc_bad = accuracy_score(y_test, model.predict(X_test_bad))\n",
        "\n",
        "model.fit(X_train_good, y_train)\n",
        "acc_good = accuracy_score(y_test, model.predict(X_test_good))\n",
        "\n",
        "print(f\"С утечкой: {acc_bad:.4f}\")\n",
        "print(f\"Без утечки: {acc_good:.4f}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* При утечке модель «знает» статистику тестового набора (например, что первый признак имеет разброс ~1000), и использует это для лучшего масштабирования. В реальности такой информации нет, и производительность падает.\n",
        "\n",
        "*После выполнения:* Разница может быть небольшой на синтетических данных, но **в реальных проектах утечка часто приводит к катастрофическому провалу в продакшене**.\n",
        "\n",
        "Эта строгость лежит в основе **Pipeline** — механизма, объединяющего предобработку и модель в единый объект, который ведёт себя как обычный Estimator:\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "pipe.score(X_test, y_test)  # Масштабирование и прогнозирование — в одном вызове\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 2. Подготовка Данных и Воспроизводимость\n",
        "\n",
        "### 2.1. Базовое деление данных и воспроизводимость\n",
        "\n",
        "Функция `train_test_split` — первый шаг в любом проекте. Ключевые параметры:\n",
        "\n",
        "- `random_state` — **обязателен** для воспроизводимости;\n",
        "- `stratify=y` — сохраняет пропорции классов в обеих выборках (критично при несбалансированных данных).\n",
        "\n",
        "**Пример: Стратификация при несбалансированных классах**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1,\n",
        "                           weights=[0.95, 0.05], random_state=42)\n",
        "\n",
        "print(\"Исходное распределение:\", Counter(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Обучающая выборка:\", Counter(y_train))\n",
        "print(\"Тестовая выборка:\", Counter(y_test))\n",
        "```\n",
        "\n",
        "*Пояснение:* Без `stratify=y` есть риск, что в тестовом наборе не окажется ни одного представителя редкого класса — модель будет невозможно оценить.\n",
        "\n",
        "### 2.2. Продвинутые стратегии валидации: Кросс-Валидация\n",
        "\n",
        "#### `StratifiedKFold` — стандарт для задач классификации\n",
        "\n",
        "Гарантирует, что **во всех фолдах сохраняется пропорция классов**. Это особенно важно при малом числе наблюдений или сильной несбалансированности.\n",
        "\n",
        "#### `TimeSeriesSplit` — единственно корректный выбор для временных рядов\n",
        "\n",
        "Нарушение временного порядка — одна из самых частых ошибок начинающих. `TimeSeriesSplit` строит сплиты так, что **тест всегда идёт после обучения**, имитируя реальный сценарий прогнозирования.\n",
        "\n",
        "**Пример: Визуализация сплитов**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=4)\n",
        "X = np.arange(20).reshape(-1, 1)\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
        "    plt.scatter(train_index, [i]*len(train_index), c='b', label='Train' if i==0 else \"\")\n",
        "    plt.scatter(test_index, [i]*len(test_index), c='r', label='Test' if i==0 else \"\")\n",
        "plt.yticks(range(4), [f\"Split {i+1}\" for i in range(4)])\n",
        "plt.xlabel(\"Индекс наблюдения\"); plt.legend(); plt.title(\"TimeSeriesSplit\")\n",
        "plt.grid(True, axis='x', alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### `ShuffleSplit` — гибкость для диагностики\n",
        "\n",
        "Полезен при построении **кривых обучения**, когда нужно оценить, как производительность зависит от размера обучающего набора.\n",
        "\n",
        "**Таблица 2: Сравнение стратегий кросс-валидации**\n",
        "\n",
        "| Стратегия | Когда использовать | Особенности |\n",
        "|----------|--------------------|-------------|\n",
        "| `KFold` | Регрессия, сбалансированная классификация | Простое разбиение на K частей |\n",
        "| `StratifiedKFold` | Классификация (особенно несбалансированная) | Сохраняет пропорции классов в каждом фолде |\n",
        "| `TimeSeriesSplit` | Временные ряды, последовательные данные | Тест всегда после обучения; нет перемешивания |\n",
        "| `ShuffleSplit` | Диагностика, кривые обучения | Гибкое число сплитов, независимые разбиения |\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 3. Преобразование Признаков I: Масштабирование и Импутация\n",
        "\n",
        "### 3.1. Обработка пропущенных значений\n",
        "\n",
        "`SimpleImputer` — стандартный инструмент. Ключевые рекомендации:\n",
        "\n",
        "- Используйте `strategy='median'` для признаков с выбросами;\n",
        "- Всегда применяйте `fit` только на `X_train`;\n",
        "- Для production-моделей установите `keep_empty_features=True`, чтобы избежать сбоев при полном отсутствии данных по признаку.\n",
        "\n",
        "### 3.2. Стандартизация и нормализация\n",
        "\n",
        "Выбор метода масштабирования зависит от модели:\n",
        "\n",
        "| Метод | Формула | Когда использовать |\n",
        "|------|--------|--------------------|\n",
        "| `StandardScaler` | \\( z = \\frac{x - \\mu}{\\sigma} \\) | SVM, линейные модели, KNN, нейросети |\n",
        "| `MinMaxScaler` | \\( x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\) | Нейросети с сигмоидой, когда нужен диапазон [0,1] |\n",
        "| `MaxAbsScaler` | \\( x' = \\frac{x}{\\max|x|} \\) | Разреженные данные, центрированные признаки |\n",
        "| `QuantileTransformer` | Нелинейное преобразование к равномерному/нормальному распределению | Признаки с выбросами, нелинейные зависимости |\n",
        "\n",
        "**Важно**: модели на основе деревьев (`RandomForest`, `XGBoost`) **не требуют масштабирования** — их можно исключить из пайплайна для ускорения.\n",
        "\n",
        "### 3.3. Пример: Корректное масштабирование (уже включён в основной текст выше)\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 4. Преобразование Признаков II: Кодирование и Извлечение\n",
        "\n",
        "### 4.1. Кодирование категориальных данных\n",
        "\n",
        "`OneHotEncoder` — основной инструмент, но требует внимания к деталям:\n",
        "\n",
        "- `handle_unknown='ignore'` — **обязателен** для production, чтобы модель не падала при новых категориях;\n",
        "- `drop='first'` — предотвращает мультиколлинеарность в линейных моделях.\n",
        "\n",
        "**Альтернативы при высокой кардинальности**:\n",
        "- `OrdinalEncoder` + `embedding` (в нейросетях);\n",
        "- `TargetEncoder` (из `category_encoders` или sklearn ≥1.3);\n",
        "- Кластеризация категорий по целевой переменной.\n",
        "\n",
        "### 4.2. Введение в работу с текстом\n",
        "\n",
        "`TfidfVectorizer` — стандарт для начальной векторизации текста. Он:\n",
        "\n",
        "- Автоматически удаляет стоп-слова (`stop_words='english'`);\n",
        "- Возвращает **разреженную матрицу** (экономит память);\n",
        "- Интегрируется в Pipeline как обычный Transformer.\n",
        "\n",
        "**Пример: Полный пайплайн для текстовой классификации**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Данные\n",
        "texts = [\"I love this movie\", \"This is terrible\", \"Great film!\", \"Worst ever\"]\n",
        "labels = [1, 0, 1, 0]\n",
        "\n",
        "# Пайплайн\n",
        "text_pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "text_pipe.fit(texts, labels)\n",
        "print(\"Прогноз для нового текста:\", text_pipe.predict([\"Amazing!\"]))\n",
        "```\n",
        "\n",
        "*Пояснение:* Весь процесс — от сырых текстов до прогноза — управляется единым объектом. Это позволяет легко настраивать гиперпараметры (`tfidf__max_features`, `clf__C`) через `GridSearchCV`.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ghx95cNtibBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Раздел 5. Сквозной Пайплайн (`Pipeline`) и Работа с Разнородными Данными\n",
        "\n",
        "Создание сквозных рабочих процессов в машинном обучении — это не просто последовательность шагов, а **единая, инкапсулированная система**, защищённая от методологических ошибок. Объекты `Pipeline` и `ColumnTransformer` являются архитектурными краеугольными камнями Scikit-learn, которые обеспечивают эту инкапсуляцию, гарантируя **воспроизводимость**, **масштабируемость** и **защиту от утечки данных**.\n",
        "\n",
        "### 5.1. Преимущества `Pipeline`\n",
        "\n",
        "`Pipeline` последовательно применяет список преобразователей и завершает процесс финальным оценщиком. Его ключевые преимущества:\n",
        "\n",
        "1. **Предотвращение утечки данных**  \n",
        "   Пайплайн гарантирует, что в ходе кросс-валидации **на каждом фолде** преобразования обучаются **только на данных этого фолда**, а затем применяются к его тестовой части. Это исключает даже теоретическую возможность утечки.\n",
        "\n",
        "2. **Инкапсуляция и модульность**  \n",
        "   Весь рабочий процесс становится единым `Estimator`, который можно передавать в `GridSearchCV`, сохранять через `joblib`, или развёртывать в production, не заботясь о порядке операций.\n",
        "\n",
        "3. **Упрощение и читаемость кода**  \n",
        "   Вместо десятков строк ручной предобработки — один объект, чья структура отражает логику проекта.\n",
        "\n",
        "### 5.2. `ColumnTransformer`: Работа с разнородными данными\n",
        "\n",
        "В реальных задачах данные редко бывают однородными: числовые, категориальные и текстовые признаки сосуществуют в одном датафрейме. Для каждого типа требуются свои методы предобработки.\n",
        "\n",
        "`ColumnTransformer` позволяет **применять разные преобразования к разным столбцам**, объединяя результаты в единую числовую матрицу.\n",
        "\n",
        "Структура: список кортежей вида  \n",
        "```python\n",
        "(name, transformer, columns)\n",
        "```\n",
        "\n",
        "- `name` — метка для отладки;\n",
        "- `transformer` — сам объект преобразования (может быть `Pipeline`);\n",
        "- `columns` — список имён, индексов или селекторов столбцов.\n",
        "\n",
        "### 5.3. Практический кейс: Полный пайплайн для смешанных данных\n",
        "\n",
        "Рассмотрим реалистичный сценарий: датасет содержит числовые признаки с пропусками и категориальные признаки с высокой кардинальностью. Нам нужно построить классификатор, защищённый от переобучения и утечек.\n",
        "\n",
        "**Пример: Создание robust-пайплайна**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Генерация синтетического датасета\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'age': [25, np.nan, 35, 45, 55, np.nan],\n",
        "    'income': [50000, 60000, np.nan, 80000, 90000, 70000],\n",
        "    'city': ['A', 'B', 'A', 'C', 'B', 'A'],\n",
        "    'education': ['BSc', 'MSc', 'PhD', 'BSc', 'MSc', 'BSc']\n",
        "})\n",
        "y = np.array([0, 1, 1, 1, 0, 0])\n",
        "\n",
        "# Разделение на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, y, test_size=0.5, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Определение селекторов столбцов\n",
        "numerical_cols = make_column_selector(dtype_exclude=['category', 'object'])\n",
        "categorical_cols = make_column_selector(dtype_include=['category', 'object'])\n",
        "\n",
        "# Числовой пайплайн: импутация → масштабирование\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Категориальный пайплайн: OHE → отбор признаков\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first')),\n",
        "    ('selector', SelectPercentile(score_func=chi2, percentile=80))\n",
        "])\n",
        "\n",
        "# Объединение в ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, numerical_cols),\n",
        "    ('cat', categorical_pipeline, categorical_cols)\n",
        "])\n",
        "\n",
        "# Финальный пайплайн: предобработка + модель\n",
        "clf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(C=1.0, max_iter=200))\n",
        "])\n",
        "\n",
        "# Обучение и оценка\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Отчёт классификации:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "*Пояснение до выполнения:*  \n",
        "Этот пайплайн автоматически:\n",
        "- заполняет пропуски в числовых признаках медианой **только по тренировочным данным**;\n",
        "- кодирует категории, игнорируя неизвестные в тесте;\n",
        "- отбирает 80% наиболее значимых бинарных признаков;\n",
        "- обучает логистическую регрессию на объединённом пространстве признаков.\n",
        "\n",
        "*После выполнения:*  \n",
        "Такой подход **полностью исключает утечку данных**, даже при наличии пропусков и новых категорий в тесте. Более того, весь пайплайн можно передать в `GridSearchCV`, чтобы совместно настраивать `classifier__C`, `cat__selector__percentile` и даже `num__imputer__strategy`.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 6. Выбор Эстиматоров I: Регрессионные Модели\n",
        "\n",
        "Линейные модели — основа регрессионного анализа. Их главные достоинства: **интерпретируемость**, **скорость** и **прозрачность**. Однако при большом числе признаков или мультиколлинеарности требуется регуляризация.\n",
        "\n",
        "### 6.1. Линейная регрессия (`LinearRegression`)\n",
        "\n",
        "Реализует метод наименьших квадратов (OLS). Минимизирует сумму квадратов остатков. Коэффициенты показывают **маргинальный вклад** каждого признака.\n",
        "\n",
        "**Ограничения**:  \n",
        "- Чувствительна к выбросам;  \n",
        "- При мультиколлинеарности коэффициенты становятся нестабильными;  \n",
        "- При \\(p > n\\) (признаков больше, чем наблюдений) — не решается.\n",
        "\n",
        "### 6.2. Регуляризация L2 (Ridge)\n",
        "\n",
        "Добавляет к функции потерь штраф \\(\\alpha \\sum w_i^2\\).  \n",
        "- **Не обнуляет** коэффициенты, но **стягивает их к нулю**;  \n",
        "- Стабилизирует решение при мультиколлинеарности;  \n",
        "- Рекомендуется, когда **все признаки потенциально полезны**, но нужно снизить дисперсию.\n",
        "\n",
        "### 6.3. Регуляризация L1 (Lasso)\n",
        "\n",
        "Добавляет штраф \\(\\alpha \\sum |w_i|\\).  \n",
        "- **Обнуляет** коэффициенты малозначимых признаков → **автоматический отбор признаков**;  \n",
        "- Создаёт **разреженную модель**, что повышает интерпретируемость;  \n",
        "- Лучше работает, когда **только небольшое подмножество признаков релевантно**.\n",
        "\n",
        "**Пример: Разреженность Lasso**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Создаём данные: 50 наблюдений, 20 признаков, но только 3 релевантны\n",
        "X, y = make_regression(n_samples=50, n_features=20, n_informative=3, noise=10, random_state=42)\n",
        "\n",
        "# Обучаем Lasso\n",
        "lasso = Lasso(alpha=10.0)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Подсчитываем ненулевые коэффициенты\n",
        "non_zero = np.sum(lasso.coef_ != 0)\n",
        "print(f\"Ненулевых коэффициентов: {non_zero} из 20\")\n",
        "print(\"Истинные релевантные признаки: 0, 1, 2\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Lasso корректно отобрал 3–4 признака, игнорируя остальные 16–17 шумовых переменных. Это демонстрирует его силу как **встроенного метода отбора признаков**.\n",
        "\n",
        "> **Выбор между Ridge и Lasso** — это выбор между **стабильностью** и **интерпретируемостью**. Для бизнес-аналитики, где важно объяснить модель стейкхолдеру, Lasso часто предпочтительнее.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 7. Выбор Эстиматоров II: Классификация и Ансамбли\n",
        "\n",
        "### 7.1. Модели ближайших соседей (`KNeighborsClassifier`)\n",
        "\n",
        "**Принцип**: классифицирует объект по большинству среди *K* ближайших соседей.\n",
        "\n",
        "**Особенности**:  \n",
        "- **Ленивое обучение**: модель = обучающий набор;  \n",
        "- **Критически зависит от масштабирования** — без `StandardScaler` результаты бессмысленны;  \n",
        "- **Чувствителен к размерности** — «проклятие размерности» делает все точки почти равноудалёнными в высоких пространствах.\n",
        "\n",
        "**Когда использовать**:  \n",
        "- Малые наборы данных;  \n",
        "- Когда важна локальная структура;  \n",
        "- Как бейзлайн.\n",
        "\n",
        "### 7.2. Ансамблевые методы\n",
        "\n",
        "#### Случайный лес (`RandomForestClassifier`)\n",
        "\n",
        "- **Бэггинг**: обучение на бутстрэп-выборках + случайный отбор признаков на каждом узле;  \n",
        "- **Снижает дисперсию**, устойчив к выбросам и пропускам;  \n",
        "- Возвращает `feature_importances_` — полезно для EDA;  \n",
        "- **Не требует масштабирования**.\n",
        "\n",
        "#### Градиентный бустинг (`HistGradientBoostingClassifier`)\n",
        "\n",
        "- **Последовательное обучение**: каждое дерево исправляет ошибки предыдущих;  \n",
        "- **Снижает смещение**, достигает высокой точности;  \n",
        "- **`HistGradientBoosting`** — оптимизированная версия:  \n",
        "  - Работает на бинах (гистограммах), а не на сырых значениях → **в 10–100× быстрее**;  \n",
        "  - **Встроенная поддержка пропущенных значений** → можно исключить импутацию из пайплайна;  \n",
        "  - Рекомендуется Scikit-learn для наборов > 10 000 строк.\n",
        "\n",
        "**Выбор ансамбля**:  \n",
        "- **Random Forest** — для устойчивости, интерпретируемости, быстрой настройки;  \n",
        "- **HistGradientBoosting** — для максимальной точности и скорости на больших данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 8. Оценка Производительности Моделей и Метрики\n",
        "\n",
        "Выбор метрики — это **перевод бизнес-цели в математическую форму**. Неправильный выбор приводит к оптимизации модели в неверном направлении.\n",
        "\n",
        "### 8.1. Метрики Классификации\n",
        "\n",
        "| Метрика | Формула | Когда использовать |\n",
        "|--------|--------|--------------------|\n",
        "| **Accuracy** | \\((TP + TN) / (TP + TN + FP + FN)\\) | Сбалансированные данные, равная стоимость ошибок |\n",
        "| **Precision** | \\(TP / (TP + FP)\\) | Минимизация ложных срабатываний (мошенничество, спам) |\n",
        "| **Recall** | \\(TP / (TP + FN)\\) | Минимизация пропусков (медицина, безопасность) |\n",
        "| **F1-Score** | \\(2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\) | Баланс между Precision и Recall |\n",
        "| **ROC AUC** | Площадь под ROC-кривой | Оценка способности к ранжированию, независимо от порога |\n",
        "\n",
        "> **Важно**: при несбалансированных данных **accuracy бесполезна**. Модель, предсказывающая всегда «0» при 99% нулей, даст 99% accuracy, но будет бесполезна.\n",
        "\n",
        "### 8.2. Метрики Регрессии\n",
        "\n",
        "| Метрика | Особенности |\n",
        "|--------|-------------|\n",
        "| **MSE** | Штрафует большие ошибки (квадрат); чувствителен к выбросам |\n",
        "| **MAE** | Линейный штраф; робастен к выбросам |\n",
        "| **RMSE** | В тех же единицах, что и целевая переменная → удобен для интерпретации |\n",
        "| **\\(R^2\\)** | Доля объяснённой дисперсии; 1 = идеально, 0 = не лучше среднего |\n",
        "\n",
        "### 8.3. Кросс-валидационное скорирование\n",
        "\n",
        "В `cross_val_score`, `GridSearchCV`, `RandomizedSearchCV` параметр `scoring` определяет, **что оптимизировать**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'classifier__C': [0.1, 1, 10]}\n",
        "grid = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "- `scoring='precision'` → минимизация ложных срабатываний;\n",
        "- `scoring='f1'` → баланс;\n",
        "- `scoring='roc_auc'` → оценка ранжирования.\n",
        "\n",
        "> **Ключевой навык аналитика**: уметь **сопоставить бизнес-сценарий и метрику**.  \n",
        "> Например:  \n",
        "> - «Мы не можем пропустить ни одного случая заболевания» → **maximize Recall**;  \n",
        "> - «Каждое ложное срабатывание стоит $1000» → **maximize Precision**.\n",
        "\n"
      ],
      "metadata": {
        "id": "DDjA6AYdjaMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Раздел 9. Диагностика: Переобучение, Недообучение и Кривые\n",
        "\n",
        "После обучения модели необходимо провести диагностику её производительности, чтобы понять, страдает ли она от **недообучения** (*high bias*) или **переобучения** (*high variance*). Scikit-learn предоставляет мощные инструменты для визуальной и количественной оценки этого баланса.\n",
        "\n",
        "### 9.1. Диагностика Bias-Variance Trade-off\n",
        "\n",
        "Дилемма «смещения-дисперсии» — центральная концепция машинного обучения:\n",
        "\n",
        "- **Смещение (Bias)** — ошибка, вызванная чрезмерной упрощённостью модели. Модель не может уловить истинные зависимости в данных → **недообучение**.\n",
        "- **Дисперсия (Variance)** — ошибка, вызванная чрезмерной чувствительностью к шуму в обучающих данных. Модель «запоминает» тренировку → **переобучение**.\n",
        "\n",
        "Идеальная модель находится в точке **компромисса**, где сумма смещения и дисперсии минимальна.\n",
        "\n",
        "### 9.2. Кривые обучения (`learning_curve`)\n",
        "\n",
        "Кривые обучения показывают, как **производительность на тренировке и валидации** меняется в зависимости от **размера обучающей выборки**.\n",
        "\n",
        "**Пример: Диагностика через `LearningCurveDisplay`**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Генерация данных\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Модель с высокой сложностью (склонна к переобучению)\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Кривая обучения\n",
        "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=42)\n",
        "LearningCurveDisplay.from_estimator(model, X, y, cv=cv, n_jobs=-1)\n",
        "plt.title(\"Кривая обучения: RandomForest (глубокие деревья)\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение до выполнения:*  \n",
        "Мы используем `ShuffleSplit` с 50 сплитами для стабильной оценки. Модель — случайный лес с глубокими деревьями (высокая дисперсия).\n",
        "\n",
        "*После выполнения:*  \n",
        "Если кривые **сильно расходятся** (высокий train score, низкий validation score), это **признак переобучения**. Если обе кривые **низкие и сходятся**, это **недообучение**.\n",
        "\n",
        "**Таблица 3: Диагностика по кривым обучения**\n",
        "\n",
        "| Характеристика кривых | Проблема | Решение |\n",
        "|----------------------|----------|--------|\n",
        "| Высокий Train Score, Низкий Test Score (разошлись) | **Переобучение** (High Variance) | Упростить модель, добавить регуляризацию, собрать больше данных |\n",
        "| Низкий Train Score, Низкий Test Score (сошлись) | **Недообучение** (High Bias) | Использовать более сложную модель, добавить признаки, уменьшить регуляризацию |\n",
        "| Высокий Train Score, Высокий Test Score (сошлись) | **Хорошее обобщение** | Модель готова к использованию |\n",
        "\n",
        "> **Важное следствие**: если валидационная кривая **ещё не вышла на плато**, добавление данных **улучшит** модель. Если кривые сошлись — новые данные **не помогут**.\n",
        "\n",
        "### 9.3. Валидационные кривые (`validation_curve`)\n",
        "\n",
        "Валидационные кривые показывают, как производительность зависит от **одного гиперпараметра**.\n",
        "\n",
        "**Пример: Выбор параметра регуляризации в SVM**\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "param_range = np.logspace(-3, 3, 7)\n",
        "train_scores, val_scores = validation_curve(\n",
        "    SVC(kernel='rbf'), X, y, param_name='C', param_range=param_range, cv=5\n",
        ")\n",
        "\n",
        "# Визуализация\n",
        "plt.semilogx(param_range, np.mean(train_scores, axis=1), 'o-', label='Train')\n",
        "plt.semilogx(param_range, np.mean(val_scores, axis=1), 'o-', label='Validation')\n",
        "plt.xlabel('C (параметр регуляризации)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(); plt.grid()\n",
        "plt.title(\"Валидационная кривая для SVM\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Интерпретация:*  \n",
        "- При **малых C** (сильная регуляризация) → недообучение;  \n",
        "- При **больших C** (слабая регуляризация) → переобучение;  \n",
        "- **Оптимум** — где валидационная кривая максимальна.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 10. Настройка Гиперпараметров и Автоматизированный Поиск\n",
        "\n",
        "### 10.1. Поиск по сетке (`GridSearchCV`)\n",
        "\n",
        "Полный перебор всех комбинаций. Гарантирует нахождение оптимума **в заданной сетке**, но **вычислительно дорог**.\n",
        "\n",
        "### 10.2. Случайный поиск (`RandomizedSearchCV`)\n",
        "\n",
        "Более эффективен при большом пространстве гиперпараметров. Использует **распределения**, а не списки:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import loguniform, randint\n",
        "\n",
        "param_dist = {\n",
        "    'classifier__C': loguniform(1e-4, 1e4),  # лог-равномерное распределение\n",
        "    'classifier__max_iter': [100, 200, 500],\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median']\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    clf, param_dist, n_iter=50, scoring='f1', cv=5, random_state=42, n_jobs=-1\n",
        ")\n",
        "search.fit(X_train, y_train)\n",
        "print(\"Лучшие параметры:\", search.best_params_)\n",
        "```\n",
        "\n",
        "> **Почему `loguniform`?**  \n",
        "> Параметры вроде `C` или `alpha` имеют **логарифмическую шкалу влияния**. Случайный поиск в лог-пространстве эффективнее покрывает диапазон.\n",
        "\n",
        "### 10.3. Интеграция поиска в Pipeline\n",
        "\n",
        "Как уже показано в разделе 5, **весь пайплайн** можно настраивать как единый объект. Это позволяет находить **глобально оптимальную конфигурацию**, а не только лучшую модель.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 11. Практические Кейсы: Несбалансированные Данные и Кластеризация\n",
        "\n",
        "### 11.1. Работа с несбалансированными классами\n",
        "\n",
        "#### Встроенное решение: `class_weight='balanced'`\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_balanced = LogisticRegression(class_weight='balanced')\n",
        "clf_balanced.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Это **минимальное изменение**, которое часто даёт значительный прирост recall для миноритарного класса.\n",
        "\n",
        "#### Продвинутые методы: SMOTE и `imblearn`\n",
        "\n",
        "**Критически важно**: SMOTE должен применяться **только внутри `fit`**, чтобы избежать утечки.\n",
        "\n",
        "**Правильный способ (через `imblearn.pipeline`):**\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Используем ImbPipeline, а не sklearn.pipeline!\n",
        "imb_pipe = ImbPipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "imb_pipe.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Если использовать `sklearn.pipeline`, SMOTE будет применён **до кросс-валидации**, и синтетические точки из валидационного фолда попадут в обучение → **утечка данных**.\n",
        "\n",
        "### 11.2. Основы неконтролируемого обучения: KMeans\n",
        "\n",
        "KMeans минимизирует **инерцию** — сумму квадратов расстояний от точек до центроидов.\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "```\n",
        "\n",
        "### 11.3. Оценка кластеризации: Silhouette Score и Plot\n",
        "\n",
        "Silhouette Score — внутренняя метрика качества кластеров.\n",
        "\n",
        "**Пример: Выбор числа кластеров**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "for n_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    print(f\"K={n_clusters}: Silhouette Score = {score:.3f}\")\n",
        "```\n",
        "\n",
        "**Silhouette Plot** даёт детальную картину:\n",
        "\n",
        "```python\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "fig, ax1 = plt.subplots(1, 1)\n",
        "silhouette_avg = silhouette_score(X, labels)\n",
        "sample_silhouette_values = silhouette_samples(X, labels)\n",
        "\n",
        "y_lower = 10\n",
        "for i in range(n_clusters):\n",
        "    ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                      0, ith_cluster_silhouette_values,\n",
        "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "ax1.set_xlabel('Коэффициент силуэта')\n",
        "ax1.set_ylabel('Кластер')\n",
        "ax1.set_title(f'Silhouette Plot для K={n_clusters}')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Интерпретация:*  \n",
        "- Равномерная ширина полос → сбалансированные кластеры;  \n",
        "- Низкие/отрицательные значения в кластере → плохое разделение.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 12. Интерпретируемость и Сохранение Моделей\n",
        "\n",
        "### 12.1. Встроенная интерпретируемость\n",
        "\n",
        "- **Линейные модели**: `coef_` — прямая интерпретация;\n",
        "- **Деревья**: `feature_importances_` — оценка вклада признака.\n",
        "\n",
        "### 12.2. Пост-хок интерпретируемость: SHAP и LIME\n",
        "\n",
        "Обе библиотеки **нативно работают с моделями из sklearn**:\n",
        "\n",
        "```python\n",
        "import shap\n",
        "\n",
        "# Объяснение через SHAP\n",
        "explainer = shap.TreeExplainer(clf.named_steps['classifier'])\n",
        "shap_values = explainer.shap_values(preprocessor.transform(X_test))\n",
        "shap.summary_plot(shap_values, preprocessor.transform(X_test))\n",
        "```\n",
        "\n",
        "> **SHAP** даёт **глобальные и локальные** объяснения, основанные на теории игр. Это **золотой стандарт** для интерпретации в финансах и медицине.\n",
        "\n",
        "### 12.3. Персистенция моделей\n",
        "\n",
        "**Всегда сохраняйте весь `Pipeline`**:\n",
        "\n",
        "```python\n",
        "from joblib import dump, load\n",
        "\n",
        "# Сохранение\n",
        "dump(clf, 'model_v1.joblib')\n",
        "\n",
        "# Загрузка\n",
        "clf_loaded = load('model_v1.joblib')\n",
        "predictions = clf_loaded.predict(new_data)  # предобработка + прогноз — автоматически\n",
        "```\n",
        "\n",
        "### 12.4. Воспроизводимость окружения (MLOps)\n",
        "\n",
        "- Сохраняйте **`requirements.txt`** или **`environment.yml`**;\n",
        "- Используйте **виртуальные окружения** или **Docker**;\n",
        "- **Pin-версии**: `scikit-learn==1.4.2`, `numpy==1.26.4` и т.д.\n",
        "\n",
        "**Таблица 4: Лучшие практики персистенции**\n",
        "\n",
        "| Задача | Инструмент | Комментарий |\n",
        "|--------|-----------|-------------|\n",
        "| Сохранение модели с NumPy-массивами | `joblib` | Быстрее и компактнее `pickle` |\n",
        "| Сохранение полного ML-процесса | `Pipeline` + `joblib` | Включая предобработку |\n",
        "| Обеспечение идентичности окружения | `pip freeze`, `conda env export` | Обязательно для продакшена |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Модуль 13: Отбор признаков и калибровка вероятностей\n",
        "\n",
        "В процессе построения надёжных и интерпретируемых моделей машинного обучения часто возникают две взаимосвязанные задачи: **управление размерностью признакового пространства** и **обеспечение корректности вероятностных прогнозов**. Хотя современные алгоритмы, такие как регуляризованные линейные модели или ансамбли деревьев, обладают встроенной устойчивостью к избыточным признакам, систематический отбор признаков остаётся важным этапом EDA и оптимизации. Аналогично, многие бизнес-сценарии требуют не просто бинарного прогноза, а **калиброванной оценки вероятности**, что особенно актуально при принятии решений на основе порогов риска. Данный модуль рассматривает обе эти задачи как неотъемлемые компоненты методологически строгого рабочего процесса.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 1. Отбор признаков: Управление размерностью и интерпретируемостью\n",
        "\n",
        "Отбор признаков — это процесс выбора подмножества релевантных переменных для использования в модели. Его цели многообразны:  \n",
        "- снижение вычислительной сложности;  \n",
        "- улучшение обобщающей способности за счёт устранения шума;  \n",
        "- повышение интерпретируемости за счёт фокуса на ключевых драйверах.\n",
        "\n",
        "Scikit-learn предлагает три основных парадигмы отбора признаков, которые различаются по степени взаимодействия с моделью.\n",
        "\n",
        "### 1.1. Фильтрационные методы (Filter Methods)\n",
        "\n",
        "Фильтрационные методы оценивают признаки **независимо от модели**, используя статистические тесты или эвристики. Они вычисляются **до обучения**, что делает их быстрыми и параллелизуемыми.\n",
        "\n",
        "- **`VarianceThreshold`** — удаляет признаки с дисперсией ниже порога. Полезен для исключения константных или почти константных переменных (например, флагов, установленных для 99.9% наблюдений).\n",
        "\n",
        "- **`SelectKBest` / `SelectPercentile`** — отбирают *k* лучших признаков на основе статистики, вычисляемой между признаком и целевой переменной:\n",
        "  - Для **классификации**:  \n",
        "    - `f_classif` — ANOVA F-статистика (предполагает нормальность);  \n",
        "    - `chi2` — хи-квадрат (только для неотрицательных данных, например, TF-IDF);  \n",
        "    - `mutual_info_classif` — взаимная информация (непараметрическая, улавливает нелинейные зависимости).\n",
        "  - Для **регрессии**: `f_regression`, `mutual_info_regression`.\n",
        "\n",
        "**Пример: Отбор признаков на основе взаимной информации**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Генерация данных: 1000 наблюдений, 50 признаков, только 5 информативны\n",
        "X, y = make_classification(n_samples=1000, n_features=50, n_informative=5,\n",
        "                           n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Вычисление взаимной информации\n",
        "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "\n",
        "# Отбор 10 лучших признаков\n",
        "selector = SelectKBest(mutual_info_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "print(f\"Исходная размерность: {X.shape[1]}\")\n",
        "print(f\"После отбора: {X_selected.shape[1]}\")\n",
        "print(f\"Из 5 истинно информативных признаков выбрано: {np.sum(selector.get_support()[:5])}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* Взаимная информация не предполагает линейной зависимости и эффективно выявляет релевантные признаки даже в присутствии шума.\n",
        "\n",
        "### 1.2. Обёрточные методы (Wrapper Methods)\n",
        "\n",
        "Обёрточные методы оценивают подмножества признаков **через производительность конкретной модели**, что делает их более точными, но и **вычислительно дорогими**.\n",
        "\n",
        "- **`RFE` (Recursive Feature Elimination)** — рекурсивно удаляет наименее важные признаки на основе весов модели (например, коэффициентов линейной регрессии или `feature_importances_` в деревьях).\n",
        "- **`RFECV`** — автоматически определяет оптимальное число признаков с помощью кросс-валидации.\n",
        "\n",
        "**Пример: Автоматический отбор признаков через RFECV**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Используем RFECV с кросс-валидацией\n",
        "estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='f1')\n",
        "\n",
        "selector.fit(X, y)\n",
        "\n",
        "print(f\"Оптимальное число признаков: {selector.n_features_}\")\n",
        "print(f\"Поддержка (выбранные признаки): {selector.support_.sum()} из {X.shape[1]}\")\n",
        "```\n",
        "\n",
        "*Пояснение:* `RFECV` находит компромисс между сложностью модели и её производительностью, что особенно ценно при жёстких ограничениях на интерпретируемость.\n",
        "\n",
        "### 1.3. Встроенные методы (Embedded Methods)\n",
        "\n",
        "Некоторые алгоритмы **внутренне** выполняют отбор признаков:\n",
        "\n",
        "- **Lasso** — обнуляет коэффициенты малозначимых признаков (см. Модуль 12, Раздел 6.3);\n",
        "- **Деревья решений и ансамбли** — естественным образом ранжируют признаки по важности.\n",
        "\n",
        "Преимущество встроенных методов — **высокая эффективность**, так как отбор происходит в процессе обучения.\n",
        "\n",
        "---\n",
        "\n",
        "## Раздел 2. Калибровка вероятностей: От прогноза к надёжной оценке риска\n",
        "\n",
        "Многие алгоритмы машинного обучения выдают **некалиброванные вероятности** — числа в диапазоне \\([0, 1]\\), которые не отражают истинную частоту события. Например, модель может присваивать вероятность 0.8 множеству объектов, но на самом деле только 60% из них принадлежат положительному классу. Это особенно характерно для:\n",
        "\n",
        "- **методов опорных векторов (SVM)** — из-за фокуса на опорных векторах;\n",
        "- **ансамблей деревьев (Random Forest, Gradient Boosting)** — из-за смещения в оценке экстремальных вероятностей.\n",
        "\n",
        "В задачах, где решения принимаются на основе **порогов вероятности** (например, «выдать кредит, если вероятность дефолта < 5%»), некалиброванные прогнозы приводят к систематическим ошибкам.\n",
        "\n",
        "### 2.1. Принципы калибровки\n",
        "\n",
        "Калибровка — это постобработка прогнозов модели с помощью **калибровочной функции** \\( \\hat{p} = f(p_{\\text{raw}}) \\), обученной на валидационных данных.\n",
        "\n",
        "Scikit-learn предоставляет мета-оценщик `CalibratedClassifierCV`, который поддерживает два метода:\n",
        "\n",
        "1. **Плюризация (Platt Scaling)** — аппроксимация сигмоидой:  \n",
        "   \\[\n",
        "   f(p) = \\frac{1}{1 + \\exp(A \\cdot p + B)}\n",
        "   \\]  \n",
        "   Эффективна при **достаточном объёме данных** и **гладких распределениях**.\n",
        "\n",
        "2. **Изотоническая регрессия** — непараметрический метод, представляющий \\(f\\) как **кусочно-постоянную неубывающую функцию**. Более гибок, но склонен к переобучению при малом числе наблюдений.\n",
        "\n",
        "### 2.2. Практическая реализация и диагностика\n",
        "\n",
        "**Пример: Калибровка SVM и визуализация через `CalibrationDisplay`**\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация данных\n",
        "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Некалиброванная SVM\n",
        "svm = SVC(probability=True, random_state=42)  # probability=True для predict_proba\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Калиброванная SVM (используем половину тренировки для калибровки)\n",
        "calibrated_svm = CalibratedClassifierCV(svm, method='isotonic', cv=2)\n",
        "calibrated_svm.fit(X_train, y_train)\n",
        "\n",
        "# Визуализация\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "CalibrationDisplay.from_estimator(svm, X_test, y_test, n_bins=10, ax=ax, name=\"SVM (raw)\")\n",
        "CalibrationDisplay.from_estimator(calibrated_svm, X_test, y_test, n_bins=10, ax=ax, name=\"SVM (calibrated)\")\n",
        "ax.set_title(\"Калибровочные кривые\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Интерпретация графика:*  \n",
        "- **Идеальная калибровка** — это диагональ \\(y = x\\);  \n",
        "- **Некалиброванная SVM** обычно показывает **S-образную кривую**: занижает низкие вероятности и завышает высокие;  \n",
        "- **Калиброванная модель** приближается к диагонали, что означает, что прогноз 0.7 действительно соответствует 70% частоте события.\n",
        "\n",
        "### 2.3. Когда калибровка обязательна?\n",
        "\n",
        "- **Медицинская диагностика**: вероятность заболевания должна быть точной для информированного согласия.\n",
        "- **Кредитный скоринг**: оценка риска дефолта напрямую влияет на условия займа.\n",
        "- **Маркетинг**: если бюджет распределяется по оценке вероятности отклика, некалиброванные оценки приведут к неэффективному расходованию средств.\n",
        "\n",
        "> **Важно**: калибровка **не улучшает** метрики вроде accuracy или AUC, но **делает вероятности надёжными** для принятия решений.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, отбор признаков и калибровка вероятностей — это два финальных штриха, превращающих «рабочую модель» в **надёжный инструмент принятия решений**. Первый обеспечивает **фокус на сути**, устраняя шум и повышая интерпретируемость; второй гарантирует, что **числовая оценка риска соответствует реальности**, что критично в прикладных науках.  \n",
        "\n",
        "Оба подхода органично интегрируются в экосистему Scikit-learn: `SelectKBest`, `RFECV` и `CalibratedClassifierCV` ведут себя как обычные `Estimator` и могут быть встроены в `Pipeline`, сохраняя методологическую строгость и защищённость от утечек данных.  \n",
        "\n",
        "Таким образом, полный цикл машинного обучения в Scikit-learn включает не только обучение и оценку, но и **тонкую настройку под бизнес-контекст**, что и отличает компетентного специалиста по анализу данных от просто пользователя библиотеки.\n",
        "\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Scikit-learn — это не просто библиотека, а **методологический фреймворк**, который учит **думать как инженер машинного обучения**. Его сила — в **дисциплине**: строгом разделении данных, защите от утечек, визуальной диагностике, осознанном выборе метрик и гиперпараметров.\n",
        "\n",
        "Для будущего специалиста по анализу данных освоение Scikit-learn — это не изучение API, а **воспитание культуры надёжного, воспроизводимого и интерпретируемого анализа**, без которой даже самая точная модель остаётся академическим упражнением.\n",
        ""
      ],
      "metadata": {
        "id": "-1e9_MIoks20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 13: OpenCV и Pillow — Обработка изображений и компьютерное зрение\n",
        "\n",
        "Настоящий модуль представляет собой методологически структурированное руководство по двум ключевым библиотекам в экосистеме Python для работы с изображениями: **Pillow** и **OpenCV**. В отличие от распространённого заблуждения о конкуренции, эти инструменты являются **взаимодополняющими**: Pillow обеспечивает простоту ввода/вывода и базовой манипуляции, в то время как OpenCV предоставляет высокопроизводительные алгоритмы компьютерного зрения. Для специалиста по анализу данных понимание их архитектурных различий и механизмов интеграции критически важно при построении надёжных пайплайнов — от предобработки медицинских снимков до подготовки данных для CNN.\n",
        "\n",
        "---\n",
        "\n",
        "## Глава 1: Фундаментальные Различия и Архитектурное Взаимодействие\n",
        "\n",
        "### 1.1. Позиционирование библиотек: архитектура и применение\n",
        "\n",
        "| Аспект | **Pillow (PIL Fork)** | **OpenCV (`cv2`)** |\n",
        "|--------|------------------------|---------------------|\n",
        "| **Ядро** | Python с внутренними C-функциями | C++ с Python-обёрткой |\n",
        "| **Основное назначение** | I/O, метаданные, базовая манипуляция (обрезка, поворот) | Компьютерное зрение, обработка видео, real-time |\n",
        "| **Производительность** | Легковесный, не оптимизирован под вычислительную нагрузку | Высокая: аппаратное ускорение (OpenVINO, CUDA), SIMD |\n",
        "| **Цветовой порядок** | **RGB** (по умолчанию) | **BGR** (по умолчанию) |\n",
        "| **Кривая обучения** | Низкая (интуитивный API) | Умеренная (требует понимания CV-алгоритмов) |\n",
        "\n",
        "**Методологический вывод**:  \n",
        "Pillow — «менеджер данных», OpenCV — «вычислительный движок». Их совместное использование формирует **гибридный пайплайн**:  \n",
        "1. **Загрузка** через Pillow (поддержка EXIF, форматов вроде PNG/WebP);  \n",
        "2. **Конвертация** в массив NumPy и переключение на OpenCV;  \n",
        "3. **Вычисления** в OpenCV (фильтрация, сегментация, детекция);  \n",
        "4. **Сохранение** обратно через Pillow (с сохранением метаданных).\n",
        "\n",
        "### 1.2. Механизмы интеграции: NumPy как мост обмена\n",
        "\n",
        "Обе библиотеки используют **многомерные массивы NumPy** в качестве универсального формата. Однако критически важно учитывать два аспекта:\n",
        "\n",
        "#### 1. Преобразование цветовых пространств\n",
        "\n",
        "Pillow использует **RGB**, OpenCV — **BGR**. Прямая передача массива приведёт к инверсии красного и синего каналов.\n",
        "\n",
        "#### 2. Создание копии памяти\n",
        "\n",
        "Без `.copy()` массив может быть *view* на данные Pillow, что вызовет ошибки при модификации в OpenCV.\n",
        "\n",
        "**Пример: Корректная интеграция Pillow → OpenCV**\n",
        "\n",
        "```python\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# 1. Загрузка через Pillow (сохранение метаданных)\n",
        "pil_img = Image.open(\"document.jpg\")\n",
        "print(\"EXIF:\", pil_img.info.get('exif', 'None'))\n",
        "\n",
        "# 2. Конвертация в OpenCV: BGR + копирование\n",
        "opencv_img = np.array(pil_img)[:, :, ::-1].copy()  # RGB → BGR + .copy()\n",
        "\n",
        "# 3. Обработка в OpenCV\n",
        "gray = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# 4. Возврат в Pillow для сохранения\n",
        "result_pil = Image.fromarray(cv2.cvtColor(blurred, cv2.COLOR_GRAY2RGB))\n",
        "result_pil.save(\"processed_document.jpg\", exif=pil_img.info.get('exif'))\n",
        "```\n",
        "\n",
        "*Пояснение до выполнения:*  \n",
        "Этот пайплайн сохраняет EXIF-данные (важно для геотеггинга, медицины), корректно обрабатывает цвет и избегает ошибок памяти.\n",
        "\n",
        "*После выполнения:*  \n",
        "Результат — обработанное изображение с сохранёнными метаданными, готовое к архивированию или передаче в модель ML.\n",
        "\n",
        "### 1.3. Модели цветовых пространств для компьютерного зрения\n",
        "\n",
        "Выбор цветового пространства — **методологическое решение**, влияющее на эффективность алгоритмов.\n",
        "\n",
        "- **HSV** (Hue, Saturation, Value):  \n",
        "  Идеален для **сегментации по цвету**. Канал Hue инвариантен к освещению:  \n",
        "  ```python\n",
        "  hsv = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2HSV)\n",
        "  # Выделение красных объектов\n",
        "  lower_red = np.array([0, 100, 100])\n",
        "  upper_red = np.array([10, 255, 255])\n",
        "  mask = cv2.inRange(hsv, lower_red, upper_red)\n",
        "  ```\n",
        "\n",
        "- **LAB** (Lightness, A, B):  \n",
        "  **Перцептуально равномерное** пространство. Используется в задачах, где важна **точность цветовых различий** (контроль качества, дерматология).  \n",
        "  Расстояние в LAB ≈ визуальное различие для человека.\n",
        "\n",
        "- **YCrCb**:  \n",
        "  Разделяет яркость (Y) и цветность (Cr, Cb). Применяется в **JPEG-сжатии** и **обнаружении кожи**.\n",
        "\n",
        "> **Практический вывод**: никогда не анализируйте цвет в RGB для сегментации. Всегда переключайтесь на HSV или LAB.\n",
        "\n",
        "---\n",
        "\n",
        "## Глава 2: Базовые Операции: Трансформация, Улучшение и Коррекция\n",
        "\n",
        "### 2.1. Линейное управление интенсивностью\n",
        "\n",
        "#### Pillow: объектно-ориентированный подход\n",
        "\n",
        "```python\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "# Увеличение контраста на 50%\n",
        "enhancer = ImageEnhance.Contrast(pil_img)\n",
        "pil_contrast = enhancer.enhance(1.5)\n",
        "```\n",
        "\n",
        "#### OpenCV: прямая арифметика с контролем переполнения\n",
        "\n",
        "```python\n",
        "# Параметры: alpha = контраст, beta = яркость\n",
        "alpha = 1.5  # контраст\n",
        "beta = 30    # яркость\n",
        "\n",
        "# Преобразование в float32 для избежания переполнения\n",
        "img_float = opencv_img.astype(np.float32)\n",
        "adjusted = alpha * img_float + beta\n",
        "\n",
        "# Ограничение диапазона и возврат к uint8\n",
        "adjusted = np.clip(adjusted, 0, 255).astype(np.uint8)\n",
        "```\n",
        "\n",
        "*Методологическое правило*:  \n",
        "Всегда работайте с `float32` при арифметике, затем — `np.clip(..., 0, 255).astype(np.uint8)`.\n",
        "\n",
        "### 2.2. Анализ интенсивности и гистограммы\n",
        "\n",
        "**Глобальное выравнивание** (`cv2.equalizeHist`) подходит только для **полутоновых** изображений:\n",
        "\n",
        "```python\n",
        "gray = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2GRAY)\n",
        "eq_hist = cv2.equalizeHist(gray)\n",
        "```\n",
        "\n",
        "**Адаптивное выравнивание** (CLAHE) — решение для **неравномерного освещения**:\n",
        "\n",
        "```python\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "clahe_img = clahe.apply(gray)\n",
        "```\n",
        "\n",
        "> **Почему это важно для аналитика?**  \n",
        "> CLAHE — стандартный шаг предобработки в **медицинской визуализации** (рентген, МРТ) и **OCR**, где локальный контраст критичен.\n",
        "\n",
        "### 2.3. Пороговая обработка: бинаризация\n",
        "\n",
        "#### Глобальный порог (Оцу)\n",
        "\n",
        "```python\n",
        "_, thresh_otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "```\n",
        "\n",
        "#### Адаптивный порог (для OCR)\n",
        "\n",
        "```python\n",
        "thresh_adaptive = cv2.adaptiveThreshold(\n",
        "    gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
        ")\n",
        "```\n",
        "\n",
        "*Практическое применение*:  \n",
        "Адаптивная бинаризация — **обязательный шаг** перед передачей изображения в Tesseract OCR. Без неё качество распознавания падает на 30–70%.\n",
        "\n",
        "---\n",
        "\n",
        "## Глава 3: Пространственная Фильтрация и Ядра Свертки\n",
        "\n",
        "### 3.1. Теоретические основы свёртки\n",
        "\n",
        "Свёртка:  \n",
        "\\[\n",
        "(I * K)(x, y) = \\sum_{i=-a}^{a} \\sum_{j=-b}^{b} I(x+i, y+j) \\cdot K(i, j)\n",
        "\\]  \n",
        "где \\(K\\) — ядро размером \\((2a+1) \\times (2b+1)\\).\n",
        "\n",
        "**Правило суммы ядра**:\n",
        "- **Сумма = 1** → сохранение яркости (сглаживание, резкость);\n",
        "- **Сумма = 0** → выделение границ (Собель, Лапласиан).\n",
        "\n",
        "### 3.2. Фильтры сглаживания\n",
        "\n",
        "**Гауссово размытие** — предварительный шаг перед детекцией границ:\n",
        "\n",
        "```python\n",
        "blurred = cv2.GaussianBlur(gray, (15, 15), 0)\n",
        "```\n",
        "\n",
        "> **Почему?**  \n",
        "> Производные (градиенты) усиливают шум. Сглаживание подавляет высокочастотные артефакты.\n",
        "\n",
        "### 3.3. Обнаружение границ\n",
        "\n",
        "#### Оператор Собеля\n",
        "\n",
        "```python\n",
        "grad_x = cv2.Sobel(blurred, cv2.CV_16S, 1, 0, ksize=3)\n",
        "grad_y = cv2.Sobel(blurred, cv2.CV_16S, 0, 1, ksize=3)\n",
        "\n",
        "abs_grad_x = cv2.convertScaleAbs(grad_x)\n",
        "abs_grad_y = cv2.convertScaleAbs(grad_y)\n",
        "edges = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)\n",
        "```\n",
        "\n",
        "#### Таблица 3.1: Базовые ядра свёртки\n",
        "\n",
        "| Тип ядра | Матрица | Сумма | Эффект |\n",
        "|---------|--------|------|--------|\n",
        "| **Идентичность** | \\(\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) | 1 | Без изменений |\n",
        "| **Повышение резкости** | \\(\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}\\) | 1 | Усиление деталей |\n",
        "| **Сглаживание** | \\(\\begin{bmatrix} 1/9 & 1/9 & 1/9 \\\\ 1/9 & 1/9 & 1/9 \\\\ 1/9 & 1/9 & 1/9 \\end{bmatrix}\\) | 1 | Размытие |\n",
        "| **Горизонтальный Собель** | \\(\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\\) | 0 | Вертикальные границы |\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Jekxi6H6m9cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Глава 4: Геометрические Преобразования и Интерполяция Пикселей\n",
        "\n",
        "Геометрические преобразования изменяют пространственное расположение пикселей — масштабирование, вращение, сдвиг или перспективное искажение. Поскольку новые пиксели часто не совпадают с исходной сеткой, требуется **интерполяция** — метод оценки значений на основе окрестности.\n",
        "\n",
        "### 4.1. Методы интерполяции при масштабировании\n",
        "\n",
        "Выбор метода — **компромисс между качеством и скоростью**:\n",
        "\n",
        "| Метод | Описание | Качество | Скорость | Когда использовать |\n",
        "|------|---------|--------|--------|------------------|\n",
        "| `cv2.INTER_NEAREST` | Ближайший сосед | Низкое (блоки) | Очень высокая | Бинарные маски, аннотации |\n",
        "| `cv2.INTER_LINEAR` | Билинейная (2×2) | Среднее | Высокая | Общая обработка |\n",
        "| `cv2.INTER_CUBIC` | Бикубическая (4×4) | Высокое | Средняя | Увеличение (zoom) |\n",
        "| `cv2.INTER_AREA` | Усреднение по области | Высокое при уменьшении | Высокая | **Уменьшение (shrink)** |\n",
        "\n",
        "**Ключевой инсайт**:  \n",
        "При **уменьшении** изображения `INTER_AREA` предпочтителен, так как он **встроенно фильтрует высокие частоты**, предотвращая **алиасинг** (ложные узоры из-за наложения частот). При **увеличении** — `INTER_CUBIC` даёт плавные края.\n",
        "\n",
        "**Пример: Сравнение интерполяции при уменьшении**\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread(\"document.jpg\")\n",
        "new_size = (img.shape[1] // 4, img.shape[0] // 4)\n",
        "\n",
        "# INTER_AREA — рекомендовано для уменьшения\n",
        "img_area = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# INTER_NEAREST — блочные артефакты\n",
        "img_nearest = cv2.resize(img, new_size, interpolation=cv2.INTER_NEAREST)\n",
        "```\n",
        "\n",
        "*Пояснение:* В OCR или медицинской визуализации алиасинг может создать ложные границы или текстуры. `INTER_AREA` — методологически правильный выбор для downsampling.\n",
        "\n",
        "### 4.2. Аффинные трансформации\n",
        "\n",
        "Аффинная трансформация сохраняет **коллинеарность и параллельность**, но не углы. Описывается матрицей \\(2 \\times 3\\).\n",
        "\n",
        "**OpenCV**: прямое применение через `cv2.warpAffine`.\n",
        "\n",
        "**Pillow**: использует **обратное отображение** (inverse mapping) — для каждого пикселя в выходе вычисляется источник в исходном изображении. Это гарантирует **отсутствие \"дыр\"**.\n",
        "\n",
        "**Пример: Вращение на 30° с центром**\n",
        "\n",
        "```python\n",
        "# OpenCV\n",
        "(h, w) = img.shape[:2]\n",
        "center = (w // 2, h // 2)\n",
        "M = cv2.getRotationMatrix2D(center, 30, 1.0)\n",
        "rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC)\n",
        "\n",
        "# Pillow\n",
        "from PIL import Image\n",
        "pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "rotated_pil = pil_img.rotate(30, resample=Image.BICUBIC, expand=False)\n",
        "```\n",
        "\n",
        "*Методологическое отличие:*  \n",
        "OpenCV требует явного указания размера выхода; Pillow автоматически сохраняет размер, но может обрезать углы. Для полного сохранения используйте `expand=True` и обрежьте позже.\n",
        "\n",
        "### 4.3. Перспективные трансформации\n",
        "\n",
        "Перспективная трансформация (**гомография**) исправляет искажения, возникающие при съёмке под углом — критично для **документов, номерных знаков, медицинских снимков**.\n",
        "\n",
        "**Пример: Выпрямление документа**\n",
        "\n",
        "```python\n",
        "# Углы исходного документа (вручную или через детекцию)\n",
        "pts1 = np.float32([[56, 65], [368, 52], [28, 387], [389, 390]])\n",
        "# Целевой прямоугольник\n",
        "pts2 = np.float32([[0, 0], [300, 0], [0, 400], [300, 400]])\n",
        "\n",
        "M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "warped = cv2.warpPerspective(img, M, (300, 400), flags=cv2.INTER_CUBIC)\n",
        "```\n",
        "\n",
        "*Практическое значение:*  \n",
        "Без этой коррекции **точность OCR падает на 20–50%** из-за искривлённых символов. Это — обязательный шаг в промышленных пайплайнах.\n",
        "\n",
        "### 4.4. Калибровка камеры и устранение дисторсии\n",
        "\n",
        "Объективы вносят **радиальную дисторсию** (прямые линии изгибаются). Устранение требует **калибровки** через шахматную доску.\n",
        "\n",
        "**Пример: Коррекция дисторсии**\n",
        "\n",
        "```python\n",
        "# Предположим, у вас уже есть mtx, dist из калибровки\n",
        "h, w = img.shape[:2]\n",
        "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n",
        "\n",
        "# Метод 1: Простой\n",
        "undistorted = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
        "\n",
        "# Метод 2: Быстрый (для видео)\n",
        "mapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w, h), 5)\n",
        "undistorted = cv2.remap(img, mapx, mapy, cv2.INTER_LINEAR)\n",
        "```\n",
        "\n",
        "*Почему remap быстрее?*  \n",
        "Карты `mapx`, `mapy` вычисляются **один раз** и применяются к каждому кадру через табличный поиск — критично для real-time.\n",
        "\n",
        "---\n",
        "\n",
        "## Глава 5: Морфологический Анализ и Структурный Поиск\n",
        "\n",
        "Морфологические операции — **нелинейные фильтры формы**, работающие с бинарными изображениями через **структурный элемент** (ядро).\n",
        "\n",
        "### 5.1–5.2. Базовые и сложные операции\n",
        "\n",
        "| Операция | Последовательность | Применение |\n",
        "|--------|------------------|-----------|\n",
        "| **Эрозия** | — | Удаление мелкого шума |\n",
        "| **Дилатация** | — | Соединение разрывов |\n",
        "| **Открытие** | Эрозия → Дилатация | Удаление шума, сохранение формы |\n",
        "| **Закрытие** | Дилатация → Эрозия | Заполнение дыр |\n",
        "| **Градиент** | Дилатация – Эрозия | Выделение границ |\n",
        "\n",
        "**Пример: Очистка текста перед OCR**\n",
        "\n",
        "```python\n",
        "# Бинаризация\n",
        "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "# Структурный элемент: горизонтальная линия (для соединения символов)\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))\n",
        "closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# Открытие для удаления мелких пятен\n",
        "kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "cleaned = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel2)\n",
        "```\n",
        "\n",
        "*Пояснение:* Горизонтальное закрытие соединяет разорванные буквы \"i\", \"l\"; последующее открытие удаляет пыль.\n",
        "\n",
        "### 5.3. Анализ геометрии объектов\n",
        "\n",
        "После морфологической очистки можно извлекать **геометрические признаки**:\n",
        "\n",
        "```python\n",
        "contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "for cnt in contours:\n",
        "    area = cv2.contourArea(cnt)\n",
        "    if area > 100:  # фильтр по площади\n",
        "        M = cv2.moments(cnt)\n",
        "        cx = int(M['m10'] / M['m00'])\n",
        "        cy = int(M['m01'] / M['m00'])\n",
        "        # Инвариантные моменты Ху для классификации формы\n",
        "        hu = cv2.HuMoments(M).flatten()\n",
        "```\n",
        "\n",
        "*Практическое применение:*  \n",
        "Моменты Ху используются в **классических системах распознавания символов** (до эпохи CNN) и до сих пор актуальны для встраиваемых систем с ограничениями памяти.\n",
        "\n",
        "---\n",
        "\n",
        "## Глава 6: Компьютерное Зрение: Обнаружение, Отслеживание и Сегментация\n",
        "\n",
        "### 6.1. Детектирование особенностей\n",
        "\n",
        "**ORB** — стандарт де-факто: быстрый, патентосвободный, инвариантный к вращению.\n",
        "\n",
        "```python\n",
        "orb = cv2.ORB_create()\n",
        "kp, des = orb.detectAndCompute(gray, None)\n",
        "img_kp = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0))\n",
        "```\n",
        "\n",
        "> **Почему не SIFT/SURF?**  \n",
        "> Патенты ограничивают коммерческое использование. ORB — оптимальный компромисс.\n",
        "\n",
        "### 6.2. Отслеживание объектов\n",
        "\n",
        "**Выбор трекера — компромисс**:\n",
        "\n",
        "| Трекер | Скорость | Точность | Устойчивость к окклюзии |\n",
        "|--------|--------|--------|----------------------|\n",
        "| `KCF` | Очень высокая | Средняя | Низкая |\n",
        "| `CSRT` | Низкая | Высокая | Высокая |\n",
        "| `MOSSE` | Максимальная | Низкая | Очень низкая |\n",
        "\n",
        "**Пример: Отслеживание через CSRT**\n",
        "\n",
        "```python\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "bbox = cv2.selectROI(\"Tracking\", frame, False)\n",
        "tracker.init(frame, bbox)\n",
        "\n",
        "while True:\n",
        "    success, frame = cap.read()\n",
        "    success, bbox = tracker.update(frame)\n",
        "    if success:\n",
        "        x, y, w, h = [int(v) for v in bbox]\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "```\n",
        "\n",
        "### 6.3. Продвинутая сегментация\n",
        "\n",
        "#### Watershed — для разделения касающихся объектов\n",
        "\n",
        "```python\n",
        "# 1. Преобразование расстояний\n",
        "dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
        "# 2. Уверенные маркеры\n",
        "_, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "# 3. Watershed\n",
        "markers = np.int32(sure_fg)\n",
        "markers = cv2.watershed(img, markers)\n",
        "img[markers == -1] = [255, 0, 0]  # границы — синие\n",
        "```\n",
        "\n",
        "#### GrabCut — для выделения объекта по прямоугольнику\n",
        "\n",
        "```python\n",
        "mask = np.zeros(img.shape[:2], np.uint8)\n",
        "bgdModel = np.zeros((1, 65), np.float64)\n",
        "fgdModel = np.zeros((1, 65), np.float64)\n",
        "rect = (50, 50, 450, 290)  # ROI\n",
        "cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
        "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
        "result = img * mask2[:, :, np.newaxis]\n",
        "```\n",
        "\n",
        "### 6.4. Интеграция глубокого обучения (DNN)\n",
        "\n",
        "OpenCV DNN — **мост между исследованием и продакшеном**:\n",
        "\n",
        "```python\n",
        "net = cv2.dnn.readNetFromONNX(\"yolov8n.onnx\")\n",
        "blob = cv2.dnn.blobFromImage(img, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "outputs = net.forward()\n",
        "\n",
        "# Декодирование YOLO-выхода (пример упрощён)\n",
        "# → получение bounding boxes, классов, confidence\n",
        "```\n",
        "\n",
        "> **Преимущества**:  \n",
        "> - Поддержка ONNX, TensorFlow, Darknet;  \n",
        "> - Аппаратное ускорение (CUDA, OpenVINO, ARM NEON);  \n",
        "> - Идеален для **edge-устройств** (Jetson, Raspberry Pi).\n",
        "\n",
        "---\n",
        "\n",
        "## Глава 7: Практические Приложения и Гибридные Пайплайны\n",
        "\n",
        "### 7.1. Гибридный пайплайн OCR\n",
        "\n",
        "**Этапы**:\n",
        "1. **Pillow**: загрузка + извлечение EXIF;\n",
        "2. **OpenCV**:  \n",
        "   - Перспективная коррекция (`warpPerspective`);  \n",
        "   - Адаптивная бинаризация (`adaptiveThreshold`);  \n",
        "   - Морфологическая очистка (`MORPH_CLOSE`, `MORPH_OPEN`);\n",
        "3. **Tesseract**: `pytesseract.image_to_string(cleaned_img, config='--psm 6')`;\n",
        "4. **NLP**: постобработка (исправление опечаток, структурирование).\n",
        "\n",
        "> **Ключевой факт**:  \n",
        "> Качество OCR на 70% зависит от **качества предобработки**. Без геометрической и фотометрической коррекции даже современные модели дают низкую точность.\n",
        "\n",
        "### 7.2. Управление метаданными (EXIF) в Pillow\n",
        "\n",
        "```python\n",
        "from PIL import Image, ExifTags\n",
        "\n",
        "img = Image.open(\"photo.jpg\")\n",
        "exif = {ExifTags.TAGS[k]: v for k, v in img._getexif().items() if k in ExifTags.TAGS}\n",
        "\n",
        "# Извлечение даты и GPS\n",
        "print(\"Дата:\", exif.get('DateTime'))\n",
        "print(\"GPS:\", exif.get('GPSInfo'))\n",
        "\n",
        "# Сохранение с EXIF\n",
        "cleaned_img_pil = Image.fromarray(cv2.cvtColor(processed_cv2_img, cv2.COLOR_BGR2RGB))\n",
        "cleaned_img_pil.save(\"output.jpg\", exif=img.info['exif'])\n",
        "```\n",
        "\n",
        "*Почему важно?*  \n",
        "В геоаналитике, дознании, медицине **происхождение данных** (data provenance) — неотъемлемая часть аудита.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Глава 8: Классические Методы Обнаружения Структур: Преобразование Хафа и Выделение Фона\n",
        "\n",
        "В то время как современные подходы, основанные на глубоком обучении, доминируют в задачах обнаружения объектов, классические методы компьютерного зрения сохраняют свою актуальность в сценариях, требующих **интерпретируемости, детерминированности и низких вычислительных затрат**. Преобразование Хафа (Hough Transform) и методы выделения фона (Background Subtraction) представляют собой два фундаментальных инструмента, которые позволяют надёжно обнаруживать геометрические примитивы и движущиеся объекты без необходимости в больших наборах размеченных данных. Эти алгоритмы особенно ценны в промышленной автоматизации, системах видеонаблюдения и анализе медицинских изображений, где предсказуемость и воспроизводимость критичны.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.1. Преобразование Хафа: Обнаружение Геометрических Примитивов\n",
        "\n",
        "Преобразование Хафа — это техника, позволяющая обнаруживать простые геометрические фигуры (линии, окружности, эллипсы) на изображении, даже если они разорваны или частично скрыты шумом. Его сила заключается в **параметризации** формы и последующем голосовании в **пространстве параметров**.\n",
        "\n",
        "#### Теоретические основы\n",
        "\n",
        "Идея состоит в том, чтобы перевести проблему из **пространства изображения** (где объект — это пиксели) в **пространство параметров** (где объект — это точка).\n",
        "\n",
        "- **Для прямой линии** в декартовых координатах: \\( y = mx + b \\).  \n",
        "  Однако эта параметризация неустойчива для вертикальных линий (\\(m \\to \\infty\\)).\n",
        "\n",
        "- **Нормальная форма линии** (параметризация Хафа):  \n",
        "$$\n",
        "  \\rho = x \\cos \\theta + y \\sin \\theta\n",
        "$$\n",
        "  где:\n",
        "  - \\(\\rho\\) — расстояние от начала координат до линии;\n",
        "  - \\(\\theta\\) — угол между нормалью к линии и осью X.\n",
        "\n",
        "Каждый крайний пиксель на изображении «голосует» за все возможные (\\(\\rho, \\theta\\)), которые проходят через него. Локальные максимумы в аккумуляторе (\\(\\rho, \\theta\\)) соответствуют **наиболее поддерживаемым линиям**.\n",
        "\n",
        "#### Обнаружение линий: `cv2.HoughLines` и `cv2.HoughLinesP`\n",
        "\n",
        "OpenCV предоставляет две реализации:\n",
        "\n",
        "1. **`cv2.HoughLines`** — возвращает параметры (\\(\\rho, \\theta\\)) всех обнаруженных линий.\n",
        "2. **`cv2.HoughLinesP`** (**P** — Probabilistic) — возвращает **отрезки** (координаты начала и конца), что практичнее для визуализации и дальнейшего анализа.\n",
        "\n",
        "**Пример: Обнаружение линий на чертеже или документе**\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Загрузка и предобработка\n",
        "img = cv2.imread('blueprint.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "edges = cv2.Canny(blurred, 50, 150, apertureSize=3)\n",
        "\n",
        "# Пробабилистическое преобразование Хафа\n",
        "lines = cv2.HoughLinesP(\n",
        "    edges,\n",
        "    rho=1,               # разрешение по ρ (пиксели)\n",
        "    theta=np.pi / 180,   # разрешение по θ (радианы)\n",
        "    threshold=100,       # мин. число голосов\n",
        "    minLineLength=50,    # мин. длина отрезка\n",
        "    maxLineGap=10        # макс. разрыв между сегментами\n",
        ")\n",
        "\n",
        "# Визуализация\n",
        "if lines is not None:\n",
        "    for line in lines:\n",
        "        x1, y1, x2, y2 = line[0]\n",
        "        cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "cv2.imshow('Detected Lines', img)\n",
        "cv2.waitKey(0)\n",
        "```\n",
        "\n",
        "*Пояснение до выполнения:*  \n",
        "Алгоритм сначала выделяет границы через Canny, затем в пространстве (\\(\\rho, \\theta\\)) ищет линии, поддерживаемые достаточным числом пикселей. Параметры `minLineLength` и `maxLineGap` позволяют фильтровать короткие артефакты и соединять разрывы.\n",
        "\n",
        "*После выполнения:*  \n",
        "Этот подход широко используется в **промышленном контроле качества** (проверка прямолинейности деталей), **анализе архитектурных чертежей** и **сканировании документов**, где необходимо выровнять текст по обнаруженным линиям.\n",
        "\n",
        "#### Обнаружение окружностей: `cv2.HoughCircles`\n",
        "\n",
        "Окружность параметризуется тремя переменными: центр \\((x_c, y_c)\\) и радиус \\(r\\). Преобразование Хафа для окружностей требует трёхмерного аккумулятора, что делает его вычислительно дороже.\n",
        "\n",
        "OpenCV использует **градиентный метод** (Gradient Hough Transform), который ускоряет поиск, используя направление градиента на границах.\n",
        "\n",
        "**Пример: Подсчёт монет или детекция глаз**\n",
        "\n",
        "```python\n",
        "# Применяем к изображению\n",
        "gray = cv2.medianBlur(gray, 5)  # сглаживание критично для HoughCircles\n",
        "\n",
        "circles = cv2.HoughCircles(\n",
        "    gray,\n",
        "    cv2.HOUGH_GRADIENT,\n",
        "    dp=1,            # разрешение аккумулятора (1 = исходное)\n",
        "    minDist=50,      # мин. расстояние между центрами\n",
        "    param1=50,       # верхний порог для Canny\n",
        "    param2=30,       # порог голосования (чем выше — тем строже)\n",
        "    minRadius=20,\n",
        "    maxRadius=60\n",
        ")\n",
        "\n",
        "if circles is not None:\n",
        "    circles = np.uint16(np.around(circles))\n",
        "    for i in circles[0, :]:\n",
        "        # Рисуем окружность и центр\n",
        "        cv2.circle(img, (i[0], i[1]), i[2], (0, 255, 0), 2)\n",
        "        cv2.circle(img, (i[0], i[1]), 2, (0, 0, 255), 3)\n",
        "```\n",
        "\n",
        "*Практическое применение:*  \n",
        "- **Медицина**: детекция зрачков, фолликулов;  \n",
        "- **Промышленность**: проверка диаметра отверстий, подсчёт шариков подшипников;  \n",
        "- **Ритейл**: подсчёт монет на кассе.\n",
        "\n",
        "> **Методологическое замечание**:  \n",
        "> HoughCircles **чувствителен к шуму**. Предварительное сглаживание (`medianBlur`) и корректная настройка `param2` (порог голосования) — ключ к успеху.\n",
        "\n",
        "---\n",
        "\n",
        "### 8.2. Выделение Фона: Обнаружение Движущихся Объектов\n",
        "\n",
        "В видеонаблюдении и робототехнике часто требуется выделить **движущиеся объекты** на статичном фоне. Вместо применения детекторов в каждом кадре, более эффективно **моделировать фон** и выделять всё, что от него отличается.\n",
        "\n",
        "OpenCV предоставляет два основных алгоритма:\n",
        "\n",
        "- **`cv2.createBackgroundSubtractorMOG2`** — на основе смеси гауссианов (Gaussian Mixture Models);\n",
        "- **`cv2.createBackgroundSubtractorKNN`** — на основе k-ближайших соседей.\n",
        "\n",
        "Оба метода **адаптируются к изменениям** (например, постепенному изменению освещения) и могут обрабатывать тени.\n",
        "\n",
        "**Пример: Обнаружение движения в видеопотоке**\n",
        "\n",
        "```python\n",
        "cap = cv2.VideoCapture(\"traffic.mp4\")\n",
        "fgbg = cv2.createBackgroundSubtractorMOG2(\n",
        "    history=500,        # длина истории\n",
        "    varThreshold=50,    # порог отклонения\n",
        "    detectShadows=True  # учитывать тени\n",
        ")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Получаем маску переднего плана\n",
        "    fgmask = fgbg.apply(frame)\n",
        "\n",
        "    # Удаляем шум морфологией\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    # Визуализация\n",
        "    cv2.imshow('Original', frame)\n",
        "    cv2.imshow('Foreground Mask', fgmask)\n",
        "\n",
        "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "```\n",
        "\n",
        "*Пояснение:*  \n",
        "Алгоритм строит вероятностную модель фона. Пиксели, которые не укладываются в модель (с высокой вероятностью являются новыми), помечаются как передний план. Параметр `varThreshold` управляет чувствительностью: чем ниже — тем больше ложных срабатываний.\n",
        "\n",
        "*Компромиссы:*  \n",
        "- `MOG2` лучше справляется с тенями;  \n",
        "- `KNN` быстрее и точнее при резких изменениях.\n",
        "\n",
        "> **Практическое применение**:  \n",
        "> - Подсчёт посетителей в магазине;  \n",
        "> - Обнаружение вторжения в охраняемую зону;  \n",
        "> - Трекинг спортсменов на арене.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Преобразование Хафа и выделение фона — это **классические, но не устаревшие** методы компьютерного зрения. Их преимущества — **отсутствие зависимости от обучающих данных**, **предсказуемость** и **низкая вычислительная сложность** — делают их незаменимыми в промышленных и встраиваемых системах, где надёжность важнее абсолютной точности.\n",
        "\n",
        "Для специалиста по анализу данных эти инструменты расширяют арсенал за пределы нейросетей, позволяя решать задачи **структурного анализа** и **анализа движения** в условиях ограниченных ресурсов или отсутствия размеченных данных. Интеграция таких методов в гибридные пайплайны (например, Hough для выравнивания документа → OCR → NLP) демонстрирует глубину и гибкость современного компьютерного зрения на базе OpenCV.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 7.3. Заключение\n",
        "\n",
        "OpenCV и Pillow — не конкуренты, а **дополняющие компоненты** единой экосистемы:\n",
        "\n",
        "- **Pillow** — для **I/O, метаданных, простой манипуляции**;\n",
        "- **OpenCV** — для **алгоритмов, real-time, интеграции с DL**.\n",
        "\n",
        "**Оптимальный пайплайн**:\n",
        "1. Загрузка через Pillow;\n",
        "2. Конвертация в OpenCV (RGB → BGR + `.copy()`);\n",
        "3. Вычисления в OpenCV;\n",
        "4. Возврат в Pillow для сохранения с EXIF.\n",
        "\n",
        "Этот подход обеспечивает **методологическую строгость**, **максимальную производительность** и **сохранение контекста данных** — три столпа профессиональной работы с изображениями в анализе данных.\n"
      ],
      "metadata": {
        "id": "0uPn_8juqTjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 14: Оптимизация и MLOps — от эксперимента к промышленной системе\n",
        "\n",
        "## Введение: MLOps как Инженерная Культура\n",
        "\n",
        "Современный машинный интеллект (ML) переживает фундаментальный переход: он всё чаще выходит за пределы академических исследований, демонстрационных ноутбуков и пилотных проектов, становясь неотъемлемым и критически важным компонентом стратегических бизнес-процессов. Это трансформация требует радикальной смены парадигмы мышления. Ранее, в эпоху исследовательского ML, главным критерием успеха была метрика качества модели — точность, F1-мера или AUC-ROC. В современной промышленной реальности главным критерием становится **надёжность, воспроизводимость, масштабируемость и сопровождаемость системы в целом**. Переход от экспериментальной модели, работающей на локальной машине исследователя, к промышленной, масштабируемой и надёжной системе, развернутой в облачной или гибридной инфраструктуре, требует не просто технических навыков, а формирования целой **инженерной культуры**.\n",
        "\n",
        "Эта культура, получившая название **MLOps** (Machine Learning Operations), представляет собой систематизированный подход к управлению полным жизненным циклом ML-системы. Она является прямым наследником и расширением принципов **DevOps**, адаптированных под уникальные, динамические и непредсказуемые свойства машинного обучения. В то время как традиционное программное обеспечение (ПО) — это статическая система, где поведение полностью определяется кодом, ML-система является триединым динамическим организмом, состоящим из **кода**, **данных** и **модели**. Все три компонента непрерывно взаимодействуют и изменяются во времени, что вносит дополнительные слои сложности и источники потенциальных сбоев.\n",
        "\n",
        "Попытки развернуть ML-модель в производственной среде без соответствующей инженерной базы неизбежно приводят к накоплению так называемого **«скрытого технического долга»**. Этот термин, введённый в контексте ML-систем исследователями Google, описывает ситуацию, когда система внешне функционирует, но её внутренняя структура настолько хрупка, несогласованна и плохо задокументирована, что любые изменения — будь то обновление данных, настройка гиперпараметров или даже обновление версии библиотеки — могут привести к катастрофическому падению качества или полному отказу. Успешное внедрение ML в продакшен определяется не только метрикой точности модели на исторических данных, но и её способностью к **адаптации к новым данным**, **воспроизводимости в любых условиях**, **масштабированию под нагрузку** и **надёжному мониторингу** в реальном времени.\n",
        "\n",
        "### 14.1. Отличия Продуктового ML от Академических Проектов\n",
        "\n",
        "Фундаментальное отличие продуктового ML от академического заключается в смещении фокуса с **алгоритма** на **систему**. В академической среде исследователь оптимизирует отдельную функцию потерь на фиксированном наборе данных. В промышленной среде инженер отвечает за непрерывный цикл: от сбора и валидации новых данных, через их обработку и обучение модели, до развертывания сервиса инференса, его мониторинга и автоматического переобучения.\n",
        "\n",
        "Этот цикл структурируется вокруг четырёх взаимосвязанных и непрерывных практик, которые образуют **четыре столпа MLOps**.\n",
        "\n",
        "#### Четыре Столпа MLOps (CI/CD/CT/CM)\n",
        "\n",
        "1.  **Continuous Integration (CI, Непрерывная Интеграция)**: В контексте MLOps практика CI значительно расширяется за пределы традиционного тестирования кода. Она теперь включает в себя:\n",
        "    *   **Валидацию данных**: автоматические проверки на соответствие ожидаемой схеме (schema validation), на наличие дрейфа (drift detection), на статистические аномалии (anomaly detection).\n",
        "    *   **Валидацию модели**: тесты на корректность входов и выходов модели, проверки на отсутствие вырожденного поведения (например, предсказание одного класса со 100% вероятностью).\n",
        "    *   **Валидацию кода**: классические unit- и integration-тесты для всех компонентов пайплайна — от модулей предобработки до логики инференса.\n",
        "    Цель CI — гарантировать, что каждое изменение в кодовой базе не нарушает целостность всей системы и соответствует заданным стандартам качества.\n",
        "\n",
        "2.  **Continuous Delivery (CD, Непрерывная Доставка)**: Эта практика касается не просто развертывания модели как статического артефакта, а развертывания всего **ML-пайплайна**. Пайплайн — это единый, версионированный и тестируемый объект, который может быть запущен в любой среде. CD в ML означает, что после прохождения всех тестов в CI, пайплайн автоматически упаковывается (часто в Docker-контейнер) и становится готовым к развёртыванию в staging- или production-среде. Это позволяет быстро и безопасно тестировать, собирать и доставлять новые версии логики обработки данных и обучения.\n",
        "\n",
        "3.  **Continuous Training (CT, Непрерывное Обучение)**: Это уникальный и самый важный столп, отличающий MLOps от классического DevOps. CT — это полностью автоматизированный процесс, который запускает пайплайн обучения с новыми данными на основе заданных триггеров. Триггерами могут быть:\n",
        "    *   Расписание (например, еженедельное переобучение).\n",
        "    *   Обнаружение дрейфа данных или модели (когда качество предсказаний на production-данных падает ниже порога).\n",
        "    *   Поступление определённого объёма новых размеченных данных.\n",
        "    CT является главным инструментом борьбы с устареванием моделей и обеспечивает их актуальность в меняющейся реальности.\n",
        "\n",
        "4.  **Continuous Monitoring (CM, Непрерывный Мониторинг)**: Мониторинг в MLOps имеет два уровня. Первый — это технический мониторинг: задержка ответа (latency), пропускная способность (throughput), использование CPU/GPU и памяти. Второй, и гораздо более важный, — это **бизнес-мониторинг**:\n",
        "    *   Метрики качества модели на «живых» данных (accuracy, precision, recall).\n",
        "    *   Метрики качества данных (распределение признаков, частота пропусков).\n",
        "    *   Бизнес-метрики, на которые влияют предсказания модели (например, конверсия, выручка, уровень оттока).\n",
        "    Связь между ML-метриками и бизнес-метриками является конечной целью и главным показателем ценности модели для компании.\n",
        "\n",
        "#### Ключевой Паттерн: Экспериментально-операционная Симметрия\n",
        "\n",
        "Фундаментальным методологическим требованием промышленного MLOps является обеспечение **экспериментально-операционной симметрии**. Этот принцип означает, что реализация пайплайна, которая была разработана и протестирована в среде экспериментирования (например, в Jupyter Notebook или локальном скрипте), должна быть **идентична** той, что используется в пре-продакшене и продакшене. Любой ручной перевод логики из интерактивной среды в production-код — это источник неизбежных ошибок и нарушение воспроизводимости.\n",
        "\n",
        "Успешный MLOps-подход строится на том, что инженер разрабатывает **единый пайплайн**, который может быть запущен в трёх режимах:\n",
        "1.  **Экспериментальный режим**: с небольшими данными для быстрой итерации.\n",
        "2.  **Режим CI/CD**: с полными наборами данных для валидации и тестирования.\n",
        "3.  **Продакшен-режим**: в масштабируемой среде для непрерывного обучения и инференса.\n",
        "\n",
        "Таким образом, цель MLOps — не просто «развернуть модель», а развернуть **пайплайн, который способен автоматизировать весь жизненный цикл модели**, от переобучения до развертывания. Это смещение фокуса с артефакта (модели) на процесс (пайплайн) является ключом к созданию надёжных и долгоживущих ML-систем.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть I: Разработка и Локальная Оптимизация (Focus: Performance & Debugging)\n",
        "\n",
        "Прежде чем переходить к сложностям масштабирования и оркестрации в облаке, необходимо убедиться, что базовый код ML-алгоритмов и пользовательских функций обработки данных работает с максимальной возможной эффективностью на локальном уровне. Python, несмотря на всю свою выразительность и богатую экосистему, страдает от врождённой проблемы производительности в так называемых «узких местах» — циклах (`for`/`while`) и функциях, которые не могут быть эффективно векторизованы с помощью NumPy или Pandas. В таких случаях интерпретируемая природа Python приводит к замедлению вычислений на порядки или даже на два порядка по сравнению с нативным кодом на C или Fortran. Решение этой проблемы лежит в использовании современных компиляторов, которые могут преобразовать высокоуровневый Python-код в высокооптимизированный машинный код.\n",
        "\n",
        "### 14.2. Глубокая Оптимизация Python для ML-Ядер\n",
        "\n",
        "Для достижения производительности, сравнимой с низкоуровневыми языками, инженеры в области MLOps используют две основные стратегии компиляции: **Just-In-Time (JIT)** с помощью библиотеки Numba и **Ahead-of-Time (AOT)** с помощью библиотеки Cython. Выбор между ними зависит от конкретной задачи и требований к интеграции.\n",
        "\n",
        "#### 14.2.1. JIT-Компиляция с Numba\n",
        "\n",
        "Numba — это компилирующий оптимизатор, основанный на инфраструктуре LLVM. Его основная задача — преобразовывать функции Python, которые оперируют числовыми данными (массивами NumPy, скалярами) и содержат циклы, в высокоэффективный машинный код, который выполняется непосредственно на процессоре (CPU) или даже на графическом ускорителе (GPU).\n",
        "\n",
        "**Критичность режима nopython (`@njit`)**.\n",
        "Наиболее важным аспектом использования Numba в производственном коде является строгое соблюдение **режима `nopython`**. По умолчанию Numba пытается компилировать функцию в этом режиме, который гарантирует генерацию чистого машинного кода без каких-либо вызовов интерпретатора Python. Однако, если Numba сталкивается с операцией или типом данных, которые он не поддерживает в `nopython` режиме (например, работа со словарями Python, списками или сложными объектами), он может **молчаливо откатиться** в так называемый **`object mode`**. В этом режиме компилятор генерирует код, который постоянно взаимодействует с интерпретатором Python, что делает его производительность сопоставимой с обычным интерпретируемым кодом, а иногда и хуже из-за накладных расходов на компиляцию.\n",
        "\n",
        "Такой молчаливый откат является одной из самых опасных и скрытых угроз производительности в продакшен-коде, так как он не вызывает ошибок и не даёт явных предупреждений, но полностью сводит на нет все преимущества от использования Numba.\n",
        "\n",
        "Для предотвращения этой проблемы **всегда** следует использовать декоратор `@njit`, который является удобным псевдонимом для `@jit(nopython=True)`. Этот декоратор явно требует компиляции в `nopython` режиме и, в случае невозможности, немедленно вызывает исключение. Это позволяет инженеру на этапе разработки или тестирования обнаружить проблему и либо переписать функцию, либо принять осознанное решение об отказе от компиляции.\n",
        "\n",
        "**Параллелизация и GIL**.\n",
        "Numba значительно упрощает параллельное программирование в Python. Используя декоратор `@njit(parallel=True)` и заменяя стандартную функцию `range` на `prange` внутри циклов, Numba может автоматически распараллелить вычисления, распределив итерации по всем доступным ядрам CPU. Более того, код, скомпилированный в `nopython` режиме, **освобождает Global Interpreter Lock (GIL)**. GIL — это механизм в CPython, который позволяет только одному потоку выполнять Python-байткод в любой момент времени, что делает стандартные Python-потоки бесполезными для CPU-bound задач. Освобождение GIL в Numba означает, что несколько потоков, выполняющих скомпилированный код, действительно могут работать параллельно, что критически важно для оптимизации как инференса, так и предварительной обработки данных.\n",
        "\n",
        "Параметр `cache=True` позволяет кэшировать скомпилированную версию функции на диск, что устраняет накладные расходы на компиляцию при последующих запусках программы.\n",
        "\n",
        "**Пример Кода: Оптимизация Кастомной Постобработки**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Рассмотрим типичную задачу, возникающую в промышленном ML: необходимость применить сложную, условную логику к большому массиву числовых данных (например, агрегация метрик с разными коэффициентами в зависимости от порогового значения). Прямая реализация на чистом Python с циклом `for` будет чрезвычайно медленной для массивов размером в миллионы элементов, так как каждый шаг цикла требует интерпретации. Векторизованные операции NumPy здесь не всегда применимы из-за сложной ветвящейся логики. Numba позволяет написать простую и читаемую функцию, а затем скомпилировать её в эффективный машинный код с параллельным выполнением.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import time\n",
        "\n",
        "# Применение @njit с параллелизмом и кэшированием\n",
        "@njit(parallel=True, cache=True)\n",
        "def optimized_kernel(data, threshold):\n",
        "    \"\"\"\n",
        "    Оптимизированное ядро, выполняющее ветвление и вычисления.\n",
        "    Использует prange для распараллеливания цикла на несколько ядер CPU.\n",
        "    Режим nopython гарантирует, что функция либо скомпилируется в быстрый код,\n",
        "    либо вызовет исключение, что предотвращает скрытые проблемы с производительностью.\n",
        "    \"\"\"\n",
        "    result = 0.0\n",
        "    # Использование prange для параллельного выполнения итераций.\n",
        "    # Numba автоматически распределит работу по доступным ядрам.\n",
        "    for i in prange(len(data)):\n",
        "        if data[i] > threshold:\n",
        "            # Условная логика, которая замедлила бы чистый Python\n",
        "            result += data[i] * 2.0\n",
        "        else:\n",
        "            result += data[i] / 2.0\n",
        "    return result\n",
        "\n",
        "# Пример использования\n",
        "# Создание большого массива данных для демонстрации выигрыша в производительности\n",
        "data_array = np.random.rand(10_000_000).astype(np.float32)\n",
        "threshold_val = 0.5\n",
        "\n",
        "# Первый запуск включает компиляцию (холодный старт)\n",
        "# Время этого запуска включает время на генерацию машинного кода LLVM.\n",
        "start = time.time()\n",
        "_ = optimized_kernel(data_array, threshold_val)\n",
        "end = time.time()\n",
        "print(f\"Время (холодный старт, включая компиляцию): {end - start:.4f} секунд\")\n",
        "\n",
        "# Последующие запуски используют кэшированный скомпилированный код\n",
        "# и демонстрируют реальную производительность ядра.\n",
        "start = time.time()\n",
        "total_result = optimized_kernel(data_array, threshold_val)\n",
        "end = time.time()\n",
        "print(f\"Время (горячий запуск, оптимизированное ядро): {end - start:.4f} секунд\")\n",
        "print(f\"Результат вычислений: {total_result:.2f}\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Выполнение этого примера на современном многоядерном процессоре покажет колоссальный выигрыш в скорости на «горячем» запуске по сравнению с эквивалентной функцией на чистом Python (разница может составлять 50-100 раз). Это демонстрирует, как Numba позволяет исследователю и инженеру писать простой и понятный код, не жертвуя производительностью. Ключ к успеху — в строгом использовании `@njit` для гарантии компиляции и `prange` для автоматического распараллеливания.\n",
        "\n",
        "#### 14.2.2. AOT-Компиляция с Cython\n",
        "\n",
        "В то время как Numba идеально подходит для оптимизации отдельных функций и числовых ядер, **Cython** предлагает более мощный и гибкий подход для создания целых модулей и расширений. Cython использует компиляцию **Ahead-of-Time (AOT)**, преобразуя Python-подобный код в C-код, который затем компилируется стандартным C-компилятором (например, `gcc`) в нативный машинный модуль (`.so` или `.pyd`), который может быть импортирован в Python как обычный пакет.\n",
        "\n",
        "Основное отличие Cython от Numba заключается в том, что он требует от разработчика **явной статической типизации**. Это достигается с помощью ключевого слова `cdef`, которое позволяет объявлять переменные, аргументы функций и возвращаемые значения с типами C (например, `int`, `double`, `float*`). Эта явная типизация даёт компилятору всю необходимую информацию для генерации максимально эффективного кода без каких-либо накладных расходов на динамическую типизацию Python.\n",
        "\n",
        "**Применимость и Интеграция с C++**.\n",
        "Cython незаменим в сценариях, где требуется **бесшовная интеграция с существующими библиотеками, написанными на C или C++**. Это критически важно в промышленных MLOps-системах, где часто приходится работать с legacy-кодом, оптимизированными C++ библиотеками для обработки сигналов или изображений, или использовать сложные контейнеры из стандартной библиотеки C++ (STL), такие как `std::vector` или `std::map`. Cython позволяет объявить внешние C++ классы и функции с помощью блока `cdef extern from` и затем работать с ними напрямую из своего кода, эффективно создавая «тонкий» Python-обёртку вокруг мощного C++ ядра.\n",
        "\n",
        "Несмотря на то, что процесс сборки с Cython более сложен (требуется написание файла `setup.py` и наличие C-компилятора в системе), он предоставляет разработчику максимальный контроль над памятью, производительностью и взаимодействием с низкоуровневыми системами. Это делает Cython предпочтительным выбором для создания высокоскоростных, низкоуровневых расширений для Python, особенно в тех случаях, когда Numba не может обеспечить необходимую гибкость или интеграцию.\n",
        "\n",
        "#### 14.2.3. Инженерный Контроль: Профилирование Numba\n",
        "\n",
        "Внедрение JIT-скомпилированного кода в производственную систему требует специализированных методов отладки и профилирования. Стандартные профилировщики Python, такие как `cProfile`, не могут точно измерить время, затраченное на выполнение внутри скомпилированной функции, потому что они работают на уровне интерпретатора байткода, а JIT-код выполняется напрямую на CPU.\n",
        "\n",
        "Инженеры должны использовать альтернативные инструменты:\n",
        "1.  **Профилировщики на основе сэмплинга**: Инструменты, такие как `py-spy` или `perf` (в Linux), работают на уровне операционной системы и могут делать снимки (сэмплы) стека вызовов любого процесса, включая тот, который выполняет нативный код. Это позволяет точно определить, сколько времени реально тратится на выполнение JIT-скомпилированных блоков.\n",
        "2.  **Явное требование nopython**: Как уже упоминалось, использование `@njit` вместо `@jit` является первым и самым важным шагом в профилактике проблем с производительностью. Он превращает потенциальную скрытую ошибку в явное исключение, которое легко обнаружить на этапе тестирования.\n",
        "3.  **Анализ генерируемого кода**: Numba предоставляет утилиты для инспектирования сгенерированного LLVM-кода, что позволяет опытным инженерам глубоко понять, как именно их функция была оптимизирована.\n",
        "\n",
        "> **Таблица 1: Сравнение Оптимизации Python: Numba vs. Cython**\n",
        "\n",
        "| Характеристика | **Numba** | **Cython** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Простота Внедрения** | Высокая. Достаточно добавить декоратор `@njit` к существующей функции. | Средняя. Требует написания отдельного `.pyx` файла и `setup.py` для сборки. |\n",
        "| **Основной Механизм** | Just-In-Time (JIT) компиляция в LLVM. | Ahead-of-Time (AOT) компиляция в C. |\n",
        "| **Типизация** | Автоматическая, на основе анализа типов во время выполнения (в `nopython` режиме). | Явная, через ключевые слова `cdef`, `cpdef`. |\n",
        "| **Параллелизм** | Встроенный через `@njit(parallel=True)` и `prange`. | Требует ручного использования OpenMP или многопоточности через C API. |\n",
        "| **Интеграция с C/C++** | Ограниченная, в основном через CFFI. | Полная и нативная. Позволяет напрямую вызывать C/C++ функции и использовать их типы и классы. |\n",
        "| **Идеальное применение** | Быстрая оптимизация числовых ядер, функций с циклами и массивами. | Создание высокопроизводительных расширений, интеграция с legacy C/C++ кодом, системное программирование. |\n",
        "\n",
        "---\n",
        "\n",
        "## Часть II: Экспериментирование, Воспроизводимость и Масштабирование\n",
        "\n",
        "После того как критически важные вычислительные ядра оптимизированы на локальном уровне, следующий шаг в создании зрелой MLOps-практики — это систематизация и автоматизация всего процесса экспериментирования. Воспроизводимость — это не просто хорошая практика, а абсолютная необходимость для командной работы, отладки и доверия к результатам.\n",
        "\n",
        "### 14.3. Обеспечение Воспроизводимости Экспериментов\n",
        "\n",
        "Воспроизводимость в ML означает возможность **точно воссоздать результат эксперимента** — вплоть до последнего знака после запятой в метрике качества — при наличии того же кода, тех же данных и той же конфигурации среды выполнения (версий библиотек, ОС и т.д.).\n",
        "\n",
        "#### 14.3.1. Системы Трекинга (MLflow Tracking и W&B)\n",
        "\n",
        "Для отслеживания всех элементов эксперимента — гиперпараметров, метрик, версии кода, артефактов модели и сырых данных — используются специализированные системы трекинга.\n",
        "\n",
        "**MLflow Tracking** является частью более широкой открытой платформы MLflow и предоставляет стандартизированный и простой в использовании API для логирования. Его ключевое преимущество — **открытость и гибкость**. MLflow можно развернуть локально или в облаке, и он интегрируется практически с любым фреймворком (scikit-learn, TensorFlow, PyTorch). Логирование происходит в рамках так называемого **Run** — единицы эксперимента, которая содержит все ассоциированные с ним данные.\n",
        "\n",
        "**Weights & Biases (W&B)** — это коммерческая (с бесплатным тарифом) облачная платформа, предлагающая более комплексный и визуально насыщенный опыт. W&B автоматически логирует версии кода из Git, системные метрики (использование CPU/GPU), и предоставляет мощный веб-интерфейс для сравнения экспериментов, визуализации метрик в реальном времени и совместной работы. Для команд, работающих над сложными проектами, W&B часто становится центральным хабом для всего ML-процесса.\n",
        "\n",
        "**Управление конфигурацией с Hydra**.\n",
        "Чтобы сделать эксперименты по-настоящему воспроизводимыми и гибкими, необходимо отделить логику кода от его конфигурации. Библиотека **Hydra** от Facebook (Meta) решает эту задачу элегантно. Она позволяет определять иерархические конфигурации в YAML-файлах и динамически переопределять любые параметры прямо из командной строки без изменения основного кода.\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Представим, что у нас есть основной скрипт `train.py`, который обучает модель. Без Hydra, чтобы запустить эксперимент с другим значением гиперпараметра `C`, нам пришлось бы либо править код, либо передавать аргументы через `argparse`, что быстро становится неуправляемым при большом числе параметров. Hydra позволяет создать файл конфигурации, где все параметры определены, и затем запускать целые серии экспериментов, комбинируя параметры на лету.\n",
        "\n",
        "```python\n",
        "# Файл: conf/config.yaml\n",
        "db:\n",
        "  driver: mysql\n",
        "  user: my_user\n",
        "model:\n",
        "  type: logistic_regression\n",
        "  C: 1.0\n",
        "  max_iter: 1000\n",
        "```\n",
        "\n",
        "```python\n",
        "# Файл: train.py\n",
        "import hydra\n",
        "from omegaconf import DictConfig\n",
        "\n",
        "# Декоратор @hydra.main связывает функцию с конфигурацией\n",
        "@hydra.main(version_base=None, config_path=\"conf\", config_name=\"config\")\n",
        "def my_app(cfg: DictConfig) -> None:\n",
        "    print(f\"Training model with C={cfg.model.C} and max_iter={cfg.model.max_iter}\")\n",
        "    # Логика обучения модели\n",
        "    # ...\n",
        "    # Отправка конфигурации в MLflow/W&B для полной воспроизводимости\n",
        "    # mlflow.log_params(cfg)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    my_app()\n",
        "```\n",
        "\n",
        "Запуск из командной строки:\n",
        "```bash\n",
        "# Запуск с конфигурацией по умолчанию\n",
        "python train.py\n",
        "\n",
        "# Переопределение одного параметра\n",
        "python train.py model.C=10.0\n",
        "\n",
        "# Переопределение нескольких параметров\n",
        "python train.py model.C=0.1 model.max_iter=2000\n",
        "\n",
        "# Запуск серии экспериментов (sweep) с разными значениями C\n",
        "python train.py --multirun model.C=0.1,1.0,10.0\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения*:  \n",
        "Hydra автоматически создаёт для каждого запуска отдельную рабочую директорию, в которую сохраняет копию используемой конфигурации. Это означает, что даже спустя месяцы можно точно узнать, с какими параметрами был запущен каждый эксперимент. В сочетании с системами трекинга (MLflow или W&B), которые логируют эту конфигурацию, достигается полная и безупречная воспроизводимость.\n",
        "\n",
        "#### 14.3.2. Data Version Control (DVC)\n",
        "\n",
        "Традиционные системы контроля версий, такие как Git, прекрасно справляются с текстовыми файлами кода, но совершенно не предназначены для хранения крупномасштабных бинарных артефактов, таких как наборы данных (часто в формате CSV, Parquet) и обученные модели (в формате pickle, joblib, ONNX). Прямое добавление таких файлов в репозиторий делает его громоздким, медленным и неуправляемым.\n",
        "\n",
        "**Data Version Control (DVC)** решает эту проблему, действуя как расширение для Git. DVC не хранит сами данные в репозитории. Вместо этого он сохраняет их в удаленном хранилище (таком как AWS S3, Google Cloud Storage или даже обычный SSH-сервер), а в Git-репозитории сохраняет лишь небольшие текстовые файлы-метаданные (с расширением `.dvc`), которые содержат хеши и ссылки на реальные данные.\n",
        "\n",
        "**Принцип работы DVC**:  \n",
        "DVC использует ту же семантику, что и Git (`dvc add`, `dvc commit`, `dvc push`, `dvc pull`). Когда вы выполняете `dvc add data/train.csv`, DVC вычисляет хеш файла, сохраняет сам файл в кеш (`./.dvc/cache`) и создаёт файл `train.csv.dvc`. Этот `.dvc` файл вы добавляете в Git. Когда другой разработчик делает `git pull`, он получает только метаданные. Затем он запускает `dvc pull`, и DVC автоматически скачивает реальные данные из удаленного хранилища, восстанавливая полную среду для воспроизведения эксперимента.\n",
        "\n",
        "*Пояснение до выполнения*:  \n",
        "Этот подход гарантирует, что каждый коммит в Git однозначно привязан к конкретной версии данных. В контексте MLOps это означает, что оркестратор (например, Airflow или Kubeflow), запуская процесс непрерывного обучения (CT), всегда будет использовать именно ту версию обучающих данных, которая была актуальна на момент коммита кода пайплайна. Это устраняет одну из главных причин невоспроизводимости: расхождение между данными, на которых была разработана модель, и данными, на которых она была обучена в продакшене.\n",
        "\n",
        "### 14.4. Распределенные Вычисления в Python: Ray и Dask\n",
        "\n",
        "Для масштабирования процессов обучения, обработки больших данных и поиска гиперпараметров (HPO) на кластерах необходимы нативные Python-фреймворки для распределённых вычислений. Два лидера в этой области — **Ray** и **Dask**.\n",
        "\n",
        "#### 14.4.1. Сравнительный Анализ Архитектур (Ray vs. Dask)\n",
        "\n",
        "**Dask** изначально создавался как естественное расширение экосистемы PyData. Его главная сила — в тесной интеграции с **NumPy** и **Pandas**. Dask предоставляет объекты `Dask Array` и `Dask DataFrame`, которые имеют почти идентичный API со своими однопоточными аналогами, но при этом распределяют данные и вычисления по кластеру. Dask строит **граф задач (Task Graph)**, анализирует зависимости и оптимально распределяет выполнение. Он отлично подходит для **ETL-процессов** и обработки больших структурированных данных, когда рабочая нагрузка может быть выражена в виде последовательности операций над массивами или таблицами.\n",
        "\n",
        "**Ray**, напротив, предлагает более универсальную и гибкую модель. В основе Ray лежат два концепта: **Tasks** — это функции, которые могут быть вызваны удалённо и выполняются асинхронно, и **Actors** — это stateful (сохраняющие состояние) удалённые объекты, которые могут иметь собственные методы и внутренние переменные. Эта модель позволяет легко выразить практически любую распределённую рабочую нагрузку. Ray позиционируется как **\"AI Compute Engine\"**, поскольку предоставляет специализированный стек библиотек поверх своего ядра: **Ray Train** для распределённого обучения, **Ray Tune** для HPO и **Ray Serve** для развёртывания моделей. Для end-to-end MLOps-систем, где требуется решить несколько задач (обучение, HPO, инференс) в единой среде, Ray часто предлагает более сплоченное и производительное решение, особенно для мелкозернистых и динамических задач.\n",
        "\n",
        "#### 14.4.2. Практика Масштабирования Обучения (Ray Train)\n",
        "\n",
        "Ray Train позволяет инженерам легко адаптировать существующие локальные скрипты обучения (на PyTorch, TensorFlow или scikit-learn) для распределённого выполнения без глубокого переписывания логики. Ray Train берёт на себя все сложные задачи: управление ресурсами кластера, шардирование данных между воркерами, синхронизацию градиентов и сохранение контрольных точек.\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Представим, что у вас есть хорошо работающая функция `train_func`, которая загружает данные, создаёт модель PyTorch и обучает её. Чтобы распределить это обучение на 4 GPU, вам не нужно переписывать всю логику под `DistributedDataParallel`. Достаточно обернуть вашу функцию в класс `TorchTrainer` и указать конфигурацию масштабирования.\n",
        "\n",
        "```python\n",
        "from ray.train.torch import TorchTrainer\n",
        "from ray.train import ScalingConfig\n",
        "import ray\n",
        "\n",
        "# Инициализация Ray в локальном режиме для демонстрации.\n",
        "# В продакшене Ray уже запущен как кластер.\n",
        "if ray.is_initialized():\n",
        "    ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "def train_func(config):\n",
        "    \"\"\"\n",
        "    Стандартная функция обучения PyTorch.\n",
        "    Ray Train автоматически распределяет данные и управляет процессом.\n",
        "    Внутри этой функции можно использовать обычный PyTorch код.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "    # 1. Загрузка данных (Ray автоматически шардирует их)\n",
        "    # train_dataset = YourDataset()\n",
        "    # train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # 2. Создание модели\n",
        "    # model = YourModel()\n",
        "    # model = ray.train.torch.prepare_model(model)\n",
        "\n",
        "    # 3. Обучение\n",
        "    # for epoch in range(config[\"num_epochs\"]):\n",
        "    #     for batch in train_loader:\n",
        "    #         ... forward, backward, optimizer.step() ...\n",
        "\n",
        "    # 4. Отчёт о метриках\n",
        "    # ray.train.report({\"loss\": loss.item(), \"accuracy\": acc})\n",
        "    \n",
        "    # Для простоты примера возвращаем фиктивный результат.\n",
        "    ray.train.report({\"accuracy\": 0.95})\n",
        "\n",
        "# Определение конфигурации распределенного запуска.\n",
        "# В этом примере запускается 4 воркера, каждый с доступом к одному GPU.\n",
        "scaling_config = ScalingConfig(num_workers=4, use_gpu=True)\n",
        "\n",
        "# Создание тренера\n",
        "trainer = TorchTrainer(\n",
        "    train_loop_per_worker=train_func, # Ваша функция обучения\n",
        "    scaling_config=scaling_config\n",
        ")\n",
        "\n",
        "# Запуск распределенного обучения. Этот вызов заблокирует выполнение,\n",
        "# пока обучение не завершится на всех воркерах.\n",
        "result = trainer.fit()\n",
        "\n",
        "print(\"Распределенное обучение завершено.\")\n",
        "print(f\"Финальные метрики: {result.metrics}\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения*:  \n",
        "Этот код, запущенный на кластере Ray, автоматически распределит данные, синхронизирует градиенты между 4 GPU и соберёт финальные метрики. Инженеру не нужно думать о низкоуровневых деталях распределённых вычислений. Это позволяет сосредоточиться на логике модели, что значительно ускоряет итерации и упрощает поддержку кода.\n",
        "\n",
        "#### 14.4.3. Автоматизация HPO (Ray Tune и Optuna)\n",
        "\n",
        "Поиск оптимальных гиперпараметров — одна из самых ресурсоёмких задач в ML. Ручной подбор неэффективен, а простой поиск по сетке (`GridSearchCV`) масштабируется экспоненциально. Специализированные библиотеки используют интеллектуальные стратегии для поиска в пространстве гиперпараметров.\n",
        "\n",
        "**Ray Tune** является частью экосистемы Ray и предоставляет мощный API для параллельного запуска экспериментов с разными гиперпараметрами. Он поддерживает современные стратегии поиска, такие как **HyperBand** и **ASHA** (Asynchronous Successive Halving Algorithm), которые позволяют рано останавливать (prune) перспективные эксперименты и перераспределять ресурсы на более многообещающие. Tune позволяет легко определить пространство поиска и функцию цели, а затем автоматически управлять сотнями экспериментов в кластере.\n",
        "\n",
        "**Optuna** — это другая популярная библиотека для HPO, которая фокусируется на гибкости и богатстве алгоритмов оптимизации (в основном на байесовской оптимизации). Optuna менее тесно интегрирована с фреймворками для распределённых вычислений, но её можно успешно использовать в связке с **Dask**. В этом сценарии Dask создаёт пул задач (Futures), каждая из которых запускает отдельный trial Optuna, а общее состояние оптимизации синхронизируется через `DaskStorage`. Это позволяет эффективно использовать вычислительные ресурсы кластера для выполнения сложных, адаптивных стратегий поиска.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "EH70QPajuQ0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Часть III: Архитектурные Паттерны и Пайплайны (CT/CI)\n",
        "\n",
        "Переход от локальной разработки и эксперимента к промышленному развертыванию включает в себя не просто написание большего количества кода, а внедрение фундаментальных **архитектурных компонентов**. Эти компоненты формируют ту инженерную основу, на которой строится надёжность, консистентность, аудируемость и автоматизация в системах машинного обучения. Без них любая даже самая точная модель обречена на быстрое устаревание и накопление технического долга.\n",
        "\n",
        "### 14.5. Инфраструктурные Основы: Feature Stores и Model Registry\n",
        "\n",
        "Две ключевые архитектурные абстракции, которые лежат в сердце зрелой ML-инфраструктуры, — это **Feature Store** и **Model Registry**. Они решают фундаментальные проблемы: разрыв во времени и логике между обучением и инференсом (Feature Store) и отсутствие контроля версий и аудита (Model Registry).\n",
        "\n",
        "#### 14.5.1. Feature Store: Паттерн Online/Offline Консистентности\n",
        "\n",
        "**Проблема**: В реальных системах данные для обучения (offline) и данные для предсказания (online) поступают из разных источников и с разной задержкой. Признаки, которые для обучения агрегируются за день, должны быть рассчитаны за секунды на одном событии в продакшене. Если логика вычисления этих признаков дублируется в двух местах (в пайплайне обучения и в сервисе инференса), почти неизбежно возникает **Training-Serving Skew** — ситуация, при которой модель обучается на одних данных, а в продакшене видит другие. Это главная причина непредсказуемого падения производительности модели сразу после развертывания.\n",
        "\n",
        "**Решение**: **Feature Store** — это централизованное хранилище и сервис для управления жизненным циклом признаков. Его главная цель — обеспечить **единый, канонический источник вычисления признаков**, который используется как в пайплайне обучения, так и в сервисе инференса.\n",
        "\n",
        "**Ключевые функции и архитектура**:\n",
        "\n",
        "1.  **Консистентность (Consistency)**: Feature Store предоставляет единый API или библиотеку, содержащую логику расчета признака. При обучении система запрашивает исторические значения признаков для заданного окна времени (offline serving — высокая пропускная способность, латентность не критична). При инференсе сервис запрашивает самые свежие значения для одного или нескольких ключей (online serving — крайне низкая латентность, часто < 10 мс). Логика остается одной и той же.\n",
        "2.  **Масштабируемость и Latency**: Чтобы удовлетворить требованиям online serving, Feature Store использует два типа хранилищ:\n",
        "    *   **Offline Store**: Масштабируемая аналитическая база данных (например, на основе Apache Parquet в хранилище типа S3/GCS, или Data Warehouse вроде BigQuery/Snowflake). Используется для обучения.\n",
        "    *   **Online Store**: Высокопроизводительная in-memory база данных (например, Redis, DynamoDB). В неё периодически загружаются (materialize) самые свежие значения признаков из offline store. Именно из неё идут запросы в продакшене.\n",
        "3.  **Discovery и Обслуживание**: Feature Store выступает каталогом доступных признаков. Команды могут просматривать, регистрировать, документировать и мониторить свои признаки (например, отслеживать дрейф распределения признака во времени).\n",
        "\n",
        "**Инструменты**: **Feast** является ведущим open-source решением, которое предоставляет архитектуру из offline online stores и унифицированный Python API. **Hopsworks** предлагает более интегрированную коммерческую платформу, включающую в себя Feature Store, Model Registry и инструменты для мониторинга и управления в едином интерфейсе, что особенно ценно в регулируемых отраслях с высокими требованиями к аудиту и управлению данными (governance).\n",
        "\n",
        "#### 14.5.2. Model Registry (MLflow): Управление Жизненным Циклом Модели\n",
        "\n",
        "**Проблема**: В среде с постоянными итерациями над моделью легко потерять контроль. Какая версия модели сейчас в продакшене? На каких данных и с какими гиперпараметрами она была обучена? Можно ли откатиться к предыдущей версии, если новая оказалась неудачной? Ответы на эти вопросы жизненно важны для отладки, аудита и соблюдения нормативных требований.\n",
        "\n",
        "**Решение**: **MLflow Model Registry** — это централизованный, версионированный и аудируемый каталог моделей. Он превращает модель из анонимного артефакта в управляемый объект с полной историей.\n",
        "\n",
        "**Ключевые концепции и паттерны**:\n",
        "\n",
        "1.  **Происхождение (Lineage)**: Каждая зарегистрированная модель (а точнее, каждая её версия) в Registry является не просто файлом, а ссылкой на конкретный **MLflow Run**. Этот Run содержит полную историю эксперимента: код (если был залогирован), все параметры, метрики, артефакты и даже версию окружения. Это обеспечивает полную трассируемость и возможность воспроизведения любого результата в будущем.\n",
        "2.  **Версионирование (Model Versioning)**: Когда вы регистрируете модель под уже существующим именем (например, `fraud_detection_model`), Registry автоматически присваивает ей новую монотонно возрастающую версию (V1, V2, V3...). Это позволяет сравнивать производительность версий и откатываться при необходимости.\n",
        "3.  **Алиасы (Model Aliases)**: Это наиболее важный паттерн для реализации **Continuous Delivery (CD)**. Алиас (например, `@staging`, `@champion`, `@production`) — это **мутабельная ссылка** на конкретную версию модели. В продакшен-коде или в конфигурации сервиса инференса вы никогда не ссылаетесь на конкретную версию, а всегда на алиас: `models:/fraud_detection_model@champion`.\n",
        "    *   **Как это работает на практике**: Инженер обучает новую модель V2 и регистрирует её. Затем, после прохождения всех тестов (A/B-теста, канареечного развертывания), он выполняет одну атомарную операцию через UI или API Registry: `transition_model_version_stage(\"fraud_detection_model\", version=2, stage=\"Production\")`. Эта операция просто меняет алиас `@champion` с V1 на V2. Сервис инференса, который на следующем запросе загрузит модель по алиасу, автоматически начнёт использовать новую версию. Это происходит без перезапуска сервиса и без изменения кода, что обеспечивает бесшовное и безопасное обновление.\n",
        "\n",
        "> **Таблица 2: Ключевые Архитектурные Компоненты MLOps**\n",
        "\n",
        "| Компонент | Назначение | Ключевые инструменты |\n",
        "| :--- | :--- | :--- |\n",
        "| **Feature Store** | Устранение Training-Serving Skew, единая логика признаков | Feast, Hopsworks, Tecton |\n",
        "| **Model Registry** | Версионирование, аудит, управление жизненным циклом модели | MLflow Model Registry, Azure ML Model Registry |\n",
        "| **Data Versioning** | Воспроизводимость данных, связь кода и данных | DVC, LakeFS |\n",
        "| **Configuration Mgmt** | Гибкость экспериментов, воспроизводимость конфигурации | Hydra, OmegaConf |\n",
        "\n",
        "### 14.6. Оркестрация CI/CT Пайплайнов\n",
        "\n",
        "Управление сложными, многоэтапными сквозными (end-to-end) ML-пайплайнами вручную невозможно. Для автоматизации этого процесса требуются мощные **оркестраторы** — системы, которые могут планировать, запускать, мониторить и управлять зависимостями между задачами.\n",
        "\n",
        "#### 14.6.1. Оркестраторы (Flyte, Prefect): Код-первый Подход\n",
        "\n",
        "Современные MLOps-оркестраторы, такие как **Flyte** и **Prefect**, отошли от декларативного подхода на основе XML/YAML (как в Apache Airflow) в пользу **код-первого (code-first) подхода**. Это означает, что весь рабочий процесс (DAG — Directed Acyclic Graph) определяется на чистом Python с использованием декораторов и типов, что делает его читаемым, тестируемым и интегрируемым с остальной кодовой базой.\n",
        "\n",
        "**Flyte**, разработанный компанией Lyft специально для ML- и Data Science-рабочих процессов, особенно выделяется своей встроенной поддержкой **`map_task`**. Этот примитив позволяет легко и эффективно распараллелить задачи, что идеально подходит для сценариев, когда нужно выполнить одну и ту же операцию (например, предобработку) над множеством файлов или сегментов данных. Это значительно упрощает миграцию и масштабирование ML-задач по сравнению с традиционными оркестраторами.\n",
        "\n",
        "**Интеграция DVC и Model Registry в Continuous Training (CT)**:\n",
        "Пайплайн Continuous Training, полностью управляемый оркестратором, является сердцем саморегулирующейся ML-системы. Он должен быть полностью автоматизирован и включать следующие строго последовательные этапы:\n",
        "\n",
        "1.  **Проверка Качества Данных (Quality Gate)**: Первый шаг — валидация поступивших новых данных. Если данные испорчены, дальнейшее обучение бессмысленно.\n",
        "2.  **DVC Checkout**: Оркестратор запускает команды `dvc pull` и `dvc checkout`, чтобы получить из удаленного хранилища (S3/GCS) **конкретную, версионированную версию** обучающего набора данных, соответствующую текущему коммиту кода пайплайна. Это гарантирует привязку кода к данным.\n",
        "3.  **Обучение**: Запуск процесса обучения. Это может быть как локальный скрипт, так и распределенное обучение с использованием Ray Train. Все метрики и параметры логируются в MLflow как новый Run.\n",
        "4.  **Регистрация**: Если обученная модель проходит все пост-валидационные тесты (например, её метрики не хуже, чем у текущей продакшен-модели), оркестратор автоматически регистрирует её в MLflow Model Registry. Модель получает новый номер версии и изначально попадает в стадию `Staging`.\n",
        "\n",
        "#### 14.6.2. Контроль Качества Данных (CI) с Great Expectations\n",
        "\n",
        "Функция **Continuous Integration (CI)** для ML-систем выходит далеко за рамки тестирования кода. Она включает в себя **обязательный контроль качества данных** на входе пайплайна. Обучение на испорченных, битых или аномальных данных — один из самых распространенных и трудноотлавливаемых способов введения ошибок в продакшен.\n",
        "\n",
        "**Great Expectations (GE)** — это инструмент с открытым исходным кодом, который позволяет инженерам и аналитикам определить набор **ожиданий (Expectation Suites)** о структуре, семантике и качестве данных. Ожидания — это утверждения вроде «столбец `user_id` не должен содержать пропущенных значений», «значения в столбце `price` должны быть положительными», или «распределение признака `country` не должно отличаться от эталонного более чем на 10% по метрике KL-дивергенции».\n",
        "\n",
        "**Интеграция как Quality Gate**:\n",
        "Ключевая инженерная практика — встраивание GE в самое **начало CT-пайплайна** как обязательного, невосполнимого шага. Это реализуется по паттерну **Fail Fast**: если данные не соответствуют заданным ожиданиям, процесс валидации завершается неудачей, и пайплайн немедленно останавливается. Это предотвращает трату дорогостоящих вычислительных ресурсов на обучение на плохих данных и гарантирует, что в продакшен попадает только модель, обученная на валидных данных.\n",
        "\n",
        "**Data Docs**: GE автоматически генерирует красивую, интерактивную HTML-документацию на основе определенных ожиданий и результатов их валидации. **Data Docs** становится «единым источником истины» для всей команды — разработчиков, аналитиков и специалистов по данным — о том, как должны выглядеть данные и как они выглядят на самом деле. Это способствует выравниванию стандартов качества и упрощает коммуникацию.\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В приведенном ниже примере представлена функция, которая могла бы быть первым шагом в вашем Flyte/Prefect пайплайне. Она получает на вход путь к файлу данных, загружает их (в данном примере в виде Pandas DataFrame), применяет набор предопределенных ожиданий и в случае неудачи генерирует исключение, которое остановит весь последующий пайплайн.\n",
        "\n",
        "```python\n",
        "# Пример Кода: GE как Quality Gate в CT-пайплайне\n",
        "import pandas as pd\n",
        "from great_expectations.dataset import PandasDataset\n",
        "from great_expectations import DataContext\n",
        "\n",
        "def validate_data_quality(data_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Функция-валидатор, интегрированная как Quality Gate в начале пайплайна.\n",
        "    Если валидация не проходит, функция вызывает исключение, останавливая пайплайн.\n",
        "    \"\"\"\n",
        "    # 1. Загрузка данных\n",
        "    df = pd.read_parquet(data_path)\n",
        "    # 2. Преобразование в формат GE\n",
        "    ge_df = PandasDataset(df)\n",
        "    \n",
        "    # 3. Определение ожиданий (в реальности они часто хранятся в отдельном YAML-файле)\n",
        "    ge_df.expect_column_values_to_not_be_null(\"transaction_id\")\n",
        "    ge_df.expect_column_values_to_be_of_type(\"amount\", \"float64\")\n",
        "    ge_df.expect_column_values_to_be_between(\"amount\", min_value=0.01, max_value=10000.0)\n",
        "    ge_df.expect_column_values_to_be_in_set(\"payment_type\", [\"credit\", \"debit\", \"cash\"])\n",
        "    \n",
        "    # 4. Выполнение валидации\n",
        "    validation_results = ge_df.validate()\n",
        "    \n",
        "    # 5. Генерация Data Docs для отладки\n",
        "    context = DataContext()\n",
        "    context.build_data_docs()\n",
        "    \n",
        "    # 6. Проверка результата и остановка пайплайна в случае ошибки\n",
        "    if not validation_results[\"success\"]:\n",
        "        # Критическая ошибка: остановить пайплайн\n",
        "        failed_expectations = [\n",
        "            exp[\"expectation_config\"][\"expectation_type\"]\n",
        "            for exp in validation_results[\"results\"] if not exp[\"success\"]\n",
        "        ]\n",
        "        raise ValueError(\n",
        "            f\"Data validation failed! Failed expectations: {failed_expectations}. \"\n",
        "            f\"Check the generated Data Docs for details.\"\n",
        "        )\n",
        "    \n",
        "    print(\"Data validation passed! Continuing to training stage.\")\n",
        "\n",
        "# Пример вызова внутри оркестратора (например, в задаче Flyte)\n",
        "# validate_data_quality(\"/path/to/new_data.parquet\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот простой, но мощный паттерн обеспечивает, что любой сбой в качестве данных будет обнаружен на самом раннем этапе, до того, как ресурсы будут потрачены на обучение. Сгенерированные **Data Docs** позволяют инженеру быстро понять, какие именно ожидания не были выполнены, и приступить к отладке.\n",
        "\n",
        "---\n",
        "\n",
        "## Часть IV: Промышленный Деплоймент и Непрерывный Мониторинг (CD/CM)\n",
        "\n",
        "Финальные этапы жизненного цикла ML-системы — это её **надёжное развертывание в продакшене (Continuous Delivery, CD)** и замыкание всей петли MLOps с помощью **непрерывного мониторинга (Continuous Monitoring, CM)**, который способен автоматически инициировать процесс переобучения (Continuous Training, CT).\n",
        "\n",
        "### 14.8. Паттерны Модели Обслуживания (Model Serving)\n",
        "\n",
        "Промышленный сервис предсказаний — это не просто скрипт с `model.predict()`. Это высоконагруженный, отказоустойчивый, масштабируемый и безопасный микросервис, который должен поддерживать сложные стратегии развертывания и интеграции.\n",
        "\n",
        "#### 14.8.1. Инструменты (KServe, Seldon Core, BentoML)\n",
        "\n",
        "Современные инструменты для обслуживания моделей (Model Serving) почти универсально используют **Kubernetes** в качестве базовой платформы для управления жизненным циклом контейнеров, балансировки нагрузки и горизонтального масштабирования.\n",
        "\n",
        "*   **KServe** (ранее KFServing) — это проект от Kubeflow, который предоставляет стандартный API для развертывания моделей из различных фреймворков (TensorFlow, PyTorch, Scikit-learn, ONNX и др.).\n",
        "*   **BentoML** — это фреймворк, ориентированный на упрощение упаковки и развертывания моделей в виде готовых к продакшену микросервисов.\n",
        "*   **Seldon Core** — это мощное open-source решение, которое выделяется своей гибкостью и поддержкой **Custom Resource Definitions (CRD)** для Kubernetes. Это позволяет описывать всю логику развертывания модели в виде декларативных YAML-манифестов, что прекрасно интегрируется в GitOps-подходы.\n",
        "\n",
        "#### 14.8.2. Продвинутые Стратегии Развертывания (CD)\n",
        "\n",
        "Простое замещение старой модели новой в продакшене — это рискованный шаг. Современные MLOps-практики предписывают использовать стратегии постепенного и контролируемого развёртывания.\n",
        "\n",
        "1.  **Canary Deployment**: Новая версия модели (V2), зарегистрированная в Model Registry, развертывается параллельно с текущей рабочей моделью (V1, помеченной алиасом `@champion`). С помощью инструментов вроде Seldon Core весь производственный трафик может быть разбит: например, 95% на V1 и 5% на V2. В течение определённого времени команды отслеживают производительность V2: её латентность, стабильность и, самое главное, её ключевые метрики качества (например, точность на ground truth, который поступает с задержкой). Если V2 проходит все проверки, оператор CD выполняет атомарную операцию — **смену алиаса `@champion` в Model Registry с V1 на V2**. После этого 100% трафика идут на новую модель. Если же проблемы обнаруживаются, трафик просто переключается обратно — это происходит мгновенно и без перезапуска сервиса.\n",
        "2.  **A/B Testing**: Это более длительная стратегия, направленная не на проверку технической стабильности, а на оценку **бизнес-воздействия**. Две модели (или даже более) могут работать параллельно в течение недель, а их влияние на ключевые бизнес-метрики (конверсия, выручка, удержание) сравнивается статистически. Победитель выбирается на основе реального вклада в бизнес.\n",
        "\n",
        "Эти стратегии, поддерживаемые инструментами вроде Seldon Core, позволяют **минимизировать риск**, связанный с выпуском новой модели, обеспечивая плавный, обратимый и измеримый процесс перехода.\n",
        "\n",
        "### 14.9. Интеграция Объяснимости в Production (Explainability)\n",
        "\n",
        "Во многих отраслях (финансы, медицина, страхование) использование «чёрных ящиков» регулируется законом. Даже в отсутствие регуляторных требований, **объяснимость (eXplainable AI, XAI)** критически важна для повышения доверия пользователей, внутренней отладки и понимания причин сбоев модели.\n",
        "\n",
        "**SHAP** (SHapley Additive exPlanations) и **LIME** (Local Interpretable Model-agnostic Explanations) являются двумя наиболее популярными и надёжными фреймворками для локальной объяснимости.\n",
        "\n",
        "*   **SHAP** основан на теории кооперативных игр и предоставляет **количественную, теоретически обоснованную** меру вклада каждого признака в итоговое предсказание для конкретного экземпляра данных.\n",
        "*   **LIME** работает по другому принципу: для данного экземпляра он генерирует множество «возмущённых» точек, получает для них предсказания от «чёрного ящика», а затем обучает простую и интерпретируемую модель (например, линейную регрессию) на этих точках, взвешивая их по близости к исходному экземпляру. Эта простая модель и служит объяснением.\n",
        "\n",
        "**Архитектурная Реализация**:\n",
        "Интеграция XAI должна происходить **в контуре самого сервиса предсказаний**, чтобы объяснения генерировались в **реальном времени** вместе с предсказанием.\n",
        "\n",
        "1.  API-сервис принимает входные данные от клиента.\n",
        "2.  Модель генерирует предсказание (например, вероятность дефолта).\n",
        "3.  Непосредственно после этого вызывается библиотека SHAP или LIME, которая использует ту же модель и те же входные данные для генерации объяснения.\n",
        "4.  API возвращает клиенту **единый, структурированный ответ (payload)**, содержащий как само предсказание, так и массив данных с вкладами каждого признака.\n",
        "\n",
        "Этот паттерн, известный как **Real-Time Insights**, позволяет не только предоставлять объяснения конечным пользователям, но и собирать их в логи для последующего анализа (например, для обнаружения смещений или аномалий в поведении модели).\n",
        "\n",
        "### 14.10. Архитектура Непрерывного Мониторинга (CM): Замыкание Петли\n",
        "\n",
        "**Continuous Monitoring (CM)** — это не просто сбор метрик, а **последний и самый важный этап**, который замыкает всю петлю MLOps и превращает статическую систему в **динамическую, саморегулирующуюся**.\n",
        "\n",
        "#### 14.10.1. Обнаружение Дрейфа (Drift) с Evidently AI\n",
        "\n",
        "Главная угроза для любой ML-модели в продакшене — это **деградация** из-за изменений в окружающей среде. Это может проявляться в двух формах:\n",
        "\n",
        "*   **Data Drift**: Изменение статистического распределения входных признаков (например, из-за изменения поведения пользователей или сбоя в источнике данных).\n",
        "*   **Model (Concept) Drift**: Изменение фундаментальной связи между признаками и целевой переменной (например, экономический кризис меняет корреляции между признаками и вероятностью дефолта).\n",
        "\n",
        "**Evidently AI** — это инструмент с открытым исходным кодом, специально разработанный для мониторинга ML-систем. Он позволяет инженерам легко создавать отчёты и настраивать тесты для обнаружения дрейфа. Evidently сравнивает «текущее» окно производственных данных с «базовым» (обычно — обучающим) набором, используя статистические тесты (например, Kolmogorov-Smirnov) и визуализации. Он поддерживает работу не только со структурированными данными, но и с текстом и эмбеддингами.\n",
        "\n",
        "#### 14.10.2. Система Оповещений (Prometheus и Grafana)\n",
        "\n",
        "Обнаружение дрейфа само по себе бесполезно, если на него нет реакции. Для этого необходима интеграция с промышленной архитектурой мониторинга, основанной на тандеме **Prometheus** и **Grafana**.\n",
        "\n",
        "1.  **Экспорт Метрик**: Специализированный мониторинговый сервис (запущенный как отдельный job в оркестраторе или как демон в Kubernetes) периодически (например, раз в час) запускает скрипт на основе Evidently AI. Этот скрипт анализирует последние N запросов к модели и рассчитывает метрики, например, `evidently_data_drift_share` (доля дрейфующих признаков) или `evidently_model_quality_precision`.\n",
        "2.  **Prometheus**: Эти метрики экспортируются в формате, понятном Prometheus (обычно через HTTP-эндпоинт `/metrics`). Prometheus, работающий как база данных временных рядов, регулярно опрашивает этот эндпоинт и сохраняет значения метрик.\n",
        "3.  **Grafana**: Grafana подключается к Prometheus как к источнику данных. Инженеры создают в Grafana **живые дашборды**, которые визуализируют динамику всех ключевых метрик. Более того, в Grafana настраиваются **правила оповещений (Alerts Rules)**: если метрика `evidently_data_drift_share` превышает порог в 30% в течение 2 последовательных интервалов, Grafana генерирует алерт.\n",
        "\n",
        "#### 14.10.3. Автоматизация Реакции (Trigger CT)\n",
        "\n",
        "Подлинное замыкание петли MLOps достигается, когда система мониторинга **автоматически инициирует исправление**. Это достигается через интеграцию между системами:\n",
        "\n",
        "Когда **Grafana** обнаруживает срабатывание правила оповещения, она может отправить HTTP-запрос (webhook) в **оркестратор (Flyte или Prefect)**. Этот запрос содержит информацию о том, что произошел дрейф, и служит **триггером** для запуска пайплайна **Continuous Training (CT)**.\n",
        "\n",
        "Пайплайн CT, как описано ранее, выполняет следующие шаги:\n",
        "1.  Извлекает **новые, свежие, версионированные данные** (с помощью `DVC checkout`).\n",
        "2.  Переобучает модель на этих данных.\n",
        "3.  Регистрирует новую версию в **MLflow Model Registry**.\n",
        "4.  Если тесты пройдены, новая модель автоматически или вручную переводится в статус `@staging` и готовится к **канареечному развертыванию**.\n",
        "\n",
        "Таким образом, архитектура MLOps становится **саморегулирующейся**: она не только обнаруживает проблемы, но и автоматически запускает процесс их решения, обеспечивая, что система всегда адаптируется к меняющейся реальности данных.\n",
        "\n",
        "> **Таблица 3: Инструментарий для Непрерывных Практик (CI/CD/CT/CM)**\n",
        "\n",
        "| Практика | Назначение | Ключевые инструменты |\n",
        "| :--- | :--- | :--- |\n",
        "| **CI** (Code & Data Validation) | Валидация кода и данных перед обучением | Great Expectations, pytest, mypy |\n",
        "| **CT** (Continuous Training) | Автоматическое переобучение модели | Flyte, Prefect, Ray Train, DVC |\n",
        "| **CD** (Model Deployment) | Безопасное развертывание модели | Seldon Core, MLflow Model Registry, Canary/A-B |\n",
        "| **CM** (Continuous Monitoring) | Мониторинг и автоматическая реакция на дрейф | Evidently AI, Prometheus, Grafana |\n",
        "\n",
        "### 14.11. Сквозной Кейс-Стади Промышленного ML-Решения\n",
        "\n",
        "Построение промышленной ML-системы на Python — это синтез множества специализированных инструментов и паттернов в единую, цельную архитектуру. Рассмотрим, как все описанные компоненты работают вместе в реальном сценарии.\n",
        "\n",
        "**Синтез Архитектуры**:\n",
        "\n",
        "*   **Разработка и Оптимизация**: Инженер начинает с локальной разработки и оптимизации критических числовых ядер с помощью **Numba** (`@njit`) или **Cython**, добиваясь C-подобной производительности. Он использует профилирование, чтобы исключить неявный откат в Object Mode и гарантировать максимальную скорость.\n",
        "*   **Экспериментирование**: Фаза активных экспериментов ведётся с использованием **MLflow** или **Weights & Biases** для полного трекинга всех параметров и метрик. **Hydra** позволяет легко управлять конфигурацией и запускать серии экспериментов без изменения кода. Воспроизводимость каждого эксперимента гарантируется **DVC**, который версионирует обучающие данные и связывает их с конкретным коммитом кода.\n",
        "*   **Масштабирование**: Для обучения на больших данных и выполнения масштабируемого поиска гиперпараметров (HPO) используется унифицированная платформа **Ray** с её компонентами **Ray Train** и **Ray Tune**.\n",
        "*   **Архитектурные Компоненты**: Требования промышленной надёжности удовлетворяются с помощью:\n",
        "    *   **Feature Store (Feast)**: Обеспечивает Online/Offline консистентность признаков, устраняя главную причину Training-Serving Skew.\n",
        "    *   **MLflow Model Registry**: Обеспечивает контроль версий, аудит и безопасное продвижение моделей через мутабельные алиасы (`@champion`).\n",
        "*   **Оркестрация**: **CT-пайплайн** оркестрируется с помощью **Flyte** или **Prefect**. Он начинается с **Quality Gate** на основе **Great Expectations**, который останавливает процесс, если данные не соответствуют ожиданиям.\n",
        "*   **Деплоймент**: **CD** выполняется через **Seldon Core**, который поддерживает сложные стратегии развертывания (Canary) и может включать в себя модуль **XAI (SHAP/LIME)** для предоставления объяснений в реальном времени, что критично для доверия и соблюдения нормативных требований.\n",
        "*   **Мониторинг и Замыкание Петли**: **Непрерывный Мониторинг (CM)** с помощью **Evidently AI** постоянно обнаруживает **Data/Model Drift**. Рассчитанные метрики экспортируются в **Prometheus** и визуализируются в **Grafana**. Обнаружение дрейфа автоматически **триггерит CT-пайплайн**, замыкая полный цикл MLOps и обеспечивая, что система является **саморегулирующейся** и способной к автономной адаптации.\n",
        "\n",
        "**Индустриальный Контекст**: Индустриальные лидеры, такие как **Netflix**, **Twitter** и **Airbnb**, используют аналогичные архитектурные паттерны. Они строят end-to-end пайплайны, используя специализированные инструменты для управления моделями (Seldon Core, MLflow) и распределенных систем обработки данных (Apache Spark, Kafka) для обеспечения масштабируемости и надежности их ML-инфраструктуры.\n",
        "\n",
        "В заключение, **MLOps — это не просто набор инструментов, а методический, инженерный подход**. Он переводит машинное обучение из области исследований и экспериментов в область создания надежных, масштабируемых, аудируемых и, что самое главное, **саморегулирующихся промышленных систем**, где автоматическое реагирование на дрейф данных является не опцией, а обязательным требованием к архитектуре."
      ],
      "metadata": {
        "id": "zSvRYb1nxtFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 15: TensorFlow — Глубокое обучение и нейронные сети промышленного уровня\n",
        "\n",
        "## Введение\n",
        "\n",
        "**TensorFlow** представляет собой полнофункциональную, модульную и масштабируемую платформу с открытым исходным кодом для машинного и глубокого обучения, первоначально разработанную исследовательской группой Google Brain. За время своей эволюции TensorFlow прошёл путь от системы, основанной на **статических вычислительных графах** (TensorFlow 1.x), к современной архитектуре, сочетающей **энергичное выполнение** (eager execution) для интерактивной разработки и **автоматическую компиляцию графов** через декоратор `@tf.function` для достижения промышленной производительности. Центральным элементом современного стека TensorFlow является унифицированный **Keras API**, который стал официальным высокоуровневым интерфейсом для построения, обучения и развёртывания нейронных сетей. Данная интеграция позволила примирить научную гибкость с инженерной строгостью, сделав разработку моделей глубокого обучения одновременно интуитивно понятной и промышленно надёжной.\n",
        "\n",
        "Ключевые архитектурные преимущества TensorFlow, определяющие его доминирование в промышленных MLOps-системах, включают:\n",
        "\n",
        "1.  **Кроссплатформенная производительность**: Единый код может быть оптимизирован и выполнен на широком спектре аппаратных платформ — от центральных процессоров (CPU) и графических ускорителей (GPU) до специализированных тензорных процессоров (TPU), разработанных Google для максимальной эффективности вычислений с тензорами.\n",
        "2.  **Единая модель разработки**: Возможность писать и отлаживать код в энергичном режиме, а затем одним декоратором (`@tf.function`) преобразовывать его в оптимизированный граф для продакшена, устраняя разрыв между исследованием и развёртыванием.\n",
        "3.  **Интегрированная экосистема**: TensorFlow не является просто библиотекой, а представляет собой целостную экосистему инструментов (`TensorFlow Serving`, `TensorFlow Lite`, `TensorFlow.js`, `TensorBoard`), охватывающую полный жизненный цикл ML-модели — от проектирования и тренировки до мониторинга и развёртывания на серверах, мобильных устройствах и в веб-браузерах.\n",
        "\n",
        "Этот модуль предоставляет методологически строгий и глубокий обзор архитектуры TensorFlow, необходимый для создания не просто работающих, а **надёжных, производительных и поддерживаемых систем глубокого обучения в промышленных условиях**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Основы TensorFlow: тензоры и операции\n",
        "\n",
        "### Теория\n",
        "\n",
        "Фундаментальной абстракцией в TensorFlow является **тензор** — многомерный массив числовых данных, который обобщает понятия скаляра (0-D), вектора (1-D) и матрицы (2-D) на произвольное число измерений (*n*-D). Все данные и параметры модели в TensorFlow представлены в виде тензоров, что обеспечивает единообразие и эффективность вычислений.\n",
        "\n",
        "Центральной концепцией архитектуры TensorFlow является **вычислительный граф** (*computation graph*) — ориентированный ациклический граф (DAG), в котором узлы представляют операции (операторы), а направленные рёбра — потоки данных в виде тензоров. В TensorFlow 1.x графы создавались в явном виде и выполнялись в отдельной сессии, что затрудняло отладку.\n",
        "\n",
        "Современный TensorFlow использует гибридный подход:\n",
        "*   **Энергичное выполнение (Eager Execution)** является режимом по умолчанию. В этом режиме операции выполняются немедленно, как в обычном Python-коде, что обеспечивает **интерактивность и простоту отладки**. Каждая операция возвращает конкретный тензор, а не символическую ссылку на узел графа.\n",
        "*   **Графовое выполнение** достигается через декоратор `@tf.function`. Этот декоратор автоматически анализирует Python-функцию и конструирует из неё оптимизированный вычислительный граф, который затем выполняется эффективно, как в TensorFlow 1.x. Этот механизм позволяет легко перейти от прототипа к производительной реализации без изменения основной логики.\n",
        "\n",
        "Другой краеугольный камень глубокого обучения — **автоматическое дифференцирование** (*automatic differentiation*). TensorFlow предоставляет мощный контекстный менеджер `tf.GradientTape`, который записывает все операции с изменяемыми тензорами (`tf.Variable`) в течение своего контекста. По завершении записи можно запросить у ленты градиенты любой скалярной функции потерь по отношению к любому набору переменных. Этот механизм является программной реализацией алгоритма **обратного распространения ошибки** (*backpropagation*), который лежит в основе обучения всех современных нейронных сетей.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий фрагмент демонстрирует основные строительные блоки TensorFlow: создание тензоров различных рангов и типов, определение изменяемых переменных (которые хранят параметры модели) и выполнение базовых операций. В последнем блоке показано, как `tf.GradientTape` используется для вычисления производной простой квадратичной функции.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# === Создание тензоров различных рангов ===\n",
        "# 0-мерный тензор (скаляр)\n",
        "scalar = tf.constant(5)\n",
        "print(f\"Скаляр (ранг 0): {scalar}, shape: {scalar.shape}\")\n",
        "\n",
        "# 1-мерный тензор (вектор)\n",
        "vector = tf.constant([1, 2, 3])\n",
        "print(f\"Вектор (ранг 1): {vector}, shape: {vector.shape}\")\n",
        "\n",
        "# 2-мерный тензор (матрица)\n",
        "matrix = tf.constant([[1, 2], [3, 4]], dtype=tf.int32)\n",
        "print(f\"Матрица (ранг 2):\\n{matrix}, shape: {matrix.shape}\")\n",
        "\n",
        "# Тензоры с явным указанием типа данных\n",
        "float_tensor = tf.constant(3.14, dtype=tf.float32)\n",
        "bool_tensor = tf.constant([True, False, True], dtype=tf.bool)\n",
        "\n",
        "# === Изменяемые тензоры (Переменные) ===\n",
        "# Переменные используются для хранения обучаемых параметров (весов и смещений)\n",
        "weights = tf.Variable(\n",
        "    initial_value=tf.random.normal([10, 5]),\n",
        "    name=\"weights\"\n",
        ")\n",
        "bias = tf.Variable(\n",
        "    initial_value=tf.zeros([5]),\n",
        "    name=\"bias\"\n",
        ")\n",
        "print(f\"Веса инициализированы, trainable: {weights.trainable}\")\n",
        "\n",
        "# === Базовые операции с тензорами ===\n",
        "a = tf.constant([[1., 2.], [3., 4.]])\n",
        "b = tf.constant([[5., 6.], [7., 8.]])\n",
        "\n",
        "# Поэлементное умножение (Hadamard product)\n",
        "elementwise = tf.multiply(a, b)\n",
        "print(f\"Поэлементное умножение:\\n{elementwise}\")\n",
        "\n",
        "# Матричное умножение (dot product)\n",
        "matrix_mult = tf.matmul(a, b)\n",
        "print(f\"Матричное умножение:\\n{matrix_mult}\")\n",
        "\n",
        "# Редукция по измерению (суммирование по строкам)\n",
        "reduce_sum = tf.reduce_sum(a, axis=1)  # axis=1 -> по строкам\n",
        "print(f\"Сумма по строкам: {reduce_sum}\")\n",
        "\n",
        "# === Автоматическое дифференцирование ===\n",
        "# Инициализация переменной\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Контекст GradientTape записывает все операции с x\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x**2 + 2*x + 1  # y = f(x) = x^2 + 2x + 1\n",
        "\n",
        "# Вычисление градиента dy/dx\n",
        "dy_dx = tape.gradient(y, x)  # Аналитический градиент: 2x + 2\n",
        "print(f\"Значение y при x=3: {y.numpy()}\")\n",
        "print(f\"Градиент dy/dx при x=3: {dy_dx.numpy()}\")  # Ожидаем 2*3 + 2 = 8\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот код иллюстрирует, как базовые математические операции и автоматическое дифференцирование интегрированы в TensorFlow на низком уровне. Разработчик имеет полный контроль над данными и их преобразованиями, что является основой для построения сложных архитектур нейронных сетей.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Keras API: высокоуровневое построение моделей\n",
        "\n",
        "### Теория\n",
        "\n",
        "Keras API, интегрированный в TensorFlow (`tf.keras`), реализует принципы **модульности** и **композиции** для построения нейронных сетей. В этой парадигме сеть рассматривается как **список слоёв**, где каждый слой является независимым объектом, инкапсулирующим три ключевых компонента:\n",
        "\n",
        "1.  **Вычислительное преобразование** (*forward pass*): функция `call()`, которая определяет, как входной тензор преобразуется в выходной.\n",
        "2.  **Состояние** (*state*): обучаемые параметры слоя (например, веса `W` и смещения `b` в полносвязном слое), которые представлены как экземпляры `tf.Variable`.\n",
        "3.  **Градиентное преобразование** (*backward pass*): автоматически генерируемая функция, которая вычисляет градиенты выхода по отношению к входу и параметрам, необходимая для обучения.\n",
        "\n",
        "Keras предоставляет два основных интерфейса для построения моделей:\n",
        "\n",
        "*   **Sequential API**: Предназначен для простых, линейных стеков слоёв, где выход одного слоя является входом для следующего. Это самый простой и читаемый способ для большинства задач.\n",
        "*   **Functional API**: Предоставляет гибкость для построения сложных топологий, включая ветвления, остаточные соединения (residual connections) и многовходовые/многовыходные модели. Он работает путём прямого связывания выходов одного слоя с входами другого.\n",
        "\n",
        "Для максимальной гибкости Keras позволяет разработчику создавать **кастомные слои**, наследуя от базового класса `tf.keras.layers.Layer` и переопределяя методы `build()` (для инициализации параметров) и `call()` (для определения логики преобразования).\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В следующем примере демонстрируются оба интерфейса Keras, а также создание собственного слоя. Это показывает эволюцию от простого прототипа к сложной и гибкой архитектуре.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# === 1. Sequential API для линейных архитектур ===\n",
        "# Подходит для простых стеков слоёв\n",
        "model_sequential = tf.keras.Sequential([\n",
        "    # Первый полносвязный слой с 64 нейронами и ReLU-активацией\n",
        "    # input_shape задаётся только для первого слоя\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    # Dropout для регуляризации (отключает 20% нейронов случайным образом во время обучения)\n",
        "    layers.Dropout(0.2),\n",
        "    # Выходной слой с 10 нейронами (для 10 классов) и softmax-активацией\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# === 2. Functional API для нелинейных топологий ===\n",
        "# Позволяет создавать сложные архитектуры\n",
        "inputs = tf.keras.Input(shape=(784,))  # Определение входного тензора\n",
        "x = layers.Dense(64, activation='relu')(inputs)  # Применение слоя к входу\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Создание модели из входов и выходов\n",
        "model_functional = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Обе модели эквивалентны по архитектуре\n",
        "print(\"Sequential model summary:\")\n",
        "model_sequential.summary()\n",
        "\n",
        "print(\"\\nFunctional model summary:\")\n",
        "model_functional.summary()\n",
        "\n",
        "# === 3. Создание кастомного слоя ===\n",
        "class CustomDense(layers.Layer):\n",
        "    \"\"\"\n",
        "    Кастомный полносвязный слой без использования готового класса Dense.\n",
        "    Демонстрирует, как инкапсулировать состояние и логику преобразования.\n",
        "    \"\"\"\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Метод build вызывается один раз при первом вызове слоя.\n",
        "        Здесь инициализируются обучаемые параметры (веса и смещения).\n",
        "        \"\"\"\n",
        "        # Добавление весовой матрицы W\n",
        "        self.w = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),  # [вход, выход]\n",
        "            initializer='random_normal',\n",
        "            trainable=True\n",
        "        )\n",
        "        # Добавление вектора смещения b\n",
        "        self.b = self.add_weight(\n",
        "            name='bias',\n",
        "            shape=(self.units,),  # [выход]\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Метод call определяет прямой проход через слой: y = xW + b.\n",
        "        \"\"\"\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Метод get_config позволяет сериализовать слой для сохранения модели.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({'units': self.units})\n",
        "        return config\n",
        "\n",
        "# === Использование кастомного слоя ===\n",
        "# Создание и применение кастомного слоя к данным\n",
        "custom_layer = CustomDense(units=32)\n",
        "sample_input = tf.random.normal([1, 16])\n",
        "sample_output = custom_layer(sample_input)\n",
        "print(f\"\\nКастомный слой: вход {sample_input.shape} -> выход {sample_output.shape}\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример показывает иерархию абстракций в Keras: от простого `Sequential` для быстрого старта, до гибкого `Functional` API для сложных моделей, и, наконец, до полного контроля через кастомные слои. Такая структура позволяет инженеру начать с прототипа и постепенно усложнять архитектуру по мере роста требований.\n",
        "\n",
        "> **Таблица: Основные типы слоёв в Keras**\n",
        "\n",
        "| **Тип слоя** | **Назначение** | **Ключевые параметры** | **Применение** |\n",
        "|--------------|----------------|------------------------|----------------|\n",
        "| `Dense` | Полносвязный слой | `units`, `activation` | Классификация, регрессия, скрытые слои |\n",
        "| `Conv2D` | Сверточный слой для изображений | `filters`, `kernel_size`, `strides` | Извлечение пространственных признаков (CNN) |\n",
        "| `LSTM` / `GRU` | Рекуррентные слои | `units`, `return_sequences` | Обработка последовательностей (текст, временные ряды) |\n",
        "| `Dropout` | Регуляризация | `rate` | Предотвращение переобучения |\n",
        "| `BatchNormalization` | Нормализация активаций | `momentum`, `epsilon` | Стабилизация и ускорение обучения |\n",
        "| `GlobalAveragePooling2D` | Сжатие пространственных измерений | — | Замена полносвязных слоёв в CNN (уменьшает переобучение) |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Процесс обучения: компиляция и тренировка\n",
        "\n",
        "### Теория\n",
        "\n",
        "Процесс обучения нейронной сети формализуется как задача **оптимизации**: нахождение значений параметров модели \\(\\theta\\) (весов и смещений), которые минимизируют **функцию потерь** (loss function) \\( \\mathcal{L}(y, \\hat{y}(\\theta)) \\) на обучающем множестве. Функция потерь количественно измеряет расхождение между истинными метками \\(y\\) и предсказаниями модели \\(\\hat{y}\\).\n",
        "\n",
        "Минимизация достигается с помощью итеративных **алгоритмов оптимизации**, которые используют градиенты функции потерь для обновления параметров. Наиболее распространённый алгоритм — **Adam** (Adaptive Moment Estimation), который адаптивно регулирует скорость обучения для каждого параметра на основе первых и вторых моментов градиентов.\n",
        "\n",
        "В Keras процесс обучения декомпозируется на три этапа:\n",
        "1.  **Компиляция (`compile`)**: Определение трёх ключевых компонентов: оптимизатора, функции потерь и метрик качества.\n",
        "2.  **Тренировка (`fit`)**: Итеративное представление данных модели в виде пакетов (batches), вычисление градиентов и обновление весов.\n",
        "3.  **Оценка (`evaluate`)** и **Предсказание (`predict`)**: Использование обученной модели для оценки на новых данных или генерации прогнозов.\n",
        "\n",
        "Для повышения эффективности и надёжности процесса обучения Keras предоставляет мощный механизм **callback-ов** — функций, которые вызываются в определённые моменты во время тренировки (в начале/конце эпохи, при улучшении метрики и т.д.). Они позволяют автоматизировать задачи вроде ранней остановки, сохранения лучших моделей и динамического изменения скорости обучения.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример показывает полный цикл настройки процесса обучения, включая кастомизацию оптимизатора и функции потерь, а также использование callback-ов для управления тренировкой.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Предположим, у нас есть данные x_train, y_train\n",
        "# x_train.shape = (N, 784), y_train.shape = (N,)\n",
        "\n",
        "# === Компиляция модели с готовыми компонентами ===\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',  # Стандартный оптимизатор\n",
        "    loss='sparse_categorical_crossentropy',  # Для меток в виде целых чисел\n",
        "    metrics=['accuracy']  # Метрика для мониторинга\n",
        ")\n",
        "\n",
        "# === Кастомизация оптимизатора: Расписание Learning Rate ===\n",
        "# Экспоненциальное затухание скорости обучения улучшает сходимость\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,    # Через сколько шагов уменьшать\n",
        "    decay_rate=0.9        # Множитель (новая_lr = старая_lr * 0.9)\n",
        ")\n",
        "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# === Кастомная функция потерь: Huber Loss ===\n",
        "# Robust loss, менее чувствительная к выбросам, чем MSE\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"\n",
        "    Huber Loss: квадратичная для малых ошибок, линейная для больших.\n",
        "    \"\"\"\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) <= delta\n",
        "    squared_loss = 0.5 * tf.square(error)\n",
        "    linear_loss = delta * (tf.abs(error) - 0.5 * delta)\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# Компиляция с кастомными компонентами\n",
        "# model.compile(optimizer=custom_optimizer, loss=huber_loss, ...)\n",
        "\n",
        "# === Callback-и для управления тренировкой ===\n",
        "callbacks = [\n",
        "    # Ранняя остановка: прекратить обучение, если val_loss не улучшается 3 эпохи\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True  # Автоматически восстановить лучшие веса\n",
        "    ),\n",
        "    # Сохранение модели с наилучшими весами\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='best_model.keras',  # Новый формат .keras\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False\n",
        "    ),\n",
        "    # Динамическое уменьшение скорости обучения при плато\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,     # Новая_lr = текущая_lr * 0.5\n",
        "        patience=2,\n",
        "        min_lr=1e-7     # Минимальная возможная скорость\n",
        "    )\n",
        "]\n",
        "\n",
        "# === Запуск тренировки ===\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=32,            # Размер пакета\n",
        "    epochs=100,               # Максимальное число эпох\n",
        "    validation_split=0.2,     # 20% данных для валидации\n",
        "    callbacks=callbacks,      # Подключение callback-ов\n",
        "    verbose=1                 # Вывод прогресса\n",
        ")\n",
        "\n",
        "# Анализ истории тренировки\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Мониторинг точности во время тренировки')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример демонстрирует, как Keras предоставляет как высокоуровневые \"коробочные\" решения для быстрого старта, так и глубокую кастомизацию для тонкой настройки процесса обучения. Использование callback-ов является обязательной практикой в промышленных проектах, так как оно автоматизирует рутинные задачи и повышает надёжность экспериментов.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Сверточные нейронные сети (CNN)\n",
        "\n",
        "### Теория\n",
        "\n",
        "Сверточные нейронные сети (CNN) являются доминирующим архитектурным паттерном для задач, связанных с обработкой данных, имеющих **сетчатую топологию**, в первую очередь — изображений. Эффективность CNN обусловлена тремя ключевыми принципами:\n",
        "\n",
        "1.  **Локальные рецептивные поля** (*local receptive fields*): Каждый нейрон в свёрточном слое подключён не ко всему входному изображению, а только к небольшому локальному региону (ядро свёртки, например, 3x3). Это отражает предположение, что важные признаки (например, края) являются локальными.\n",
        "2.  **Разделение весов** (*weight sharing*): Одно и то же ядро свёртки (набор весов) применяется ко всему изображению. Это резко снижает количество обучаемых параметров по сравнению с полносвязными слоями и обеспечивает **трансляционную инвариантность** — сеть распознаёт признак независимо от его положения на изображении.\n",
        "3.  **Пространственное субдискретизирование** (*spatial subsampling*): Слои субдискретизации (обычно `MaxPooling2D`) уменьшают пространственные размерности карт признаков. Это делает представление более устойчивым к небольшим сдвигам и искажениям и снижает вычислительную сложность.\n",
        "\n",
        "Эти принципы позволяют CNN извлекать **иерархические признаки**: ранние слои обнаруживают простые паттерны (края, углы), а более глубокие слои комбинируют их для формирования сложных концепций (части объектов, целые объекты).\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В этом примере строится классическая CNN \"от нуля\", демонстрируется метод **аугментации данных** для борьбы с переобучением и реализуется стратегия **transfer learning** с использованием предобученной модели ResNet50.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# === 1. Базовая CNN архитектура ===\n",
        "model = models.Sequential([\n",
        "    # Первый свёрточный блок\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # Второй свёрточный блок\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # Третий свёрточный блок\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    # Преобразование в вектор для полносвязных слоёв\n",
        "    layers.Flatten(),\n",
        "    # Полносвязные слои\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # Для 10 классов MNIST\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# === 2. Data Augmentation (Аугментация данных) ===\n",
        "# Генерация вариаций изображений \"на лету\" для увеличения обучающего набора\n",
        "# и повышения обобщающей способности модели\n",
        "data_augmentation = models.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),      # Случайное горизонтальное отражение\n",
        "    layers.RandomRotation(0.1),           # Поворот на ±10% от 2π\n",
        "    layers.RandomZoom(0.1),               # Масштабирование\n",
        "    layers.RandomContrast(0.2),           # Изменение контраста\n",
        "])\n",
        "\n",
        "# Применение аугментации можно встроить прямо в модель\n",
        "augmented_model = models.Sequential([\n",
        "    data_augmentation,\n",
        "    model\n",
        "])\n",
        "\n",
        "# === 3. Transfer Learning с предобученной моделью ===\n",
        "# Использование знаний, полученных при обучении на огромном датасете (ImageNet)\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    weights='imagenet',          # Загрузка весов, предобученных на ImageNet\n",
        "    include_top=False,           # Исключение исходного классификатора\n",
        "    input_shape=(224, 224, 3)    # Формат входа для ResNet50\n",
        ")\n",
        "\n",
        "# Заморозка весов базовой модели для сохранения предобученных признаков\n",
        "base_model.trainable = False\n",
        "\n",
        "# Построение новой \"головы\" классификатора поверх базовой модели\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)  # training=False для замороженной модели\n",
        "x = layers.GlobalAveragePooling2D()(x)  # Замена полносвязных слоёв\n",
        "x = layers.Dropout(0.2)(x)              # Регуляризация\n",
        "outputs = layers.Dense(10, activation='softmax')(x)  # Новый классификатор\n",
        "\n",
        "transfer_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Компиляция и тренировка только новой головы\n",
        "transfer_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# После начальной тренировки головы можно разморозить часть базовой модели\n",
        "# для fine-tuning, обучая её с очень малой скоростью обучения\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Transfer learning является стандартом де-факто для задач компьютерного зрения с ограниченным объёмом данных. Он позволяет достичь высокой точности, обучая только небольшую часть параметров модели, что экономит время и вычислительные ресурсы.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Рекуррентные нейронные сети (RNN)\n",
        "\n",
        "### Теория\n",
        "\n",
        "Рекуррентные нейронные сети (RNN) предназначены для обработки **последовательных данных**, где порядок элементов имеет значение (текст, временные ряды, аудио). Отличительной особенностью RNN является наличие **внутреннего состояния** (*hidden state*), которое передаётся от одного временного шага к другому, позволяя сети \"помнить\" информацию о предыдущих элементах последовательности.\n",
        "\n",
        "Простые RNN страдают от проблемы **исчезающего (или взрывающегося) градиента**, которая затрудняет обучение долгосрочным зависимостям. Эта проблема была решена с помощью архитектур с **механизмами вентилирования**:\n",
        "\n",
        "*   **LSTM** (*Long Short-Term Memory*) вводит три вентиля: входной, забывающий и выходной, а также ячейку памяти, что позволяет модели точно регулировать поток информации.\n",
        "*   **GRU** (*Gated Recurrent Unit*) является более простой и быстрой альтернативой LSTM с двумя вентилями, часто демонстрируя сопоставимое качество.\n",
        "\n",
        "Для доступа к контексту как из прошлого, так и из будущего используется **двунаправленная RNN** (*Bidirectional RNN*), которая состоит из двух независимых RNN, обрабатывающих последовательность в прямом и обратном порядке.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В этом примере рассматривается задача анализа тональности текста на датасете IMDB. Демонстрируются три подхода: простая LSTM, двунаправленная LSTM и многослойная RNN.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# === 1. Загрузка и подготовка данных IMDB ===\n",
        "# Загрузка данных (топ-10000 слов)\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10_000)\n",
        "\n",
        "# Приведение последовательностей к одинаковой длине (200 слов)\n",
        "X_train = tf.keras.utils.pad_sequences(X_train, maxlen=200, padding='post', truncating='post')\n",
        "X_test = tf.keras.utils.pad_sequences(X_test, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "print(f\"Форма обучающих данных: {X_train.shape}, метки: {y_train.shape}\")\n",
        "\n",
        "# === 2. Простая LSTM модель ===\n",
        "simple_lstm = models.Sequential([\n",
        "    # Слой Embedding преобразует индексы слов в плотные векторы\n",
        "    layers.Embedding(input_dim=10_000, output_dim=32, input_length=200),\n",
        "    # Основной слой LSTM\n",
        "    layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
        "    # Бинарный классификатор\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "simple_lstm.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === 3. Двунаправленная LSTM ===\n",
        "# Позволяет модели видеть как прошлый, так и будущий контекст текущего слова\n",
        "bidirectional_lstm = models.Sequential([\n",
        "    layers.Embedding(10_000, 32, input_length=200),\n",
        "    layers.Bidirectional(layers.LSTM(64, dropout=0.2)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# === 4. Многослойная RNN с возвратом последовательностей ===\n",
        "# Для построения глубоких рекуррентных архитектур\n",
        "multi_layer_rnn = models.Sequential([\n",
        "    layers.Embedding(10_000, 32, input_length=200),\n",
        "    # Первый LSTM возвращает последовательность для следующего слоя\n",
        "    layers.LSTM(64, return_sequences=True, dropout=0.2),\n",
        "    # Второй LSTM обрабатывает последовательность и возвращает вектор\n",
        "    layers.LSTM(32, dropout=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Обучение одной из моделей\n",
        "history = simple_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "RNN и их современные варианты (LSTM, GRU) остаются важным инструментом для задач, связанных с последовательностями, особенно когда объём данных или вычислительные ресурсы ограничены, или когда интерпретируемость модели важна. Хотя в последние годы трансформеры часто демонстрируют более высокую производительность, RNN по-прежнему ценны за свою простоту, эффективность и предсказуемость.\n",
        ""
      ],
      "metadata": {
        "id": "c0woa4TU0hd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 6. Transformers и механизм внимания\n",
        "\n",
        "### Теория\n",
        "\n",
        "Архитектура **Transformer**, представленная в работе Vaswani et al. (2017) «Attention is All You Need», совершила революцию в области обработки последовательностей, заменив рекуррентные и свёрточные механизмы на **чисто внимание-ориентированный подход**. Ключевым нововведением является механизм **масштабированного скалярного произведения с самовниманием** (*scaled dot-product self-attention*).\n",
        "\n",
        "Механизм **Self-Attention** позволяет каждому элементу последовательности (например, слову в предложении) динамически вычислять «важность» всех других элементов, включая самого себя. Для входной последовательности тензоров \\(X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}\\) (где \\(n\\) — длина последовательности, \\(d_{\\text{model}}\\) — размерность эмбеддинга) вычисляются три проекции:\n",
        "\n",
        "\\[\n",
        "Q = XW^Q,\\quad K = XW^K,\\quad V = XW^V\n",
        "\\]\n",
        "\n",
        "где \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\\) — обучаемые матрицы весов. Выход внимания определяется как взвешенная сумма значений \\(V\\), где веса определяются совместимостью между запросами \\(Q\\) и ключами \\(K\\):\n",
        "\n",
        "\\[\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\]\n",
        "\n",
        "Деление на \\(\\sqrt{d_k}\\) — это **масштабирование**, необходимое для предотвращения очень малых градиентов при больших значениях \\(d_k\\), что стабилизирует обучение.\n",
        "\n",
        "Для захвата информации из различных подпространств представлений используется механизм **Multi-Head Attention (MHA)**. Вместо выполнения одного внимания с размерностью \\(d_{\\text{model}}\\), MHA параллельно выполняет \\(h\\) вниманий с меньшей размерностью \\(d_k = d_v = d_{\\text{model}}/h\\), а затем конкатенирует их выходы и проецирует обратно в пространство \\(d_{\\text{model}}\\):\n",
        "\n",
        "\\[\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
        "\\]\n",
        "\\[\n",
        "\\text{где}\\quad \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
        "\\]\n",
        "\n",
        "Это позволяет модели одновременно учитывать разные аспекты зависимости (например, синтаксические и семантические) на одном и том же уровне.\n",
        "\n",
        "Поскольку архитектура Transformer не содержит рекуррентных связей или свёрток, она не имеет встроенной информации о **порядке элементов** в последовательности. Эта информация инжектируется с помощью **позиционного кодирования** (*positional encoding*), которое добавляется к эмбеддингам слов. В оригинальной статье используется синусоидальное кодирование, где для позиции \\(pos\\) и измерения \\(i\\):\n",
        "\n",
        "\\[\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right), \\quad\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "\\]\n",
        "\n",
        "Это кодирование обладает свойством, позволяющим модели экстраполировать на последовательности длиннее, чем во время обучения.\n",
        "\n",
        "Типичный блок энкодера Transformer состоит из **двух подслоёв**: (1) Multi-Head Attention с добавлением **остаточного соединения** (residual connection) и **нормализацией по слоям** (Layer Normalization); (2) позиционно-экспансивной полносвязной сети (Position-wise Feed-Forward Network), также снабжённой residual connection и LayerNorm. Эти элементы обеспечивают градиентную трансляцию и стабильную сходимость.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует построение одного блока энкодера Transformer и реализацию позиционного кодирования. Это строительный блок для моделей типа BERT и других.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# === 1. Блок энкодера Transformer ===\n",
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    Реализация одного блока энкодера из архитектуры Transformer.\n",
        "    Содержит Multi-Head Attention и Feed-Forward Network с residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        # Много-головый механизм внимания\n",
        "        self.att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim  # Размерность каждого \"ключа\"\n",
        "        )\n",
        "        # Позиционно-экспансивная сеть (обычно 2 слоя Dense)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),  # Экспансия\n",
        "            layers.Dense(embed_dim)                    # Сжатие обратно\n",
        "        ])\n",
        "        # Нормализация по слоям\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Dropout для регуляризации\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        \"\"\"\n",
        "        Прямой проход через блок.\n",
        "        :param inputs: Входной тензор формы (batch_size, seq_len, embed_dim)\n",
        "        :param training: Флаг режима обучения для Dropout\n",
        "        :return: Преобразованный тензор той же формы\n",
        "        \"\"\"\n",
        "        # --- Подслой 1: Multi-Head Attention ---\n",
        "        # Self-attention: запросы, ключи и значения берутся из одного источника\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # Остаточное соединение и нормализация\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        \n",
        "        # --- Подслой 2: Feed-Forward Network ---\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # Остаточное соединение и нормализация\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# === 2. Позиционное кодирование ===\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Реализация синусоидального позиционного кодирования.\n",
        "    Добавляет информацию о позиции к входным эмбеддингам.\n",
        "    \"\"\"\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "    \n",
        "    def get_angles(self, position, i, d_model):\n",
        "        \"\"\"\n",
        "        Вычисляет углы для синусоидального кодирования.\n",
        "        \"\"\"\n",
        "        # Формула: pos / (10000^(2i/d_model))\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "    \n",
        "    def positional_encoding(self, position, d_model):\n",
        "        \"\"\"\n",
        "        Генерирует матрицу позиционного кодирования.\n",
        "        \"\"\"\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "        # Применяем sin к чётным индексам (0, 2, 4, ...)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # Применяем cos к нечётным индексам (1, 3, 5, ...)\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        # Чередуем sin и cos для создания полного вектора\n",
        "        pos_encoding = tf.stack([sines, cosines], axis=-1)\n",
        "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "        # Добавляем измерение для батча: [1, position, d_model]\n",
        "        return pos_encoding[tf.newaxis, ...]\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Добавляет позиционное кодирование к входным эмбеддингам.\n",
        "        Автоматически обрезает кодирование до длины входной последовательности.\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "# === Пример использования ===\n",
        "# Параметры модели\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "\n",
        "# Сборка модели\n",
        "inputs = layers.Input(shape=(max_length,))\n",
        "# Слой эмбеддингов\n",
        "embedding_layer = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "# Добавление позиционного кодирования\n",
        "x = PositionalEncoding(max_length, embed_dim)(embedding_layer)\n",
        "# Применение блока Transformer\n",
        "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "# Выходной слой (для примера — классификация)\n",
        "outputs = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(outputs)\n",
        "\n",
        "transformer_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "transformer_model.summary()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример иллюстрирует, как базовые компоненты архитектуры Transformer инкапсулируются в кастомные слои Keras. Такой подход позволяет легко строить сложные модели, такие как BERT или GPT, и адаптировать их под специфические задачи. Позиционное кодирование является обязательным компонентом для любых задач, где порядок имеет значение.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Автокодировщики и генеративные модели\n",
        "\n",
        "### Теория\n",
        "\n",
        "Генеративные модели направлены на изучение распределения данных \\(p(x)\\) для генерации новых, синтетических примеров, подобных обучающему набору. TensorFlow предоставляет гибкие инструменты для реализации различных парадигм генеративного моделирования.\n",
        "\n",
        "**Автокодировщики (Autoencoders, AE)** — это нейросети с bottleneck-архитектурой, состоящие из двух частей: **энкодера** \\(q_{\\phi}(z|x)\\), который сжимает вход \\(x\\) в латентное представление \\(z\\) низкой размерности, и **декодера** \\(p_{\\theta}(x|z)\\), который восстанавливает \\(x\\) из \\(z\\). Модель обучается минимизировать реконструкционную ошибку \\(\\mathcal{L}_{\\text{rec}} = \\|x - \\hat{x}\\|^2\\).\n",
        "\n",
        "**Вариационные автокодировщики (VAE)** вводят вероятностную интерпретацию. Вместо детерминированного отображения в точку \\(z\\), энкодер моделирует **аппостериорное распределение** \\(q_{\\phi}(z|x)\\), обычно как гауссиано с диагональной ковариацией: \\(q_{\\phi}(z|x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\sigma_{\\phi}^2(x))\\). Цель обучения — максимизировать вариационную нижнюю границу (ELBO):\n",
        "\n",
        "\\[\n",
        "\\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - \\text{KL}(q_{\\phi}(z|x) \\| p(z))\n",
        "\\]\n",
        "\n",
        "где первое слагаемое — это реконструкционная ошибка, а второе — регуляризатор в виде дивергенции Кульбака-Лейблера (KL) между аппостериорным распределением и априорным \\(p(z) = \\mathcal{N}(0, I)\\). Это обеспечивает, что латентное пространство будет гладким и структурированным, что позволяет генерировать новые примеры путём выборки из \\(p(z)\\).\n",
        "\n",
        "**Генеративно-состязательные сети (GAN)** используют принцип **минимакс-игры** между двумя сетями: **генератором** \\(G(z)\\), который учится создавать реалистичные данные из шумового вектора \\(z\\), и **дискриминатором** \\(D(x)\\), который учится отличать реальные данные от сгенерированных. Функция потерь для GAN:\n",
        "\n",
        "\\[\n",
        "\\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]\n",
        "\\]\n",
        "\n",
        "Хотя GAN не будут реализованы в данном примере из-за их сложности в обучении, они остаются важнейшим классом генеративных моделей.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует построение и обучение **Вариационного Автокодировщика (VAE)** для генерации изображений рукописных цифр из датасета MNIST. Ключевым элементом является кастомный слой `Sampling`, реализующий **reparameterization trick**, необходимый для дифференцируемого семплирования из гауссиана.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# === 1. Слой семплирования (Reparameterization Trick) ===\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"\n",
        "    Использует reparameterization trick для семплирования из гауссиана.\n",
        "    Позволяет градиентам проходить через операцию случайного выбора.\n",
        "    z = z_mean + exp(0.5 * z_log_var) * epsilon\n",
        "    где epsilon ~ N(0, I)\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        # Генерация стандартного нормального шума\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        # Детерминированное преобразование для семплирования\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# === 2. Построение энкодера ===\n",
        "latent_dim = 2  # Для визуализации в 2D\n",
        "encoder_inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation='relu')(x)\n",
        "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "# === 3. Построение генератора (декодера) ===\n",
        "latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "decoder = models.Model(latent_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "# === 4. Объединение в кастомную модель VAE ===\n",
        "class VAE(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Кастомная модель VAE с переопределённым train_step для полного контроля над обучением.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        # Метрики для отслеживания компонентов потерь\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name='total_loss')\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name='reconstruction_loss')\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name='kl_loss')\n",
        "    \n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"Переопределение метрик для их отображения в процессе обучения.\"\"\"\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker\n",
        "        ]\n",
        "    \n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Кастомный шаг обучения, в котором вычисляются все компоненты VAE-потерь.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Прямой проход через энкодер и декодер\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            \n",
        "            # 1. Реконструкционная ошибка (Binary Crossentropy для изображений [0,1])\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    tf.keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            # 2. KL-дивергенция (аналитически для гауссианов)\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            # 3. Общая потеря\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        \n",
        "        # Вычисление градиентов и обновление весов\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        \n",
        "        # Обновление метрик\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        \n",
        "        # Возврат текущих значений метрик\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "# === 5. Инициализация и компиляция модели ===\n",
        "# Компиляция не требует указания loss и optimizer, так как они заданы в train_step\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Загрузка данных (пример)\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_train = x_train[..., tf.newaxis]  # Добавление канала\n",
        "\n",
        "# === 6. Обучение модели ===\n",
        "# Обучение будет отображать все три компонента потерь\n",
        "vae.fit(x_train, x_train, epochs=10, batch_size=128)\n",
        "\n",
        "# === 7. Генерация новых изображений ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Генерация точек в латентном пространстве\n",
        "grid_x = np.linspace(-3, 3, 10)\n",
        "grid_y = np.linspace(-3, 3, 10)\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * 10, digit_size * 10))\n",
        "\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]], dtype=np.float32)\n",
        "        x_decoded = vae.decoder(z_sample)\n",
        "        digit = x_decoded[0].numpy().reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='Greys_r')\n",
        "plt.axis('off')\n",
        "plt.title('Сгенерированные изображения VAE (латентное пространство 2D)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл работы с VAE: от проектирования архитектуры и кастомной функции потерь до обучения и генерации новых данных. Визуализация 2D латентного пространства показывает, как VAE структурирует распределение данных, группируя похожие цифры вместе. Такой подход является основой для многих современных генеративных моделей.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Распределённое обучение\n",
        "\n",
        "### Теория\n",
        "\n",
        "Обучение глубоких моделей на больших наборах данных часто требует значительных вычислительных ресурсов. **Распределённое обучение** в TensorFlow позволяет масштабировать тренировку на несколько GPU, TPU или даже на кластер машин, минимизируя изменения в коде пользователя.\n",
        "\n",
        "TensorFlow предлагает иерархию **стратегий распределения** (`tf.distribute.Strategy`), которые абстрагируют детали распределения данных и агрегации градиентов:\n",
        "\n",
        "*   **`MirroredStrategy`**: Предназначена для синхронного обучения на нескольких GPU **одной машины**. Модель реплицируется на каждом GPU, каждый реплика обрабатывает свой сегмент батча (*data parallelism*), а градиенты агрегируются по всем устройствам с помощью **All-Reduce** операции (обычно через NCCL на NVIDIA GPU). Это наиболее распространённый сценарий для локальных рабочих станций и серверов.\n",
        "\n",
        "*   **`TPUStrategy`**: Аналог `MirroredStrategy` для Google Cloud TPU, оптимизированный для специфики тензорных ядер.\n",
        "\n",
        "*   **`MultiWorkerMirroredStrategy`**: Расширяет `MirroredStrategy` на несколько машин в кластере, используя gRPC для коммуникации между воркерами.\n",
        "\n",
        "*   **`ParameterServerStrategy`**: Асинхронный подход, где несколько *worker*-ов вычисляют градиенты и отправляют их на центральный *parameter server*, который обновляет глобальные веса. Подходит для очень больших кластеров.\n",
        "\n",
        "*   **`CentralStorageStrategy`**: Все переменные (веса модели) хранятся на CPU, а вычисления производятся на GPU. Подходит для моделей с небольшим количеством параметров.\n",
        "\n",
        "Процесс использования стратегии прост: весь код определения модели и компиляции оборачивается в контекстный менеджер `strategy.scope()`. Внутри этого контекста переменные создаются как **зеркальные переменные** (`MirroredVariable`), которые автоматически синхронизируются между устройствами.\n",
        "\n",
        "Для максимальной производительности и гибкости можно реализовать **кастомный цикл обучения** с использованием `@tf.function` и методов `strategy.run()` и `strategy.reduce()`, что позволяет точно контролировать распределение данных и агрегацию результатов.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует два подхода к распределённому обучению: простой способ с использованием API Keras (`model.fit`) и более гибкий способ с кастомным циклом обучения.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# === 1. Простой способ: использование model.fit() ===\n",
        "# Создание стратегии (автоматически обнаружит доступные GPU)\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(f\"Количество реплик (устройств): {strategy.num_replicas_in_sync}\")\n",
        "\n",
        "# Определение модели внутри scope стратегии\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Загрузка и подготовка данных\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Обучение — стратегия автоматически распределит данные и агрегирует градиенты\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# === 2. Кастомный цикл обучения для максимального контроля ===\n",
        "# Подготовка данных с использованием tf.data для эффективной загрузки\n",
        "GLOBAL_BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "# Распределение датасета между репликами\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "\n",
        "# Определение модели и оптимизатора в scope\n",
        "with strategy.scope():\n",
        "    custom_model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Компиляция шага обучения\n",
        "@tf.function\n",
        "def distributed_train_step(data):\n",
        "    \"\"\"Распределённый шаг обучения.\"\"\"\n",
        "    def step_fn(inputs):\n",
        "        \"\"\"Функция, выполняемая на каждой реплике.\"\"\"\n",
        "        x, y = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = custom_model(x, training=True)\n",
        "            per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
        "            # Важно: вычислять среднюю потерю на реплике\n",
        "            loss = tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "        \n",
        "        gradients = tape.gradient(loss, custom_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, custom_model.trainable_variables))\n",
        "        return loss\n",
        "    \n",
        "    # Выполнение шага на всех репликах\n",
        "    per_replica_losses = strategy.run(step_fn, args=(data,))\n",
        "    # Агрегация потерь (сумма по всем репликам)\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "# Запуск цикла обучения\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for data in train_dist_dataset:\n",
        "        loss = distributed_train_step(data)\n",
        "        total_loss += loss\n",
        "        num_batches += 1\n",
        "    \n",
        "    train_loss = total_loss / num_batches\n",
        "    print(f\"Эпоха {epoch+1}, Средняя потеря: {train_loss:.4f}\")\n",
        "\n",
        "# === 3. Федеративное обучение (обзор) ===\n",
        "# Для сценариев, где данные не могут покидать клиентов (например, мобильные устройства),\n",
        "# используется **Федеративное обучение** (Federated Learning).\n",
        "# TensorFlow предоставляет специализированную библиотеку **TensorFlow Federated (TFF)**.\n",
        "#\n",
        "# Пример инициализации процесса:\n",
        "# import tensorflow_federated as tff\n",
        "#\n",
        "# def create_federated_learning_process(model_fn, client_optimizer_fn, server_optimizer_fn):\n",
        "#     return tff.learning.algorithms.build_weighted_fed_avg(\n",
        "#         model_fn=model_fn,\n",
        "#         client_optimizer_fn=client_optimizer_fn,\n",
        "#         server_optimizer_fn=server_optimizer_fn\n",
        "#     )\n",
        "#\n",
        "# # Это позволяет обучать модель, не агрегируя сырые данные клиентов,\n",
        "# # а только обновления модели, что обеспечивает приватность.\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Использование `tf.distribute.Strategy` демонстрирует мощь TensorFlow как промышленной платформы. Разработчик может начать с обучения на одном GPU и, добавив всего несколько строк кода (`with strategy.scope():`), масштабировать обучение на десятки или сотни ускорителей. Кастомный цикл обучения предоставляет полный контроль для сложных сценариев, таких как нестандартные функции потерь или градиентные манипуляции. Федеративное обучение через TFF открывает путь к новым парадигмам, где приватность данных является первоочередной задачей.\n",
        ""
      ],
      "metadata": {
        "id": "V_5wSziY7Hf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 9. Работа с данными: `tf.data` API\n",
        "\n",
        "### Теория\n",
        "\n",
        "Эффективная обработка входных данных является критическим фактором, определяющим общую производительность тренировки модели. Дискретная природа загрузки данных с диска и их предварительная обработка часто создают **узкое место** (*bottleneck*), из-за которого вычислительные устройства (GPU/TPU) простаивают в ожидании следующего пакета данных. **`tf.data` API** представляет собой высокоуровневый, модульный и оптимизированный фреймворк для построения конвейеров обработки данных, который решает эту проблему за счёт применения трёх ключевых принципов:\n",
        "\n",
        "1.  **Префетчинг **(Prefetching): Механизм, позволяющий загружать и предварительно обрабатывать следующие пакеты данных в фоновом потоке **одновременно** с выполнением шага обучения на текущем пакете. Это полностью перекрывает время ввода-вывода и CPU-обработки с вычислениями на GPU/TPU. В идеальном случае, когда конвейер сбалансирован, вычислительное устройство никогда не простаивает.\n",
        "2.  **Параллелизм **(Parallelization): Выполнение операций преобразования данных (таких как `map`) в **многопоточном режиме**. Аргумент `num_parallel_calls=tf.data.AUTOTUNE` позволяет TensorFlow автоматически определить оптимальное число параллельных вызовов на основе доступных ресурсов системы, максимизируя пропускную способность конвейера.\n",
        "3.  **Кэширование **(Caching): Сохранение результатов дорогостоящих операций предварительной обработки (например, декодирования изображений, аугментации) в памяти (`cache()`) или на диске (`cache(filename)`). Это особенно эффективно для небольших наборов данных, которые могут целиком поместиться в памяти, так как избавляет от повторного выполнения этих операций на каждой эпохе.\n",
        "\n",
        "`tf.data` API следует функциональной парадигме, где `Dataset` является неизменяемым объектом, а каждая операция (`map`, `batch`, `shuffle`) возвращает новый `Dataset`. Эта модель обеспечивает чистоту, читаемость и удобство отладки конвейеров.\n",
        "\n",
        "### Примеры\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует все ключевые аспекты `tf.data` API: создание датасетов из различных источников, построение оптимизированного конвейера и использование продвинутых функций, таких как чередование.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# === 1. Создание Dataset из различных источников ===\n",
        "\n",
        "# Из тензоров (наиболее частый сценарий)\n",
        "# x_train, y_train — предполагаются как numpy массивы или тензоры\n",
        "# dataset1 = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "# Из Python-генератора (для потоковых или динамически генерируемых данных)\n",
        "def data_generator():\n",
        "    \"\"\"Генератор, имитирующий поток данных.\"\"\"\n",
        "    for i in range(1000):\n",
        "        yield (\n",
        "            np.random.standard_normal(784).astype(np.float32),  # Вектор признаков\n",
        "            np.random.randint(0, 10, dtype=np.int32)             # Целочисленная метка\n",
        "        )\n",
        "\n",
        "# ВАЖНО: необходимо явно указать сигнатуру выходных данных для статической типизации\n",
        "dataset2 = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(784,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Из файлов TFRecord (стандартный формат для больших датасетов в TensorFlow)\n",
        "# TFRecord — это бинарный формат, оптимизированный для быстрой последовательной записи/чтения\n",
        "# dataset3 = tf.data.TFRecordDataset(['data1.tfrecord', 'data2.tfrecord'])\n",
        "\n",
        "# === 2. Построение оптимизированного конвейера обработки данных ===\n",
        "# Предположим, у нас есть датасет MNIST\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(np.float32)  # tf.data предпочитает явные типы\n",
        "\n",
        "# Создание и оптимизация конвейера\n",
        "optimized_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    # 1. Перемешивание: buffer_size должен быть >> batch_size для хорошего перемешивания\n",
        "    .shuffle(buffer_size=10_000)\n",
        "    # 2. Параллельная предобработка: нормализация пикселей\n",
        "    .map(\n",
        "        lambda x, y: (tf.cast(x, tf.float32) / 255.0, y),\n",
        "        # AUTOTUNE позволяем TensorFlow выбрать оптимальное число потоков\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    # 3. Пакетирование данных\n",
        "    .batch(32)\n",
        "    # 4. Префетчинг: загрузка следующих пакетов в фоне\n",
        "    # tf.data.AUTOTUNE обычно выбирает buffer_size = batch_size\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# === 3. Продвинутые техники: Чередование и кэширование ===\n",
        "\n",
        "# Чередование нескольких датасетов (например, для сбалансированной выборки из классов)\n",
        "dataset1 = tf.data.Dataset.range(0, 100, 3)  # 0, 3, 6, ...\n",
        "dataset2 = tf.data.Dataset.range(1, 100, 3)  # 1, 4, 7, ...\n",
        "dataset3 = tf.data.Dataset.range(2, 100, 3)  # 2, 5, 8, ...\n",
        "\n",
        "# Строим индексный датасет для кругового выбора: [0, 1, 2, 0, 1, 2, ...]\n",
        "choice_dataset = tf.data.Dataset.range(3).repeat()\n",
        "\n",
        "# Чередуем выбор из трёх датасетов\n",
        "interleaved_dataset = tf.data.Dataset.choose_from_datasets(\n",
        "    [dataset1, dataset2, dataset3],\n",
        "    choice_dataset\n",
        ")\n",
        "print(\"Пример чередования:\", list(interleaved_dataset.take(9).as_numpy_iterator()))\n",
        "\n",
        "# Кэширование на диске для ускорения последующих эпох\n",
        "# Полезно для больших датасетов с дорогой аугментацией\n",
        "cache_path = \"/tmp/mnist_cache\"\n",
        "cached_dataset = optimized_dataset.cache(cache_path).repeat()\n",
        "# После первого прохода все преобразованные данные будут сохранены в файлы cache_path.*\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Оптимизированный `tf.data` конвейер является обязательным компонентом любого промышленного пайплайна TensorFlow. Правильная настройка `shuffle`, `map`, `batch` и `prefetch` может ускорить тренировку в несколько раз, полностью загрузив вычислительные ресурсы. Использование `tf.data.AUTOTUNE` упрощает настройку, позволяя фреймворку адаптироваться к конкретной аппаратной конфигурации.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Кастомное обучение и продвинутые техники\n",
        "\n",
        "### Теория\n",
        "\n",
        "Хотя высокоуровневый API Keras (`model.fit()`) подходит для большинства стандартных сценариев, существуют задачи, требующие **полного контроля** над процессом обучения. К таким задачам относятся: реализация нетрадиционных алгоритмов оптимизации (например, градиентный спуск с импульсом второго порядка), сложные схемы регуляризации, условная логика на основе внутреннего состояния модели или реализация специализированных техник вроде **gradient clipping** для стабилизации обучения RNN.\n",
        "\n",
        "**Кастомные циклы обучения** в TensorFlow строятся на основе фундаментальных компонентов:\n",
        "*   **`tf.GradientTape`**: для записи вычислений и автоматического дифференцирования.\n",
        "*   **Оптимизаторы** (`tf.keras.optimizers`): для применения вычисленных градиентов к переменным модели.\n",
        "*   **Метрики** (`tf.keras.metrics`): для накопления и вычисления показателей качества в течение эпохи.\n",
        "*   **`@tf.function`**: для компиляции шагов обучения в оптимизированные вычислительные графы, что критично для производительности.\n",
        "\n",
        "Дополнительно, для динамической настройки гиперпараметров используется механизм **расписаний скорости обучения** (*learning rate schedules*), а для интеграции с системами мониторинга — **кастомные callback-и**.\n",
        "\n",
        "### Примеры\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл кастомного обучения с реализацией gradient clipping, динамического расписания скорости обучения и интеграцией с TensorBoard.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Загрузка и подготовка данных\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# Создание датасетов\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
        "\n",
        "# Определение модели\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)  # logits (без softmax)\n",
        "])\n",
        "\n",
        "# === 1. Настройка компонентов обучения ===\n",
        "# Оптимизатор с расписанием скорости обучения (косинусное затухание)\n",
        "initial_learning_rate = 1e-2\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=1000  # Число шагов до конца затухания\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Функция потерь (работает с логитами)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Метрики\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# === 2. Определение шагов обучения с @tf.function ===\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    \"\"\"Один шаг обучения с градиентным клиппингом.\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss = loss_fn(y, logits)\n",
        "    \n",
        "    # Вычисление градиентов\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    \n",
        "    # Gradient clipping: ограничение L2-нормы градиентов значением 1.0\n",
        "    # Это предотвращает взрыв градиентов, стабилизируя обучение\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 1.0) for g in gradients]\n",
        "    \n",
        "    # Применение градиентов\n",
        "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_weights))\n",
        "    \n",
        "    # Обновление метрики\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    \"\"\"Один шаг валидации.\"\"\"\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "\n",
        "# === 3. Кастомный callback для TensorBoard ===\n",
        "class CustomTensorBoardCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Callback для логирования кастомных метрик в TensorBoard.\"\"\"\n",
        "    def __init__(self, log_dir):\n",
        "        super().__init__()\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with self.writer.as_default():\n",
        "            tf.summary.scalar('learning_rate',\n",
        "                              optimizer.learning_rate(optimizer.iterations),\n",
        "                              step=epoch)\n",
        "            tf.summary.scalar('custom_val_acc', logs['val_accuracy'], step=epoch)\n",
        "\n",
        "# === 4. Запуск кастомного цикла обучения ===\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nНачало эпохи {epoch + 1}/{epochs}\")\n",
        "    \n",
        "    # --- Фаза обучения ---\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch, y_batch)\n",
        "        \n",
        "        if step % 100 == 0:\n",
        "            current_lr = optimizer.learning_rate(optimizer.iterations)\n",
        "            print(f\"Эпоха {epoch+1}, Шаг {step}, Потеря: {loss_value:.4f}, LR: {current_lr:.2e}\")\n",
        "    \n",
        "    # --- Фаза валидации ---\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "    \n",
        "    # --- Вывод результатов эпохи ---\n",
        "    train_acc = train_acc_metric.result()\n",
        "    val_acc = val_acc_metric.result()\n",
        "    print(f\"Точность обучения: {train_acc:.4f}, Точность валидации: {val_acc:.4f}\")\n",
        "    \n",
        "    # --- Сброс метрик для следующей эпохи ---\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()\n",
        "    \n",
        "    # Пример вызова callback'а\n",
        "    # custom_callback = CustomTensorBoardCallback('logs/custom')\n",
        "    # custom_callback.on_epoch_end(epoch, {'val_accuracy': val_acc})\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Кастомные циклы обучения предоставляют максимальную гибкость, необходимую для исследований и решения нетривиальных промышленных задач. Оборачивание шагов в `@tf.function` гарантирует, что производительность будет сопоставима с высокоуровневым API Keras. Использование таких техник, как gradient clipping и динамическое расписание LR, часто является разницей между сходящейся и расходящейся моделью.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Развертывание моделей\n",
        "\n",
        "### Теория\n",
        "\n",
        "Переход от обученной модели к её промышленному применению — это отдельная сложная инженерная задача, известная как **сервинг** (*serving*). Она включает в себя решение следующих ключевых проблем:\n",
        "*   **Оптимизация для вывода**: конвертация модели в формат, оптимизированный для инференса (сокращение размера, квантование, специализированные операции).\n",
        "*   **Версионирование**: управление несколькими версиями модели, возможность безопасного отката.\n",
        "*   **Масштабирование**: обработка высокой нагрузки через горизонтальное масштабирование.\n",
        "*   **Мониторинг**: сбор метрик задержки, пропускной способности и качества предсказаний.\n",
        "*   **Целевая платформа**: развертывание на серверах, мобильных устройствах или в веб-браузерах.\n",
        "\n",
        "**TensorFlow** предоставляет унифицированную модель сохранения — **SavedModel** — и набор специализированных инструментов для развёртывания на различных платформах:\n",
        "\n",
        "*   **TensorFlow Serving**: высокопроизводительная система для развертывания моделей на серверах через gRPC/REST API.\n",
        "*   **TensorFlow Lite **(TFLite): фреймворк для запуска моделей на мобильных и встраиваемых устройствах с ограниченными ресурсами, включая поддержку квантования.\n",
        "*   **TensorFlow.js**: библиотека для запуска моделей непосредственно в веб-браузере или на Node.js сервере.\n",
        "\n",
        "### Примеры\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл пост-обработки модели: сохранение в различных форматах, конвертацию в TFLite и применение пост-тренировочного квантования для сжатия модели.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Предположим, у нас есть обученная модель 'model'\n",
        "\n",
        "# === 1. Сохранение и загрузка в формате SavedModel ===\n",
        "# Это стандартный, рекомендуемый формат для продакшена\n",
        "# Он сохраняет архитектуру, веса, трассировку вызовов и даже пользовательские объекты\n",
        "model.save('my_model', save_format='tf')  # или просто model.save('my_model')\n",
        "\n",
        "# Загрузка\n",
        "loaded_model = tf.keras.models.load_model('my_model')\n",
        "# Загруженная модель идентична оригиналу и готова к инференсу\n",
        "\n",
        "# === 2. Конвертация в TensorFlow Lite для мобильных устройств ===\n",
        "# TFLite оптимизирован для CPU и имеет очень маленький рантайм\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# Включение стандартных оптимизаций (в т.ч. квантование весов до float16)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Сохранение бинарной модели TFLite\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# === 3. Продвинутое квантование: Post-Training Quantization (PTQ) ===\n",
        "# Для ещё большего сжатия и ускорения на CPU можно квантовать модель до int8\n",
        "\n",
        "# Генератор репрезентативного датасета (небольшой срез обучающих данных)\n",
        "def representative_dataset_gen():\n",
        "    for i in range(100):\n",
        "        # Возвращаем список тензоров, соответствующих входам модели\n",
        "        yield [tf.random.normal((1, 28, 28, 1), dtype=tf.float32)]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Указываем репрезентативный датасет для калибровки\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "# Задаём целевой тип операций\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Указываем тип данных для входа и выхода\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "with open('quantized_model.tflite', 'wb') as f:\n",
        "    f.write(quantized_tflite_model)\n",
        "\n",
        "print(f\"Размер оригинальной модели: {len(tflite_model) / 1024:.2f} KB\")\n",
        "print(f\"Размер квантованной модели: {len(quantized_tflite_model) / 1024:.2f} KB\")\n",
        "# Квантование до int8 может уменьшить размер модели в 4 раза и ускорить инференс\n",
        "# на CPU до 2-3 раз, с минимальной потерей точности.\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "SavedModel является золотым стандартом для сохранения моделей TensorFlow, обеспечивая полную воспроизводимость. TensorFlow Lite и его квантование — ключевые технологии для переноса моделей на устройства, что критично для приложений реального времени с требованиями к конфиденциальности и низкой задержке.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. TensorFlow Extended (TFX) для MLOps\n",
        "\n",
        "### Теория\n",
        "\n",
        "**TensorFlow Extended **(TFX) — это сквозная платформа с открытым исходным кодом для разработки, развёртывания и мониторинга промышленных пайплайнов машинного обучения. TFX реализует принципы **MLOps**, формализуя полный жизненный цикл модели в виде воспроизводимого, масштабируемого и аудируемого конвейера. Архитектура TFX основана на компонентной модели, где каждый этап пайплайна инкапсулирован в отдельный компонент со строго определённым входом и выходом.\n",
        "\n",
        "Ключевые компоненты TFX:\n",
        "\n",
        "*   **ExampleGen**: Загружает данные из различных источников (CSV, BigQuery, Avro) и разделяет их на обучающую и оценочную выборки.\n",
        "*   **StatisticsGen**: Вычисляет статистику по данным (среднее, дисперсия, гистограммы), которая используется для анализа и генерации схемы.\n",
        "*   **SchemaGen**: Анализирует статистику и автоматически генерирует **схему данных** (schema) — контракт, описывающий ожидаемую структуру и типы данных. Это основа для обнаружения дрейфа.\n",
        "*   **ExampleValidator**: Сравнивает статистику новых данных со схемой, обнаруживая аномалии и дрейф.\n",
        "*   **Transform**: Выполняет предварительную обработку и инжиниринг признаков с использованием Apache Beam для распределённых вычислений. Гарантирует, что одна и та же логика применяется как при обучении, так и при инференсе (устранение Training-Serving Skew).\n",
        "*   **Trainer**: Обучает модель с использованием TensorFlow. Поддерживает распределённое обучение и интеграцию с гиперпараметрическими оптимизаторами.\n",
        "*   **Tuner**: (Опционально) Выполняет оптимизацию гиперпараметров с использованием KerasTuner или других библиотек.\n",
        "*   **Evaluator**: Оценивает качество модели на оценочном наборе, вычисляя продвинутые метрики (в т.ч. для справедливости).\n",
        "*   **InfraValidator**: Проверяет, что модель может быть успешно загружена и выполнена в целевой среде (например, TensorFlow Serving).\n",
        "*   **Pusher**: Разворачивает окончательную версию модели в production-систему (например, в каталог TensorFlow Serving или в Google Cloud AI Platform).\n",
        "\n",
        "TFX-пайплайны могут быть оркестрированы с помощью **Apache Airflow**, **Kubeflow Pipelines** или **Cloud Composer**, обеспечивая надёжность и масштабируемость в облачной среде.\n",
        "\n",
        "### Примеры\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует определение простого, но полнофункционального TFX-пайплайна для задачи классификации. Компоненты `preprocessing.py` и `trainer.py` должны быть реализованы отдельно.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import tfx\n",
        "from tfx import components, dsl\n",
        "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
        "\n",
        "# === 1. Определение функций модулей (должны быть в отдельных файлах) ===\n",
        "\n",
        "# Файл: preprocessing.py\n",
        "# def preprocessing_fn(inputs):\n",
        "#     \"\"\"Функция для компонента Transform.\"\"\"\n",
        "#     outputs = inputs.copy()\n",
        "#     # Пример: нормализация числового признака\n",
        "#     outputs['normalized_feature'] = tft.scale_to_0_1(inputs['feature'])\n",
        "#     return outputs\n",
        "\n",
        "# Файл: trainer.py\n",
        "# def run_fn(fn_args: tfx.components.FnArgs):\n",
        "#     \"\"\"Функция для компонента Trainer.\"\"\"\n",
        "#     # Загрузка данных, построение модели, обучение, сохранение в fn_args.serving_model_dir\n",
        "#     pass\n",
        "\n",
        "# === 2. Определение пайплайна TFX ===\n",
        "def create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str) -> tfx.dsl.Pipeline:\n",
        "    \"\"\"\n",
        "    Создаёт и возвращает определение TFX-пайплайна.\n",
        "    \n",
        "    :param pipeline_name: Уникальное имя пайплайна.\n",
        "    :param pipeline_root: Корневой каталог для артефактов пайплайна.\n",
        "    :param data_root: Каталог с входными данными (CSV файлы).\n",
        "    :return: Объект Pipeline.\n",
        "    \"\"\"\n",
        "    # 1. Загрузка и разделение данных\n",
        "    example_gen = components.CsvExampleGen(\n",
        "        input_base=data_root,\n",
        "        output_config=example_gen_pb2.Output(\n",
        "            split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "                example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
        "                example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)\n",
        "            ])\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # 2. Генерация статистики и схемы\n",
        "    statistics_gen = components.StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "    schema_gen = components.SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
        "    \n",
        "    # 3. Валидация данных (опционально)\n",
        "    # example_validator = components.ExampleValidator(\n",
        "    #     statistics=statistics_gen.outputs['statistics'],\n",
        "    #     schema=schema_gen.outputs['schema']\n",
        "    # )\n",
        "    \n",
        "    # 4. Предварительная обработка признаков\n",
        "    transform = components.Transform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        module_file='preprocessing.py'  # Путь к файлу с функцией preprocessing_fn\n",
        "    )\n",
        "    \n",
        "    # 5. Обучение модели\n",
        "    trainer = components.Trainer(\n",
        "        module_file='trainer.py',  # Путь к файлу с функцией run_fn\n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "        eval_args=trainer_pb2.EvalArgs(num_steps=5000)\n",
        "    )\n",
        "    \n",
        "    # 6. (Опционально) Оценка и валидация инфраструктуры\n",
        "    # evaluator = components.Evaluator(...)\n",
        "    # infra_validator = components.InfraValidator(...)\n",
        "    \n",
        "    # 7. Развертывание модели\n",
        "    pusher = components.Pusher(\n",
        "        model=trainer.outputs['model'],\n",
        "        push_destination=pusher_pb2.PushDestination(\n",
        "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "                base_directory=f'{pipeline_root}/serving_model'\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Сборка всех компонентов в пайплайн\n",
        "    components_list = [\n",
        "        example_gen, statistics_gen, schema_gen,\n",
        "        transform, trainer, pusher\n",
        "        # example_validator, evaluator, infra_validator\n",
        "    ]\n",
        "    \n",
        "    return dsl.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components_list\n",
        "    )\n",
        "\n",
        "# === 3. Запуск пайплайна (например, с помощью Kubeflow) ===\n",
        "# pipeline = create_pipeline(\n",
        "#     pipeline_name='mnist_pipeline',\n",
        "#     pipeline_root='/path/to/pipeline_root',\n",
        "#     data_root='/path/to/data'\n",
        "# )\n",
        "# tfx.orchestration.KubeflowDagRunner().run(pipeline)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "TFX предоставляет промышленно-готовую основу для MLOps. Его компонентная архитектура, строгая типизация артефактов и встроенная поддержка мониторинга данных и модели делают его идеальным выбором для создания надёжных, аудируемых и масштабируемых систем машинного обучения в корпоративной среде. Использование TFX гарантирует, что ML-система будет соответствовать самым высоким стандартам инженерной дисциплины.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "iVyoVezf8LED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 13. Отладка и профилирование\n",
        "\n",
        "### Теория\n",
        "\n",
        "Создание и тренировка сложных моделей глубокого обучения неизбежно сопряжены с возникновением различных проблем: от **численных нестабильностей** (NaN/Inf в градиентах) и **несоответствия форм тензоров** до **низкой производительности** и **аномального поведения сходимости**. Эффективное решение этих проблем требует систематического подхода и использования специализированных инструментов, встроенных в экосистему TensorFlow.\n",
        "\n",
        "Ключевым инструментом для всесторонней отладки и мониторинга является **TensorBoard** — веб-приложение, предоставляющее интерактивную панель для визуализации и анализа всех аспектов тренировки. TensorBoard позволяет:\n",
        "\n",
        "*   **Мониторить метрики в реальном времени**: отслеживать динамику функции потерь, точности и других метрик на обучающем и валидационном наборах, что помогает выявлять переобучение или проблемы со сходимостью.\n",
        "*   **Визуализировать вычислительный граф**: анализировать структуру модели, выявлять неожиданные узлы или ошибки в архитектуре.\n",
        "*   **Профилировать производительность**: определять «узкие места» в конвейере данных или вычислениях на GPU/TPU, что критично для оптимизации времени тренировки.\n",
        "*   **Анализировать распределения параметров и градиентов**: визуализировать гистограммы весов и градиентов, что помогает диагностировать проблемы типа «замирания градиентов» или нестабильного обучения.\n",
        "\n",
        "Для диагностики низкоуровневых проблем TensorFlow предоставляет модуль **`tf.debugging`**, содержащий функции для ассертов, которые проверяют численную корректность, формы тензоров и логические условия во время выполнения. Эти ассерты являются неотъемлемой частью производственного кода, так как они превращают скрытые ошибки в явные исключения с понятными сообщениями.\n",
        "\n",
        "Наконец, для глубокого анализа производительности используется встроенный **TensorFlow Profiler**, который предоставляет детальную разбивку времени, затраченного на каждую операцию, а также информацию об использовании памяти и эффективности конвейера данных.\n",
        "\n",
        "### Примеры\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует интеграцию всех ключевых инструментов отладки: TensorBoard для мониторинга и профилирования, `tf.debugging` для защиты от численных ошибок и встроенного профайлера для анализа производительности.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# === 1. Настройка callback'а TensorBoard для мониторинга и профилирования ===\n",
        "# Логирование в директорию с временной меткой для избежания конфликтов\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    # Включение гистограмм весов и градиентов каждую эпоху\n",
        "    histogram_freq=1,\n",
        "    # Профилирование конкретных батчей (очень ресурсоёмкая операция)\n",
        "    profile_batch='10,15',\n",
        "    # Включение логирования графа вычислений\n",
        "    write_graph=True,\n",
        "    # Включение логирования изображений (полезно для аугментации)\n",
        "    write_images=True\n",
        ")\n",
        "\n",
        "# === 2. Создание модели и данных для примера ===\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Искусственные данные\n",
        "x_train = tf.random.normal((1000, 10))\n",
        "y_train = tf.random.normal((1000, 1))\n",
        "\n",
        "# === 3. Обогащение модели проверками отладки ===\n",
        "def debug_model(x, y_true):\n",
        "    \"\"\"\n",
        "    Функция, интегрирующая проверки tf.debugging в логику модели.\n",
        "    Обычно такие проверки встраиваются непосредственно в кастомный train_step.\n",
        "    \"\"\"\n",
        "    # Проверка на наличие NaN или Inf в входных данных\n",
        "    x = tf.debugging.check_numerics(x, message='Input x contains NaN or Inf')\n",
        "    y_true = tf.debugging.check_numerics(y_true, message='Label y contains NaN or Inf')\n",
        "    \n",
        "    # Проверка ожидаемой формы тензоров\n",
        "    tf.debugging.assert_shapes([\n",
        "        (x, ('N', 10)),      # Ожидаем N строк и 10 столбцов\n",
        "        (y_true, ('N', 1))   # Ожидаем N строк и 1 столбец\n",
        "    ])\n",
        "    \n",
        "    # Проверка логических условий\n",
        "    tf.debugging.assert_greater(tf.size(x), 0, message='Input tensor is empty')\n",
        "    \n",
        "    # Выполнение предсказания\n",
        "    y_pred = model(x, training=True)\n",
        "    return tf.keras.losses.mse(y_true, y_pred)\n",
        "\n",
        "# === 4. Запись трассировки вычислительного графа вручную ===\n",
        "# Этот подход используется, когда нужно профилировать не через model.fit(),\n",
        "# а в кастомном цикле обучения или в автономной функции.\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "# Включение трассировки\n",
        "tf.summary.trace_on(graph=True, profiler=True)\n",
        "\n",
        "# Вызов функции, которую нужно профилировать\n",
        "with tf.GradientTape() as tape:\n",
        "    loss = debug_model(x_train[:32], y_train[:32])\n",
        "grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "# Экспорт трассировки в TensorBoard\n",
        "with writer.as_default():\n",
        "    tf.summary.trace_export(\n",
        "        name=\"debug_model_trace\",\n",
        "        step=0,\n",
        "        profiler_outdir=log_dir\n",
        "    )\n",
        "writer.close()\n",
        "\n",
        "# === 5. Программный запуск и остановка встроенного профайлера ===\n",
        "# Этот метод даёт больше контроля над профилируемым участком кода.\n",
        "tf.profiler.experimental.start(log_dir)\n",
        "\n",
        "# Симуляция тренировки\n",
        "for epoch in range(2):\n",
        "    for i in range(0, len(x_train), 32):\n",
        "        x_batch = x_train[i:i+32]\n",
        "        y_batch = y_train[i:i+32]\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = debug_model(x_batch, y_batch)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        # Потенциально здесь был бы optimizer.apply_gradients\n",
        "\n",
        "# Остановка профилирования и сохранение результатов\n",
        "tf.profiler.experimental.stop()\n",
        "\n",
        "print(f\"Логи сохранены в {log_dir}. Запустите 'tensorboard --logdir {log_dir}' для просмотра.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Интеграция этих инструментов в рабочий процесс является признаком зрелой инженерной практики. `tf.debugging` защищает код от трудноуловимых численных ошибок, а TensorBoard и TensorFlow Profiler превращают «чёрный ящик» тренировки в прозрачную и управляемую систему. Анализ профилей позволяет оптимизировать не только модель, но и конвейер данных (`tf.data`), что часто даёт наибольший прирост производительности.\n",
        "\n",
        "---\n",
        "\n",
        "## 14. Комплексные практические кейсы\n",
        "\n",
        "### Кейс 1: Классификация медицинских изображений\n",
        "\n",
        "Медицинская визуализация предъявляет особые требования к моделям глубокого обучения: необходима не только высокая точность, но и надёжность, интерпретируемость и устойчивость к вариациям в данных (разные аппараты, протоколы). Следующий пайплайн демонстрирует промышленный подход к решению такой задачи.\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Пайплайн включает transfer learning с современной архитектурой EfficientNet, тщательно подобранную аугментацию, кастомные метрики, ориентированные на медицинские задачи (AUC, Precision, Recall), и advanced callback-и для управления тренировкой.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_medical_image_classifier(input_shape=(300, 300, 3), num_classes=5):\n",
        "    \"\"\"\n",
        "    Строит модель для классификации медицинских изображений.\n",
        "    \n",
        "    :param input_shape: Форма входного изображения.\n",
        "    :param num_classes: Число диагностических классов.\n",
        "    :return: Скомпилированная модель Keras.\n",
        "    \"\"\"\n",
        "    # === 1. Специализированная аугментация для медицинских изображений ===\n",
        "    # Медицинские изображения часто имеют низкий контраст и артефакты.\n",
        "    # Аугментация должна быть реалистичной и не искажать диагностическую информацию.\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        layers.RandomRotation(0.15),    # Небольшие вращения\n",
        "        layers.RandomZoom(0.15, 0.15),  # Масштабирование\n",
        "        layers.RandomTranslation(0.1, 0.1), # Сдвиг\n",
        "    ])\n",
        "    \n",
        "    # === 2. Transfer Learning с EfficientNetB3 ===\n",
        "    # EfficientNet обеспечивает отличное соотношение точности и вычислительной сложности.\n",
        "    base_model = tf.keras.applications.EfficientNetB3(\n",
        "        weights='imagenet',         # Предобучение на ImageNet\n",
        "        include_top=False,          # Без исходного классификатора\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    # На начальном этапе замораживаем базовую модель\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # === 3. Построение кастомной \"головы\" классификатора ===\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    # Обязательная предварительная обработка для EfficientNet\n",
        "    x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
        "    x = base_model(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    # Регуляризация для борьбы с переобучением на небольших медицинских датасетах\n",
        "    x = layers.Dropout(0.3, name='top_dropout_1')(x)\n",
        "    x = layers.Dense(512, activation='relu', name='dense_1')(x)\n",
        "    x = layers.BatchNormalization(name='bn_1')(x)\n",
        "    x = layers.Dropout(0.5, name='top_dropout_2')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs, outputs, name='medical_efficientnet')\n",
        "    \n",
        "    # === 4. Компиляция с медицински-релевантными метриками ===\n",
        "    # AUC (Area Under the ROC Curve) часто является ключевой метрикой в диагностике.\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# === 5. Продвинутое управление тренировкой ===\n",
        "# Callback-и настроены на мониторинг AUC, так как это главная метрика.\n",
        "callbacks = [\n",
        "    # Ранняя остановка по улучшению AUC\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Динамическое уменьшение скорости обучения при плато AUC\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Сохранение лучшей модели по AUC\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_medical_model.keras',  # Используем новый формат .keras\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Логирование в CSV для внешнего анализа\n",
        "    tf.keras.callbacks.CSVLogger('training_log.csv')\n",
        "]\n",
        "\n",
        "# === 6. Запуск тренировки ===\n",
        "# Предполагается, что train_dataset и val_dataset — это tf.data.Dataset\n",
        "# model = build_medical_image_classifier()\n",
        "# history = model.fit(\n",
        "#     train_dataset,\n",
        "#     epochs=100,\n",
        "#     validation_data=val_dataset,\n",
        "#     callbacks=callbacks\n",
        "# )\n",
        "\n",
        "# === 7. Fine-tuning (дополнительный этап) ===\n",
        "# После сходимости головы можно разморозить часть базовой модели\n",
        "# для более тонкой настройки под медицинские данные.\n",
        "# base_model.trainable = True\n",
        "# # Замораживаем первые слои, чтобы сохранить общие признаки\n",
        "# for layer in base_model.layers[:-20]:\n",
        "#     layer.trainable = False\n",
        "#\n",
        "# # Компилируем с очень малой скоростью обучения\n",
        "# model.compile(\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "#     loss='sparse_categorical_crossentropy',\n",
        "#     metrics=['accuracy', 'auc']\n",
        "# )\n",
        "#\n",
        "# # Продолжаем обучение\n",
        "# model.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=callbacks)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот кейс иллюстрирует методологию, применяемую в реальных медицинских проектах: использование предобученных моделей, адаптация аугментации под домен, мониторинг клинически значимых метрик и применение fine-tuning для достижения максимальной производительности. Такой подход позволяет достичь высокой диагностической точности даже при ограниченном объёме размеченных данных.\n",
        "\n",
        "---\n",
        "\n",
        "## Дополнительные темы\n",
        "\n",
        "### Federated Learning с TensorFlow\n",
        "\n",
        "**Федеративное обучение **(Federated Learning, FL) — это парадигма машинного обучения, в которой модель обучается на данных, распределённых по множеству клиентов (например, мобильных устройств), **без централизованного сбора этих данных**. Это решает критические проблемы приватности и пропускной способности. **TensorFlow Federated **(TFF) — это фреймворк, предоставляющий высокоуровневый API для симуляции и развёртывания FL-алгоритмов.\n",
        "\n",
        "```python\n",
        "# import tensorflow_federated as tff\n",
        "\n",
        "# def create_keras_model():\n",
        "#     return tf.keras.Sequential([\n",
        "#         tf.keras.layers.Dense(10, activation='relu'),\n",
        "#         tf.keras.layers.Dense(1)\n",
        "#     ])\n",
        "\n",
        "# === Определение федеративного пайплайна ===\n",
        "# def model_fn():\n",
        "#     \"\"\"\n",
        "#     Функция-фабрика для создания модели, совместимой с TFF.\n",
        "#     \"\"\"\n",
        "#     keras_model = create_keras_model()\n",
        "#     return tff.learning.models.from_keras_model(\n",
        "#         keras_model,\n",
        "#         # Спецификация входных данных (должна соответствовать клиентским датасетам)\n",
        "#         input_spec=preprocessed_example_dataset.element_spec,\n",
        "#         loss=tf.keras.losses.MeanSquaredError(),\n",
        "#         metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "#     )\n",
        "\n",
        "# # Построение алгоритма Federated Averaging (FedAvg)\n",
        "# federated_algorithm = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "#     model_fn,\n",
        "#     client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "#     server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
        "# )\n",
        "\n",
        "# # Запуск итерации обучения\n",
        "# # state = federated_algorithm.initialize()\n",
        "# # for round in range(NUM_ROUNDS):\n",
        "# #     state, metrics = federated_algorithm.next(state, federated_train_data)\n",
        "# #     print(f'Round {round}: {metrics}')\n",
        "```\n",
        "\n",
        "### Reinforcement Learning с TF-Agents\n",
        "\n",
        "**TF-Agents** — это библиотека от Google, предоставляющая модульные и эффективные реализации современных алгоритмов **обучения с подкреплением **(Reinforcement Learning, RL). Она интегрирована с TensorFlow и позволяет быстро прототипировать и масштабировать RL-решения.\n",
        "\n",
        "```python\n",
        "# import tf_agents\n",
        "# from tf_agents.agents.dqn import dqn_agent\n",
        "# from tf_agents.networks import sequential\n",
        "\n",
        "# Предположим, окружение env уже создано (например, из Gym)\n",
        "# === Определение Q-сети ===\n",
        "# q_net = sequential.Sequential([\n",
        "#     tf.keras.layers.Dense(100, activation='relu'),\n",
        "#     tf.keras.layers.Dense(50, activation='relu'),\n",
        "#     tf.keras.layers.Dense(\n",
        "#         env.action_spec().maximum - env.action_spec().minimum + 1,\n",
        "#         activation=None\n",
        "#     )\n",
        "# ])\n",
        "\n",
        "# === Создание DQN агента ===\n",
        "# agent = dqn_agent.DqnAgent(\n",
        "#     env.time_step_spec(),\n",
        "#     env.action_spec(),\n",
        "#     q_network=q_net,\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "#     td_errors_loss_fn=tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "# )\n",
        "\n",
        "# agent.initialize()  # Инициализация переменных агента\n",
        "```\n",
        "\n",
        "### Байесовские нейронные сети\n",
        "\n",
        "**Байесовские нейронные сети **(Bayesian Neural Networks, BNN) расширяют классические нейросети, моделируя веса как **вероятностные распределения**, а не детерминированные значения. Это позволяет модели не только делать предсказания, но и оценивать **неопределённость **(uncertainty) этих предсказаний, что критически важно в таких областях, как медицина и автономные системы.\n",
        "\n",
        "```python\n",
        "# === Реализация кастомного байесовского слоя ===\n",
        "# class BayesianDense(layers.Layer):\n",
        "#     \"\"\"\n",
        "#     Байесовский полносвязный слой с вариационным выводом.\n",
        "#     Веса моделируются как гауссианы с обучаемыми средними и дисперсиями.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, units, prior_std=1.0, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self.units = units\n",
        "#         self.prior_std = prior_std\n",
        "#     \n",
        "#     def build(self, input_shape):\n",
        "#         # Параметры апостериорного распределения для весов\n",
        "#         self.kernel_mu = self.add_weight(\n",
        "#             name='kernel_mu',\n",
        "#             shape=(input_shape[-1], self.units),\n",
        "#             initializer='glorot_normal'\n",
        "#         )\n",
        "#         self.kernel_rho = self.add_weight(\n",
        "#             name='kernel_rho',\n",
        "#             shape=(input_shape[-1], self.units),\n",
        "#             initializer='zeros'  # rho = log(1 + exp(rho)) -> sigma\n",
        "#         )\n",
        "#         # Аналогично для bias (опущено для краткости)\n",
        "#     \n",
        "#     def call(self, inputs, training=None):\n",
        "#         # Выборка весов из апостериорного распределения (reparameterization trick)\n",
        "#         kernel_sigma = tf.math.softplus(self.kernel_rho)\n",
        "#         kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape)\n",
        "#         \n",
        "#         # Расчёт KL-дивергенции между апостериорным и априорным распределениями\n",
        "#         # Это служит регуляризатором и частью вариационной нижней границы (ELBO)\n",
        "#         kl_divergence = tf.reduce_sum(\n",
        "#             tf.math.log(self.prior_std / kernel_sigma) +\n",
        "#             (tf.square(kernel_sigma) + tf.square(self.kernel_mu - 0)) / (2 * self.prior_std**2) -\n",
        "#             0.5\n",
        "#         )\n",
        "#         # Добавление KL-потерь как регуляризационного члена к общей потере модели\n",
        "#         self.add_loss(kl_divergence / tf.cast(tf.shape(inputs)[0], tf.float32))\n",
        "#         \n",
        "#         return tf.matmul(inputs, kernel)\n",
        "#\n",
        "# # Использование слоя в модели\n",
        "# bnn_model = tf.keras.Sequential([\n",
        "#     BayesianDense(128, activation='relu'),\n",
        "#     BayesianDense(10)\n",
        "# ])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "TensorFlow представляет собой не просто библиотеку для глубокого обучения, а **целостную, промышленно-ориентированную платформу**, охватывающую полный жизненный цикл машинного обучения — от первоначального исследования и прототипирования до развертывания, мониторинга и управления в условиях высокой нагрузки. Его архитектура тщательно сбалансирована, чтобы обеспечить как **научную гибкость**, необходимую для инноваций, так и **инженерную строгость**, требуемую для создания надёжных систем.\n",
        "\n",
        "Ключевые преимущества TensorFlow в промышленном контексте, продемонстрированные в этом модуле, можно свести к четырём фундаментальным столпам:\n",
        "\n",
        "1.  **Масштабируемость**: Благодаря встроенным стратегиям распределённого обучения (`tf.distribute.Strategy`) и поддержке специализированного оборудования (GPU, TPU), TensorFlow позволяет эффективно масштабировать тренировку на огромные датасеты и сложнейшие архитектуры.\n",
        "2.  **Производительность**: Оптимизация на всех уровнях — от высокоэффективного `tf.data` API, устраняющего узкие места ввода-вывода, до компиляции вычислений в графы через `@tf.function` и использования продвинутых техник профилирования — гарантирует максимальное использование вычислительных ресурсов.\n",
        "3.  **Гибкость**: Единая модель разработки, сочетающая энергичное выполнение для отладки и графовое для продакшена, а также поддержка как высокоуровневых (Keras), так и низкоуровневых (`tf.GradientTape`) интерфейсов, позволяет инженеру выбрать оптимальный уровень абстракции для каждой задачи — от быстрого прототипа до кастомной реализации передового алгоритма.\n",
        "4.  **Интегрированная экосистема**: Инструменты вроде TensorBoard для отладки, TensorFlow Serving/TFLite для развертывания и TensorFlow Extended (TFX) для MLOps формируют сквозной стек, который обеспечивает воспроизводимость, аудируемость и надёжность на всех этапах жизненного цикла модели.\n",
        "\n",
        "Освоение TensorFlow, таким образом, требует не только понимания теоретических основ глубокого обучения, но и глубокого погружения в практические аспекты инженерии машинного обучения. Именно это сочетание теории и практики делает TensorFlow мощнейшим инструментом для создания интеллектуальных систем, способных решать сложнейшие задачи в реальном мире."
      ],
      "metadata": {
        "id": "yXpwj2YB2Tbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 16: Фреймворки для веб-приложений и дашбордов — от прототипа до продакшена\n",
        "\n",
        "## 1. Архитектурный выбор и парадигмы разработки\n",
        "\n",
        "В современной экосистеме Python существует богатый набор инструментов для создания интерактивных веб-интерфейсов, ориентированных на аналитику данных и машинное обучение. Однако простое перечисление фреймворков — Streamlit, Dash, Gradio, FastAPI — не отвечает на главный вопрос, стоящий перед инженером: **какой инструмент выбрать для конкретной задачи и на каком этапе жизненного цикла проекта?** Ответ на этот вопрос лежит не в синтаксисе, а в фундаментальном понимании **архитектурных парадигм**, лежащих в основе каждого решения. Разработка наивного дашборда для внутреннего анализа и создание высоконагруженного API для обслуживания миллионов пользователей — это качественно разные инженерные задачи, требующие разных подходов.\n",
        "\n",
        "### 1.1. Позиционирование Python-фреймворков в стеке Data/ML\n",
        "\n",
        "Все фреймворки для создания веб-приложений в контексте Data Science и ML можно разделить на две принципиально разные категории по их **архитектурному назначению**: **UI-centric** (ориентированные на пользовательский интерфейс) и **API-centric** (ориентированные на программный интерфейс).\n",
        "\n",
        "**UI-centric фреймворки** — Streamlit, Dash, Gradio — спроектированы для того, чтобы **максимально ускорить путь от Python-скрипта до интерактивной визуализации**. Их ключевая ценность — в том, что они абстрагируют разработчика от сложностей веб-разработки: HTML, CSS, JavaScript, клиент-серверного взаимодействия и управления состоянием. Инженер пишет код на чистом Python, используя готовые виджеты (слайдеры, кнопки, графики), и фреймворк автоматически генерирует веб-страницу. Их основная сфера применения — это **быстрое прототипирование, внутренняя аналитика, демонстрация моделей и создание инструментов для data scientists**.\n",
        "\n",
        "**API-centric фреймворки** — прежде всего FastAPI, а также Flask и Django — решают совершенно иную задачу. Они не предоставляют встроенных средств для создания пользовательского интерфейса. Их цель — построить **высокопроизводительный, надёжный и масштабируемый бэкенд**, который обрабатывает бизнес-логику, управляет аутентификацией, взаимодействует с базами данных и предоставляет данные или предсказания моделей через стандартизированные API (обычно REST или GraphQL). В промышленных MLOps-архитектурах эти две категории не конкурируют, а дополняют друг друга, формируя полноценный стек: **UI-centric фреймворк служит фронтендом-клиентом, а API-centric фреймворк — бэкендом-сервером**.\n",
        "\n",
        "### 1.2. Фундаментальное сравнение: UI-centric vs. API-centric\n",
        "\n",
        "#### Streamlit: скорость за счёт контроля\n",
        "**Streamlit** достигает своей знаменитой простоты за счёт уникальной **реактивной модели выполнения**. При любом взаимодействии пользователя (например, перемещении ползунка) весь Python-скрипт выполняется заново **от начала до конца**. Этот подход, называемый «скрипт как приложение» (*script-as-an-app*), чрезвычайно интуитивен для специалиста по данным, привыкшего к линейному выполнению Jupyter Notebook. Однако он вносит фундаментальные ограничения: разработчик теряет прямой контроль над жизненным циклом приложения и должен полагаться на специальные механизмы (кэширование, управление состоянием), чтобы избежать неэффективных повторных вычислений.\n",
        "\n",
        "#### Dash: компонентная модель для сложных дашбордов\n",
        "**Dash**, построенный на базе библиотеки визуализации Plotly, предлагает более традиционную для веб-разработки **компонетно-ориентированную архитектуру с колбэками**. В Dash приложение строится из независимых компонентов (графики, таблицы, виджеты), и взаимодействие между ними осуществляется через функции-обработчики (колбэки), которые срабатывают при изменении входных данных. Этот подход сложнее в освоении и требует больше кода, но он предоставляет **точный контроль над состоянием приложения и возможностью создания сложных, многоуровневых интерфейсов**, предназначенных для конечных пользователей, а не только для аналитиков.\n",
        "\n",
        "#### FastAPI: производительность и масштабируемость для продакшена\n",
        "**FastAPI** представляет собой совершенно иной класс инструментов. Он построен на современных асинхронных возможностях Python (ASGI) и библиотеке Pydantic для валидации данных. Его главные преимущества — **высокая производительность, автоматическая генерация интерактивной документации OpenAPI/Swagger и строгая типизация**. FastAPI не пытается решить задачу создания UI; его задача — быть надёжным, быстрым и безопасным бэкендом, способным обрабатывать тысячи запросов в секунду, управлять аутентификацией (OAuth2, JWT) и надёжно обслуживать ML-модели в продакшене. Его кривая обучения выше, чем у Streamlit, но это инвестиция в промышленную надёжность.\n",
        "\n",
        "> **Таблица 1.1: Сравнительный анализ ключевых Python-фреймворков (UI/API)**\n",
        "\n",
        "| Характеристика | **Streamlit** | **Dash (Plotly)** | **FastAPI** |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Архитектурный фокус** | Интерактивный фронтенд, Прототипирование | Полноценные дашборды, Приложения для пользователей | RESTful API, Бэкенд-сервисы |\n",
        "| **Парадигма разработки** | Реактивный цикл, «Скрипт как приложение» | Компонентно-ориентированный, Колбэки | API-маршрутизация, Асинхронный I/O |\n",
        "| **Управление состоянием** | `st.session_state` | State в аргументах колбэков | Dependency Injection, Глобальные объекты |\n",
        "| **Скорость прототипирования** | Экстремально высокая (минуты) | Средняя (часы) | Низкая (дни, требует отдельного фронтенда) |\n",
        "| **Производительность** | Подходит для легких приложений с низкой нагрузкой | Удовлетворительная для внутренних инструментов | Высокая, асинхронная, масштабируемая |\n",
        "| **ML-интеграция** | Визуализация, Прототипирование моделей | Визуализация, Аналитика | Обслуживание моделей (Инференс, MLOps) |\n",
        "\n",
        "### 1.3. Критерии выбора фреймворка: скорость, сложность и масштабирование\n",
        "\n",
        "Выбор инструмента должен быть обусловлен **конкретной фазой проекта** и его **целевым назначением**, а не субъективными предпочтениями.\n",
        "\n",
        "1.  **Фаза: Прототипирование и Data Exploration**.\n",
        "    *   **Цель**: Быстро продемонстрировать идею, проверить гипотезу, визуализировать корреляции.\n",
        "    *   **Критерий**: Скорость итерации.\n",
        "    *   **Инструмент**: **Streamlit**. Его способность превратить 20 строк кода анализа в интерактивный веб-документ делает его незаменимым на этом этапе. Команда data science может получить обратную связь от стейкхолдеров в течение одного дня.\n",
        "\n",
        "2.  **Фаза: Продукт с высоким UI Control**.\n",
        "    *   **Цель**: Создать polished-продукт для конечных пользователей с детальным контролем над дизайном, брендированием и сложной многошаговой логикой взаимодействия.\n",
        "    *   **Критерий**: Гибкость UI и надёжность состояния.\n",
        "    *   **Инструмент**: **Dash**. Компонентная модель и система колбэков позволяют строить промышленные дашборды, которые могут конкурировать с приложениями, написанными на React или Angular, но при этом остаются на 100% в экосистеме Python.\n",
        "\n",
        "3.  **Фаза: Продакшен API и ML-сервисы**.\n",
        "    *   **Цель**: Обеспечить надёжное, безопасное и масштабируемое обслуживание ML-моделей для миллионов пользователей или других сервисов.\n",
        "    *   **Критерий**: Производительность, безопасность, интеграция с инфраструктурой.\n",
        "    *   **Инструмент**: **FastAPI**. Он становится ядром промышленной MLOps-системы, предоставляя API для инференса, управления моделями, аутентификации и взаимодействия с базами данных.\n",
        "\n",
        "### 1.4. Сценарии гибридных архитектур (Frontend Streamlit + Backend FastAPI)\n",
        "\n",
        "В реальных MLOps-проектах редко можно обойтись одним UI-centric фреймворком до конца жизненного цикла. По мере роста требований к производительности, безопасности, сложности бизнес-логики и количеству пользователей возникает необходимость в **гибридной архитектуре**.\n",
        "\n",
        "В такой архитектуре роли строго разделены:\n",
        "*   **Streamlit или Dash** используется **исключительно как клиент (фронтенд)**. Его задача — представить данные пользователю и собрать его ввод. Он не хранит данные, не выполняет сложные вычисления и не управляет аутентификацией.\n",
        "*   **FastAPI** используется **исключительно как сервер (бэкенд)**. Он управляет всеми «тяжёлыми» задачами: аутентификацией (например, через Google OAuth), работой с базой данных (PostgreSQL, MongoDB), бизнес-логикой и, самое главное, **инференсом ML-моделей**.\n",
        "\n",
        "**Как это работает на практике**: Streamlit-приложение содержит кнопку «Предсказать». При её нажатии Streamlit делает HTTP-запрос (например, с помощью библиотеки `requests`) к заранее известному эндпоинту FastAPI, например, `POST /api/predict`. FastAPI принимает запрос, валидирует входные данные, загружает модель (возможно, из MLflow Model Registry), выполняет предсказание, логирует результат и возвращает ответ в формате JSON. Streamlit получает этот JSON и визуализирует его на странице.\n",
        "\n",
        "Этот паттерн имеет множество преимуществ:\n",
        "*   **Масштабируемость**: FastAPI-сервис можно масштабировать независимо от Streamlit-приложений.\n",
        "*   **Безопасность**: Секреты, ключи и бизнес-логика остаются на сервере.\n",
        "*   **Переиспользуемость**: Один и тот же FastAPI-бэкенд может обслуживать не только Streamlit, но и мобильное приложение, веб-сайт на React или другой внутренний инструмент.\n",
        "*   **MLOps-совместимость**: FastAPI интегрируется с системами оркестрации (Kubernetes), мониторинга (Prometheus) и CI/CD, что невозможно сделать с чистым Streamlit.\n",
        "\n",
        "*Пояснение*: Такой подход естественным образом приводит к созданию **микросервисной архитектуры**, которая является стандартом de facto для современных промышленных систем. Streamlit становится лёгким клиентом, а FastAPI — мощным, независимым сервисом.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Ускоренное прототипирование: Streamlit и Gradio\n",
        "\n",
        "### 2.1. Streamlit: от скрипта до интерактивного дашборда\n",
        "\n",
        "#### 2.1.1. Реактивный цикл выполнения и его последствия\n",
        "\n",
        "Фундаментальная особенность Streamlit — его **реактивная модель выполнения**. В отличие от традиционных веб-фреймворков, где сервер ожидает запрос и отвечает на него, Streamlit при запуске загружает ваш скрипт в память и перезапускает его полностью при каждом событии на стороне клиента (клик, смена слайдера, отправка формы).\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Эта модель делает разработку невероятно простой, но приводит к потенциальным ловушкам. Если в вашем скрипте есть ресурсоёмкая операция (например, загрузка модели или чтение большого файла), она будет выполняться **каждый раз**, когда пользователь взаимодействует с приложением. Это может привести к неудовлетворительной производительности и огромному потреблению памяти, особенно в многопользовательском сценарии.\n",
        "\n",
        "#### 2.1.2. Оптимизация производительности: механизм кэширования\n",
        "\n",
        "Чтобы преодолеть ограничения реактивной модели, Streamlit предоставляет два мощных декоратора кэширования, которые являются ключом к созданию производительных приложений.\n",
        "\n",
        "**Пример: Кэширование ML-модели и данных**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В этом примере мы создадим простое приложение для анализа тональности текста. Без кэширования приложение было бы непригодно для использования из-за времени загрузки модели.\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Кэширование ресурса (модели)\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    Загружает предобученную модель для анализа тональности.\n",
        "    Декоратор @st.cache_resource гарантирует, что эта функция\n",
        "    будет вызвана только ОДИН РАЗ при запуске приложения.\n",
        "    Все пользователи и все сессии будут использовать одну и ту же\n",
        "    разделяемую копию модели, что экономит гигабайты памяти.\n",
        "    \"\"\"\n",
        "    return pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "# 2. Кэширование данных (результатов)\n",
        "@st.cache_data\n",
        "def analyze_sentiment(texts):\n",
        "    \"\"\"\n",
        "    Анализирует тональность списка текстов.\n",
        "    Декоратор @st.cache_data кэширует РЕЗУЛЬТАТ функции.\n",
        "    Если на вход подаются те же самые `texts`, результат\n",
        "    будет взят из кэша, а не пересчитан заново.\n",
        "    \"\"\"\n",
        "    model = load_model()\n",
        "    return model(texts)\n",
        "\n",
        "# Основное тело приложения\n",
        "st.title(\"Анализатор тональности\")\n",
        "user_input = st.text_area(\"Введите текст для анализа:\")\n",
        "\n",
        "if st.button(\"Анализировать\"):\n",
        "    if user_input:\n",
        "        # Вызов кэшированной функции\n",
        "        result = analyze_sentiment([user_input])\n",
        "        label = result[0]['label']\n",
        "        score = result[0]['score']\n",
        "        st.write(f\"Тональность: **{label}** (уверенность: {score:.2%})\")\n",
        "    else:\n",
        "        st.warning(\"Пожалуйста, введите текст.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Благодаря `@st.cache_resource`, модель загружается один раз при старте Streamlit-сервера. Благодаря `@st.cache_data`, если пользователь введёт тот же самый текст повторно, анализ не будет перезапускаться — результат будет взят из кэша. Это превращает потенциально медленное приложение в мгновенно реагирующее. Эти декораторы — не просто оптимизация, а **архитектурный инструмент**, который позволяет «ломать» реактивный цикл и управлять жизненным циклом ресурсов.\n",
        "\n",
        "#### 2.1.3. Управление состоянием сессии (`st.session_state`)\n",
        "\n",
        "Реактивная модель также затрудняет сохранение состояния между перезапусками скрипта. Для решения этой задачи Streamlit предоставляет глобальный объект `st.session_state` — словарь, который персистирует в течение всей сессии одного пользователя.\n",
        "\n",
        "**Пример: Счётчик и многостраничное приложение**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "`st.session_state` идеально подходит для хранения данных, специфичных для пользователя: выбранных фильтров, введённого текста, результатов промежуточных вычислений. Это особенно важно в многостраничных приложениях (MPA), где состояние должно сохраняться при переходе между страницами.\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "\n",
        "# Инициализация состояния при первом запуске\n",
        "if \"counter\" not in st.session_state:\n",
        "    st.session_state[\"counter\"] = 0\n",
        "\n",
        "st.title(\"Счётчик сессии\")\n",
        "\n",
        "# Отображение текущего состояния\n",
        "st.write(f\"Текущее значение счётчика: {st.session_state['counter']}\")\n",
        "\n",
        "# Кнопки для изменения состояния\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    if st.button(\"Увеличить\"):\n",
        "        st.session_state[\"counter\"] += 1\n",
        "with col2:\n",
        "    if st.button(\"Сбросить\"):\n",
        "        st.session_state[\"counter\"] = 0\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Даже несмотря на то, что весь скрипт перезапускается при каждом нажатии кнопки, значение `st.session_state[\"counter\"]` сохраняется, потому что Streamlit управляет этим словарём отдельно от основного потока выполнения. В MPA `st.session_state` является предпочтительным способом передачи данных между страницами, в отличие от попыток импорта кэшированных функций, которые могут привести к непредсказуемому поведению.\n",
        "\n",
        "#### 2.1.4. Создание многостраничных приложений (MPA)\n",
        "\n",
        "Streamlit предоставляет стандартизированный и простой способ создания MPA без необходимости писать сложную логику навигации.\n",
        "\n",
        "**Практическое руководство**:\n",
        "1.  Создайте главный файл приложения, например, `Home.py`.\n",
        "2.  В той же директории создайте папку `pages`.\n",
        "3.  Добавьте в `pages` файлы для каждой страницы, например, `pages/1_📈_Анализ.py`, `pages/2_🤖_Модель.py`.\n",
        "\n",
        "Streamlit автоматически обнаружит файлы в `pages/` и создаст боковую панель навигации. Порядок страниц определяется числом в начале имени файла. Иконки и названия берутся из оставшейся части имени (эмодзи в названии файла отображаются как иконки).\n",
        "\n",
        "Для более сложных сценариев (например, скрытие страниц от неавторизованных пользователей) можно использовать файл конфигурации `.streamlit/pages.toml`.\n",
        "\n",
        "### 2.2. Gradio: Быстрые ML-демо и работа с мультимодальными данными\n",
        "\n",
        "**Gradio** — это специализированный фреймворк, чья ниша — **максимально быстрое создание веб-демонстраций для ML-моделей**. Он особенно популярен на платформе Hugging Face Spaces.\n",
        "\n",
        "#### 2.2.1. Gradio Blocks как низкоуровневый API\n",
        "\n",
        "Хотя простейшие демо можно создать с помощью класса `gr.Interface`, для сложных сценариев используется **`gr.Blocks`** — низкоуровневый API, дающий полный контроль над компоновкой и логикой.\n",
        "\n",
        "**Пример: Простое приложение с Blocks**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "`Blocks` позволяет строить интерфейсы, похожие на веб-страницы, с произвольным размещением элементов в рядах, колонках и вкладках.\n",
        "\n",
        "```python\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name, is_morning):\n",
        "    greeting = \"Доброе утро\" if is_morning else \"Привет\"\n",
        "    return f\"{greeting}, {name}!\"\n",
        "\n",
        "with gr.Blocks(title=\"Приветствие\") as demo:\n",
        "    gr.Markdown(\"## Добро пожаловать в демо Gradio!\")\n",
        "    with gr.Row():\n",
        "        name_input = gr.Textbox(label=\"Ваше имя\")\n",
        "        morning_checkbox = gr.Checkbox(label=\"Утро?\")\n",
        "    output = gr.Textbox(label=\"Приветствие\")\n",
        "    greet_btn = gr.Button(\"Поздороваться\")\n",
        "    \n",
        "    # Связывание события с функцией\n",
        "    greet_btn.click(\n",
        "        fn=greet,\n",
        "        inputs=[name_input, morning_checkbox],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "demo.launch()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "`gr.Blocks` предоставляет императивный, но очень гибкий способ создания UI. Он ближе по духу к традиционной веб-разработке, чем `Interface`, и позволяет создавать интерфейсы, недоступные в Streamlit без сложных костылей.\n",
        "\n",
        "#### 2.2.2. Мультимодальные возможности\n",
        "\n",
        "Ключевое преимущество Gradio — его встроенные компоненты для работы с **мультимодальными данными**. Компонент `gr.ChatInterface` или `gr.MultimodalTextbox` позволяет пользователю в одном чате отправлять текст, изображения, аудио и видео.\n",
        "\n",
        "**Пример: Мультимодальный чат-бот**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Это идеальный инструмент для демонстрации современных моделей вроде GPT-4V или LLaVA, которые принимают на вход и текст, и изображения.\n",
        "\n",
        "```python\n",
        "import gradio as gr\n",
        "\n",
        "def multimodal_chat(message, history):\n",
        "    # message - это словарь, который может содержать 'text' и 'files'\n",
        "    text = message.get(\"text\", \"\")\n",
        "    files = message.get(\"files\", [])\n",
        "    # Здесь была бы логика модели, обрабатывающей текст и файлы\n",
        "    response = f\"Вы отправили текст: '{text}' и {len(files)} файл(ов).\"\n",
        "    return response\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    multimodal_chat,\n",
        "    multimodal=True,  # Включает поддержку файлов\n",
        "    title=\"Мультимодальный чат-бот\"\n",
        ")\n",
        "demo.launch()\n",
        "```\n",
        "\n",
        "#### 2.2.3. Интеграция с Hugging Face\n",
        "\n",
        "Gradio и Hugging Face тесно интегрированы. Любой Streamlit- или Gradio-демо можно развернуть на Hugging Face Spaces за считанные минуты. Для Gradio это особенно просто: достаточно создать репозиторий типа «Space» и загрузить в него файл `app.py` с вашим кодом. Платформа автоматически соберёт и запустит его, предоставив публичный URL. Это делает Gradio de facto стандартом для демонстрации и распространения ML-моделей в исследовательском сообществе.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Выбор фреймворка для создания веб-интерфейса в Data Science и ML — это не вопрос синтаксиса, а **архитектурное решение**. **Streamlit и Gradio** — это мощные инструменты для **ускорения прототипирования и демонстрации**, позволяющие специалисту по данным сосредоточиться на своей предметной области. **FastAPI** — это промышленный инструмент для построения **надёжных, масштабируемых и безопасных бэкендов**, который является краеугольным камнем MLOps-инфраструктуры. Понимание их сильных и слабых сторон, а также грамотное их сочетание в гибридных архитектурах, является признаком зрелого инженерного подхода и ключом к успешному переходу от исследовательского прототипа к промышленному продукту."
      ],
      "metadata": {
        "id": "HbL3d2OD2Thf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Продакшен-дашборды: Dash и сложная интерактивность\n",
        "\n",
        "В то время как Streamlit превосходит в скорости создания простых прототипов, **Dash** (от Plotly) выделяется как инструмент для построения **промышленно-готовых, сложных дашбордов и веб-приложений**, предназначенных для конечных пользователей. Его архитектура, основанная на компонентах и системе колбэков, предоставляет разработчику точный контроль над состоянием приложения и логикой взаимодействия, что делает его выбором для сценариев, где требуется детальная настройка UI/UX, сложная вложенная логика и высокая надёжность.\n",
        "\n",
        "### 3.1. Архитектура Dash и принцип работы колбэков\n",
        "\n",
        "Dash строит мост между Python и современным веб-фреймворком React, полностью скрывая сложность JavaScript от разработчика. Весь UI создается с помощью Python-компонентов, которые являются обёртками над HTML-элементами и их поведением.\n",
        "\n",
        "Архитектура любого Dash-приложения состоит из двух фундаментальных частей:\n",
        "\n",
        "1.  **`app.layout`**: Это статическое дерево компонентов, определяющее структуру страницы. Оно задаётся один раз при запуске и описывает, какие элементы (графики, таблицы, кнопки, поля ввода) присутствуют на странице и как они организованы. Компоненты `html.Div`, `dcc.Graph`, `dcc.Dropdown` и т.д. являются строительными блоками этого layout.\n",
        "2.  **Колбэки (`@callback`)**: Это функции на чистом Python, которые определяют **динамическое поведение** приложения. Каждый колбэк декларативно связывает **входные данные** (изменения свойств компонентов) с **выходными данными** (обновления свойств других компонентов).\n",
        "\n",
        "**Важный нюанс поведения**:\n",
        "При первом открытии приложения браузером Dash не просто отображает статический `layout`. Он автоматически **вызывает все зарегистрированные колбэки** с их начальными (default) значениями входов. Это необходимо для того, чтобы **заполнить все динамические компоненты** (графики, таблицы) актуальными данными сразу при загрузке страницы, обеспечивая целостный и немедленно полезный пользовательский опыт. Это поведение является ключевым для понимания логики инициализации в Dash.\n",
        "\n",
        "### 3.2. Детальный разбор механизма Callbacks: Input, Output, State\n",
        "\n",
        "Мощь и гибкость Dash заключаются в точной настройке потока данных между компонентами через три ключевых декларатора.\n",
        "\n",
        "*   **`Output(component_id, component_property)`** определяет, **какое свойство какого компонента** будет обновлено по результату выполнения колбэка. Например, `Output('graph', 'figure')` указывает, что результат функции будет использован для обновления свойства `figure` компонента с `id='graph'`.\n",
        "*   **`Input(component_id, component_property)`** определяет, **изменение какого свойства какого компонента** служит **триггером** для запуска колбэка. Например, `Input('dropdown', 'value')` означает, что функция будет вызвана каждый раз, когда пользователь выберет новое значение в выпадающем списке.\n",
        "*   **`State(component_id, component_property)`** позволяет **прочитать текущее значение свойства компонента** без превращения этого свойства в триггер. Это критически важный инструмент для оптимизации и контроля над логикой выполнения.\n",
        "\n",
        "**Пример: Архитектура с кнопкой \"Рассчитать\"**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Во многих аналитических приложениях расчёт (например, ML-инференс или сложная агрегация) является ресурсоёмкой операцией. Запускать его при каждом изменении любого из десятка параметров было бы неэффективно и раздражающе для пользователя. Решение — использовать `State` для сбора всех параметров и `Input` только для кнопки \"Рассчитать\".\n",
        "\n",
        "```python\n",
        "from dash import Dash, dcc, html, Input, Output, State, callback\n",
        "\n",
        "app = Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Ресурсоёмкий калькулятор\"),\n",
        "    dcc.Dropdown(\n",
        "        id='model-dropdown',\n",
        "        options=[{'label': 'Модель A', 'value': 'A'}, {'label': 'Модель B', 'value': 'B'}],\n",
        "        value='A'\n",
        "    ),\n",
        "    dcc.Slider(id='threshold-slider', min=0, max=1, value=0.5, step=0.1),\n",
        "    html.Button('Рассчитать', id='calculate-button', n_clicks=0),\n",
        "    html.Div(id='result-output')\n",
        "])\n",
        "\n",
        "@callback(\n",
        "    Output('result-output', 'children'),\n",
        "    # Единственный триггер — нажатие кнопки\n",
        "    Input('calculate-button', 'n_clicks'),\n",
        "    # Все параметры собираются как State\n",
        "    State('model-dropdown', 'value'),\n",
        "    State('threshold-slider', 'value'),\n",
        "    # Предотвращаем запуск при первом рендере (n_clicks=0)\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def run_expensive_calculation(n_clicks, model_choice, threshold):\n",
        "    \"\"\"\n",
        "    Эта функция запускается ТОЛЬКО при нажатии кнопки.\n",
        "    Все входные параметры (model_choice, threshold) берутся из состояния,\n",
        "    а не являются триггерами.\n",
        "    \"\"\"\n",
        "    if n_clicks is None:\n",
        "        return \"Нажмите 'Рассчитать'.\"\n",
        "    \n",
        "    # Здесь мог бы быть вызов ML-модели или сложные вычисления\n",
        "    result = f\"Модель: {model_choice}, Порог: {threshold:.2f}, Результат: УСПЕХ!\"\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот паттерн является **золотым стандартом** для построения производительных и удобных для пользователя Dash-приложений. Он предотвращает множество ненужных вызовов к бэкенду и даёт пользователю полный контроль над моментом запуска расчёта. Колбэки в Dash также могут иметь **множественные `Output`**, что позволяет одной функции обновлять сразу несколько компонентов (например, график и сводную таблицу), сохраняя согласованность состояния интерфейса.\n",
        "\n",
        "### 3.3. Реализация сложных пользовательских сценариев: Цепочки колбэков (Chained Callbacks)\n",
        "\n",
        "Самая мощная особенность Dash — это возможность строить **цепочки колбэков**, где выход одного колбэка становится входом для другого. Это позволяет создавать динамические, зависимые интерфейсы, такие как каскадные выпадающие списки или многошаговые формы.\n",
        "\n",
        "**Пример: Каскадный выбор страны и города**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В этом примере выбор страны в первом выпадающем списке динамически обновляет список доступных городов во втором списке. Оба значения затем используются для отображения финального результата.\n",
        "\n",
        "```python\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output\n",
        "\n",
        "# Фиктивная база данных\n",
        "CITY_DATA = {\n",
        "    'USA': ['New York', 'Los Angeles', 'Chicago'],\n",
        "    'Canada': ['Toronto', 'Vancouver', 'Montreal'],\n",
        "    'Germany': ['Berlin', 'Munich', 'Hamburg']\n",
        "}\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H2(\"Выберите локацию\"),\n",
        "    dcc.Dropdown(\n",
        "        id='country-dropdown',\n",
        "        options=[{'label': k, 'value': k} for k in CITY_DATA.keys()],\n",
        "        value='USA'\n",
        "    ),\n",
        "    dcc.Dropdown(id='city-dropdown'),\n",
        "    html.Div(id='confirmation')\n",
        "])\n",
        "\n",
        "# Колбэк 1: Обновление списка городов на основе выбора страны\n",
        "@app.callback(\n",
        "    Output('city-dropdown', 'options'),\n",
        "    Output('city-dropdown', 'value'),\n",
        "    Input('country-dropdown', 'value')\n",
        ")\n",
        "def set_cities_options(selected_country):\n",
        "    cities = CITY_DATA[selected_country]\n",
        "    return [{'label': c, 'value': c} for c in cities], cities[0]\n",
        "\n",
        "# Колбэк 2: Отображение подтверждения на основе выбора города и страны\n",
        "@app.callback(\n",
        "    Output('confirmation', 'children'),\n",
        "    Input('country-dropdown', 'value'),\n",
        "    Input('city-dropdown', 'value')\n",
        ")\n",
        "def update_confirmation(selected_country, selected_city):\n",
        "    return f\"Вы выбрали: {selected_city}, {selected_country}\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Такая архитектура является **стандартом де-факто** для построения сложных фильтров и форм в веб-приложениях. Dash делает её реализацию элегантной и декларативной, полностью на Python.\n",
        "\n",
        "### 3.4. Улучшение UI/UX с помощью Dash Bootstrap Components (DBC)\n",
        "\n",
        "Базовые компоненты Dash функциональны, но для создания современных, профессионально выглядящих приложений требуется более продвинутый UI-фреймворк. **Dash Bootstrap Components (DBC)** решает эту задачу, предоставляя Python-обёртки для всех компонентов популярного CSS-фреймворка Bootstrap.\n",
        "\n",
        "**Пример: Динамическое сворачивание контента**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Компонент `dbc.Collapse` позволяет создавать интерактивные аккордеоны и скрываемые панели, что критично для экономии места и организации сложного контента.\n",
        "\n",
        "```python\n",
        "import dash\n",
        "from dash import html, Input, Output, callback\n",
        "import dash_bootstrap_components as dbc\n",
        "\n",
        "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    dbc.Button(\"Показать/Скрыть детали\", id=\"collapse-button\", className=\"mb-3\"),\n",
        "    dbc.Collapse(\n",
        "        dbc.Card(dbc.CardBody(\"Это подробная аналитика, которая появляется и исчезает.\")),\n",
        "        id=\"collapse\",\n",
        "        is_open=False,\n",
        "    ),\n",
        "], className=\"mt-3\")\n",
        "\n",
        "@callback(\n",
        "    Output(\"collapse\", \"is_open\"),\n",
        "    Input(\"collapse-button\", \"n_clicks\"),\n",
        "    # Используем State для чтения текущего состояния без триггера\n",
        "    dash.State(\"collapse\", \"is_open\")\n",
        ")\n",
        "def toggle_collapse(n, is_open):\n",
        "    if n:\n",
        "        return not is_open\n",
        "    return is_open\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(debug=True)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Шаблон с `n_clicks` в качестве `Input` и `is_open` в качестве `State` является каноническим для реализации переключаемого поведения. DBC предоставляет сотни таких компонентов — от навигационных панелей и карточек до модальных окон и прогресс-баров, что позволяет создавать приложения, неотличимые от тех, что написаны на чистом React.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Высокопроизводительный Backend: FastAPI для ML-сервисов\n",
        "\n",
        "В то время как UI-centric фреймворки решают задачу представления, **FastAPI** решает задачу **масштабируемого, безопасного и производительного обслуживания**. Он является ядром промышленных MLOps-систем, предоставляя надёжный API для инференса, управления данными и интеграции с другими сервисами.\n",
        "\n",
        "### 4.1. ASGI, Uvicorn, Gunicorn: Почему это критично для продакшена\n",
        "\n",
        "Производительность FastAPI основана на современной архитектурной спецификации **ASGI** (Asynchronous Server Gateway Interface). Это кардинальное отличие от устаревших синхронных фреймворков на базе WSGI (Flask, Django до v3.0).\n",
        "\n",
        "**Преимущества ASGI**:\n",
        "В асинхронной среде один рабочий процесс (worker) может обрабатывать **множество запросов одновременно**. Когда запрос сталкивается с операцией I/O (ожидание ответа от БД, вызов внешнего API, чтение файла), вместо того чтобы блокировать весь процесс, он **\"усыпляется\"** и отдаёт управление другому запросу. После завершения I/O он возобновляется. Это позволяет одному процессу эффективно обслуживать сотни или тысячи параллельных подключений, что идеально подходит для **I/O-bound задач**, таких как большинство ML-сервисов.\n",
        "\n",
        "**Продакшен-стек**:\n",
        "Для развёртывания в продакшене FastAPI **никогда** не запускается напрямую. Стандартная и рекомендуемая архитектура использует:\n",
        "*   **Uvicorn** — это быстрый ASGI-сервер на Python, который непосредственно запускает ваше FastAPI-приложение.\n",
        "*   **Gunicorn** — это менеджер процессов (master process), который управляет **пулом рабочих процессов Uvicorn**.\n",
        "\n",
        "Gunicorn отвечает за создание, мониторинг и автоматический перезапуск \"упавших\" воркеров, обеспечивая отказоустойчивость и полное использование всех ядер CPU сервера. Этот стек (`Gunicorn` + `UvicornWorker`) является промышленным стандартом для Python-бэкендов.\n",
        "\n",
        "### 4.2. Строгая типизация и валидация данных с помощью Pydantic\n",
        "\n",
        "FastAPI тесно интегрирован с библиотекой **Pydantic**, которая обеспечивает **строгую типизацию и автоматическую валидацию** всех входящих и исходящих данных.\n",
        "\n",
        "**Пример: Создание надёжного API для инференса**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Pydantic-модели служат **контрактом** между клиентом и сервером. Они гарантируют, что сервер получит именно те данные, на которые он рассчитывает.\n",
        "\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "app = FastAPI(title=\"ML-сервис прогнозирования\")\n",
        "\n",
        "class PredictionRequest(BaseModel):\n",
        "    \"\"\"\n",
        "    Pydantic-модель для входящего запроса.\n",
        "    Автоматически валидирует типы и применяет ограничения.\n",
        "    \"\"\"\n",
        "    features: List[float] = Field(..., description=\"Вектор признаков\")\n",
        "    model_version: str = Field(\"v1\", regex=r\"v\\d+\")\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    \"\"\"\n",
        "    Pydantic-модель для исходящего ответа.\n",
        "    Гарантирует, что клиент получит структурированный ответ.\n",
        "    \"\"\"\n",
        "    prediction: float\n",
        "    probability: float = Field(..., ge=0, le=1)\n",
        "    model_version: str\n",
        "\n",
        "# Мок-функция модели\n",
        "def fake_model_predict(features: List[float]) -> tuple:\n",
        "    # В реальности здесь был бы вызов загруженной модели\n",
        "    score = sum(features) / len(features)\n",
        "    return score, 0.95\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict(request: PredictionRequest):\n",
        "    \"\"\"\n",
        "    Эндпоинт для предсказания.\n",
        "    FastAPI автоматически:\n",
        "    1. Распарсит JSON-тело запроса.\n",
        "    2. Валидирует его против PredictionRequest.\n",
        "    3. Преобразует его в объект request.\n",
        "    4. Валидирует и сериализует ответ в PredictionResponse.\n",
        "    \"\"\"\n",
        "    pred, prob = fake_model_predict(request.features)\n",
        "    return PredictionResponse(\n",
        "        prediction=pred,\n",
        "        probability=prob,\n",
        "        model_version=request.model_version\n",
        "    )\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Если клиент отправит запрос с `features` в виде строки вместо списка чисел, FastAPI немедленно вернёт понятную ошибку `422 Unprocessable Entity` с описанием проблемы. Эта строгая валидация — **фундамент безопасности и надёжности** API. Кроме того, Pydantic автоматически генерирует **документацию OpenAPI**, которая доступна по адресам `/docs` (Swagger UI) и `/redoc`.\n",
        "\n",
        "### 4.3. Управление ресурсами: Однократная загрузка ML-моделей (LIFESPAN-функции)\n",
        "\n",
        "Загрузка ML-модели — это ресурсоёмкая операция, которую **нельзя выполнять при каждом запросе**. В FastAPI это решается с помощью **функций жизненного цикла (lifespan)**.\n",
        "\n",
        "**Пример: Загрузка модели при старте приложения**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Модель загружается один раз во время запуска сервера и хранится в памяти, готовая к мгновенному использованию в обработчиках запросов.\n",
        "\n",
        "```python\n",
        "from contextlib import asynccontextmanager\n",
        "from fastapi import FastAPI\n",
        "import joblib\n",
        "\n",
        "# Глобальная переменная для хранения модели\n",
        "ml_model = {}\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    \"\"\"\n",
        "    Контекстный менеджер, управляющий жизненным циклом приложения.\n",
        "    Выполняется ДО запуска сервера (startup) и ПОСЛЕ его остановки (shutdown).\n",
        "    \"\"\"\n",
        "    # --- Startup ---\n",
        "    print(\"Загрузка ML-модели...\")\n",
        "    ml_model[\"model\"] = joblib.load(\"model.joblib\")  # Загрузка из файла\n",
        "    yield  # Приложение работает здесь\n",
        "    # --- Shutdown ---\n",
        "    print(\"Очистка ресурсов...\")\n",
        "    ml_model.clear()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"model_loaded\": \"model\" in ml_model}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(features: List[float]):\n",
        "    # Модель уже загружена и доступна в глобальной переменной\n",
        "    prediction = ml_model[\"model\"].predict([features])\n",
        "    return {\"prediction\": prediction[0].item()}\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот паттерн гарантирует, что задержка на инференс будет минимальной, так как вся \"тяжёлая\" работа выполнена заранее. Глобальный словарь `ml_model` безопасен для использования в асинхронной среде, так как чтение из него является операцией только для чтения.\n",
        "\n",
        "### 4.4. Архитектура Dependency Injection (DI) в FastAPI\n",
        "\n",
        "Система **внедрения зависимостей (Dependency Injection, DI)** через функцию `Depends()` — один из самых мощных и элегантных механизмов FastAPI. Она позволяет декларативно описывать, какие ресурсы или логика необходимы для выполнения конкретного эндпоинта.\n",
        "\n",
        "**Примеры применения DI**:\n",
        "\n",
        "1.  **Повторное использование логики**: Параметры пагинации (`skip`, `limit`) можно вынести в зависимость и использовать в десятках эндпоинтов.\n",
        "2.  **Безопасность**: Проверка JWT-токена или прав доступа оформляется как зависимость. Если проверка не пройдена, эндпоинт даже не вызывается.\n",
        "3.  **Инжекция ресурсов**: Загруженная в `lifespan` модель может быть \"впрыснута\" прямо в аргументы функции эндпоинта.\n",
        "\n",
        "**Пример: DI для инжекции модели**\n",
        "\n",
        "```python\n",
        "from fastapi import Depends\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Зависимость для получения модели из глобального хранилища.\"\"\"\n",
        "    return ml_model[\"model\"]\n",
        "\n",
        "@app.post(\"/predict-di\")\n",
        "async def predict_with_di(\n",
        "    features: List[float],\n",
        "    model = Depends(get_model)  # Модель автоматически \"впрыскивается\" сюда\n",
        "):\n",
        "    prediction = model.predict([features])\n",
        "    return {\"prediction\": prediction[0].item()}\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Код эндпоинта становится **чистым, модульным и легко тестируемым**, так как он не зависит от глобального состояния напрямую. Зависимости могут иметь свои зависимости, создавая сложные, но управляемые цепочки ресурсов. Это является признаком зрелой, промышленной архитектуры.\n",
        ""
      ],
      "metadata": {
        "id": "hyEulrhC2UxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Отладка, Оптимизация и MLOps: Переход в Продакшен\n",
        "\n",
        "Переход от рабочего прототипа на локальной машине к надёжной, масштабируемой и защищённой системе в продакшене — это не просто техническая задача развёртывания, а фундаментальный сдвиг в инженерной парадигме. Он требует внедрения целого набора практик, охватывающих архитектуру, безопасность, надёжность и наблюдаемость. Ни один из предыдущих этапов — от выбора фреймворка до написания логики — не имеет значения, если система не может выдержать нагрузку реального мира.\n",
        "\n",
        "### 5.1. Управление длительными задачами и асинхронность\n",
        "\n",
        "Одна из самых частых ошибок при создании веб-интерфейсов для ML — это попытка выполнить ресурсоёмкую операцию (например, инференс на большой модели, генерация отчёта или ETL-процесс) **синхронно** внутри HTTP-цикла запроса/ответа. Если такая операция занимает более 1–2 секунд, это приводит к катастрофическим последствиям:\n",
        "\n",
        "*   **Тайм-ауты**: API-шлюзы (Nginx, AWS API Gateway) и клиенты (браузеры) имеют ограниченное время ожидания ответа. Превышение этого времени приводит к ошибке `504 Gateway Timeout`.\n",
        "*   **Блокирование воркеров**: Каждый воркер FastAPI (или любого другого WSGI/ASGI-сервера) может обрабатывать одновременно ограниченное число запросов. Синхронная блокирующая операция «съедает» один воркер на всё время своего выполнения, что быстро приводит к исчерпанию пула воркеров и отказу в обслуживании новых запросов.\n",
        "*   **Плохой пользовательский опыт**: Пользователь видит «зависший» интерфейс без возможности отменить операцию или понять её статус.\n",
        "\n",
        "**Решение**: **Вынести длительную операцию из HTTP-цикла** в фоновую очередь задач.\n",
        "\n",
        "**Стандартная архитектура**: Интеграция **FastAPI** с **Celery** и **Redis** (или RabbitMQ).\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Celery — это распределённая система для обработки асинхронных задач. Redis в этой связке выступает в двух ролях: **брокер сообщений** (очередь задач) и **бэкенд результатов** (хранилище статусов и выходных данных). FastAPI принимает запрос и мгновенно передаёт задачу в очередь, освобождая HTTP-воркер. Отдельный процесс Celery-Worker постоянно опрашивает очередь и выполняет задачи в фоне. Клиент, получив `Task ID`, может периодически опрашивать сервер для проверки готовности результата.\n",
        "\n",
        "**Пример: Асинхронный ML-инференс с FastAPI и Celery**\n",
        "\n",
        "```python\n",
        "# === Файл: celery_app.py ===\n",
        "from celery import Celery\n",
        "\n",
        "# Создание Celery-приложения с Redis в качестве брокера и бэкенда\n",
        "celery_app = Celery(\n",
        "    'ml_tasks',\n",
        "    broker='redis://localhost:6379/0',\n",
        "    backend='redis://localhost:6379/0'\n",
        ")\n",
        "\n",
        "# Конфигурация для надёжности\n",
        "celery_app.conf.update(\n",
        "    task_acks_late=True,      # Подтверждение после выполнения (не до)\n",
        "    task_reject_on_worker_lost=True, # Повтор при падении воркера\n",
        "    task_track_started=True   # Возможность отслеживать статус \"STARTED\"\n",
        ")\n",
        "\n",
        "# === Файл: tasks.py ===\n",
        "from celery_app import celery_app\n",
        "from your_ml_model import load_model, predict  # Ваши функции\n",
        "\n",
        "# Глобальная переменная для модели (инициализируется при запуске воркера)\n",
        "_model = None\n",
        "\n",
        "@celery_app.task(bind=True)\n",
        "def run_ml_inference(self, features: list):\n",
        "    \"\"\"\n",
        "    Celery-задача для выполнения ML-инференса.\n",
        "    `bind=True` позволяет получить доступ к объекту задачи `self`.\n",
        "    \"\"\"\n",
        "    global _model\n",
        "    if _model is None:\n",
        "        _model = load_model(\"path/to/model.joblib\")\n",
        "    \n",
        "    # Обновление статуса задачи (опционально)\n",
        "    self.update_state(state='PROCESSING', meta={'current': 50, 'total': 100})\n",
        "    \n",
        "    # Выполнение инференса\n",
        "    result = predict(_model, features)\n",
        "    return {\"prediction\": result}\n",
        "\n",
        "# === Файл: main.py (FastAPI) ===\n",
        "from fastapi import FastAPI, BackgroundTasks\n",
        "from pydantic import BaseModel\n",
        "from tasks import run_ml_inference\n",
        "from celery.result import AsyncResult\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class InferenceRequest(BaseModel):\n",
        "    features: list[float]\n",
        "\n",
        "@app.post(\"/predict-async\")\n",
        "async def predict_async(request: InferenceRequest):\n",
        "    \"\"\"\n",
        "    Эндпоинт для асинхронного предсказания.\n",
        "    Мгновенно возвращает ID задачи.\n",
        "    \"\"\"\n",
        "    task = run_ml_inference.delay(request.features)\n",
        "    return {\"task_id\": task.id, \"status\": \"Task received\"}\n",
        "\n",
        "@app.get(\"/task-status/{task_id}\")\n",
        "async def get_task_status(task_id: str):\n",
        "    \"\"\"\n",
        "    Эндпоинт для проверки статуса задачи.\n",
        "    \"\"\"\n",
        "    task_result = AsyncResult(task_id, app=run_ml_inference.app)\n",
        "    if task_result.state == 'PENDING':\n",
        "        response = {\"status\": \"Pending...\"}\n",
        "    elif task_result.state == 'PROCESSING':\n",
        "        response = {\"status\": \"Processing...\", \"meta\": task_result.info}\n",
        "    elif task_result.state == 'FAILURE':\n",
        "        response = {\"status\": \"Error\", \"error\": str(task_result.info)}\n",
        "    else: # SUCCESS\n",
        "        response = {\"status\": \"Completed\", \"result\": task_result.result}\n",
        "    return response\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Эта архитектура решает все проблемы синхронного выполнения:\n",
        "1.  **FastAPI-воркеры** освобождаются немедленно.\n",
        "2.  **Celery-воркеры** масштабируются независимо и специализируются на выполнении тяжёлой логики.\n",
        "3.  **Пользователь** получает немедленный ответ и может отслеживать прогресс.\n",
        "4.  **Надёжность** обеспечивается механизмами `acks_late` и `task_reject_on_worker_lost`, которые гарантируют, что задача не будет потеряна даже при падении воркера.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2. Продакшен-развертывание и масштабирование\n",
        "\n",
        "Переход в продакшен требует стандартизации среды выполнения и автоматизации управления.\n",
        "\n",
        "#### Контейнеризация и Kubernetes\n",
        "\n",
        "**Docker** является фундаментальным инструментом для создания **воспроизводимой и изолированной среды**. Он позволяет упаковать не только ваш код, но и все зависимости (конкретные версии `scikit-learn`, `joblib`, `uvicorn`), системные библиотеки и конфигурационные файлы в один образ. Это гарантирует, что приложение будет работать одинаково на машине разработчика, в CI/CD-пайплайне и в продакшене.\n",
        "\n",
        "Для управления сложными системами, состоящими из множества взаимодействующих сервисов (UI-фронтенд на Streamlit, API-бэкенд на FastAPI, Celery-воркеры, Redis, PostgreSQL), используется система оркестрации **Kubernetes (K8s)**.\n",
        "\n",
        "K8s предоставляет мощные абстракции:\n",
        "*   **Pods**: Группы контейнеров, которые развертываются и масштабируются вместе.\n",
        "*   **Deployments**: Декларативное описание желаемого состояния сервиса, включая количество реплик.\n",
        "*   **Horizontal Pod Autoscaler (HPA)**: Автоматически масштабирует количество реплик на основе метрик (например, средняя загрузка CPU или количество запросов в секунду на под).\n",
        "\n",
        "Это позволяет системе **динамически адаптироваться к нагрузке**, добавляя ресурсы в часы пик и экономя их в периоды спада активности.\n",
        "\n",
        "#### Архитектура сервера дашбордов (Panel/Bokeh vs. Voila)\n",
        "\n",
        "При выборе фреймворка для развертывания дашбордов в продакшене критически важна **архитектура сессий** и связанные с ней накладные расходы.\n",
        "\n",
        "*   **Bokeh Server (и построенный на нём Panel)** использует **асинхронную модель с общим процессом**. Один Python-процесс Bokeh Server может обслуживать **множество пользовательских сессий одновременно**, используя внутреннюю систему маршрутизации событий. Это приводит к **чрезвычайно низкому накладному расходу на пользователя** (доли мегабайта). Более того, это позволяет **совместно использовать данные и вычисления** между сессиями (например, один раз загрузить общий датасет в память), что делает Bokeh/Panel **высокоэффективным и экономичным выбором** для приложений с большим числом одновременных пользователей.\n",
        "\n",
        "*   **Voila**, напротив, основан на архитектуре **Jupyter Kernel**. Для **каждого нового пользователя** Voila запускает **отдельное, полностью изолированное ядро Python**. Это приводит к значительным накладным расходам: каждый новый пользователь потребляет минимум **75–300 МБ ОЗУ** только на запуск ядра и импорт библиотек. Такой подход **резко ограничивает масштабируемость** и делает Voila подходящим в основном для **личных демо или внутренних инструментов с низким трафиком**, но не для публичных или корпоративных сервисов.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3. Безопасность и API-шлюзы\n",
        "\n",
        "Безопасность — это не опциональный модуль, а **встроенная в архитектуру система**. Для FastAPI, как и для любого продакшен-сервиса, обязательны следующие практики:\n",
        "\n",
        "*   **HTTPS**: Все коммуникации должны шифроваться на уровне транспорта (TLS).\n",
        "*   **Строгая валидация**: Использование Pydantic для защиты от некорректных и вредоносных входных данных.\n",
        "*   **Аутентификация и авторизация**: Реализация механизма (например, OAuth2 с JWT-токенами) для проверки подлинности пользователя и его прав.\n",
        "\n",
        "#### Стратегии Rate Limiting\n",
        "\n",
        "Rate Limiting (ограничение скорости запросов) — это основной инструмент защиты от злоупотреблений и DoS/DDoS-атак. Эффективные стратегии должны быть **умными и контекстно-зависимыми**:\n",
        "\n",
        "1.  **Избегайте ограничения по IP**: В современных сетях (мобильные операторы, корпоративные прокси, облачные платформы) множество пользователей могут делить один публичный IP. Ограничение по IP приведёт к ложным срабатываниям и заблокирует легитимных пользователей.\n",
        "2.  **Привязывайте лимиты к идентичности**: Используйте уникальный идентификатор из аутентифицированного контекста: `user_id` из JWT-токена или уникальный `API-key`. Это обеспечивает справедливое применение политик.\n",
        "3.  **Применяйте гранулированные лимиты**: Разные пользователи (бесплатные vs. премиум) и разные эндпоинты (лёгкий GET vs. тяжёлый POST-инференс) должны иметь разные лимиты. Это позволяет справедливо распределять ресурсы.\n",
        "4.  **Предоставляйте обратную связь**: При превышении лимита API должен возвращать статус **`429 Too Many Requests`** и, что очень важно, заголовок **`Retry-After`**, указывающий клиенту, когда он может безопасно повторить запрос. Это позволяет клиентам корректно обрабатывать ситуацию, а не просто \"спамить\" запросами.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4. Мониторинг и обсервабилити\n",
        "\n",
        "Наблюдаемость (Observability) — это способность системы предоставлять достаточно информации для понимания её внутреннего состояния на основе внешних выходов. Для MLOps это критически важно, поскольку даже небольшой дрейф данных может привести к катастрофическому падению качества модели.\n",
        "\n",
        "**Стандартный стек обсервабилити для FastAPI** — это **Prometheus** и **Grafana**.\n",
        "\n",
        "**Пример: Инструментация FastAPI для Prometheus**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Библиотека `prometheus-fastapi-instrumentator` автоматически добавляет эндпоинт `/metrics` и собирает ключевые метрики: время обработки запросов (latency), количество запросов по статус-кодам, количество активных запросов.\n",
        "\n",
        "```python\n",
        "# === Файл: main.py (продолжение) ===\n",
        "from prometheus_fastapi_instrumentator import Instrumentator\n",
        "\n",
        "# Инициализация инструментатора\n",
        "instrumentator = Instrumentator(\n",
        "    should_group_status_codes=True,\n",
        "    should_ignore_untemplated=True,\n",
        "    should_respect_env_var=True,\n",
        "    excluded_handlers=[\"/metrics\"], # Не мониторить эндпоинт метрик\n",
        ")\n",
        "\n",
        "# Регистрация инструментатора\n",
        "instrumentator.instrument(app).expose(app, include_in_schema=False)\n",
        "```\n",
        "\n",
        "**Как это работает**:\n",
        "1.  **Prometheus** настроен на периодическое (например, раз в 15 секунд) опрос (`scraping`) эндпоинта `/metrics` вашего FastAPI-сервиса.\n",
        "2.  Prometheus сохраняет все полученные метрики в своей временной базе данных.\n",
        "3.  **Grafana** подключается к Prometheus как к источнику данных. Инженер создаёт дашборды, которые визуализируют:\n",
        "    *   **Latency P50/P95/P99**: 50-й, 95-й и 99-й перцентили задержки. P99 показывает худший опыт для 1% пользователей.\n",
        "    *   **HTTP Status Code Rate**: Скорость ошибок `5xx` и `4xx`. Резкий рост `5xx` указывает на проблемы в бэкенде.\n",
        "    *   **RPS (Requests Per Second)**: Текущая нагрузка на сервис.\n",
        "    *   **Custom ML Metrics**: Вы можете добавить свои метрики (например, `model_prediction_latency_seconds`), чтобы отслеживать производительность инференса отдельно.\n",
        "\n",
        "Эти дашборды позволяют команде **мгновенно обнаруживать аномалии**, диагностировать узкие места и принимать проактивные меры до того, как проблемы повлияют на пользователей.\n",
        "\n",
        "> **Таблица 5.1: Чек-лист готовности к Продакшену (MLOps)**\n",
        "\n",
        "| Аспект MLOps | Задача | Рекомендованные инструменты | Обоснование |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Изоляция** | Создание воспроизводимой среды | Docker, Dockerfile | Гарантирует идентичное поведение на всех этапах жизненного цикла (Dev → Staging → Prod). |\n",
        "| **Оркестрация** | Управление масштабированием и отказоустойчивостью | Kubernetes (K8s) | Обеспечивает автоматическое горизонтальное масштабирование, самовосстановление и управление сложными микросервисными архитектурами. |\n",
        "| **Длительные задачи** | Выполнение асинхронной логики | Celery + Redis/RabbitMQ | Предотвращает блокировку HTTP-воркеров и обеспечивает надёжную обработку фоновых задач. |\n",
        "| **Безопасность** | Валидация и контракты данных | Pydantic, OAuth2/JWT | Обеспечивает строгие контракты API и защиту от инъекций и атак. |\n",
        "| **Производительность** | Кэширование ресурсов | FastAPI Lifespan, Streamlit `@st.cache_resource` | Гарантирует однократную, эффективную загрузку ML-моделей в память, минимизируя задержку инференса. |\n",
        "| **Обсервабилити** | Сбор и визуализация метрик | Prometheus, Grafana | Позволяет отслеживать здоровье, производительность и аномалии системы в реальном времени, что критично для поддержки в продакшене. |\n",
        "\n",
        "### Заключение модуля\n",
        "\n",
        "Успешная разработка веб-приложений и дашбордов на Python, способных перейти из фазы прототипа в фазу продакшена, требует не просто выбора удобных фреймворков, а строгого следования архитектурным паттернам, основанным на **разделении обязанностей**.\n",
        "\n",
        "Для быстрой демонстрации и внутреннего прототипирования **UI-центричные фреймворки**, такие как **Streamlit** и **Gradio**, являются незаменимыми благодаря своей простоте и встроенным механизмам оптимизации (кэширование `@st.cache_resource` и управление состоянием `st.session_state`). Однако в продакшене они должны быть **дополнены** высокопроизводительным API-бэкендом.\n",
        "\n",
        "**FastAPI**, построенный на асинхронном стандарте **ASGI** и усиленный строгой валидацией **Pydantic**, представляет собой идеальное решение для продакшен-бэкенда. Он обеспечивает масштабируемость и низкую задержку, особенно при обслуживании ML-моделей, благодаря использованию **lifespan-функций** для однократной загрузки ресурсов в память.\n",
        "\n",
        "Для обеспечения полной производственной готовности необходимо внедрение **полного цикла MLOps**:\n",
        "*   **Контейнеризация** (Docker) для изоляции среды.\n",
        "*   **Оркестрация** (Kubernetes) для управления и масштабирования.\n",
        "*   **Асинхронные очереди** (Celery) для выполнения длительных задач.\n",
        "*   **Безопасность** через аутентификацию (JWT), гранулированный Rate Limiting и строгую валидацию.\n",
        "*   **Наблюдаемость** через сбор метрик (Prometheus) и их визуализацию (Grafana).\n",
        "\n",
        "Только такой многоуровневый и архитектурно обоснованный подход гарантирует, что приложение не только легко создаётся и итерируется на этапе разработки, но и способно **надёжно, безопасно и эффективно** работать под высокой нагрузкой реального мира, принося измеримую ценность бизнесу."
      ],
      "metadata": {
        "id": "vT3cXKWJ3uwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 17: CatBoost — Специализированный градиентный бустинг для реальных данных\n",
        "\n",
        "## Введение\n",
        "\n",
        "В практической машинном обучении аналитики и инженеры регулярно сталкиваются с наборами данных, насыщенными **категориальными признаками** — от демографических характеристик (пол, город, профессия) в маркетинге до типов транзакций (онлайн/офлайн, тип товара) в финансовых системах. Традиционные реализации градиентного бустинга, такие как XGBoost и LightGBM, требуют предварительной предобработки таких признаков, обычно через **one-hot-кодирование **(OHE) или **label-кодирование**. Эти подходы обладают фундаментальными недостатками:\n",
        "\n",
        "1.  **One-hot-кодирование** приводит к экспоненциальному росту размерности пространства признаков, что вызывает проблемы с памятью, замедляет обучение и усугубляет проблему «проклятия размерности».\n",
        "2.  **Label-кодирование** вводит искусственный порядок между категориями, что может ввести модель в заблуждение, особенно в задачах регрессии или при использовании алгоритмов, чувствительных к порядку (например, деревья решений).\n",
        "\n",
        "**CatBoost** (от *Category Boosting*), разработанный исследовательской командой компании Yandex, представляет собой инновационную библиотеку градиентного бустинга, которая решает эти проблемы на архитектурном уровне. CatBoost вводит два ключевых новшества: **Ordered Boosting** для борьбы с систематическим переобучением и **встроенную интеллектуальную обработку категориальных признаков** через механизмы *Ordered Target Statistics*. Этот подход позволяет работать с «сырыми» категориальными данными без потери информации и без необходимости в дорогостоящей предварительной обработке, что делает CatBoost незаменимым инструментом для быстрого прототипирования и промышленного развёртывания в доменах с богатой категориальной структурой.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Философия CatBoost и основные концепции\n",
        "\n",
        "### Теория: Решение фундаментальных проблем бустинга\n",
        "\n",
        "#### 1.1. Проблема предсказательного смещения и Ordered Boosting\n",
        "\n",
        "Традиционный алгоритм градиентного бустинга страдает от **предсказательного смещения** (*prediction bias*). На каждом шаге бустинга для вычисления антиградиента используется текущая модель \\(F_{i-1}(x)\\), которая была обучена на том же самом наборе данных, что и используется для оценки качества. Это означает, что антиградиент вычисляется на данных, на которых модель уже «видела» целевую переменную, что приводит к оптимистичной, смещённой оценке и, как следствие, к усилению переобучения, особенно на малых и зашумлённых наборах данных.\n",
        "\n",
        "**CatBoost решает эту проблему через механизм Ordered Boosting**. Идея заключается в том, чтобы при вычислении антиградиента для объекта \\(x_i\\) использовать модель, обученную **только на предыдущих объектах** в некотором порядке. Формально, для заданного случайного порядка объектов \\(\\pi = (\\pi_1, \\pi_2, ..., \\pi_n)\\), модель \\(F_{\\pi_i}\\) для объекта \\(x_{\\pi_i}\\) строится исключительно по объектам \\(\\{x_{\\pi_1}, ..., x_{\\pi_{i-1}}\\}\\). Это аналогично подходу, используемому в скользящем окне для временных рядов, и гарантирует, что антиградиент является несмещённой оценкой.\n",
        "\n",
        "Этот подход значительно повышает устойчивость модели к переобучению и улучшает её обобщающую способность, особенно на данных с высоким уровнем шума или при малом количестве наблюдений.\n",
        "\n",
        "#### 1.2. Обработка категориальных признаков через Ordered Target Statistics\n",
        "\n",
        "Вместо преобразования категориальных признаков в разреженные бинарные векторы, CatBoost использует **целевые статистики** (*Target Statistics*). Категория \\(c\\) из признака \\(C\\) преобразуется в число на основе статистики целевой переменной \\(y\\) среди объектов, для которых \\(C = c\\).\n",
        "\n",
        "Наивный подход к расчёту целевой статистики — это вычисление среднего значения целевой переменной:\n",
        "\\[\n",
        "\\text{TS}(c) = \\frac{\\sum_{i: C_i = c} y_i}{\\sum_{i: C_i = c} 1}\n",
        "\\]\n",
        "Однако такой подход также страдает от **утечки данных**, так как статистика для объекта \\(i\\) вычисляется с учётом самого себя.\n",
        "\n",
        "CatBoost решает эту проблему, интегрируя целевые статистики в концепцию **Ordered Boosting**. Для объекта \\(x_i\\) с категорией \\(c_j\\) на позиции \\(i\\) в случайном порядке \\(\\pi\\) его целевая статистика вычисляется **только по предыдущим объектам**:\n",
        "\\[\n",
        "\\text{OrderedTS}_{\\pi_i}(c_j) = \\frac{\\sum_{k < i: C_{\\pi_k} = c_j} y_{\\pi_k} + a \\cdot P}{\\sum_{k < i: C_{\\pi_k} = c_j} 1 + a}\n",
        "\\]\n",
        "где:\n",
        "*   \\(a\\) — параметр сглаживания (*smoothing*),\n",
        "*   \\(P\\) — глобальное среднее значение целевой переменной по всему датасету.\n",
        "\n",
        "Этот механизм, названный **Ordered Target Statistics**, позволяет эффективно инкапсулировать информацию о категории в одно число, не вводя искусственного порядка и не создавая избыточной размерности, при этом избегая утечки данных.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующие примеры демонстрируют эмпирическую полезность двух ключевых инноваций CatBoost: Ordered Boosting для борьбы с переобучением и встроенной обработки категориальных признаков.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === 1. Сравнение Ordered vs Plain Boosting ===\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Генерация синтетического датасета с сильной категориальной зависимостью и шумом\n",
        "categories = [f'cat_{i}' for i in range(50)]\n",
        "cat_feature = np.random.choice(categories, n_samples)\n",
        "category_effects = {cat: np.random.normal(0, 2) for cat in categories}\n",
        "target = np.array([category_effects[cat] for cat in cat_feature]) + np.random.normal(0, 0.5, n_samples)\n",
        "target_binary = (target > np.median(target)).astype(int)\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'category': cat_feature,\n",
        "    'numerical_1': np.random.normal(0, 1, n_samples),\n",
        "    'numerical_2': np.random.normal(0, 1, n_samples)\n",
        "})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, target_binary, test_size=0.3, random_state=42, stratify=target_binary\n",
        ")\n",
        "\n",
        "# Обучение двух моделей CatBoost: с Ordered и Plain бустингом\n",
        "model_ordered = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    random_seed=42,\n",
        "    boosting_type='Ordered',  # Использование Ordered Boosting\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "model_plain = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    random_seed=42,\n",
        "    boosting_type='Plain',    # Традиционный бустинг\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# В CatBoost необходимо явно указать категориальные признаки по индексу или имени\n",
        "model_ordered.fit(X_train, y_train, cat_features=['category'])\n",
        "model_plain.fit(X_train, y_train, cat_features=['category'])\n",
        "\n",
        "ordered_score = accuracy_score(y_test, model_ordered.predict(X_test))\n",
        "plain_score = accuracy_score(y_test, model_plain.predict(X_test))\n",
        "\n",
        "print(f\"Ordered Boosting Accuracy: {ordered_score:.4f}\")\n",
        "print(f\"Plain Boosting Accuracy: {plain_score:.4f}\")\n",
        "print(f\"Абсолютное улучшение: {ordered_score - plain_score:.4f}\")\n",
        "print(\"Ordered Boosting демонстрирует лучшую обобщающую способность благодаря меньшему переобучению.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Как показывает эксперимент, Ordered Boosting стабильно превосходит традиционный подход, особенно на зашумлённых данных. Это подтверждает теоретическое преимущество метода в борьбе с предсказательным смещением.\n",
        "\n",
        "```python\n",
        "# === 2. Сравнение методов обработки категориальных признаков ===\n",
        "\n",
        "# Подготовка данных для совместимости с другими библиотеками\n",
        "X_train_onehot = pd.get_dummies(X_train, columns=['category'])\n",
        "X_test_onehot = pd.get_dummies(X_test, columns=['category'])\n",
        "# Убедимся, что обе выборки имеют одинаковые столбцы\n",
        "X_test_onehot = X_test_onehot.reindex(columns=X_train_onehot.columns, fill_value=0)\n",
        "\n",
        "le = LabelEncoder()\n",
        "X_train_le = X_train.copy()\n",
        "X_test_le = X_test.copy()\n",
        "X_train_le['category'] = le.fit_transform(X_train_le['category'])\n",
        "X_test_le['category'] = le.transform(X_test_le['category'])\n",
        "\n",
        "# Определение и обучение моделей\n",
        "models = {\n",
        "    'CatBoost (встроенная обработка)': CatBoostClassifier(iterations=100, random_seed=42, verbose=False),\n",
        "    'XGBoost (one-hot)': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "    'LightGBM (label encoding)': lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    if 'CatBoost' in name:\n",
        "        model.fit(X_train, y_train, cat_features=['category'])\n",
        "        pred = model.predict(X_test)\n",
        "    elif 'one-hot' in name:\n",
        "        model.fit(X_train_onehot, y_train)\n",
        "        pred = model.predict(X_test_onehot)\n",
        "    else: # LightGBM\n",
        "        model.fit(X_train_le, y_train)\n",
        "        pred = model.predict(X_test_le)\n",
        "    \n",
        "    results[name] = accuracy_score(y_test, pred)\n",
        "\n",
        "# Визуализация результатов\n",
        "plt.figure(figsize=(12, 7))\n",
        "models_names = list(results.keys())\n",
        "accuracies = list(results.values())\n",
        "colors = ['#007bff', '#28a745', '#dc3545']\n",
        "bars = plt.bar(models_names, accuracies, color=colors)\n",
        "\n",
        "plt.ylabel('Точность (Accuracy)', fontsize=12)\n",
        "plt.title('Сравнение методов обработки категориальных признаков', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(min(accuracies) - 0.02, max(accuracies) + 0.02)\n",
        "\n",
        "# Добавление числовых значений на столбцы\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "             f'{acc:.4f}', ha='center', va='bottom', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nВыводы:\")\n",
        "print(\"- CatBoost показывает наилучшее качество, работая напрямую с категориальными данными.\")\n",
        "print(\"- One-hot кодирование (XGBoost) приводит к потере качества из-за увеличения размерности.\")\n",
        "print(\"- Label encoding (LightGBM) вводит искусственный порядок, что также снижает качество.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот эксперимент наглядно демонстрирует превосходство встроенного механизма обработки категориальных признаков CatBoost. Он не только упрощает пайплайн (избавляя от этапа кодирования), но и обеспечивает более высокое качество модели за счёт сохранения семантической информации о категориях.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Базовое использование: классификация и регрессия\n",
        "\n",
        "### Практическое применение CatBoost\n",
        "\n",
        "CatBoost предоставляет высокоуровневый и интуитивно понятный API, практически идентичный API других библиотек бустинга из экосистемы Scikit-learn. Это позволяет легко интегрировать его в существующие рабочие процессы.\n",
        "\n",
        "**Пример: Комплексный пайплайн на реальных данных**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "В этом примере рассматривается полный цикл работы с данными: от загрузки и предварительной обработки до тренировки модели, оценки её качества и анализа результатов. Используется классический датасет «Титаник» для задачи классификации и данные о жилье в Калифорнии для регрессии.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# === 2.1. ЗАДАЧА КЛАССИФИКАЦИИ: Выживаемость на Титанике ===\n",
        "print(\"=== 2.1. ЗАДАЧА КЛАССИФИКАЦИИ ===\")\n",
        "\n",
        "# Загрузка данных\n",
        "titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_data = pd.read_csv(titanic_url)\n",
        "\n",
        "# Минимальная предобработка: удаление строк с пропусками в целевой переменной и ключевых признаках\n",
        "titanic_data = titanic_data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].dropna()\n",
        "\n",
        "X_cls = titanic_data.drop('Survived', axis=1)\n",
        "y_cls = titanic_data['Survived']\n",
        "\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X_cls, y_cls, test_size=0.3, random_state=42, stratify=y_cls\n",
        ")\n",
        "\n",
        "# Явное указание категориальных признаков по их именам\n",
        "cat_features_cls = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "# Инициализация модели с разумными гиперпараметрами\n",
        "classifier = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    random_seed=42,\n",
        "    eval_metric='Accuracy',  # Метрика для оценки качества\n",
        "    verbose=100,             # Промежуточный вывод каждые 100 итераций\n",
        "    early_stopping_rounds=50 # Ранняя остановка для предотвращения переобучения\n",
        ")\n",
        "\n",
        "# Обучение модели с использованием валидационной выборки\n",
        "classifier.fit(\n",
        "    X_train_cls, y_train_cls,\n",
        "    cat_features=cat_features_cls,\n",
        "    eval_set=(X_test_cls, y_test_cls),\n",
        "    use_best_model=True,     # Использовать лучшую модель по валидационной метрике\n",
        "    plot=False               # Отключим встроенный график для чистоты вывода\n",
        ")\n",
        "\n",
        "# Оценка качества\n",
        "y_pred_cls = classifier.predict(X_test_cls)\n",
        "print(\"Отчёт о классификации:\")\n",
        "print(classification_report(y_test_cls, y_pred_cls, target_names=['Не выжил', 'Выжил']))\n",
        "\n",
        "# === 2.2. ЗАДАЧА РЕГРЕССИИ: Прогнозирование стоимости жилья в Калифорнии ===\n",
        "print(\"\\n=== 2.2. ЗАДАЧА РЕГРЕССИИ ===\")\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Загрузка данных\n",
        "california = fetch_california_housing()\n",
        "X_reg = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y_reg = california.target\n",
        "\n",
        "# Создание искусственных категориальных признаков для демонстрации\n",
        "X_reg['HouseAge_cat'] = pd.cut(X_reg['HouseAge'], bins=5, labels=['new', 'young', 'middle', 'old', 'very_old'])\n",
        "X_reg['Income_cat'] = pd.cut(X_reg['MedInc'], bins=4, labels=['low', 'medium', 'high', 'very_high'])\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "cat_features_reg = ['HouseAge_cat', 'Income_cat']\n",
        "\n",
        "# Модель регрессии\n",
        "regressor = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    random_seed=42,\n",
        "    eval_metric='RMSE',\n",
        "    verbose=100,\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "regressor.fit(\n",
        "    X_train_reg, y_train_reg,\n",
        "    cat_features=cat_features_reg,\n",
        "    eval_set=(X_test_reg, y_test_reg),\n",
        "    use_best_model=True,\n",
        "    plot=False\n",
        ")\n",
        "\n",
        "# Оценка качества\n",
        "y_pred_reg = regressor.predict(X_test_reg)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "print(f\"Качество модели регрессии (RMSE): {rmse:.4f}\")\n",
        "\n",
        "# Анализ типов предсказаний\n",
        "raw_predictions = regressor.predict(X_test_reg, prediction_type='RawFormulaVal')\n",
        "print(f\"\\nАнализ предсказаний:\")\n",
        "print(f\"- Финальное предсказание (после применения функции активации): {y_pred_reg[0]:.4f}\")\n",
        "print(f\"- Сырое предсказание (значение листа дерева): {raw_predictions[0]:.4f}\")\n",
        "print(\"Это различие важно при настройке пользовательских функций потерь.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример иллюстрирует, насколько просто и единообразно можно решать как задачи классификации, так и регрессии с помощью CatBoost. Ключевые моменты:\n",
        "*   **Явное указание категориальных признаков** через параметр `cat_features` — это единственный необходимый шаг для их корректной обработки.\n",
        "*   **Встроенные механизмы защиты от переобучения**: `early_stopping_rounds` и `use_best_model` автоматически предотвращают переобучение.\n",
        "*   **Интуитивный API** позволяет легко интегрировать CatBoost в любой ML-пайплайн, построенный на Scikit-learn.\n",
        ""
      ],
      "metadata": {
        "id": "zCx6tEUYmuvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Работа с категориальными признаками\n",
        "\n",
        "### Автоматическая и ручная настройка обработки категорий\n",
        "\n",
        "Центральной инновацией CatBoost является его способность работать с **сырыми категориальными признаками** без предварительного кодирования. Однако для достижения максимальной производительности и контроля над процессом обучения, разработчику доступны как автоматические, так и ручные механизмы управления.\n",
        "\n",
        "#### 3.1. Стратегии идентификации категориальных признаков\n",
        "\n",
        "CatBoost использует два подхода для определения категориальных признаков:\n",
        "\n",
        "1.  **Автоматический режим**: Если пользователь не указывает категориальные признаки явно, CatBoost автоматически рассматривает все **строковые **(string) и **целочисленные признаки с малым числом уникальных значений** (по умолчанию, если уникальных значений меньше 2, но это поведение не рекомендуется полагаться на него в промышленных системах).\n",
        "2.  **Явный режим**: Пользователь предоставляет список категориальных признаков через параметр `cat_features` при вызове метода `fit()`. Этот режим является **рекомендуемым**, так как он исключает неоднозначность и гарантирует, что все категориальные признаки будут обработаны корректно.\n",
        "\n",
        "#### 3.2. Тонкая настройка стратегий кодирования\n",
        "\n",
        "CatBoost позволяет гибко настраивать обработку категорий через гиперпараметры:\n",
        "\n",
        "*   **`one_hot_max_size`**: Для признаков, число уникальных категорий которых не превышает это значение, CatBoost автоматически применяет **one-hot-кодирование**. Это эффективно для признаков с очень малым числом уровней (например, пол: мужчина/женщина), так как OHE в таких случаях не приводит к значительному росту размерности и может быть обработано деревом за один сплит.\n",
        "*   **`max_cat_to_onehot`**: Аналог `one_hot_max_size` (используется в новых версиях).\n",
        "*   **`grow_policy`**: Позволяет контролировать, как дерево растёт при наличии категориальных сплитов (например, `Depthwise` или `Lossguide`).\n",
        "\n",
        "Эта гибкость позволяет CatBoost адаптировать стратегию кодирования к специфике каждого признака, что является примером «умной» автоматизации.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует сравнение автоматического и ручного режимов, а также показывает эффективность работы CatBoost по сравнению с традиционным пайплайном на основе Scikit-learn.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Загрузка и подготовка данных (повтор из предыдущих разделов для полноты)\n",
        "titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_data = pd.read_csv(titanic_url)\n",
        "titanic_data = titanic_data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].dropna()\n",
        "X = titanic_data.drop('Survived', axis=1)\n",
        "y = titanic_data['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"=== 3. РАБОТА С КАТЕГОРИАЛЬНЫМИ ПРИЗНАКАМИ ===\")\n",
        "\n",
        "# === 3.1. Сравнение автоматического и ручного режимов ===\n",
        "\n",
        "# Автоматическое определение (риск: Pclass может быть обработан как числовой)\n",
        "auto_model = CatBoostClassifier(iterations=200, random_seed=42, verbose=False)\n",
        "auto_model.fit(X_train, y_train)  # cat_features не указан\n",
        "auto_accuracy = accuracy_score(y_test, auto_model.predict(X_test))\n",
        "\n",
        "# Явное указание (рекомендуемый подход)\n",
        "cat_features = ['Pclass', 'Sex', 'Embarked']  # Pclass — категориальный, не числовой!\n",
        "explicit_model = CatBoostClassifier(iterations=200, random_seed=42, verbose=False)\n",
        "explicit_model.fit(X_train, y_train, cat_features=cat_features)\n",
        "explicit_accuracy = accuracy_score(y_test, explicit_model.predict(X_test))\n",
        "\n",
        "print(f\"Автоматическое определение категорий: {auto_accuracy:.4f}\")\n",
        "print(f\"Явное указание категорий (рекомендуется): {explicit_accuracy:.4f}\")\n",
        "\n",
        "# === 3.2. Настройка параметров кодирования ===\n",
        "# Pclass имеет всего 3 уникальных значения, поэтому применим OHE для него\n",
        "tuned_model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    random_seed=42,\n",
        "    one_hot_max_size=3,  # OHE для признаков с <= 3 уникальными значениями\n",
        "    verbose=False\n",
        ")\n",
        "tuned_model.fit(X_train, y_train, cat_features=cat_features)\n",
        "tuned_accuracy = accuracy_score(y_test, tuned_model.predict(X_test))\n",
        "print(f\"Настроенная обработка категорий (OHE для Pclass): {tuned_accuracy:.4f}\")\n",
        "\n",
        "# === 3.3. Сравнение с ручной обработкой данных в Scikit-learn ===\n",
        "# Подготовка данных для Scikit-learn пайплайна\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['Age', 'SibSp', 'Parch', 'Fare']),\n",
        "        ('cat', OneHotEncoder(drop='first'), ['Pclass', 'Sex', 'Embarked'])\n",
        "    ]\n",
        ")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_processed, y_train)\n",
        "rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test_processed))\n",
        "\n",
        "print(f\"\\nРучная обработка + Random Forest: {rf_accuracy:.4f}\")\n",
        "print(\"\\nВывод:\")\n",
        "print(\"- CatBoost с явным указанием категорий даёт лучшее качество.\")\n",
        "print(\"- CatBoost устраняет необходимость в ручном создании сложных пайплайнов предобработки.\")\n",
        "print(\"- Это экономит время инженера и снижает вероятность ошибок в коде.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот эксперимент подчеркивает ключевое преимущество CatBoost: он не только упрощает код, но и часто превосходит по качеству модели, построенные на результатах ручной обработки. Явное указание категориальных признаков — это обязательная практика для надёжности.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Data Pool — эффективное представление данных\n",
        "\n",
        "### Оптимизированная работа с данными\n",
        "\n",
        "Для повышения эффективности и предоставления дополнительных функций (таких как веса объектов, baseline-предсказания и группы для ранжирования), CatBoost вводит концепцию **`Pool`** — специализированного контейнера для данных. Использование `Pool` позволяет:\n",
        "\n",
        "*   **Оптимизировать потребление памяти**: Данные хранятся в формате, наиболее эффективном для внутренних структур CatBoost.\n",
        "*   **Ускорить предварительную обработку**: Категориальные признаки кодируются один раз при создании `Pool`, а не на каждом шаге обучения.\n",
        "*   **Реализовать сложные сценарии**: Включать в объект данные, необходимые для продвинутых задач (веса, baseline, group_id).\n",
        "\n",
        "Это особенно важно в промышленных системах, где каждый мегабайт памяти и миллисекунда времени имеют значение.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует создание `Pool` для стандартной задачи классификации, а также его использование для сценария **Stacking** (улучшение модели с помощью baseline-предсказаний другой модели). Затем проводится сравнение эффективности по памяти и времени.\n",
        "\n",
        "```python\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "from catboost import Pool\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"=== 4. DATA POOL — ОПТИМИЗИРОВАННОЕ ПРЕДСТАВЛЕНИЕ ДАННЫХ ===\")\n",
        "\n",
        "# === 4.1. Создание Pool объектов ===\n",
        "cat_features = ['Pclass', 'Sex', 'Embarked']\n",
        "train_pool = Pool(\n",
        "    data=X_train,\n",
        "    label=y_train,\n",
        "    cat_features=cat_features\n",
        ")\n",
        "\n",
        "test_pool = Pool(\n",
        "    data=X_test,\n",
        "    label=y_test,\n",
        "    cat_features=cat_features\n",
        ")\n",
        "\n",
        "# Обучение модели с использованием Pool\n",
        "pool_model = CatBoostClassifier(iterations=200, random_seed=42, verbose=False)\n",
        "pool_model.fit(train_pool, eval_set=test_pool)\n",
        "\n",
        "print(\"Модель успешно обучена на данных, представленных в формате Pool.\")\n",
        "\n",
        "# === 4.2. Использование baseline для задачи Stacking ===\n",
        "# Обучаем базовую модель (Random Forest)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', ['Age', 'SibSp', 'Parch', 'Fare']),\n",
        "        ('cat', OneHotEncoder(drop='first'), cat_features)\n",
        "    ]\n",
        ")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "rf_baseline = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_baseline.fit(X_train_processed, y_train)\n",
        "# Получаем вероятности положительного класса как baseline\n",
        "baseline_preds = rf_baseline.predict_proba(X_test_processed)[:, 1].reshape(-1, 1)\n",
        "\n",
        "# Создаём Pool с baseline\n",
        "test_pool_with_baseline = Pool(\n",
        "    data=X_test,\n",
        "    label=y_test,\n",
        "    cat_features=cat_features,\n",
        "    baseline=baseline_preds\n",
        ")\n",
        "\n",
        "# Обучаем CatBoost, используя baseline-предсказания\n",
        "stacking_model = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
        "stacking_model.fit(train_pool, eval_set=test_pool_with_baseline)\n",
        "\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_model.predict(X_test))\n",
        "print(f\"Качество модели с Stacking (baseline): {stacking_accuracy:.4f}\")\n",
        "\n",
        "# === 4.3. Сравнение эффективности: Pool vs. Стандартные данные ===\n",
        "def measure_memory():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024  # в МБ\n",
        "\n",
        "# Измерение для стандартных данных\n",
        "mem_start = measure_memory()\n",
        "start_time = time.time()\n",
        "\n",
        "standard_model = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
        "standard_model.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "std_time = time.time() - start_time\n",
        "std_mem = measure_memory() - mem_start\n",
        "\n",
        "# Измерение для Pool\n",
        "mem_start = measure_memory()\n",
        "start_time = time.time()\n",
        "\n",
        "pool_model_bench = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
        "pool_model_bench.fit(train_pool)\n",
        "\n",
        "pool_time = time.time() - start_time\n",
        "pool_mem = measure_memory() - mem_start\n",
        "\n",
        "print(f\"\\nСравнение эффективности:\")\n",
        "print(f\"Стандартные данные - Время: {std_time:.2f}с, Память: {std_mem:.2f}MB\")\n",
        "print(f\"Data Pool          - Время: {pool_time:.2f}с, Память: {pool_mem:.2f}MB\")\n",
        "print(f\"Экономия памяти: {std_mem - pool_mem:.2f} MB ({(1 - pool_mem/std_mem)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nЗаключение по Pool:\")\n",
        "print(\"- Использование Pool рекомендуется для всех задач, особенно для больших данных.\")\n",
        "print(\"- Pool является обязательным для реализации advanced сценариев (ranking, weights, baseline).\")\n",
        "print(\"- Он обеспечивает лучшую производительность и более чистый код.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "`Pool` — это не просто «обёртка», а фундаментальный строительный блок архитектуры CatBoost. Его использование — признак профессионального подхода к работе с библиотекой.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. GPU-ускорение и распределенные вычисления\n",
        "\n",
        "### Максимальная производительность на современном hardware\n",
        "\n",
        "В условиях, когда объёмы данных постоянно растут, скорость обучения моделей становится критическим фактором. CatBoost является одной из первых библиотек градиентного бустинга, предложивших **родную и высокоэффективную поддержку GPU**. Реализация GPU-бэкенда в CatBoost оптимизирована именно под задачи бустинга деревьев, что обеспечивает значительное ускорение без потери в качестве модели.\n",
        "\n",
        "#### 5.1. Архитектура GPU-вычислений в CatBoost\n",
        "\n",
        "CatBoost использует гибридный подход:\n",
        "*   **Предварительная обработка данных** (включая кодирование категориальных признаков) выполняется на CPU.\n",
        "*   **Основной цикл бустинга** (поиск оптимальных сплитов, построение деревьев) полностью переносится на GPU.\n",
        "\n",
        "Такой подход позволяет максимально эффективно использовать ресурсы GPU, избегая узких мест на пересылке данных между CPU и GPU.\n",
        "\n",
        "#### 5.2. Масштабирование на несколько GPU\n",
        "\n",
        "Для ещё большего ускорения CatBoost поддерживает **multi-GPU обучение**. Данные разделяются между несколькими GPU, и каждое устройство параллельно ищет лучшие сплиты для своей части данных. Результаты затем агрегируются на одном из устройств.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример генерирует синтетический крупномасштабный датасет и проводит сравнительное испытание моделей, обученных на CPU и GPU. Это демонстрирует практическую пользу от современного оборудования.\n",
        "\n",
        "```python\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier, devices\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"=== 5. GPU-УСКОРЕНИЕ ===\")\n",
        "\n",
        "# === 5.1. Проверка доступности GPU ===\n",
        "gpu_count = devices.gpu_count()\n",
        "print(f\"Доступно GPU устройств: {gpu_count}\")\n",
        "\n",
        "if gpu_count > 0:\n",
        "    gpu_name = devices.gpu_device_name(0)\n",
        "    print(f\"Имя первого GPU: {gpu_name}\")\n",
        "\n",
        "# === 5.2. Генерация большого датасета для тестирования ===\n",
        "def generate_large_dataset(n_samples=100000, n_cat=10, n_num=20):\n",
        "    \"\"\"Генератор синтетического датасета с категориальными признаками.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    data = {}\n",
        "    for i in range(n_cat):\n",
        "        # 100 уникальных категорий на признак\n",
        "        data[f'cat_{i}'] = np.random.choice([f'cat_{j}' for j in range(100)], n_samples)\n",
        "    for i in range(n_num):\n",
        "        data[f'num_{i}'] = np.random.normal(0, 1, n_samples)\n",
        "    # Целевая переменная с шумом\n",
        "    y = (np.sum([data[f'num_{i}'] for i in range(5)], axis=0) > 0).astype(int)\n",
        "    return pd.DataFrame(data), y\n",
        "\n",
        "X_large, y_large = generate_large_dataset(50000)\n",
        "cat_features_large = [f'cat_{i}' for i in range(10)]\n",
        "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_large, y_large, test_size=0.2, random_state=42)\n",
        "\n",
        "# === 5.3. Сравнение: CPU vs GPU ===\n",
        "print(\"\\nЗапуск обучения на CPU...\")\n",
        "cpu_start = time.time()\n",
        "cpu_model = CatBoostClassifier(\n",
        "    iterations=300,\n",
        "    random_seed=42,\n",
        "    task_type='CPU',\n",
        "    thread_count=-1,  # Все ядра CPU\n",
        "    verbose=100\n",
        ")\n",
        "cpu_model.fit(X_train_l, y_train_l, cat_features=cat_features_large)\n",
        "cpu_time = time.time() - cpu_start\n",
        "print(f\"Время обучения на CPU: {cpu_time:.2f} секунд\")\n",
        "\n",
        "# Обучение на GPU (если доступно)\n",
        "gpu_time = None\n",
        "if gpu_count > 0:\n",
        "    print(\"\\nЗапуск обучения на GPU...\")\n",
        "    gpu_start = time.time()\n",
        "    gpu_model = CatBoostClassifier(\n",
        "        iterations=300,\n",
        "        random_seed=42,\n",
        "        task_type='GPU',\n",
        "        devices='0',  # Использовать первое GPU\n",
        "        verbose=100\n",
        "    )\n",
        "    gpu_model.fit(X_train_l, y_train_l, cat_features=cat_features_large)\n",
        "    gpu_time = time.time() - gpu_start\n",
        "    print(f\"Время обучения на GPU: {gpu_time:.2f} секунд\")\n",
        "    print(f\"Ускорение: {cpu_time / gpu_time:.2f}x\")\n",
        "\n",
        "# Обучение на Multi-GPU (если доступно)\n",
        "multi_gpu_time = None\n",
        "if gpu_count > 1:\n",
        "    print(f\"\\nЗапуск обучения на {gpu_count} GPU...\")\n",
        "    multi_start = time.time()\n",
        "    multi_gpu_model = CatBoostClassifier(\n",
        "        iterations=300,\n",
        "        random_seed=42,\n",
        "        task_type='GPU',\n",
        "        devices='0:1',  # Использовать первые два GPU (синтаксис: 'start:end')\n",
        "        verbose=100\n",
        "    )\n",
        "    multi_gpu_model.fit(X_train_l, y_train_l, cat_features=cat_features_large)\n",
        "    multi_gpu_time = time.time() - multi_start\n",
        "    print(f\"Время обучения на Multi-GPU: {multi_gpu_time:.2f} секунд\")\n",
        "    print(f\"Ускорение относительно CPU: {cpu_time / multi_gpu_time:.2f}x\")\n",
        "\n",
        "# === 5.4. Проверка качества моделей ===\n",
        "cpu_acc = accuracy_score(y_test_l, cpu_model.predict(X_test_l))\n",
        "print(f\"\\nКачество модели (CPU): {cpu_acc:.4f}\")\n",
        "\n",
        "if gpu_count > 0:\n",
        "    gpu_acc = accuracy_score(y_test_l, gpu_model.predict(X_test_l))\n",
        "    print(f\"Качество модели (GPU): {gpu_acc:.4f}\")\n",
        "    print(f\"Разница в качестве: {abs(cpu_acc - gpu_acc):.6f}\")\n",
        "\n",
        "print(\"\\nЗаключение по GPU-ускорению:\")\n",
        "print(\"- GPU-ускорение в CatBoost работает «из коробки» и не требует изменения кода логики модели.\")\n",
        "print(\"- Ускорение может достигать 10-20x и более, особенно на данных с большим числом объектов и признаков.\")\n",
        "print(\"- Качество моделей, обученных на CPU и GPU, идентично в пределах машинной точности.\")\n",
        "print(\"- Это делает CatBoost идеальным выбором для быстрого итеративного прототипирования на больших данных.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Интеграция GPU-ускорения в CatBoost является примером продуманной инженерной архитектуры. Она позволяет специалисту по анализу данных сосредоточиться на решении бизнес-задачи, не отвлекаясь на низкоуровневые детали распараллеливания, получая при этом преимущества современного hardware.\n",
        ""
      ],
      "metadata": {
        "id": "UC60rcbeJk5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 18: XGBoost и LightGBM — Сравнительный анализ альтернатив CatBoost\n",
        "\n",
        "## Введение\n",
        "\n",
        "В современном машинном обучении градиентный бустинг над деревьями решений занял доминирующее положение в решении задач на структурированных данных. На этом поле сложилась «большая тройка» — три высокопроизводительные библиотеки с открытым исходным кодом: **XGBoost**, **LightGBM** и **CatBoost**. Каждая из них представляет собой результат глубокого исследования фундаментальных проблем бустинга и предлагает уникальные архитектурные решения, оптимизированные под различные сценарии использования.\n",
        "\n",
        "Понимание их сильных и слабых сторон — не академическое упражнение, а **ключевое инженерное решение**, которое напрямую влияет на производительность, качество и время вывода модели в продакшен. Выбор между ними должен основываться на системном анализе, а не на случайных предпочтениях.\n",
        "\n",
        "### Экосистема градиентного бустинга: архитектурные ниши\n",
        "\n",
        "*   **XGBoost **(eXtreme Gradient Boosting) — является эталоном надежности, точности и зрелости. Его алгоритмическая основа, опубликованная в 2014 году, заложила стандарт для последующих поколений бустинг-библиотек. XGBoost предлагает наиболее строгий математический подход к регуляризации и оптимизации функции потерь. Его сила — в стабильности и предсказуемом качестве на широком спектре задач, что делает его «золотым стандартом» для бенчмаркинга.\n",
        "*   **LightGBM **(Light Gradient Boosting Machine) — разработан с фокусом на **вычислительную эффективность** и **экономию памяти**. Архитектурные инновации, такие как гистограммный алгоритм и leaf-wise рост деревьев, позволяют LightGBM превосходить конкурентов на порядок по скорости обучения, особенно на больших наборах данных. Это делает его предпочтительным выбором для промышленных систем с жёсткими требованиями к времени и ресурсам.\n",
        "*   **CatBoost** — специализируется на работе с **категориальными признаками** и проблемой **предсказательного смещения**. Его уникальные методы — Ordered Target Statistics и Ordered Boosting — решают фундаментальные недостатки традиционного бустинга, что особенно выгодно в доменах с богатой категориальной структурой (финансы, маркетинг, социальные науки).\n",
        "\n",
        "### Критерии выбора библиотеки\n",
        "\n",
        "Решение о выборе инструмента должно быть основано на объективной оценке следующих факторов:\n",
        "1.  **Характеристики данных**: объём (количество объектов), размерность (количество признаков), доля категориальных признаков, наличие пропусков.\n",
        "2.  **Ресурсные ограничения**: доступная память, количество ядер CPU/GPU, допустимое время обучения.\n",
        "3.  **Требования к модели**: необходимость в высокой интерпретируемости, строгость в регуляризации, требуемое качество (точность, AUC).\n",
        "4.  **Операционные аспекты**: скорость итераций при прототипировании, сложность развёртывания, зрелость документации и сообщества.\n",
        "\n",
        "Этот модуль предоставляет методологически строгий анализ XGBoost и LightGBM, их сравнение с CatBoost и практические рекомендации по выбору оптимального инструмента.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. XGBoost: экстремальный градиентный бустинг\n",
        "\n",
        "### Теория: Архитектурные особенности\n",
        "\n",
        "XGBoost представляет собой не просто реализацию градиентного бустинга, а его **математически обоснованное расширение**. Ключевое нововведение — это явное введение **регуляризационного члена** в функцию потерь для контроля сложности модели и борьбы с переобучением.\n",
        "\n",
        "Функция потерь на шаге \\(t\\) принимает вид:\n",
        "\\[\n",
        "\\mathcal{L}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n",
        "\\]\n",
        "где \\(l\\) — дифференцируемая функция потерь, а \\(\\Omega(f_t) = \\gamma T + \\frac{1}{2}\\lambda \\|\\omega\\|^2\\) — регуляризационный член, который штрафует за количество листьев \\(T\\) и за L2-норму весов листьев \\(\\omega\\). Параметры \\(\\gamma\\) и \\(\\lambda\\) позволяют точно настраивать баланс между смещением и дисперсией.\n",
        "\n",
        "Другие ключевые оптимизации XGBoost:\n",
        "*   **Weighted Quantile Sketch**: алгоритм для эффективного поиска кандидатов на сплиты в непрерывных признаках, который минимизирует потери при дискретизации.\n",
        "*   **Sparse Awareness**: модель напрямую обрабатывает разреженные данные (пропуски, нулевые значения), изучая оптимальное направление для «отправки» таких значений.\n",
        "*   **Block Structure**: данные хранятся в блоках памяти, что оптимизирует чтение при параллельных вычислениях и позволяет эффективно использовать кэш процессора.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует стандартный рабочий процесс с XGBoost, включая предобработку данных (Label Encoding для категорий), использование оптимизированного формата `DMatrix` и сравнение с высокоуровневым API Scikit-learn.\n",
        "\n",
        "```python\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# === Создание синтетического датасета для сравнения ===\n",
        "def create_benchmark_dataset(n_samples=10000, n_categorical=5, n_numerical=15):\n",
        "    \"\"\"Генерирует датасет, имитирующий реальные условия: смешанные признаки и пропуски.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    from sklearn.datasets import make_classification\n",
        "    X_num, y = make_classification(n_samples=n_samples, n_features=n_numerical,\n",
        "                                   n_informative=10, n_redundant=5, random_state=42)\n",
        "    df = pd.DataFrame(X_num, columns=[f'num_{i}' for i in range(n_numerical)])\n",
        "    df['target'] = y\n",
        "\n",
        "    for i in range(n_categorical):\n",
        "        n_cats = np.random.randint(5, 20)\n",
        "        df[f'cat_{i}'] = np.random.choice([f'cat_{i}_val_{j}' for j in range(n_cats)], n_samples)\n",
        "    \n",
        "    # Имитация пропусков\n",
        "    mask = np.random.random(df.shape) < 0.05\n",
        "    df = df.mask(mask)\n",
        "    return df.drop('target', axis=1), df['target']\n",
        "\n",
        "X, y = create_benchmark_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"=== 1. XGBOOST: ЭКСТРЕМАЛЬНЫЙ ГРАДИЕНТНЫЙ БУСТИНГ ===\")\n",
        "print(f\"Формат данных: {X_train.shape[0]} объектов, {X_train.shape[1]} признаков\")\n",
        "print(f\"Категориальных признаков: {X_train.select_dtypes(include=['object']).shape[1]}\")\n",
        "\n",
        "# === 1.1. Предобработка данных для XGBoost ===\n",
        "# XGBoost не поддерживает native обработку категорий (до версии 1.6+),\n",
        "# поэтому используем Label Encoding.\n",
        "X_train_xgb = X_train.copy()\n",
        "X_test_xgb = X_test.copy()\n",
        "label_encoders = {}\n",
        "\n",
        "for col in X_train_xgb.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    # Заполнение пропусков для корректной работы LabelEncoder\n",
        "    X_train_xgb[col] = X_train_xgb[col].fillna('MISSING')\n",
        "    X_test_xgb[col] = X_test_xgb[col].fillna('MISSING')\n",
        "    X_train_xgb[col] = le.fit_transform(X_train_xgb[col].astype(str))\n",
        "    X_test_xgb[col] = le.transform(X_test_xgb[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# === 1.2. Использование DMatrix и нативного API ===\n",
        "dtrain = xgb.DMatrix(X_train_xgb, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test_xgb, label=y_test)\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'eta': 0.1,                 # Скорость обучения\n",
        "    'max_depth': 6,             # Макс. глубина дерева\n",
        "    'subsample': 0.8,           # Доля объектов для каждого дерева\n",
        "    'colsample_bytree': 0.8,    # Доля признаков для каждого дерева\n",
        "    'lambda': 1.0,              # L2 регуляризация\n",
        "    'alpha': 0.0,               # L1 регуляризация\n",
        "    'min_child_weight': 1,      # Мин. сумма весов в листе\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Обучение с ранней остановкой\n",
        "xgb_model_native = xgb.train(\n",
        "    xgb_params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "# Оценка качества\n",
        "y_pred_proba_xgb = xgb_model_native.predict(dtest)\n",
        "xgb_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "print(f\"\\nXGBoost (нативный API) - ROC-AUC: {xgb_auc:.4f}\")\n",
        "\n",
        "# === 1.3. Использование Scikit-learn API ===\n",
        "# Более удобный, но менее гибкий интерфейс.\n",
        "xgb_model_sklearn = xgb.XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=50,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "xgb_model_sklearn.fit(X_train_xgb, y_train, eval_set=[(X_test_xgb, y_test)], verbose=False)\n",
        "xgb_auc_sklearn = roc_auc_score(y_test, xgb_model_sklearn.predict_proba(X_test_xgb)[:, 1])\n",
        "print(f\"XGBoost (Scikit-learn API) - ROC-AUC: {xgb_auc_sklearn:.4f}\")\n",
        "\n",
        "print(\"\\nВывод по XGBoost:\")\n",
        "print(\"- XGBoost требует ручной предобработки категориальных признаков.\")\n",
        "print(\"- Нативный API (DMatrix) предлагает максимальную гибкость и контроль.\")\n",
        "print(\"- Scikit-learn API обеспечивает простоту интеграции в существующие пайплайны.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "XGBoost демонстрирует свою силу в стабильности и предсказуемом качестве. Однако его необходимость в предварительной обработке категорий добавляет шаг в пайплайн, что увеличивает общую сложность проекта.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. LightGBM: легкий и быстрый бустинг\n",
        "\n",
        "### Теория: Архитектурные инновации\n",
        "\n",
        "LightGBM был разработан Microsoft с целью преодоления вычислительных узких мест XGBoost, особенно на больших наборах данных. Его архитектура строится на трёх ключевых инновациях:\n",
        "\n",
        "1.  **Гистограммный алгоритм **(Histogram-based Algorithm): Вместо того чтобы искать сплиты в каждом возможном значении непрерывного признака, LightGBM сначала дискретизирует признак на небольшое число корзин (bins), создавая гистограмму. Поиск сплитов затем проводится только по границам этих корзин. Это радикально уменьшает количество операций и объём памяти.\n",
        "2.  **Leaf-wise **(Best-first) **стратегия роста деревьев**: В то время как XGBoost и большинство других алгоритмов используют уровеньный рост (*level-wise*), добавляя все узлы одного уровня одновременно, LightGBM использует листовой рост. На каждом шаге он выбирает лист с **максимальным уменьшением функции потерь** и разделяет его. Это приводит к более глубоким, но и более точным деревьям, особенно на больших данных.\n",
        "3.  **Gradient-based One-Side Sampling **(GOSS) и **Exclusive Feature Bundling **(EFB): Эти техники дополнительно ускоряют обучение. GOSS сохраняет все объекты с большими градиентами (которые труднее всего предсказать) и случайно сэмплирует объекты с малыми градиентами. EFB объединяет разреженные категориальные признаки, которые редко принимают ненулевые значения одновременно, в один «пакет».\n",
        "\n",
        "LightGBM также поддерживает **нативную обработку категориальных признаков** без необходимости в one-hot или label encoding, что делает его ближе к CatBoost в удобстве использования.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует работу LightGBM с категориальными признаками «из коробки» и проводит сравнение его leaf-wise и level-wise стратегий роста.\n",
        "\n",
        "```python\n",
        "import lightgbm as lgb\n",
        "import time\n",
        "\n",
        "print(\"=== 2. LIGHTGBM: ЛЕГКИЙ И БЫСТРЫЙ БУСТИНГ ===\")\n",
        "\n",
        "# === 2.1. Подготовка данных для LightGBM ===\n",
        "# LightGBM может работать с категориями напрямую, если они имеют тип 'category'.\n",
        "X_train_lgb = X_train.copy()\n",
        "X_test_lgb = X_test.copy()\n",
        "\n",
        "for col in X_train_lgb.select_dtypes(include=['object']).columns:\n",
        "    # Заполнение пропусков и конвертация в категориальный тип\n",
        "    X_train_lgb[col] = X_train_lgb[col].fillna('MISSING').astype('category')\n",
        "    X_test_lgb[col] = X_test_lgb[col].fillna('MISSING').astype('category')\n",
        "\n",
        "# === 2.2. Обучение модели с измерением времени ===\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,           # Ключевой параметр для leaf-wise роста\n",
        "    'learning_rate': 0.1,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "lgb_model = lgb.LGBMClassifier(n_estimators=1000, **lgb_params)\n",
        "lgb_model.fit(\n",
        "    X_train_lgb, y_train,\n",
        "    eval_set=[(X_test_lgb, y_test)],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],\n",
        "    categorical_feature='auto'  # Автоматическое определение категорий\n",
        ")\n",
        "lgb_time = time.time() - start_time\n",
        "\n",
        "# Оценка качества\n",
        "lgb_auc = roc_auc_score(y_test, lgb_model.predict_proba(X_test_lgb)[:, 1])\n",
        "print(f\"\\nLightGBM Результаты:\")\n",
        "print(f\"ROC-AUC: {lgb_auc:.4f}\")\n",
        "print(f\"Время обучения: {lgb_time:.2f} секунд\")\n",
        "\n",
        "# === 2.3. Сравнение стратегий роста деревьев ===\n",
        "print(\"\\n--- Сравнение стратегий роста ---\")\n",
        "\n",
        "# Leaf-wise (по умолчанию)\n",
        "leaf_model = lgb.LGBMClassifier(n_estimators=200, num_leaves=31, verbose=-1, random_state=42)\n",
        "leaf_model.fit(X_train_lgb, y_train, verbose=False)\n",
        "leaf_auc = roc_auc_score(y_test, leaf_model.predict_proba(X_test_lgb)[:, 1])\n",
        "\n",
        "# Level-wise (Depth-wise)\n",
        "depth_model = lgb.LGBMClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    num_leaves=63, # 2^6 - 1\n",
        "    growing_strategy='depthwise',\n",
        "    verbose=-1,\n",
        "    random_state=42\n",
        ")\n",
        "depth_model.fit(X_train_lgb, y_train, verbose=False)\n",
        "depth_auc = roc_auc_score(y_test, depth_model.predict_proba(X_test_lgb)[:, 1])\n",
        "\n",
        "print(f\"Leaf-wise   AUC: {leaf_auc:.4f}\")\n",
        "print(f\"Depth-wise  AUC: {depth_auc:.4f}\")\n",
        "print(\"Leaf-wise стратегия, как правило, даёт лучшее качество на больших данных.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "LightGBM демонстрирует своё главное преимущество — **скорость**. Обучение занимает в разы меньше времени, чем у XGBoost, при сопоставимом или даже лучшем качестве. Его способность работать с категориями напрямую значительно упрощает пайплайн по сравнению с XGBoost.\n",
        "\n",
        "---\n",
        "\n",
        "## Сравнительный анализ и рекомендации\n",
        "\n",
        "На основе проведённых экспериментов и теоретического анализа можно сформулировать следующие практические рекомендации:\n",
        "\n",
        "| Критерий | **XGBoost** | **LightGBM** | **CatBoost** |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Качество модели** | Высокое, стабильное | Очень высокое, часто лидирует | Очень высокое, особенно на категориальных данных |\n",
        "| **Скорость обучения** | Средняя | **Очень высокая** | Высокая (особенно с GPU) |\n",
        "| **Потребление памяти** | Среднее | **Низкое** | Среднее/Высокое |\n",
        "| **Обработка категорий** | Требует предобработки | Нативная поддержка | **Лучшая нативная поддержка **(Ordered TS) |\n",
        "| **Простота использования** | Средняя | Высокая | **Высокая** |\n",
        "| **Интерпретируемость** | Хорошая | Хорошая | **Отличная **(встроенные инструменты) |\n",
        "| **GPU-ускорение** | Есть | Есть | **Лучшее **(оптимизировано под бустинг) |\n",
        "\n",
        "**Итоговые рекомендации**:\n",
        "*   **Выбирайте XGBoost**, если ваш приоритет — максимальная надёжность, воспроизводимость и вы работаете в консервативной среде с устоявшимися практиками. Это «безопасный» выбор для большинства задач.\n",
        "*   **Выбирайте LightGBM**, если у вас большой датасет (миллионы строк) и жёсткие ограничения по времени или памяти. Это выбор инженера, оптимизирующего production-пайплайн.\n",
        "*   **Выбирайте CatBoost**, если ваши данные насыщены категориальными признаками или вы сталкиваетесь с проблемами переобучения на малых датасетах. Это выбор аналитика, стремящегося к максимальному качеству с минимальными усилиями по предобработке.\n",
        "\n",
        "Правильный выбор инструмента — это уже половина успеха в решении задачи машинного обучения. Глубокое понимание архитектуры XGBoost, LightGBM и CatBoost позволяет сделать этот выбор осознанно и уверенно."
      ],
      "metadata": {
        "id": "_7T824v3Mcvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Сравнительная архитектура\n",
        "\n",
        "### Детальный анализ различий в подходах\n",
        "\n",
        "Выбор между XGBoost, LightGBM и CatBoost не должен основываться на априорных предпочтениях, а на **систематическом сравнении** их производительности в идентичных условиях. Такой подход позволяет выявить их истинные сильные и слабые стороны и сформулировать объективные рекомендации.\n",
        "\n",
        "Для честного бенчмарка необходимо:\n",
        "1.  Использовать один и тот же датасет, богатый как числовыми, так и категориальными признаками.\n",
        "2.  Сконфигурировать гиперпараметры моделей так, чтобы они были сопоставимы по сложности (например, `max_depth=6` в XGBoost/CatBoost эквивалентен `num_leaves=31` в LightGBM).\n",
        "3.  Измерять не только качество модели (точность, AUC), но и ключевые инженерные метрики: время обучения и потребление памяти.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий код реализует методологически строгий бенчмарк, где все три библиотеки обучаются на одних и тех же данных с эквивалентными настройками. Это позволяет провести объективное сравнение.\n",
        "\n",
        "```python\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# === Функция для генерации стандартизированного датасета ===\n",
        "def create_benchmark_dataset(n_samples=10000, n_categorical=5, n_numerical=15):\n",
        "    \"\"\"Генерирует датасет с контролируемыми характеристиками.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    from sklearn.datasets import make_classification\n",
        "    X_num, y = make_classification(n_samples=n_samples, n_features=n_numerical,\n",
        "                                   n_informative=10, n_redundant=5, random_state=42)\n",
        "    df = pd.DataFrame(X_num, columns=[f'num_{i}' for i in range(n_numerical)])\n",
        "    df['target'] = y\n",
        "\n",
        "    for i in range(n_categorical):\n",
        "        n_cats = np.random.randint(5, 20)\n",
        "        df[f'cat_{i}'] = np.random.choice([f'cat_{i}_val_{j}' for j in range(n_cats)], n_samples)\n",
        "    \n",
        "    # Имитация пропусков\n",
        "    mask = np.random.random(df.shape) < 0.05\n",
        "    df = df.mask(mask)\n",
        "    return df.drop('target', axis=1), df['target']\n",
        "\n",
        "# Создание датасета\n",
        "X, y = create_benchmark_dataset(10000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "categorical_features = [f'cat_{i}' for i in range(5)]\n",
        "\n",
        "def benchmark_models(X_train, y_train, X_test, y_test, cat_features):\n",
        "    \"\"\"Проводит честный бенчмарк трёх библиотек.\"\"\"\n",
        "    results = {}\n",
        "    times = {}\n",
        "    \n",
        "    # === Подготовка данных для каждой библиотеки ===\n",
        "    # XGBoost: Label Encoding\n",
        "    X_train_xgb, X_test_xgb = X_train.copy(), X_test.copy()\n",
        "    for col in cat_features:\n",
        "        le = LabelEncoder()\n",
        "        X_train_xgb[col] = le.fit_transform(X_train_xgb[col].fillna('MISSING').astype(str))\n",
        "        X_test_xgb[col] = le.transform(X_test_xgb[col].fillna('MISSING').astype(str))\n",
        "    \n",
        "    # LightGBM: Native категориальные признаки\n",
        "    X_train_lgb, X_test_lgb = X_train.copy(), X_test.copy()\n",
        "    for col in cat_features:\n",
        "        X_train_lgb[col] = X_train_lgb[col].fillna('MISSING').astype('category')\n",
        "        X_test_lgb[col] = X_test_lgb[col].fillna('MISSING').astype('category')\n",
        "    \n",
        "    # CatBoost: Работает с сырыми данными\n",
        "    X_train_cb, X_test_cb = X_train.copy(), X_test.copy()\n",
        "    \n",
        "    # === Обучение моделей ===\n",
        "    common_params = {\n",
        "        'n_estimators': 500,\n",
        "        'learning_rate': 0.1,\n",
        "        'random_state': 42,\n",
        "        'early_stopping_rounds': 50\n",
        "    }\n",
        "    \n",
        "    # XGBoost\n",
        "    print(\"Обучение XGBoost...\")\n",
        "    start = time.time()\n",
        "    xgb_model = xgb.XGBClassifier(max_depth=6, subsample=0.8, colsample_bytree=0.8, **common_params)\n",
        "    xgb_model.fit(X_train_xgb, y_train, eval_set=[(X_test_xgb, y_test)], verbose=False)\n",
        "    times['XGBoost'] = time.time() - start\n",
        "    results['XGBoost'] = accuracy_score(y_test, xgb_model.predict(X_test_xgb))\n",
        "    \n",
        "    # LightGBM\n",
        "    print(\"Обучение LightGBM...\")\n",
        "    start = time.time()\n",
        "    lgb_model = lgb.LGBMClassifier(num_leaves=31, feature_fraction=0.8, bagging_fraction=0.8, **common_params)\n",
        "    lgb_model.fit(X_train_lgb, y_train, eval_set=[(X_test_lgb, y_test)], verbose=False)\n",
        "    times['LightGBM'] = time.time() - start\n",
        "    results['LightGBM'] = accuracy_score(y_test, lgb_model.predict(X_test_lgb))\n",
        "    \n",
        "    # CatBoost\n",
        "    print(\"Обучение CatBoost...\")\n",
        "    start = time.time()\n",
        "    cb_model = cb.CatBoostClassifier(depth=6, **common_params, verbose=False)\n",
        "    cb_model.fit(X_train_cb, y_train, cat_features=cat_features, eval_set=(X_test_cb, y_test), verbose=False)\n",
        "    times['CatBoost'] = time.time() - start\n",
        "    results['CatBoost'] = accuracy_score(y_test, cb_model.predict(X_test_cb))\n",
        "    \n",
        "    return results, times\n",
        "\n",
        "# === Запуск бенчмарка и визуализация ===\n",
        "results, times = benchmark_models(X_train, y_train, X_test, y_test, categorical_features)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Точность\n",
        "models = list(results.keys())\n",
        "accs = list(results.values())\n",
        "bars1 = ax1.bar(models, accs, color=['#007bff', '#28a745', '#dc3545'])\n",
        "ax1.set_ylabel('Точность (Accuracy)', fontsize=12)\n",
        "ax1.set_title('Сравнение качества моделей', fontsize=14)\n",
        "ax1.set_ylim(min(accs)-0.01, max(accs)+0.01)\n",
        "for bar, acc in zip(bars1, accs):\n",
        "    ax1.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005, f'{acc:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Время обучения\n",
        "t_vals = list(times.values())\n",
        "bars2 = ax2.bar(models, t_vals, color=['#007bff', '#28a745', '#dc3545'])\n",
        "ax2.set_ylabel('Время обучения (секунды)', fontsize=12)\n",
        "ax2.set_title('Сравнение скорости обучения', fontsize=14)\n",
        "for bar, t in zip(bars2, t_vals):\n",
        "    ax2.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.1, f'{t:.2f}s', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nИтоги сравнительного анализа:\")\n",
        "for m in models:\n",
        "    print(f\"{m:12} | Точность: {results[m]:.4f} | Время: {times[m]:.2f}с\")\n",
        "print(f\"\\nПотребление памяти: {psutil.Process(os.getpid()).memory_info().rss/1024/1024:.2f} MB\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Такой бенчмарк показывает, что **LightGBM** демонстрирует наилучшее соотношение скорости и качества, в то время как **CatBoost** и **XGBoost** обеспечивают сопоставимое качество при большем времени обучения. Это подтверждает их архитектурную специализацию.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Производительность и масштабируемость\n",
        "\n",
        "### Сравнение на датасетах разного размера\n",
        "\n",
        "Эффективность алгоритма не является постоянной величиной — она зависит от объёма данных. Анализ **масштабируемости** позволяет определить, какая библиотека лучше всего подходит для работы с малыми, средними и крупными наборами данных.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот эксперимент измеряет время обучения каждой библиотеки на датасетах, чей размер варьируется от 1 000 до 20 000 наблюдений, и строит графики зависимости времени от объёма данных.\n",
        "\n",
        "```python\n",
        "# === Тестирование масштабируемости ===\n",
        "print(\"=== 4. ПРОИЗВОДИТЕЛЬНОСТЬ И МАСШТАБИРУЕМОСТЬ ===\")\n",
        "dataset_sizes = [1000, 5000, 10000, 20000]\n",
        "scalability = {'XGBoost': [], 'LightGBM': [], 'CatBoost': []}\n",
        "\n",
        "for size in dataset_sizes:\n",
        "    print(f\"Тестирование на датасете из {size} объектов...\")\n",
        "    X_temp, y_temp = create_benchmark_dataset(size)\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n",
        "    _, t = benchmark_models(X_tr, y_tr, X_te, y_te, categorical_features)\n",
        "    for model in scalability:\n",
        "        scalability[model].append(t[model])\n",
        "\n",
        "# Визуализация масштабируемости\n",
        "plt.figure(figsize=(12, 7))\n",
        "for model, t_list in scalability.items():\n",
        "    plt.plot(dataset_sizes, t_list, marker='o', linewidth=3, markersize=8, label=model)\n",
        "plt.xlabel('Размер датасета (количество объектов)', fontsize=12)\n",
        "plt.ylabel('Время обучения (секунды)', fontsize=12)\n",
        "plt.title('Масштабируемость алгоритмов градиентного бустинга', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# === Анализ GPU-ускорения ===\n",
        "def check_gpu_support():\n",
        "    \"\"\"Проверяет поддержку GPU каждой библиотекой.\"\"\"\n",
        "    support = {}\n",
        "    try:  # XGBoost\n",
        "        _ = xgb.XGBClassifier(tree_method='gpu_hist', n_estimators=1).fit(X_train.head(10), y_train.head(10))\n",
        "        support['XGBoost'] = True\n",
        "    except: support['XGBoost'] = False\n",
        "    \n",
        "    try:  # LightGBM\n",
        "        _ = lgb.LGBMClassifier(device='gpu', n_estimators=1).fit(X_train_lgb.head(10), y_train.head(10))\n",
        "        support['LightGBM'] = True\n",
        "    except: support['LightGBM'] = False\n",
        "    \n",
        "    try:  # CatBoost\n",
        "        _ = cb.CatBoostClassifier(task_type='GPU', n_estimators=1, verbose=False).fit(X_train.head(10), y_train.head(10), cat_features=categorical_features)\n",
        "        support['CatBoost'] = True\n",
        "    except: support['CatBoost'] = False\n",
        "    return support\n",
        "\n",
        "gpu_info = check_gpu_support()\n",
        "print(\"\\nПоддержка GPU:\")\n",
        "for m, s in gpu_info.items():\n",
        "    print(f\"  {m:12}: {'✓ Доступно' if s else '✗ Недоступно'}\")\n",
        "\n",
        "# Сравнение CPU vs GPU (на подвыборке)\n",
        "if any(gpu_info.values()):\n",
        "    print(\"\\nСравнение CPU vs GPU:\")\n",
        "    X_samp, y_samp = X_train.head(5000), y_train.head(5000)\n",
        "    for m, s in gpu_info.items():\n",
        "        if s:\n",
        "            # CPU\n",
        "            start = time.time()\n",
        "            if m == 'XGBoost':\n",
        "                _ = xgb.XGBClassifier(n_estimators=100, random_state=42).fit(X_train_xgb.head(5000), y_samp)\n",
        "            elif m == 'LightGBM':\n",
        "                _ = lgb.LGBMClassifier(n_estimators=100, verbose=-1, random_state=42).fit(X_train_lgb.head(5000), y_samp)\n",
        "            else: # CatBoost\n",
        "                _ = cb.CatBoostClassifier(n_estimators=100, verbose=False, random_state=42).fit(X_samp, y_samp, cat_features=categorical_features)\n",
        "            cpu_t = time.time() - start\n",
        "            \n",
        "            # GPU\n",
        "            start = time.time()\n",
        "            if m == 'XGBoost':\n",
        "                _ = xgb.XGBClassifier(n_estimators=100, tree_method='gpu_hist', random_state=42).fit(X_train_xgb.head(5000), y_samp)\n",
        "            elif m == 'LightGBM':\n",
        "                _ = lgb.LGBMClassifier(n_estimators=100, device='gpu', random_state=42, verbose=-1).fit(X_train_lgb.head(5000), y_samp)\n",
        "            else: # CatBoost\n",
        "                _ = cb.CatBoostClassifier(n_estimators=100, task_type='GPU', verbose=False, random_state=42).fit(X_samp, y_samp, cat_features=categorical_features)\n",
        "            gpu_t = time.time() - start\n",
        "            \n",
        "            print(f\"  {m:12} | CPU: {cpu_t:.2f}с, GPU: {gpu_t:.2f}с, Ускорение: {cpu_t/gpu_t:.2f}x\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "График масштабируемости наглядно демонстрирует, что **LightGBM** имеет наименьший наклон, что говорит о его превосходной эффективности на больших данных. GPU-ускорение предоставляет значительный прирост производительности (в 5–20 раз) для всех трёх библиотек, но реализация в **CatBoost** часто оказывается самой стабильной и простой в настройке.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Гиперпараметры и настройка моделей\n",
        "\n",
        "### Сравнение подходов к оптимизации\n",
        "\n",
        "Даже самая совершенная архитектура не сможет продемонстрировать своё истинное качество без правильной настройки гиперпараметров. Этот раздел сравнивает эффективность и стоимость процесса оптимизации для каждой библиотеки.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Используя `RandomizedSearchCV`, мы настраиваем ключевые гиперпараметры каждой модели и сравниваем как итоговое качество, так и затраченное на поиск время.\n",
        "\n",
        "```python\n",
        "# === Настройка гиперпараметров ===\n",
        "print(\"=== 5. НАСТРОЙКА ГИПЕРПАРАМЕТРОВ ===\")\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grids = {\n",
        "    'XGBoost': {\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'num_leaves': [15, 31, 63],\n",
        "        'feature_fraction': [0.8, 0.9, 1.0]\n",
        "    },\n",
        "    'CatBoost': {\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'depth': [4, 6, 8],\n",
        "        'l2_leaf_reg': [1, 3, 5]\n",
        "    }\n",
        "}\n",
        "\n",
        "def optimize_model(name, X, y):\n",
        "    print(f\"\\nНастройка {name}...\")\n",
        "    if name == 'XGBoost':\n",
        "        model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "    elif name == 'LightGBM':\n",
        "        model = lgb.LGBMClassifier(n_estimators=100, verbose=-1, random_state=42)\n",
        "    else: # CatBoost\n",
        "        model = cb.CatBoostClassifier(n_estimators=100, verbose=False, random_state=42)\n",
        "    \n",
        "    search = RandomizedSearchCV(model, param_grids[name], n_iter=9, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "    start = time.time()\n",
        "    search.fit(X, y)\n",
        "    opt_time = time.time() - start\n",
        "    print(f\"Лучшая CV-точность: {search.best_score_:.4f}, Время: {opt_time:.2f}с\")\n",
        "    return search.best_estimator_, search.best_score_, opt_time\n",
        "\n",
        "# Оптимизация всех моделей\n",
        "opt_models, opt_results = {}, {}\n",
        "opt_models['XGBoost'], opt_results['XGBoost']['score'], opt_results['XGBoost']['time'] = optimize_model('XGBoost', X_train_xgb, y_train)\n",
        "opt_models['LightGBM'], opt_results['LightGBM']['score'], opt_results['LightGBM']['time'] = optimize_model('LightGBM', X_train_lgb, y_train)\n",
        "opt_models['CatBoost'], opt_results['CatBoost']['score'], opt_results['CatBoost']['time'] = optimize_model('CatBoost', X_train, y_train)\n",
        "\n",
        "# Тестирование на hold-out выборке\n",
        "final_acc = {}\n",
        "for name, model in opt_models.items():\n",
        "    if name == 'XGBoost':\n",
        "        final_acc[name] = accuracy_score(y_test, model.predict(X_test_xgb))\n",
        "    elif name == 'LightGBM':\n",
        "        final_acc[name] = accuracy_score(y_test, model.predict(X_test_lgb))\n",
        "    else: # CatBoost\n",
        "        final_acc[name] = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "# Визуализация улучшений\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "x = np.arange(3)\n",
        "width = 0.35\n",
        "before = [results[m] for m in ['XGBoost', 'LightGBM', 'CatBoost']]\n",
        "after = [final_acc[m] for m in ['XGBoost', 'LightGBM', 'CatBoost']]\n",
        "ax.bar(x - width/2, before, width, label='До оптимизации', alpha=0.8)\n",
        "ax.bar(x + width/2, after, width, label='После оптимизации', alpha=0.8)\n",
        "ax.set_ylabel('Точность', fontsize=12)\n",
        "ax.set_title('Влияние настройки гиперпараметров на качество моделей', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(['XGBoost', 'LightGBM', 'CatBoost'])\n",
        "ax.legend()\n",
        "for i, (b, a) in enumerate(zip(before, after)):\n",
        "    ax.annotate(f'{a-b:+.4f}', (i, max(b, a)+0.005), ha='center')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nИтоги настройки гиперпараметров:\")\n",
        "for m in ['XGBoost', 'LightGBM', 'CatBoost']:\n",
        "    print(f\"{m:12} | CV: {opt_results[m]['score']:.4f} | Test: {final_acc[m]:.4f} | Время: {opt_results[m]['time']:.2f}с\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Настройка гиперпараметров приносит улучшение для всех моделей, но стоимость этого улучшения различна. **LightGBM** и **CatBoost** часто достигают оптимальных параметров быстрее благодаря своей архитектуре, в то время как **XGBoost** может потребовать больше времени для исследования пространства гиперпараметров.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение по Модулю 18\n",
        "\n",
        "Проведённый сравнительный анализ позволяет сформулировать четкие и практические рекомендации по выбору инструмента из «большой тройки»:\n",
        "\n",
        "*   **XGBoost** — это выбор для **консервативных и критически важных систем**, где стабильность, воспроизводимость и зрелость экосистемы важнее абсолютной скорости. Его строгая математическая основа делает его надёжным эталоном.\n",
        "\n",
        "*   **LightGBM** — это выбор для **промышленных систем с большими данными**, где время обучения и потребление памяти являются ключевыми ограничениями. Его архитектурные оптимизации позволяют обрабатывать миллионы строк в считанные минуты.\n",
        "\n",
        "*   **CatBoost** — это выбор для **аналитиков и исследователей**, работающих с данными, насыщенными категориальными признаками. Его интеллектуальная обработка категорий и встроенные механизмы борьбы с переобучением позволяют достигать высокого качества с минимальными усилиями по подготовке данных.\n",
        "\n",
        "Идеальный современный MLOps-пайплайн часто включает в себя **ансамбль из двух или трёх моделей**, построенных на разных библиотеках. Такой подход позволяет воспользоваться сильными сторонами каждой из них, обеспечивая максимальную производительность и надёжность. Понимание их архитектурных различий, представленное в этом модуле, является ключом к успешному применению этого продвинутого метода."
      ],
      "metadata": {
        "id": "1G-mG_GTNniy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Модуль 19: NLP — от традиционных методов к современным трансформерам\n",
        "\n",
        "## Введение\n",
        "\n",
        "Обработка естественного языка (Natural Language Processing, NLP) представляет собой одну из наиболее динамичных и стратегически важных областей искусственного интеллекта. За последние три десятилетия эта дисциплина прошла путь от систем, основанных на **жёстких лингвистических правилах**, до **глубоких нейронных сетей**, способных к семантическому пониманию и генерации языка на уровне, приближающемся к человеческому. Эта эволюция не только радикально повысила качество решений ключевых NLP-задач (таких как машинный перевод, анализ тональности или ответы на вопросы), но и полностью изменила методологию разработки, сместив фокус с ручного проектирования признаков на автоматическое обучение представлений из данных.\n",
        "\n",
        "Ключевым технологическим прорывом, определившим современную эру NLP, стала архитектура **Transformer**, представленная в работе Vaswani et al. (2017). Её внедрение в такие модели, как BERT, GPT и T5, дало начало эпохе **transfer learning**, где предобученные на огромных корпусах языковые модели могут быть эффективно дообучены (fine-tuned) для решения конкретных задач с минимальным объёмом размеченных данных. В экосистеме Python эта революция была обеспечена благодаря библиотекам `transformers` от Hugging Face, которые демократизировали доступ к передовым моделям.\n",
        "\n",
        "### Эволюция NLP в Python: архитектурные парадигмы\n",
        "\n",
        "*   **Правило-ориентированные системы **(1990-2000-е): Эра, доминируемая библиотекой **NLTK** (Natural Language Toolkit). Решение задач базировалось на создании вручную правил (grammar rules), словарей (lexicons) и шаблонов. Такой подход требовал глубоких лингвистических знаний, был трудоёмким и плохо масштабировался на новые домены или языки.\n",
        "*   **Статистические и машинно-обучные методы **(2000-2010-е): Внедрение фреймворков **Scikit-learn** и **Gensim** перевело NLP в парадигму машинного обучения. Текст стал представляться в виде числовых векторов (например, TF-IDF), которые затем подавались в классические ML-модели (логистическая регрессия, SVM). Этот подход был более автоматизированным и масштабируемым, но всё ещё сильно зависел от качества инжиниринга признаков.\n",
        "*   **Нейросетевые репрезентации **(2010-2017): Появление алгоритмов вроде **Word2Vec** и **GloVe** позволило получать плотные векторные представления слов (embeddings), захватывающие семантические и синтаксические связи. Архитектуры на основе **рекуррентных нейронных сетей **(RNN/LSTM) и **свёрточных сетей **(CNN) использовались для моделирования последовательностей, что привело к значительному росту качества в задачах классификации и генерации текста.\n",
        "*   **Эра Transformer и Transfer Learning **(2018-настоящее): Архитектура Transformer, основанная на механизме **внимания **(Attention), устранила фундаментальные ограничения RNN (медленное последовательное вычисление) и CNN (ограниченное контекстное окно). Модели, предобученные на задачах маскированного языкового моделирования (BERT) или автогрессивной генерации (GPT), научились создавать контекстуально-зависимые эмбеддинги, которые стали универсальным отправным пунктом для решения практически любой NLP-задачи.\n",
        "\n",
        "### Экосистема Python для NLP\n",
        "\n",
        "Современный NLP-стек в Python представляет собой многослойную экосистему, где каждая библиотека решает свои специфические задачи:\n",
        "*   **NLTK** и **spaCy**: фундаментальная обработка текста (токенизация, лемматизация, NER).\n",
        "*   **Gensim**: работа с тематическим моделированием и word embeddings.\n",
        "*   **Scikit-learn**: традиционные ML-модели для классификации и регрессии на текстовых признаках.\n",
        "*   **Hugging Face Transformers**: предоставление доступа к тысячам предобученных моделей и инструментов для их fine-tuning.\n",
        "*   **PyTorch/TensorFlow**: фреймворки для создания и тренировки кастомных нейросетей.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. NLTK: фундаментальные методы обработки текста\n",
        "\n",
        "### Лингвистические основы и традиционные подходы\n",
        "\n",
        "**NLTK **(Natural Language Toolkit) — это не просто библиотека, а полноценная образовательная платформа, содержащая обширные корпуса данных, лингвистические ресурсы и инструменты для исследования структуры языка. Несмотря на появление более производительных альтернатив (например, spaCy), NLTK остаётся незаменимым инструментом для обучения и понимания фундаментальных концепций NLP.\n",
        "\n",
        "Процесс обработки текста в традиционном NLP-пайплайне состоит из последовательных этапов, каждый из которых решает конкретную задачу:\n",
        "\n",
        "1.  **Токенизация **(Tokenization) — разбиение текста на атомарные единицы: слова, пунктуацию, числа.\n",
        "2.  **Нормализация **(Normalization) — приведение слов к их канонической форме (лемматизация) или корню (стемминг).\n",
        "3.  **Частеречная разметка **(Part-of-Speech Tagging, POS) — определение грамматической категории каждого токена (существительное, глагол, прилагательное и т.д.).\n",
        "4.  **Синтаксический анализ **(Parsing) — выявление грамматической структуры предложения, например, идентификация именных (NP) и глагольных (VP) групп.\n",
        "5.  **Извлечение именованных сущностей **(Named Entity Recognition, NER) — обнаружение и классификация сущностей, таких как имена людей, организаций, локаций.\n",
        "6.  **Анализ тональности **(Sentiment Analysis) — определение эмоциональной окраски текста.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует полный цикл традиционной обработки текста с использованием NLTK, от самого первого шага (токенизации) до конечного аналитического результата (анализа тональности).\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.chunk import RegexpParser\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# === Инициализация NLTK ===\n",
        "# Загрузка необходимых лингвистических ресурсов\n",
        "nltk_resources = [\n",
        "    'punkt', 'stopwords', 'averaged_perceptron_tagger',\n",
        "    'maxent_ne_chunker', 'words', 'wordnet', 'vader_lexicon'\n",
        "]\n",
        "for resource in nltk_resources:\n",
        "    nltk.download(resource, quiet=True)\n",
        "\n",
        "print(\"=== 1. NLTK: ФУНДАМЕНТАЛЬНЫЕ МЕТОДЫ ОБРАБОТКИ ТЕКСТА ===\")\n",
        "\n",
        "# Текст для анализа\n",
        "sample_text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer science,\n",
        "and artificial intelligence concerned with the interactions between computers and human language.\n",
        "It was founded in the 1950s as machine translation. Modern NLP includes many tasks like\n",
        "sentiment analysis, named entity recognition, and text generation. Companies like Google,\n",
        "Microsoft, and Amazon use NLP in their products every day.\n",
        "\"\"\"\n",
        "\n",
        "# === 1.1. Токенизация ===\n",
        "print(\"\\n1.1. Токенизация\")\n",
        "sentences = sent_tokenize(sample_text)\n",
        "print(f\"Количество предложений: {len(sentences)}\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"  {i}. {sent.strip()}\")\n",
        "\n",
        "words = word_tokenize(sample_text)\n",
        "print(f\"\\nОбщее количество токенов: {len(words)}\")\n",
        "print(f\"Первые 15 токенов: {words[:15]}\")\n",
        "\n",
        "# === 1.2. Нормализация ===\n",
        "print(\"\\n1.2. Нормализация\")\n",
        "\n",
        "# Удаление стоп-слов и пунктуации\n",
        "stop_words = set(stopwords.words('english')).union(set(string.punctuation))\n",
        "content_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "# Стемминг (Porter Stemmer) и Лемматизация (WordNet)\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [stemmer.stem(word) for word in content_words]\n",
        "lemmatized = [lemmatizer.lemmatize(word.lower(), pos='v') for word in content_words]\n",
        "\n",
        "print(\"Пример нормализации:\")\n",
        "for orig, stem, lemm in zip(content_words[:8], stemmed[:8], lemmatized[:8]):\n",
        "    print(f\"{orig:15} -> Стем: {stem:10} -> Лемма: {lemm}\")\n",
        "\n",
        "# === 1.3. Частеречная разметка (POS Tagging) ===\n",
        "print(\"\\n1.3. Частеречная разметка\")\n",
        "pos_tags = pos_tag(content_words[:12])  # Анализ первых 12 слов\n",
        "\n",
        "pos_guide = {\n",
        "    'NN': 'существительное (ед.ч.)', 'NNS': 'существительное (мн.ч.)',\n",
        "    'VB': 'глагол (инфинитив)', 'VBD': 'глагол (прош.вр.)',\n",
        "    'JJ': 'прилагательное', 'RB': 'наречие', 'IN': 'предлог'\n",
        "}\n",
        "\n",
        "print(\"Примеры POS-тегов:\")\n",
        "for word, tag in pos_tags:\n",
        "    explanation = pos_guide.get(tag, 'другое')\n",
        "    print(f\"{word:12} -> {tag:4} ({explanation})\")\n",
        "\n",
        "# === 1.4. Синтаксический анализ (Chunking) ===\n",
        "print(\"\\n1.4. Синтаксический анализ\")\n",
        "# Определение грамматических правил для извлечения именных групп (NP)\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "# Применение к предложению\n",
        "sample_sent = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sent_pos = pos_tag(word_tokenize(sample_sent))\n",
        "chunked = chunk_parser.parse(sent_pos)\n",
        "\n",
        "print(\"Результат синтаксического анализа:\")\n",
        "print(chunked)\n",
        "# chunked.draw() # Для визуализации дерева (раскомментировать в ноутбуке)\n",
        "\n",
        "# === 1.5. Извлечение именованных сущностей (NER) ===\n",
        "print(\"\\n1.5. Извлечение именованных сущностей\")\n",
        "ner_text = \"Google was founded by Larry Page and Sergey Brin in Mountain View, California.\"\n",
        "ner_tree = ne_chunk(pos_tag(word_tokenize(ner_text)))\n",
        "\n",
        "print(\"Результат NER:\")\n",
        "print(ner_tree)\n",
        "\n",
        "# Анализ структуры дерева для извлечения сущностей\n",
        "entities = []\n",
        "for chunk in ner_tree:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        entity = ' '.join([token for token, pos in chunk.leaves()])\n",
        "        entities.append((entity, chunk.label()))\n",
        "\n",
        "print(\"\\nИзвлечённые сущности:\")\n",
        "for entity, label in entities:\n",
        "    print(f\"  {label}: {entity}\")\n",
        "\n",
        "# === 1.6. Анализ тональности ===\n",
        "print(\"\\n1.6. Анализ тональности с VADER\")\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "test_texts = [\n",
        "    \"I love this product! It's absolutely amazing!\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"The product is okay. Nothing special, but it works.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    scores = sia.polarity_scores(text)\n",
        "    compound = scores['compound']\n",
        "    sentiment = \"ПОЗИТИВНЫЙ\" if compound >= 0.05 else \"НЕГАТИВНЫЙ\" if compound <= -0.05 else \"НЕЙТРАЛЬНЫЙ\"\n",
        "    print(f\"\\nТекст: {text}\")\n",
        "    print(f\"Тональность: {sentiment} (compound: {compound:.3f})\")\n",
        "\n",
        "# === 1.7. Статистический анализ текста ===\n",
        "print(\"\\n1.7. Статистический анализ\")\n",
        "\n",
        "# Частотный анализ слов\n",
        "word_freq = Counter([w.lower() for w in content_words])\n",
        "most_common = word_freq.most_common(10)\n",
        "\n",
        "print(\"10 самых частых слов:\")\n",
        "for word, freq in most_common:\n",
        "    print(f\"  {word}: {freq}\")\n",
        "\n",
        "# Визуализация\n",
        "words, freqs = zip(*most_common)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(words, freqs)\n",
        "plt.xlabel('Частота')\n",
        "plt.title('Топ-10 слов в тексте')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример иллюстрирует, что даже с использованием традиционных методов можно извлечь из текста богатую структурированную информацию: от базовой статистики до синтаксических и семантических отношений. Однако каждый шаг требует ручной настройки и сильно зависит от качества лингвистических ресурсов, что делает этот подход трудоёмким для масштабирования. Современные нейросетевые методы, которые будут рассмотрены далее, автоматизируют многие из этих задач, обучаясь выявлять сложные паттерны непосредственно из данных.\n",
        ""
      ],
      "metadata": {
        "id": "qckbjHpTO_iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. SpaCy: промышленная обработка текста\n",
        "\n",
        "### Эффективная лингвистически точная обработка\n",
        "\n",
        "В то время как NLTK превосходит в образовательных целях и гибкости, **SpaCy** представляет собой инструмент, спроектированный исключительно для **промышленной эксплуатации**. Его архитектура оптимизирована под три ключевых требования современных NLP-систем: **высокая производительность**, **лингвистическая точность** и **предсказуемость**. В отличие от модульного подхода NLTK, SpaCy предоставляет единый, согласованный пайплайн обработки, который «из коробки» решает большинство стандартных задач обработки текста.\n",
        "\n",
        "Центральным элементом архитектуры SpaCy является **конфигурируемый пайплайн** (*pipeline*), состоящий из последовательных компонентов (компонентов обработки). Каждый компонент модифицирует объект `Doc` — центральную структуру данных, представляющую полный проанализированный текст со всеми его лингвистическими аннотациями. Такой подход обеспечивает:\n",
        "*   **Эффективность**: данные обрабатываются один раз и все аннотации хранятся в одном месте.\n",
        "*   **Согласованность**: все компоненты работают с одним и тем же представлением текста, что исключает ошибки рассогласования.\n",
        "*   **Расширяемость**: разработчик может легко добавлять свои кастомные компоненты в пайплайн, не нарушая его целостности.\n",
        "\n",
        "SpaCy предоставляет предобученные модели (например, `en_core_web_sm`, `en_core_web_lg`), которые включают в себя не только правила токенизации и лемматизации, но и сложные статистические модели для POS-теггинга, NER и анализа зависимостей, обученные на крупных лингвистических корпусах. Более того, большие модели (`_lg`, `_trf`) содержат **предобученные word embeddings**, что открывает доступ к семантическому анализу без дополнительных усилий.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует промышленный подход к обработке текста с использованием SpaCy, включая анализ лингвистических атрибутов, извлечение сущностей, семантический анализ и работу с производительностью.\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import time\n",
        "from spacy.lang.en import English\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "print(\"=== 2. SPACY: ПРОМЫШЛЕННАЯ ОБРАБОТКА ТЕКСТА ===\")\n",
        "\n",
        "# === 2.1. Инициализация и обработка текста ===\n",
        "# Попытка загрузить стандартную модель\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"Загружена модель: en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Модель en_core_web_sm не найдена. Используется пустая английская модель для демонстрации базовых функций.\")\n",
        "    nlp = English()\n",
        "    # В продакшене это недопустимо — модель должна быть установлена\n",
        "\n",
        "doc_text = \"\"\"\n",
        "Apple Inc. is planning to open a new store in Paris in 2024.\n",
        "The company, founded by Steve Jobs and Steve Wozniak, reported\n",
        "revenue of $365 billion last year. Dr. Smith, the CEO, said:\n",
        "\"We're excited about this expansion into European markets.\"\n",
        "\"\"\"\n",
        "\n",
        "doc = nlp(doc_text)\n",
        "\n",
        "# === 2.2. Детальный лингвистический анализ ===\n",
        "print(\"\\n2.2. Детальный лингвистический анализ токенов\")\n",
        "token_data = []\n",
        "for token in doc[:15]:  # Анализ первых 15 токенов\n",
        "    token_data.append({\n",
        "        'Токен': token.text,\n",
        "        'Лемма': token.lemma_,\n",
        "        'Часть речи (POS)': token.pos_,\n",
        "        'Финер POS-тег': token.tag_,\n",
        "        'Синтаксическая роль': token.dep_,\n",
        "        'Голова': token.head.text if token.head else 'ROOT',\n",
        "        'Шейп (форма)': token.shape_,\n",
        "        'Стоп-слово': token.is_stop,\n",
        "        'Пунктуация': token.is_punct,\n",
        "        'Число': token.like_num\n",
        "    })\n",
        "\n",
        "df_tokens = pd.DataFrame(token_data)\n",
        "with pd.option_context('display.max_columns', None, 'display.width', 1000):\n",
        "    print(df_tokens.to_string(index=False))\n",
        "\n",
        "# === 2.3. Извлечение именованных сущностей (NER) ===\n",
        "print(\"\\n2.3. Извлечение именованных сущностей\")\n",
        "print(f\"Обнаружено {len(doc.ents)} сущностей:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"  {ent.text:25} | {ent.label_:10} | {spacy.explain(ent.label_)}\")\n",
        "\n",
        "# === 2.4. Синтаксический анализ зависимостей ===\n",
        "print(\"\\n2.4. Синтаксический анализ зависимостей\")\n",
        "first_sent = list(doc.sents)[0]\n",
        "print(f\"Анализ первого предложения: '{first_sent.text.strip()}'\")\n",
        "\n",
        "dep_data = []\n",
        "for token in first_sent:\n",
        "    dep_data.append({\n",
        "        'Токен': token.text,\n",
        "        'Зависимость': token.dep_,\n",
        "        'Голова': token.head.text\n",
        "    })\n",
        "\n",
        "df_deps = pd.DataFrame(dep_data)\n",
        "print(df_deps.to_string(index=False))\n",
        "\n",
        "# === 2.5. Семантическое сходство (требует модели с векторами) ===\n",
        "print(\"\\n2.5. Семантическое сходство\")\n",
        "try:\n",
        "    nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "    print(\"Загружена модель с векторами: en_core_web_lg\")\n",
        "    \n",
        "    # Сравнение сходства слов\n",
        "    words = [\"cat\", \"dog\", \"car\", \"computer\", \"animal\"]\n",
        "    vectors = {word: nlp_lg(word) for word in words}\n",
        "    \n",
        "    # Матрица сходства\n",
        "    sim_matrix = []\n",
        "    for w1 in words:\n",
        "        row = [vectors[w1].similarity(vectors[w2]) for w2 in words]\n",
        "        sim_matrix.append(row)\n",
        "    \n",
        "    df_sim = pd.DataFrame(sim_matrix, columns=words, index=words)\n",
        "    print(\"\\nМатрица косинусного сходства:\")\n",
        "    print(df_sim.round(3))\n",
        "    \n",
        "except OSError:\n",
        "    print(\"Модель en_core_web_lg не найдена. Семантический анализ недоступен.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Ошибка сходства: {e}\")\n",
        "\n",
        "# === 2.6. Производительность: Batch Processing ===\n",
        "print(\"\\n2.6. Производительность: Batch Processing\")\n",
        "\n",
        "large_texts = [doc_text] * 100  # Имитация 100 документов\n",
        "\n",
        "# Последовательная обработка\n",
        "start_time = time.time()\n",
        "for text in large_texts:\n",
        "    _ = nlp(text)\n",
        "sequential_time = time.time() - start_time\n",
        "\n",
        "# Batch-обработка\n",
        "start_time = time.time()\n",
        "docs = list(nlp.pipe(large_texts, batch_size=16, n_process=1))\n",
        "batch_time = time.time() - start_time\n",
        "\n",
        "print(f\"Последовательная обработка: {sequential_time:.2f}с\")\n",
        "print(f\"Batch-обработка:           {batch_time:.2f}с\")\n",
        "print(f\"Ускорение:                 {sequential_time / batch_time:.2f}x\")\n",
        "\n",
        "# === 2.7. Кастомизация пайплайна ===\n",
        "print(\"\\n2.7. Кастомизация пайплайна\")\n",
        "\n",
        "# Добавление пользовательского расширения к Doc\n",
        "if not Doc.has_extension(\"tech_entities\"):\n",
        "    Doc.set_extension(\"tech_entities\", default=[])\n",
        "\n",
        "def tech_entity_component(doc):\n",
        "    \"\"\"Кастомный компонент для извлечения технических сущностей\"\"\"\n",
        "    tech_terms = {\"AI\", \"NLP\", \"ML\", \"Python\", \"Apple\", \"Paris\", \"CEO\"}\n",
        "    doc._.tech_entities = [ent.text for ent in doc.ents if ent.text in tech_terms]\n",
        "    return doc\n",
        "\n",
        "# Добавление компонента в пайплайн (опционально)\n",
        "# nlp.add_pipe(tech_entity_component, after=\"ner\")\n",
        "\n",
        "# Применение компонента вручную\n",
        "final_doc = tech_entity_component(doc)\n",
        "print(f\"Технические сущности: {final_doc._.tech_entities}\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "SpaCy демонстрирует, как можно построить надёжный, производительный и расширяемый NLP-пайплайн. Его подход «всё в одном» минимизирует сложность интеграции и делает его идеальным выбором для промышленных систем, где стабильность и скорость критически важны.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Традиционные ML подходы для NLP\n",
        "\n",
        "### От текста к признакам: классические методы представления текста\n",
        "\n",
        "До появления нейросетевых репрезентаций, ключевой задачей в NLP было преобразование неструктурированного текста в числовые векторы, которые можно было бы подать на вход классическим алгоритмам машинного обучения. Этот процесс, известный как **векторизация текста** (*text vectorization*), лежит в основе большинства традиционных ML-подходов. Эффективность модели напрямую зависела от качества инжиниринга этих векторных представлений.\n",
        "\n",
        "Основные методы векторизации:\n",
        "1.  **Bag-of-Words **(BoW): Представляет документ как мультимножество слов, игнорируя порядок и синтаксис. Вектор BoW содержит частоты или бинарные индикаторы присутствия слов из словаря.\n",
        "2.  **TF-IDF **(Term Frequency-Inverse Document Frequency): Улучшает BoW, понижая вес слов, которые часто встречаются во всех документах (стоп-слова), и повышая вес редких, но специфичных для документа слов.\n",
        "3.  **Тематическое моделирование **(LDA, NMF): Позволяет извлечь латентные темы из коллекции документов, представляя каждый документ как распределение по темам.\n",
        "4.  **Distributed Representations **(Word2Vec, Doc2Vec): Представляют слова и документы в виде плотных векторов в непрерывном пространстве, где семантические и синтаксические отношения между словами отражаются геометрическими свойствами этого пространства (например, `king - man + woman ≈ queen`).\n",
        "\n",
        "Эти подходы остаются актуальными и сегодня, особенно в сценариях с ограниченными вычислительными ресурсами или при недостатке данных для обучения больших нейросетей.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл традиционного ML-подхода к NLP: от векторизации текста через BoW и TF-IDF до тематического моделирования, создания word embeddings и, наконец, построения классификатора текста.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== 3. ТРАДИЦИОННЫЕ ML ПОДХОДЫ ДЛЯ NLP ===\")\n",
        "\n",
        "# === 3.1. Создание демонстрационного датасета ===\n",
        "documents = [\n",
        "    \"machine learning algorithms improve with more data\",\n",
        "    \"deep learning neural networks have many layers\",\n",
        "    \"natural language processing understands human language\",\n",
        "    \"computer vision algorithms analyze images and videos\",\n",
        "    \"reinforcement learning agents learn from environment\",\n",
        "    \"supervised learning uses labeled training data\",\n",
        "    \"unsupervised learning finds patterns without labels\",\n",
        "    \"transformers are state of the art in nlp\",\n",
        "    \"convolutional networks are good for images\",\n",
        "    \"recurrent networks work well with sequences\"\n",
        "]\n",
        "categories = [\"ml_basics\", \"deep_learning\", \"nlp\", \"computer_vision\",\n",
        "             \"rl\", \"ml_basics\", \"ml_basics\", \"nlp\", \"deep_learning\", \"deep_learning\"]\n",
        "\n",
        "# === 3.2. Векторизация текста: BoW и TF-IDF ===\n",
        "print(\"\\n3.2. Векторизация текста\")\n",
        "\n",
        "# Bag-of-Words\n",
        "bow_vectorizer = CountVectorizer(stop_words='english', max_features=20, ngram_range=(1, 2))\n",
        "X_bow = bow_vectorizer.fit_transform(documents)\n",
        "print(f\"BoW: {X_bow.shape[1]} признаков\")\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=15, ngram_range=(1, 2))\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "print(f\"TF-IDF: {X_tfidf.shape[1]} признаков\")\n",
        "\n",
        "# Анализ весов TF-IDF для первого документа\n",
        "feat_names = tfidf_vectorizer.get_feature_names_out()\n",
        "first_doc_weights = X_tfidf[0].toarray().flatten()\n",
        "top_indices = np.argsort(first_doc_weights)[::-1][:5]\n",
        "print(\"\\nТоп-5 TF-IDF весов для первого документа:\")\n",
        "for idx in top_indices:\n",
        "    if first_doc_weights[idx] > 0:\n",
        "        print(f\"  {feat_names[idx]}: {first_doc_weights[idx]:.3f}\")\n",
        "\n",
        "# === 3.3. Тематическое моделирование (LDA) ===\n",
        "print(\"\\n3.3. Тематическое моделирование (LDA)\")\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=3, random_state=42, n_jobs=-1)\n",
        "lda.fit(X_bow)\n",
        "\n",
        "feat_names_bow = bow_vectorizer.get_feature_names_out()\n",
        "print(\"\\nТемы, обнаруженные LDA:\")\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_idx = topic.argsort()[-5:][::-1]\n",
        "    top_words = [feat_names_bow[i] for i in top_words_idx]\n",
        "    print(f\"Тема {topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "# === 3.4. Распределённые представления (Word2Vec и Doc2Vec) ===\n",
        "print(\"\\n3.4. Распределённые представления\")\n",
        "\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# Word2Vec\n",
        "word2vec = Word2Vec(sentences=tokenized_docs, vector_size=50, window=3, min_count=1, seed=42, workers=1)\n",
        "print(\"Word2Vec модель обучена.\")\n",
        "\n",
        "# Демонстрация семантики\n",
        "try:\n",
        "    sim_words = word2vec.wv.most_similar('learning', topn=2)\n",
        "    print(f\"\\nСлова, похожие на 'learning': {sim_words}\")\n",
        "except KeyError:\n",
        "    print(\"Слово 'learning' не найдено в словаре Word2Vec.\")\n",
        "\n",
        "# Doc2Vec\n",
        "tagged_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
        "doc2vec = Doc2Vec(documents=tagged_docs, vector_size=50, window=2, min_count=1, epochs=50, workers=1)\n",
        "doc_vectors = np.array([doc2vec.dv[str(i)] for i in range(len(documents))])\n",
        "print(f\"Doc2Vec векторы документов: {doc_vectors.shape}\")\n",
        "\n",
        "# === 3.5. Классификация текста с помощью пайплайна Scikit-learn ===\n",
        "print(\"\\n3.5. Классификация текста\")\n",
        "\n",
        "# Создание расширенного датасета\n",
        "news_docs = [\n",
        "    \"Stocks market rally continues as investors gain confidence\",\n",
        "    \"Company earnings report shows strong growth this quarter\",\n",
        "    \"Football team wins championship in dramatic final match\",\n",
        "    \"Basketball player sets new scoring record in league history\",\n",
        "    \"Scientists discover new species in amazon rainforest\",\n",
        "    \"Research team develops breakthrough cancer treatment\",\n",
        "    \"Political leaders meet to discuss climate change agreement\",\n",
        "    \"Government announces new economic stimulus package\"\n",
        "]\n",
        "news_cats = [\"finance\", \"finance\", \"sports\", \"sports\", \"science\", \"science\", \"politics\", \"politics\"]\n",
        "\n",
        "# Создание пайплайна\n",
        "text_classifier = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=100, ngram_range=(1, 2))),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Обучение и оценка\n",
        "X_train, X_test, y_train, y_test = train_test_split(news_docs, news_cats, test_size=0.25, random_state=42)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "y_pred = text_classifier.predict(X_test)\n",
        "\n",
        "print(f\"\\nТочность классификации: {text_classifier.score(X_test, y_test):.3f}\")\n",
        "print(\"\\nОтчет по классификации:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Анализ важности признаков\n",
        "feature_names = text_classifier.named_steps['tfidf'].get_feature_names_out()\n",
        "importances = text_classifier.named_steps['classifier'].feature_importances_\n",
        "top_feat_idx = np.argsort(importances)[-10:][::-1]\n",
        "\n",
        "print(\"\\nТоп-10 важных признаков:\")\n",
        "for idx in top_feat_idx:\n",
        "    print(f\"  {feature_names[idx]}: {importances[idx]:.4f}\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример показывает, что даже без использования глубокого обучения можно построить эффективные NLP-системы. Ключ к успеху лежит в правильном выборе метода векторизации и тщательной настройке пайплайна. Хотя современные трансформеры часто превосходят традиционные методы в качестве, они требуют значительно больше ресурсов, что делает классические подходы по-прежнему актуальными в реальных промышленных задачах.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "tfqqKpqKQHGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Архитектура трансформеров\n",
        "\n",
        "### Революция self-attention механизмов\n",
        "\n",
        "Появление архитектуры **Transformer** в работе Vaswani et al. (2017) «Attention is All You Need» стало поворотным моментом в истории обработки естественного языка. Этот подход полностью отказался от рекуррентных и свёрточных механизмов, которые ранее доминировали в задачах моделирования последовательностей, и вместо этого построил всю вычислительную модель исключительно на основе механизма **внимания **(Attention). Это решение устранило два фундаментальных ограничения предыдущих архитектур: **последовательную природу вычислений** в RNN и **ограниченное контекстное окно** в CNN.\n",
        "\n",
        "Центральным нововведением Transformer является механизм **масштабированного скалярного произведения с самовниманием** (*scaled dot-product self-attention*). Он позволяет каждому элементу последовательности (токену) напрямую взаимодействовать со всеми другими элементами, независимо от их расстояния в последовательности. Это обеспечивает **глобальный контекст** на каждом шаге обработки и позволяет модели параллельно вычислять представления для всех токенов, что радикально ускоряет обучение.\n",
        "\n",
        "Формально, для входной последовательности тензоров \\(X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}\\) (где \\(n\\) — длина последовательности, \\(d_{\\text{model}}\\) — размерность эмбеддинга) сначала вычисляются три проекции:\n",
        "\n",
        "\\[\n",
        "Q = XW^Q,\\quad K = XW^K,\\quad V = XW^V\n",
        "\\]\n",
        "\n",
        "где \\(Q\\) (запросы, *queries*), \\(K\\) (ключи, *keys*) и \\(V\\) (значения, *values*) — это матрицы, а \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\\) — обучаемые веса. Затем вычисляется выход внимания как взвешенная сумма значений:\n",
        "\n",
        "\\[\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\]\n",
        "\n",
        "Деление на \\(\\sqrt{d_k}\\) (масштабирование) — это критически важный шаг, необходимый для предотвращения очень малых градиентов при больших значениях \\(d_k\\), что стабилизирует процесс обучения.\n",
        "\n",
        "Для захвата информации из различных подпространств представлений используется механизм **Multi-Head Attention **(MHA). Он параллельно применяет \\(h\\) функций внимания и конкатенирует их результаты:\n",
        "\n",
        "\\[\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
        "\\]\n",
        "\\[\n",
        "\\text{где}\\quad \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
        "\\]\n",
        "\n",
        "Это позволяет модели одновременно учитывать разные аспекты зависимости, например, синтаксические и семантические связи.\n",
        "\n",
        "Поскольку Transformer не содержит рекуррентных связей, он не имеет встроенной информации о **порядке элементов** в последовательности. Эта информация инжектируется с помощью **позиционного кодирования** (*positional encoding*), которое добавляется к эмбеддингам слов. В оригинальной статье используется синусоидальное кодирование, которое позволяет модели экстраполировать на последовательности длиннее, чем во время обучения.\n",
        "\n",
        "Типичный блок энкодера Transformer состоит из **двух подслоёв**:\n",
        "1.  **Multi-Head Attention** с добавлением **остаточного соединения** (residual connection) и **нормализации по слоям** (Layer Normalization).\n",
        "2.  **Позиционно-экспансивная полносвязная сеть** (Position-wise Feed-Forward Network — FFN), также снабжённая residual connection и LayerNorm.\n",
        "\n",
        "Эти элементы обеспечивают стабильную градиентную трансляцию и сходимость глубоких архитектур.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует пошаговую реализацию ключевых компонентов архитектуры Transformer на PyTorch, позволяя глубже понять её внутреннюю работу.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== 4. АРХИТЕКТУРА ТРАНСФОРМЕРОВ ===\")\n",
        "\n",
        "# === 4.1. Self-Attention механизм ===\n",
        "print(\"\\n4.1. Self-Attention механизм\")\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Реализация масштабированного скалярного произведения с самовниманием.\n",
        "    Это основная вычислительная единица архитектуры Transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k  # Размерность ключа (и запроса)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Вычисляет выход внимания и веса.\n",
        "        \n",
        "        :param Q: Запросы, форма [batch_size, seq_len, d_k]\n",
        "        :param K: Ключи, форма [batch_size, seq_len, d_k]\n",
        "        :param V: Значения, форма [batch_size, seq_len, d_v] (обычно d_v = d_k)\n",
        "        :param mask: Опциональная маска для маскирования будущих токенов или padding\n",
        "        :return: output, attention_weights\n",
        "        \"\"\"\n",
        "        # Вычисление скалярных произведений: [batch_size, seq_len, seq_len]\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        # Применение маски (если задана)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Применение softmax для получения весов внимания\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # Взвешенная сумма значений\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Демонстрация Self-Attention\n",
        "batch_size, seq_len, d_k = 2, 5, 64\n",
        "Q = torch.randn(batch_size, seq_len, d_k)\n",
        "K = torch.randn(batch_size, seq_len, d_k)\n",
        "V = torch.randn(batch_size, seq_len, d_k)\n",
        "\n",
        "self_attn = ScaledDotProductAttention(d_k=d_k)\n",
        "output, attn_weights = self_attn(Q, K, V)\n",
        "\n",
        "print(f\"Входные данные: Q{Q.shape}, K{K.shape}, V{V.shape}\")\n",
        "print(f\"Выход: {output.shape}\")\n",
        "print(f\"Веса внимания: {attn_weights.shape}\")\n",
        "print(f\"Сумма весов для первого запроса: {attn_weights[0, 0, :].sum().item():.4f} (должно быть 1.0)\")\n",
        "\n",
        "# === 4.2. Multi-Head Attention ===\n",
        "print(\"\\n4.2. Multi-Head Attention\")\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Реализация Multi-Head Attention механизма.\n",
        "    Позволяет модели совместно обращать внимание на информацию из разных подпространств.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model должно делиться на num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Размерность каждого 'головы'\n",
        "        \n",
        "        # Линейные проекции для Q, K, V\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        # Финальная проекция после конкатенации\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.attention = ScaledDotProductAttention(self.d_k)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        \n",
        "        # Сначала применяем линейные преобразования\n",
        "        Q = self.W_Q(Q)  # [batch_size, seq_len, d_model]\n",
        "        K = self.W_K(K)\n",
        "        V = self.W_V(V)\n",
        "        \n",
        "        # Изменяем форму для разделения на 'головы'\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, num_heads, d_k]\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        \n",
        "        # Транспонируем для корректного умножения матриц\n",
        "        # [batch_size, num_heads, seq_len, d_k]\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Применяем внимание ко всем 'головам' параллельно\n",
        "        output, attn_weights = self.attention(Q, K, V, mask)\n",
        "        \n",
        "        # Конкатенация 'голов': [batch_size, num_heads, seq_len, d_k] -> [batch_size, seq_len, d_model]\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, -1, self.d_model)\n",
        "        \n",
        "        # Финальная линейная проекция\n",
        "        output = self.W_O(output)\n",
        "        \n",
        "        return output, attn_weights\n",
        "\n",
        "# Демонстрация Multi-Head Attention\n",
        "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
        "mha_output, mha_weights = mha(Q, K, V)\n",
        "\n",
        "print(f\"Multi-Head Attention выход: {mha_output.shape}\")\n",
        "print(f\"Количество 'голов': {mha.num_heads}, размерность 'головы': {mha.d_k}\")\n",
        "\n",
        "# === 4.3. Позиционное кодирование ===\n",
        "print(\"\\n4.3. Позиционное кодирование\")\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Синусоидальные позиционные кодировки, как в оригинальной статье.\n",
        "    Добавляют информацию о позиции токена в последовательности.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Создаём матрицу позиционных кодировок [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # Вычисляем делитель для синусоид\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # Чётные индексы - синус, нечётные - косинус\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # Добавляем измерение для батча: [max_len, 1, d_model]\n",
        "        pe = pe.unsqueeze(1)\n",
        "        # Регистрируем как буфер (не обучаемый параметр)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Добавляет позиционное кодирование к входным эмбеддингам.\n",
        "        :param x: Входные эмбеддинги, форма [seq_len, batch_size, d_model]\n",
        "        \"\"\"\n",
        "        # Обрезаем pe до длины входной последовательности\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "# Демонстрация\n",
        "pos_enc = PositionalEncoding(d_model=64)\n",
        "# В PyTorch трансформеры часто ожидают форму [seq_len, batch_size, d_model]\n",
        "dummy_emb = torch.zeros(10, 3, 64)  # [seq_len, batch_size, d_model]\n",
        "pos_output = pos_enc(dummy_emb)\n",
        "\n",
        "print(f\"Входные эмбеддинги: {dummy_emb.shape}\")\n",
        "print(f\"Выход с позиционным кодированием: {pos_output.shape}\")\n",
        "\n",
        "# === 4.4. Полный блок энкодера Transformer ===\n",
        "print(\"\\n4.4. Полный блок энкодера Transformer\")\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Позиционно-экспансивная полносвязная сеть (FFN).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Один блок энкодера из архитектуры Transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        # Подслой 1: Multi-Head Self-Attention\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        \n",
        "        # Подслой 2: Feed-Forward Network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Демонстрация блока энкодера\n",
        "encoder_block = EncoderBlock(d_model=64, num_heads=8, d_ff=256)\n",
        "# Вход: [batch_size, seq_len, d_model]\n",
        "input_seq = torch.randn(2, 10, 64)\n",
        "output_seq = encoder_block(input_seq)\n",
        "\n",
        "print(f\"Вход блока энкодера: {input_seq.shape}\")\n",
        "print(f\"Выход блока энкодера: {output_seq.shape}\")\n",
        "\n",
        "# === 4.5. Визуализация механизма внимания ===\n",
        "print(\"\\n4.5. Визуализация механизма внимания\")\n",
        "\n",
        "def plot_attention_map(attention_weights, tokens, title=\"Attention Map\"):\n",
        "    \"\"\"\n",
        "    Визуализирует матрицу весов внимания.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    \n",
        "    # Берём веса первой 'головы' первого элемента батча\n",
        "    weights = attention_weights[0, 0, :, :].detach().numpy()\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(weights,\n",
        "                xticklabels=tokens,\n",
        "                yticklabels=tokens,\n",
        "                annot=True,\n",
        "                fmt=\".2f\",\n",
        "                cmap=\"viridis\",\n",
        "                cbar_kws={'label': 'Weight'})\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Key Tokens\")\n",
        "    plt.ylabel(\"Query Tokens\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Пример для визуализации\n",
        "sample_tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "Q_sample = torch.randn(1, len(sample_tokens), 64)\n",
        "K_sample = torch.randn(1, len(sample_tokens), 64)\n",
        "V_sample = torch.randn(1, len(sample_tokens), 64)\n",
        "\n",
        "mha_sample = MultiHeadAttention(d_model=64, num_heads=8)\n",
        "_, sample_attn = mha_sample(Q_sample, K_sample, V_sample)\n",
        "\n",
        "print(\"Матрица весов внимания для примера:\")\n",
        "print(sample_attn[0, 0, :, :].detach().numpy())\n",
        "\n",
        "# Раскомментируйте для визуализации\n",
        "# plot_attention_map(sample_attn, sample_tokens, \"Multi-Head Attention Visualization\")\n",
        "\n",
        "print(\"\\nАнализ:\")\n",
        "print(\"- Веса внимания показывают, на какие 'ключи' внимание обращает 'запрос'.\")\n",
        "print(\"- Высокие веса на диагонали указывают на фокус на самом себе.\")\n",
        "print(\"- Внедиагональные веса показывают связь между разными токенами.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример не просто показывает, как работает Transformer, а позволяет **исследовать его внутреннюю структуру**. Понимание того, как вычисляются веса внимания, как они агрегируются в Multi-Head Attention и как позиционное кодирование добавляет информацию о порядке, является ключом к интерпретации и отладке современных NLP-моделей. Именно эта архитектура лежит в основе таких революционных моделей, как BERT, GPT и T5, которые доминируют в NLP-задачах сегодня.\n",
        "\n",
        "Этот раздел завершает теоретическую основу модуля, подготавливая почву для перехода к практическому применению трансформеров через библиотеку Hugging Face Transformers в следующих разделах. Понимание внутреннего устройства Transformer позволяет не просто использовать предобученные модели как «чёрный ящик», а осознанно настраивать их, интерпретировать их результаты и разрабатывать кастомные архитектуры для решения специфических задач."
      ],
      "metadata": {
        "id": "-J5xaHysRYPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 20: Hugging Face Ecosystem — Комплексная платформа для современных NLP проектов\n",
        "\n",
        "## Введение\n",
        "\n",
        "Экосистема **Hugging Face** представляет собой не просто набор библиотек с открытым исходным кодом, а **целостную методологическую платформу**, созданную для демократизации искусственного интеллекта и ускорения жизненного цикла проектов машинного обучения. В основе её архитектуры лежит принцип **согласованной интеграции**, который устраняет традиционные барьеры между исследованием, разработкой и промышленным развёртыванием. Эта экосистема позволяет специалисту по данным перейти от идеи к production-системе с минимальными усилиями, обеспечивая при этом воспроизводимость, масштабируемость и эффективность.\n",
        "\n",
        "Центральным элементом экосистемы является **Hugging Face Hub** — централизованный репозиторий, который хранит не только тысячи предобученных моделей и датасетов, но и конфигурации, метрики и даже демонстрационные приложения (Spaces). Вокруг Hub строятся ключевые компоненты:\n",
        "*   **🤗 Transformers** — унифицированный интерфейс для работы с архитектурами на основе трансформеров.\n",
        "*   **🤗 Datasets** — инструмент для эффективной загрузки, обработки и версионирования наборов данных любого размера.\n",
        "*   **🤗 Tokenizers** — оптимизированные библиотеки для токенизации текста на Rust.\n",
        "*   **🤗 Accelerate** — библиотека для абстрагирования аппаратно-зависимого кода обучения.\n",
        "*   **PEFT **(Parameter-Efficient Fine-Tuning) — набор методов для эффективной адаптации больших моделей.\n",
        "\n",
        "### Ключевые архитектурные преимущества\n",
        "\n",
        "1.  **Согласованность API**: Все компоненты используют единые принципы именования и интерфейсы (например, `from_pretrained`), что минимизирует когнитивную нагрузку и ускоряет освоение новых инструментов.\n",
        "2.  **Сквозная воспроизводимость**: Каждый эксперимент, модель и датасет снабжён уникальным идентификатором и контрольной суммой, что гарантирует полное воспроизведение результатов.\n",
        "3.  **Автоматическая оптимизация**: Библиотеки интеллектуально адаптируются к доступному оборудованию (CPU/GPU/TPU) и автоматически применяют современные техники оптимизации (микс-пресижн, градиентный чекпоинтинг).\n",
        "4.  **Снижение порога входа**: Благодаря концепции `pipeline` и моделям с предобученными весами, можно реализовать сложные NLP-задачи всего в несколько строк кода, что делает передовые технологии доступными для широкого круга специалистов.\n",
        "\n",
        "Этот модуль предоставляет методологически строгий и практический обзор всей экосистемы Hugging Face, позволяя читателю не только использовать её инструменты, но и понимать лежащие в их основе архитектурные решения.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Transformers Library: унифицированный доступ к моделям\n",
        "\n",
        "### Теория: Архитектура и принципы библиотеки\n",
        "\n",
        "Библиотека **🤗 Transformers** является краеугольным камнем всей экосистемы. Её архитектура построена на концепции **AutoClasses** (`AutoModel`, `AutoTokenizer`, `AutoConfig`), которые выступают в роли фабрик для создания экземпляров конкретных моделей и компонентов. При вызове метода `from_pretrained` AutoClass анализирует метаданные, хранящиеся в репозитории модели на Hub (обычно файл `config.json`), и автоматически загружает соответствующую архитектуру, конфигурацию и токенизатор.\n",
        "\n",
        "Этот подход решает фундаментальную проблему совместимости, которая возникает при работе с сотнями различных архитектур (BERT, GPT-2, T5, Whisper и др.). Разработчику больше не нужно помнить, какой именно класс модели использовать; достаточно указать её имя или путь. Это обеспечивает **согласованность интерфейсов** на всех этапах жизненного цикла модели: от загрузки и инференса до fine-tuning и развёртывания.\n",
        "\n",
        "Другой ключевой концепцией является **Pipeline API** — высокоуровневый интерфейс, который инкапсулирует весь процесс обработки: предобработку (токенизация), выполнение инференса и постобработку (преобразование логитов в читаемые метки). Pipeline абстрагирует всю сложность работы с тензорами и позволяет выполнять задачи, такие как классификация текста или извлечение именованных сущностей, с помощью простого вызова функции.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует как базовое, так и продвинутое использование библиотеки Transformers, включая zero-shot inference через `pipeline`, загрузку модели через `AutoClasses` и настройку продакшен-готовой конфигурации.\n",
        "\n",
        "```python\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import torch\n",
        "\n",
        "print(\"=== 1. TRANSFORMERS LIBRARY: УНИФИЦИРОВАННЫЙ ДОСТУП ===\")\n",
        "\n",
        "# === 1.1. Zero-shot Inference через Pipeline ===\n",
        "print(\"\\n1.1. Zero-shot Inference\")\n",
        "\n",
        "# Создание пайплайна в одну строку\n",
        "classifier = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "# Выполнение инференса\n",
        "text = \"Hugging Face Transformers is revolutionizing NLP!\"\n",
        "result = classifier(text)\n",
        "print(f\"Текст: {text}\")\n",
        "print(f\"Результат: {result[0]['label']} (уверенность: {result[0]['score']:.4f})\")\n",
        "\n",
        "# === 1.2. Загрузка модели через AutoClasses ===\n",
        "print(\"\\n1.2. Загрузка модели и токенизатора\")\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Автоматическая загрузка соответствующей архитектуры и токенизатора\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2  # Переопределяем число классов для новой задачи\n",
        ")\n",
        "\n",
        "# Ручной процесс инференса (для понимания внутренней работы)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "    predictions = torch.softmax(logits, dim=-1)\n",
        "\n",
        "print(f\"Ручной инференс (BERT): Позитив: {predictions[0][1]:.4f}, Негатив: {predictions[0][0]:.4f}\")\n",
        "\n",
        "# === 1.3. Продвинутая конфигурация для Production ===\n",
        "print(\"\\n1.3. Продакшен-конфигурация Pipeline\")\n",
        "\n",
        "# Загрузка GPU-совместимой модели\n",
        "ner_model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=ner_model_name,\n",
        "    tokenizer=ner_model_name,\n",
        "    aggregation_strategy=\"simple\",  # Объединяет смежные токены в сущности\n",
        "    device=0 if torch.cuda.is_available() else -1,  # Автоматический выбор устройства\n",
        "    batch_size=8  # Пакетная обработка для повышения скорости\n",
        ")\n",
        "\n",
        "# Пример NER\n",
        "ner_text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
        "ner_results = ner_pipeline(ner_text)\n",
        "print(f\"\\nNER для текста: {ner_text}\")\n",
        "for entity in ner_results:\n",
        "    print(f\"  {entity['word']}: {entity['entity_group']} ({entity['score']:.3f})\")\n",
        "\n",
        "# === 1.4. Fine-tuning с Trainer API ===\n",
        "print(\"\\n1.4. Fine-tuning с Trainer API\")\n",
        "\n",
        "# Функция токенизации для датасета\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "# Создание фиктивного датасета для примера\n",
        "from datasets import Dataset\n",
        "fake_train_data = {\n",
        "    \"text\": [\"This is a positive example.\"] * 100 + [\"This is a negative example.\"] * 100,\n",
        "    \"label\": [1] * 100 + [0] * 100\n",
        "}\n",
        "fake_eval_data = {\n",
        "    \"text\": [\"Another positive.\", \"Another negative.\"],\n",
        "    \"label\": [1, 0]\n",
        "}\n",
        "train_dataset = Dataset.from_dict(fake_train_data).map(tokenize_function, batched=True)\n",
        "eval_dataset = Dataset.from_dict(fake_eval_data).map(tokenize_function, batched=True)\n",
        "\n",
        "# Коллатор для динамического паддинга\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Конфигурация обучения\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_finetuned\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "# Создание и запуск тренера\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Обучение (в реальном сценарии здесь были бы настоящие данные)\n",
        "print(\"Запуск обучения (на фиктивных данных)...\")\n",
        "# trainer.train() # Раскомментировать для реального обучения\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример иллюстрирует эволюцию от самого простого использования (`pipeline`) к полному контролю над процессом обучения (`Trainer`). Библиотека Transformers предоставляет гибкость, позволяя исследователю начать с быстрого прототипа и постепенно усложнять пайплайн по мере роста требований проекта, сохраняя при этом согласованность кодовой базы.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Продвинутый fine-tuning и кастомизация\n",
        "\n",
        "### Теория: Методы адаптации моделей\n",
        "\n",
        "С появлением гигантских моделей с миллиардами параметров (LLaMA, BLOOM, OPT) традиционный подход к fine-tuning — обновление всех весов модели — стал вычислительно и экономически невыгодным для большинства организаций. Это привело к развитию области **Parameter-Efficient Fine-Tuning **(PEFT), которая фокусируется на адаптации моделей путём внесения минимальных изменений в их архитектуру.\n",
        "\n",
        "Ключевые методы PEFT:\n",
        "*   **Adapter Layers**: Вставка небольших двухслойных нейросетей между слоями трансформера. Во время обучения обновляются только веса адаптеров.\n",
        "*   **Prefix-Tuning / Prompt Tuning**: Добавление и оптимизация непрерывных векторов (prompt-эмбеддингов) в начало входной последовательности. Обычные веса модели замораживаются.\n",
        "*   **LoRA **(Low-Rank Adaptation): Разложение матриц обновления весов (например, в проекциях Q и V) на произведение двух малых матриц низкого ранга. Это позволяет эффективно аппроксимировать полное обновление весов с использованием лишь доли исходных параметров.\n",
        "\n",
        "Эти методы позволяют сократить потребление памяти и вычислительные затраты на fine-tuning на порядки, делая адаптацию больших моделей доступной даже на одном GPU.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует практическое применение метода LoRA для эффективной настройки большой модели, а также показывает, как можно создать кастомную голову классификации для решения специфических задач.\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "print(\"=== 2. ПРОДВИНУТЫЙ FINE-TUNING И КАСТОМИЗАЦИЯ ===\")\n",
        "\n",
        "# === 2.1. Parameter-Efficient Fine-Tuning с LoRA ===\n",
        "print(\"\\n2.1. LoRA: Parameter-Efficient Fine-Tuning\")\n",
        "\n",
        "# Загрузка большой модели (в реальности это может быть LLaMA, BLOOM и т.д.)\n",
        "base_model_name = \"distilbert-base-uncased\"  # Используем DistilBERT для демонстрации\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Конфигурация LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,  # Задача: классификация последовательностей\n",
        "    inference_mode=False,        # False для обучения, True для инференса\n",
        "    r=8,                         # Ранг низкоранговой декомпозиции\n",
        "    lora_alpha=32,               # Масштабирующий параметр\n",
        "    lora_dropout=0.1,            # Dropout для адаптеров\n",
        "    target_modules=[\"q_lin\", \"v_lin\"]  # Для DistilBERT; для BERT: [\"query\", \"value\"]\n",
        ")\n",
        "\n",
        "# Применение LoRA к модели\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()  # Показывает, что обучается лишь ~0.5-1% параметров\n",
        "\n",
        "# === 2.2. Кастомная голова классификации ===\n",
        "print(\"\\n2.2. Кастомная голова классификации\")\n",
        "\n",
        "class CustomClassificationHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Расширенная голова классификации с дополнительной нелинейностью и dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # Берём [CLS] токен\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "# Замена стандартной головы на кастомную\n",
        "model.classifier = CustomClassificationHead(model.config)\n",
        "\n",
        "# Фиктивные данные для теста\n",
        "dummy_input = {\n",
        "    \"input_ids\": torch.randint(0, 1000, (2, 128)),\n",
        "    \"attention_mask\": torch.ones(2, 128)\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**dummy_input)\n",
        "    print(f\"Выход кастомной головы: {outputs.logits.shape}\")\n",
        "\n",
        "print(\"\\nLoRA и кастомные головы позволяют гибко адаптировать модели под специфические задачи, \"\n",
        "      \"сохраняя при этом вычислительную эффективность и качество.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Использование PEFT-методов, таких как LoRA, меняет экономику fine-tuning. Теперь исследователь может экспериментировать с адаптацией моделей, которые ранее были недоступны из-за требований к ресурсам. Кастомизация головы классификации, в свою очередь, предоставляет полный контроль над финальным слоем модели, что критично для задач с нестандартной структурой выхода.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Оптимизация процесса обучения\n",
        "\n",
        "### Теория: Техники ускорения и экономии памяти\n",
        "\n",
        "Обучение современных моделей требует продвинутых методов оптимизации для эффективного использования памяти и вычислительных ресурсов. Ключевые техники, интегрированные в экосистему Hugging Face:\n",
        "\n",
        "1.  **Mixed Precision Training**: Использование 16-битных чисел с плавающей точкой (FP16 или BF16) для большинства операций, что уменьшает потребление памяти в 2 раза и ускоряет вычисления на GPU, поддерживающих Tensor Cores.\n",
        "2.  **Gradient Accumulation**: Накопление градиентов от нескольких мини-батчей перед выполнением шага оптимизатора. Это позволяет эмулировать больший эффективный размер батча, не увеличивая потребление памяти.\n",
        "3.  **Gradient Checkpointing**: Сохранение в памяти только входов и выходов каждого блока, а не всех промежуточных активаций. Во время обратного прохода активации пересчитываются по требованию. Это значительно экономит память за счёт небольшого замедления.\n",
        "4.  **Офлоудинг **(Offloading): Перемещение редко используемых данных (например, оптимизатора) из GPU в оперативную память CPU, что позволяет обучать модели, размер которых превышает объём видеопамяти.\n",
        "5.  **DeepSpeed Integration**: Глубокая интеграция с фреймворком DeepSpeed от Microsoft, который предоставляет промышленно-ориентированные реализации всех вышеперечисленных техник, включая Zero Redundancy Optimizer (ZeRO).\n",
        "\n",
        "Эти техники могут быть легко активированы через аргументы `TrainingArguments` или конфигурационные файлы, что делает их доступными без глубокого погружения в низкоуровневые детали.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует, как настроить `TrainingArguments` для максимальной эффективности обучения на ограниченных ресурсах.\n",
        "\n",
        "```python\n",
        "from transformers import TrainingArguments\n",
        "# from accelerate import Accelerator  # Для продвинутой кастомизации\n",
        "\n",
        "print(\"=== 3. ОПТИМИЗАЦИЯ ПРОЦЕССА ОБУЧЕНИЯ ===\")\n",
        "\n",
        "# === 3.1. Конфигурация высокооптимизированного обучения ===\n",
        "print(\"\\n3.1. Оптимизированная конфигурация TrainingArguments\")\n",
        "\n",
        "optimized_args = TrainingArguments(\n",
        "    output_dir=\"./optimized_training\",\n",
        "    # Основные параметры\n",
        "    num_train_epochs=3,\n",
        "    \n",
        "    # Параметры батча и памяти\n",
        "    per_device_train_batch_size=4,    # Малый размер для экономии памяти\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=8,    # Эффективный размер батча: 4 * 8 = 32\n",
        "    gradient_checkpointing=True,      # Экономия памяти\n",
        "    \n",
        "    # Mixed Precision\n",
        "    fp16=torch.cuda.is_available(),   # FP16 для NVIDIA GPU\n",
        "    # bf16=torch.cuda.is_bf16_supported(), # BF16 для новых GPU (A100, H100)\n",
        "    \n",
        "    # Производительность загрузки данных\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True,\n",
        "    \n",
        "    # Стратегия оценки и логирования\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    \n",
        "    # Запуск на нескольких GPU\n",
        "    # fp16_full_eval=True,  # Для ускорения валидации\n",
        "    # local_rank=int(os.environ.get(\"LOCAL_RANK\", -1)) # Для запуска с torch.distributed\n",
        ")\n",
        "\n",
        "print(\"Конфигурация оптимизированного обучения создана.\")\n",
        "print(f\"Эффективный размер батча: {optimized_args.per_device_train_batch_size * optimized_args.gradient_accumulation_steps}\")\n",
        "\n",
        "# === 3.2. Конфигурация DeepSpeed для экстремальной оптимизации ===\n",
        "print(\"\\n3.2. DeepSpeed конфигурация (для очень больших моделей)\")\n",
        "\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": \"auto\",  # Автоматический расчёт\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,  # ZeRO-2: разделение градиентов и оптимизатора\n",
        "        \"allgather_partitions\": True,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"reduce_scatter\": True,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"cpu_offload\": True  # Офлоудинг оптимизатора в CPU\n",
        "    },\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"wall_clock_breakdown\": False\n",
        "}\n",
        "\n",
        "print(\"DeepSpeed конфигурация готова для обучения моделей с миллиардами параметров.\")\n",
        "print(\"Это позволяет обучать LLaMA-13B на 4x A100 40GB.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Интеграция этих техник в `TrainingArguments` и поддержка DeepSpeed превращают Hugging Face Transformers из исследовательской библиотеки в мощный инструмент промышленного масштаба. Теперь специалист по данным может не только экспериментировать с моделями, но и эффективно их обучать в production-среде.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Datasets Library: эффективная работа с данными\n",
        "\n",
        "### Теория: Принципы работы с большими наборами данных\n",
        "\n",
        "Библиотека **🤗 Datasets** решает фундаментальную проблему машинного обучения — эффективную работу с данными. Её архитектура основана на нескольких ключевых принципах:\n",
        "\n",
        "1.  **Аппаратно-оптимизированные форматы**: Использование Apache Arrow в качестве внутреннего формата данных обеспечивает высокую скорость загрузки и обработки, особенно для колончатых операций.\n",
        "2.  **Ленивая загрузка и обработка** (*Lazy Evaluation*): Данные загружаются и обрабатываются только тогда, когда это необходимо для выполнения конкретной операции. Это позволяет строить сложные конвейеры предобработки без промежуточного хранения.\n",
        "3.  **Потоковый режим **(Streaming Mode): Для датасетов, превышающих объём оперативной памяти, Datasets может работать в потоковом режиме, загружая и обрабатывая данные по одному примеру или батчу за раз.\n",
        "4.  **Встроенная воспроизводимость**: Каждый датасет снабжён хешем, который гарантирует, что при повторной загрузке будет получена абсолютно идентичная версия данных. Это критически важно для научной строгости и отладки.\n",
        "5.  **Интеграция с Hugging Face Hub**: Прямой доступ к тысячам датасетов с централизованного репозитория с автоматическим кэшированием.\n",
        "\n",
        "Эти принципы делают Datasets незаменимым инструментом для работы как с небольшими, так и с гигантскими наборами данных.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл работы с данными: от загрузки и потоковой обработки до создания кастомных датасетов и метрик.\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset, Dataset, load_metric, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=== 4. DATASETS LIBRARY: ЭФФЕКТИВНАЯ РАБОТА С ДАННЫМИ ===\")\n",
        "\n",
        "# === 4.1. Загрузка датасетов из Hub ===\n",
        "print(\"\\n4.1. Загрузка датасетов\")\n",
        "\n",
        "# Загрузка небольшого датасета в память\n",
        "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "print(f\"MRPC датасет загружен. Размер тренировки: {len(mrpc_dataset['train'])}\")\n",
        "\n",
        "# Загрузка большого датасета в потоковом режиме\n",
        "# imdb_stream = load_dataset(\"imdb\", streaming=True, split=\"train\")\n",
        "# print(\"IMDB загружен в потоковом режиме. Пример:\")\n",
        "# print(next(iter(imdb_stream)))\n",
        "\n",
        "# === 4.2. Создание и объединение кастомных датасетов ===\n",
        "print(\"\\n4.2. Кастомные датасеты\")\n",
        "\n",
        "# Создание из словаря\n",
        "custom_data = {\n",
        "    \"text\": [\"NLP is fascinating!\", \"Transformers are powerful.\"],\n",
        "    \"label\": [1, 1]\n",
        "}\n",
        "custom_dataset = Dataset.from_dict(custom_data)\n",
        "\n",
        "# Создание из Pandas DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\"This is negative.\", \"Another negative text.\"],\n",
        "    \"label\": [0, 0]\n",
        "})\n",
        "df_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Объединение датасетов\n",
        "combined_dataset = concatenate_datasets([custom_dataset, df_dataset])\n",
        "print(f\"Объединённый датасет: {len(combined_dataset)} примеров\")\n",
        "\n",
        "# === 4.3. Предобработка с токенизацией ===\n",
        "print(\"\\n4.3. Предобработка и токенизация\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Применение предобработки с отложенным выполнением\n",
        "tokenized_dataset = mrpc_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=[\"sentence1\", \"sentence2\", \"idx\"]  # Удаляем ненужные столбцы\n",
        ")\n",
        "\n",
        "print(f\"Токенизированный датасет. Ключи: {tokenized_dataset['train'].features.keys()}\")\n",
        "\n",
        "# === 4.4. Фильтрация и преобразование ===\n",
        "print(\"\\n4.4. Фильтрация и агрегация\")\n",
        "\n",
        "# Фильтрация по длине\n",
        "filtered_dataset = tokenized_dataset.filter(\n",
        "    lambda example: sum(example[\"attention_mask\"]) > 5\n",
        ")\n",
        "print(f\"После фильтрации: {len(filtered_dataset['train'])} примеров\")\n",
        "\n",
        "# Преобразование в Pandas для анализа\n",
        "analysis_df = filtered_dataset[\"train\"].to_pandas()\n",
        "print(f\"Преобразовано в DataFrame. Размер: {analysis_df.shape}\")\n",
        "\n",
        "# === 4.5. Кастомные метрики ===\n",
        "print(\"\\n4.5. Кастомные метрики\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Функция для вычисления метрик во время оценки.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Загрузка встроенных метрик\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "    f1 = load_metric(\"f1\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    }\n",
        "\n",
        "# Пример использования (в реальном тренере)\n",
        "print(\"Функция compute_metrics готова для использования в Trainer.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Библиотека Datasets обеспечивает гладкий переход от ручной работы с данными в Pandas к высокооптимизированной обработке, необходимой для обучения современных моделей. Её интеграция с Transformers и Hub создаёт бесшовный пайплайн «данные -> модель -> результат», который лежит в основе современных MLOps-практик для NLP.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Nxhp4sg2UADM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 5. Accelerate Library: унифицированное распределенное обучение\n",
        "\n",
        "### Теория: Абстракции для распределенных вычислений\n",
        "\n",
        "Распространение гигантских моделей и рост объёмов данных сделали распределённое обучение не опцией, а необходимостью. Однако реализация распределённых стратегий (Multi-GPU, TPU, Multi-Node) традиционно требовала глубоких знаний в области параллельных вычислений и приводила к фрагментации кодовой базы. **Библиотека Accelerate** от Hugging Face решает эту проблему, предоставляя **универсальную и прозрачную абстракцию** над аппаратно-зависимым кодом.\n",
        "\n",
        "Ключевой принцип Accelerate — **единая кодовая база**. Разработчик пишет свой кастомный цикл обучения один раз, используя стандартные PyTorch API. Затем, с помощью простой обёртки `Accelerator`, этот код автоматически адаптируется для работы на любой конфигурации: от одного CPU до кластера из сотен GPU. Accelerator берёт на себя всю сложность:\n",
        "*   **Распределение данных** между процессами/устройствами.\n",
        "*   **Оборачивание модели** в соответствующую распределённую обёртку (DDP, FSDP).\n",
        "*   **Синхронизацию градиентов** и параметров после каждого шага.\n",
        "*   **Упреждающую инициализацию** смешанной точности (FP16/BF16).\n",
        "*   **Обработку коллизий** при сборе данных для метрик.\n",
        "\n",
        "Этот подход не только ускоряет разработку, но и гарантирует **воспроизводимость** и **портативность** кода между различными вычислительными средами.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует, как превратить стандартный цикл обучения PyTorch в распределённый, добавив всего несколько строк кода с использованием Accelerate.\n",
        "\n",
        "```python\n",
        "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, Any\n",
        "\n",
        "print(\"=== 5. ACCELERATE: УНИФИЦИРОВАННОЕ РАСПРЕДЕЛЕННОЕ ОБУЧЕНИЕ ===\")\n",
        "\n",
        "# === 5.1. Вспомогательные функции для примера ===\n",
        "def get_dummy_data_loader(batch_size: int = 8, num_samples: int = 100) -> DataLoader:\n",
        "    \"\"\"Создаёт фиктивный DataLoader для демонстрации.\"\"\"\n",
        "    input_ids = torch.randint(0, 1000, (num_samples, 128))\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    labels = torch.randint(0, 2, (num_samples,))\n",
        "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def get_optimizer_and_scheduler(model: nn.Module, num_training_steps: int):\n",
        "    \"\"\"Инициализирует оптимизатор и scheduler.\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "    return optimizer, scheduler\n",
        "\n",
        "# === 5.2. Основной распределённый цикл обучения ===\n",
        "print(\"\\n5.2. Настройка и запуск распределённого обучения\")\n",
        "\n",
        "# Инициализация Accelerator с продвинутыми настройками\n",
        "# DistributedDataParallelKwargs полезен для моделей с неиспользуемыми параметрами\n",
        "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=\"fp16\",           # Включение FP16 автоматически\n",
        "    gradient_accumulation_steps=4,    # Эффективный батч: per_device * grad_acc\n",
        "    kwargs_handlers=[ddp_kwargs],     # Добавление кастомных аргументов\n",
        "    log_with=\"tensorboard\",           # Интеграция с системами логирования\n",
        "    project_dir=\"./accelerate_logs\"\n",
        ")\n",
        "\n",
        "# Загрузка модели и токенизатора\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Создание даталоадеров и оптимизатора\n",
        "train_dataloader = get_dummy_data_loader(batch_size=8)\n",
        "eval_dataloader = get_dummy_data_loader(batch_size=8, num_samples=20)\n",
        "num_epochs = 1\n",
        "num_training_steps = num_epochs * len(train_dataloader) // accelerator.gradient_accumulation_steps\n",
        "optimizer, lr_scheduler = get_optimizer_and_scheduler(model, num_training_steps)\n",
        "\n",
        "# === ВАЖНО: prepare() должен вызываться ПОСЛЕ создания всех компонентов ===\n",
        "# Accelerator автоматически обернёт их в распределённые версии\n",
        "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "# Запуск логирования только на основном процессе\n",
        "if accelerator.is_main_process:\n",
        "    accelerator.init_trackers(\"nlp_training\")\n",
        "\n",
        "print(f\"Обучение запущено с использованием Accelerator.\")\n",
        "print(f\"Устройство: {accelerator.device}\")\n",
        "print(f\"Количество процессов: {accelerator.num_processes}\")\n",
        "print(f\"Эффективный размер батча: {8 * 4 * accelerator.num_processes}\")\n",
        "\n",
        "# === Кастомный цикл обучения ===\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # accelerator.accumulate автоматически обрабатывает gradient accumulation\n",
        "        with accelerator.accumulate(model):\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            # Автоматически масштабирует градиенты в FP16 и запускает backward pass\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        # Логирование только на основном процессе для избежания дублирования\n",
        "        if accelerator.is_main_process and step % 10 == 0:\n",
        "            accelerator.print(f\"[Эпоха {epoch+1}/{num_epochs}] Шаг {step}, Потеря: {loss.item():.4f}\")\n",
        "            accelerator.log({\"train_loss\": loss.item()}, step=step)\n",
        "    \n",
        "    # === Оценка модели ===\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "    \n",
        "    for batch in eval_dataloader:\n",
        "        with torch.no_grad():\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        \n",
        "        # Сбор предсказаний со ВСЕХ процессов на ВСЕХ устройствах\n",
        "        # Это гарантирует, что метрики будут вычислены по полному датасету\n",
        "        predictions, references = accelerator.gather_for_metrics((predictions, labels))\n",
        "        all_predictions.append(predictions)\n",
        "        all_references.append(references)\n",
        "    \n",
        "    # Вычисление итоговой метрики на основном процессе\n",
        "    if accelerator.is_main_process:\n",
        "        all_predictions = torch.cat(all_predictions)\n",
        "        all_references = torch.cat(all_references)\n",
        "        accuracy = (all_predictions == all_references).float().mean().item()\n",
        "        accelerator.log({\"eval_accuracy\": accuracy}, step=epoch)\n",
        "        accelerator.print(f\"Эпоха {epoch+1} - Точность на валидации: {accuracy:.4f}\")\n",
        "\n",
        "# === Сохранение модели ===\n",
        "# Ждём завершения всех процессов перед сохранением\n",
        "accelerator.wait_for_everyone()\n",
        "if accelerator.is_main_process:\n",
        "    # Распаковка модели из распределённой обёртки\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    # Сохранение локально\n",
        "    unwrapped_model.save_pretrained(\"./final_accelerate_model\")\n",
        "    tokenizer.save_pretrained(\"./final_accelerate_model\")\n",
        "    print(\"Модель успешно сохранена.\")\n",
        "    \n",
        "    # Завершение логирования\n",
        "    accelerator.end_training()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Пример демонстрирует элегантность подхода Accelerate. Разработчик сосредоточен на логике модели и данных, в то время как все детали распределённых вычислений управляются библиотекой. Этот код будет работать одинаково хорошо на ноутбуке с CPU, на рабочей станции с 4 GPU и в облаке на TPU-pod, не требуя ни одной строки изменения.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Hugging Face Hub: управление моделями и датасетами\n",
        "\n",
        "### Теория: Централизованный репозиторий для ML артефактов\n",
        "\n",
        "**Hugging Face Hub** — это не просто хостинг, а **централизованная платформа управления жизненным циклом ML-артефактов**. Он решает фундаментальные проблемы, с которыми сталкиваются команды: фрагментация моделей по локальным дискам, отсутствие документации, сложность воспроизведения результатов и отсутствие централизованного доступа.\n",
        "\n",
        "Hub предоставляет:\n",
        "*   **Версионирование Git**: Каждая модель и датасет — это Git-репозиторий, что позволяет отслеживать изменения, откатываться к предыдущим версиям и использовать familiar workflow.\n",
        "*   **Метаданные и документация**: **Model Cards** и **Dataset Cards** — это структурированные документы в формате YAML/Markdown, которые описывают лицензию, метрики, датасеты, использованные для обучения, и примеры использования. Это обеспечивает прозрачность и воспроизводимость.\n",
        "*   **Политики доступа**: Поддержка публичных и приватных репозиториев, а также организационных аккаунтов, позволяет гибко управлять доступом в enterprise-средах.\n",
        "*   **Встроенная экосистема**: Прямая интеграция с библиотеками Transformers, Datasets и Spaces создаёт бесшовный поток «обучение -> публикация -> демо -> использование».\n",
        "\n",
        "Hub превращает модель из анонимного файла весов в **первоклассный объект** с полной историей, документацией и контекстом использования.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл работы с Hub: от загрузки модели и датасета до их дообучения, публикации и последующего использования.\n",
        "\n",
        "```python\n",
        "from huggingface_hub import (\n",
        "    HfApi,\n",
        "    create_repo,\n",
        "    notebook_login  # Для авторизации в ноутбуках\n",
        ")\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"=== 6. HUGGING FACE HUB: УПРАВЛЕНИЕ МОДЕЛЯМИ И ДАТАСЕТАМИ ===\")\n",
        "\n",
        "# === 6.1. Авторизация (требуется для публикации) ===\n",
        "# Раскомментировать для запуска в ноутбуке\n",
        "# notebook_login()\n",
        "\n",
        "# Или установить токен вручную\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"your_hf_token\")\n",
        "\n",
        "# === 6.2. Загрузка и использование артефактов из Hub ===\n",
        "print(\"\\n6.2. Загрузка моделей и датасетов\")\n",
        "\n",
        "# Загрузка предобученной модели и датасета\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "dataset_name = \"glue\"\n",
        "task_name = \"mrpc\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dataset = load_dataset(dataset_name, task_name)\n",
        "\n",
        "print(f\"Модель {model_name} и датасет {dataset_name}/{task_name} успешно загружены.\")\n",
        "\n",
        "# === 6.3. Fine-tuning и публикация модели на Hub ===\n",
        "print(\"\\n6.3. Fine-tuning и публикация на Hub\")\n",
        "\n",
        "# Функция токенизации\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
        "\n",
        "# Применение токенизации\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Настройка обучения с автоматической публикацией\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mrpc-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    # Публикация на Hub\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"my-username/distilbert-base-uncased-finetuned-mrpc\",  # Уникальный ID\n",
        "    hub_strategy=\"every_save\",  # Публиковать каждую сохранённую модель\n",
        "    hub_token=None,  # Использует токен из кэша\n",
        "    report_to=\"none\",  # Отключим логирование для примера\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Запуск обучения и автоматическая публикация\n",
        "# trainer.train() # Раскомментировать для реального обучения\n",
        "\n",
        "print(\"Fine-tuning запущен. Модель будет автоматически опубликована на Hub.\")\n",
        "print(\"URL модели: https://huggingface.co/my-username/distilbert-base-uncased-finetuned-mrpc\")\n",
        "\n",
        "# === 6.4. Ручная публикация и создание Model Card ===\n",
        "print(\"\\n6.4. Ручная публикация и создание Model Card\")\n",
        "\n",
        "# Создание приватного репозитория для enterprise\n",
        "# api = HfApi()\n",
        "# api.create_repo(\n",
        "#     repo_id=\"my-company/enterprise-classifier\",\n",
        "#     repo_type=\"model\",\n",
        "#     private=True,\n",
        "#     token=\"your_enterprise_token\"\n",
        "# )\n",
        "\n",
        "# Создание Model Card вручную\n",
        "model_card_content = \"\"\"\n",
        "---\n",
        "language: en\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- text-classification\n",
        "- sentence-similarity\n",
        "- mrpc\n",
        "- distilbert\n",
        "datasets:\n",
        "- glue/mrpc\n",
        "metrics:\n",
        "- accuracy\n",
        "- f1\n",
        "---\n",
        "# DistilBERT Fine-tuned on MRPC\n",
        "\n",
        "This model is a `distilbert-base-uncased` model fine-tuned on the [MRPC](https://huggingface.co/datasets/glue#mrpc) dataset for sentence similarity classification.\n",
        "\n",
        "## Model Description\n",
        "- **Base Model**: `distilbert-base-uncased`\n",
        "- **Task**: Sentence Pair Classification\n",
        "- **Evaluation Metrics**: Accuracy = 0.88, F1 = 0.92\n",
        "\n",
        "## Intended Uses & Limitations\n",
        "This model is intended for academic and research purposes. It may not perform well on out-of-domain text.\n",
        "\"\"\"\n",
        "\n",
        "# Сохранение Model Card\n",
        "model_card_path = \"./mrpc-finetuned/README.md\"\n",
        "with open(model_card_path, \"w\") as f:\n",
        "    f.write(model_card_content)\n",
        "\n",
        "# Загрузка Model Card на Hub (если публикация не автоматическая)\n",
        "# api.upload_file(\n",
        "#     path_or_fileobj=model_card_path,\n",
        "#     path_in_repo=\"README.md\",\n",
        "#     repo_id=\"my-username/distilbert-base-uncased-finetuned-mrpc\"\n",
        "# )\n",
        "\n",
        "print(\"Model Card создан и готов к публикации.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Интеграция с Hugging Face Hub превращает процесс публикации и совместного использования моделей в тривиальную задачу. Это не только упрощает работу внутри команды, но и способствует открытой науке и воспроизводимости исследований в сообществе.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Spaces: мгновенное развертывание демо\n",
        "\n",
        "### Теория: Бесплатный хостинг для ML демо\n",
        "\n",
        "**Hugging Face Spaces** — это сервис, который предоставляет **бесплатную и простую в использовании инфраструктуру** для развёртывания интерактивных демо-приложений ML-моделей. Spaces решает проблему «демо-пропасти» — ситуации, когда исследователь обучил отличную модель, но не может быстро показать её потенциал стейкхолдерам из-за сложности развёртывания веб-сервиса.\n",
        "\n",
        "Spaces поддерживает несколько фреймворков для создания UI:\n",
        "*   **Gradio**: Идеален для быстрых, простых демонстраций. Создание интерфейса занимает 3-5 строк кода.\n",
        "*   **Streamlit**: Подходит для более сложных и интерактивных приложений с множеством компонентов.\n",
        "*   **Static HTML/JS**: Для полностью кастомных интерфейсов.\n",
        "\n",
        "Преимущества Spaces:\n",
        "*   **Бесплатные ресурсы**: Включая CPU и GPU (T4), что делает его доступным для всех.\n",
        "*   **Git-интеграция**: Демо развертывается напрямую из репозитория на GitHub, Hugging Face Hub или GitLab.\n",
        "*   **Автоматическое масштабирование**: Обработка нагрузки от одного пользователя до сотен без вмешательства.\n",
        "*   **Нативная интеграция с Hub**: Прямой доступ к моделям и датасетам без дополнительной аутентификации.\n",
        "\n",
        "Spaces превращает процесс демонстрации модели из многочасовой задачи в 10-минутную.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример показывает, как создать два разных демо-приложения — одно на Gradio, другое на Streamlit — для одной и той же модели, размещённой на Hugging Face Hub.\n",
        "\n",
        "```python\n",
        "# === 7.1. Gradio Demo (app.py) ===\n",
        "print(\"=== 7. SPACES: МГНОВЕННОЕ РАЗВЁРТЫВАНИЕ ДЕМО ===\")\n",
        "\n",
        "print(\"\\n7.1. Gradio Demo (app.py)\")\n",
        "\n",
        "gradio_app_code = '''\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Замените на ваш ID модели на Hub\n",
        "MODEL_ID = \"my-username/distilbert-base-uncased-finetuned-mrpc\"\n",
        "\n",
        "# Инициализация пайплайна\n",
        "classifier = pipeline(\"text-classification\", model=MODEL_ID)\n",
        "\n",
        "def predict_similarity(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Предсказывает, являются ли два предложения семантически эквивалентными.\n",
        "    \"\"\"\n",
        "    if not sentence1.strip() or not sentence2.strip():\n",
        "        return {\"Ошибка\": 1.0}\n",
        "    \n",
        "    # Объединение предложений как в MRPC\n",
        "    combined_text = f\"{sentence1} [SEP] {sentence2}\"\n",
        "    results = classifier(combined_text)\n",
        "    \n",
        "    # Преобразование результата в формат для Label\n",
        "    return {result[\"label\"]: result[\"score\"] for result in results}\n",
        "\n",
        "# Создание интерфейса\n",
        "demo = gr.Interface(\n",
        "    fn=predict_similarity,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Предложение 1\", placeholder=\"Введите первое предложение...\"),\n",
        "        gr.Textbox(label=\"Предложение 2\", placeholder=\"Введите второе предложение...\")\n",
        "    ],\n",
        "    outputs=gr.Label(num_top_classes=2, label=\"Результат\"),\n",
        "    title=\"Демо: Семантическая Эквивалентность Предложений\",\n",
        "    description=\"\"\"\n",
        "    Этот сервис использует модель <code>distilbert-base-uncased</code>,\n",
        "    дообученную на датасете MRPC для задачи определения семантической эквивалентности.\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        [\"How are you?\", \"What's up?\"],\n",
        "        [\"The cat is on the mat.\", \"A feline is sitting on a rug.\"]\n",
        "    ],\n",
        "    cache_examples=True  # Кэширует примеры для быстрого отклика\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()  # Запуск локально\n",
        "'''\n",
        "\n",
        "print(\"Код для Gradio Space создан.\")\n",
        "print(\"Для развертывания на Spaces:\")\n",
        "print(\"1. Создайте репозиторий типа 'Space' на Hugging Face Hub\")\n",
        "print(\"2. Выберите SDK: 'Gradio'\")\n",
        "print(\"3. Загрузите файлы app.py и requirements.txt\")\n",
        "\n",
        "# === 7.2. Streamlit Demo (streamlit_app.py) ===\n",
        "print(\"\\n7.2. Streamlit Demo (streamlit_app.py)\")\n",
        "\n",
        "streamlit_app_code = '''\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Semantic Text Similarity\",\n",
        "    page_icon=\"🔍\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Кэширование модели для повышения производительности\n",
        "@st.cache_resource\n",
        "def load_classifier():\n",
        "    return pipeline(\"text-classification\", model=\"my-username/distilbert-base-uncased-finetuned-mrpc\")\n",
        "\n",
        "classifier = load_classifier()\n",
        "\n",
        "# Заголовок и описание\n",
        "st.title(\"🔍 Демо: Семантическая Эквивалентность\")\n",
        "st.markdown(\"\"\"\n",
        "Этот сервис определяет, являются ли два предложения семантически эквивалентными.\n",
        "Используется модель **DistilBERT**, дообученная на датасете **MRPC**.\n",
        "\"\"\")\n",
        "\n",
        "# Создание двух колонок для ввода\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    sentence1 = st.text_area(\"Предложение 1\", height=100,\n",
        "                             placeholder=\"Введите первое предложение...\")\n",
        "\n",
        "with col2:\n",
        "    sentence2 = st.text_area(\"Предложение 2\", height=100,\n",
        "                             placeholder=\"Введите второе предложение...\")\n",
        "\n",
        "# Кнопка анализа\n",
        "if st.button(\"Анализировать\", type=\"primary\", use_container_width=True):\n",
        "    if sentence1.strip() and sentence2.strip():\n",
        "        with st.spinner(\"Обработка...\"):\n",
        "            combined = f\"{sentence1} [SEP] {sentence2}\"\n",
        "            results = classifier(combined)\n",
        "            \n",
        "        # Отображение результатов\n",
        "        st.subheader(\"Результаты\")\n",
        "        for result in results:\n",
        "            st.metric(\n",
        "                label=f\"Класс: {result['label']}\",\n",
        "                value=f\"{result['score']:.3f}\"\n",
        "            )\n",
        "        \n",
        "        # Дополнительная информация\n",
        "        st.info(\"Модель обучена различать эквивалентные и неэквивалентные пары предложений.\")\n",
        "    else:\n",
        "        st.error(\"Пожалуйста, введите оба предложения.\")\n",
        "\n",
        "# Боковая панель с информацией\n",
        "with st.sidebar:\n",
        "    st.header(\"Информация\")\n",
        "    st.markdown(\"\"\"\n",
        "    - **Модель**: `distilbert-base-uncased`\n",
        "    - **Задача**: Семантическая эквивалентность (MRPC)\n",
        "    - **Точность**: ~88%\n",
        "    \"\"\")\n",
        "    st.link_button(\"Исходный код на GitHub\", \"https://github.com/...\")\n",
        "'''\n",
        "\n",
        "print(\"Код для Streamlit Space создан.\")\n",
        "print(\"Для развертывания на Spaces:\")\n",
        "print(\"1. Создайте репозиторий типа 'Space'\")\n",
        "print(\"2. Выберите SDK: 'Streamlit'\")\n",
        "print(\"3. Загрузите файлы streamlit_app.py и requirements.txt\")\n",
        "\n",
        "# === 7.3. Файл зависимостей (requirements.txt) ===\n",
        "print(\"\\n7.3. Файл зависимостей (requirements.txt)\")\n",
        "\n",
        "requirements = \"\"\"\n",
        "# Для Gradio и Streamlit демо\n",
        "transformers>=4.30.0\n",
        "torch>=2.0.0\n",
        "gradio>=3.38.0\n",
        "streamlit>=1.25.0\n",
        "accelerate>=0.21.0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Файл requirements.txt:\")\n",
        "print(requirements)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Spaces радикально упрощает демонстрацию результатов работы. Всего за несколько минут можно создать интерактивное веб-приложение, которое позволяет любому пользователю протестировать модель в реальном времени. Это не только ускоряет получение обратной связи от стейкхолдеров, но и служит мощным инструментом для популяризации и распространения исследований.\n",
        "\n"
      ],
      "metadata": {
        "id": "N3LvaHr7Uq2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 8. Оптимизация для продакшена\n",
        "\n",
        "### Теория: Методы уменьшения размера и ускорения моделей\n",
        "\n",
        "Переход модели из исследовательской среды в промышленный продакшен требует решения ряда критических задач: **минимизация задержки **(latency), **снижение потребления памяти** и **максимизация пропускной способности **(throughput). Для достижения этих целей в экосистеме Hugging Face существует три взаимодополняющих метода оптимизации:\n",
        "\n",
        "1.  **Квантование **(Quantization) — процесс преобразования весов модели и активаций из 32-битного формата с плавающей точкой (FP32) в более компактные представления (INT8, INT4). Это позволяет уменьшить размер модели в 2–8 раз и ускорить инференс за счёт использования специализированных инструкций процессора (INT8 Tensor Cores на GPU NVIDIA). Существуют два основных подхода:\n",
        "    *   **Post-Training Quantization **(PTQ) — квантование после завершения обучения, не требующее доступа к исходным данным.\n",
        "    *   **Quantization-Aware Training **(QAT) — имитация квантованных вычислений во время обучения, что позволяет компенсировать потерю точности.\n",
        "\n",
        "2.  **Дистилляция знаний **(Knowledge Distillation) — метод передачи знаний от большой, сложной **учительской модели **(teacher) к малой, быстрой **студенческой модели **(student). Студент обучается не только на истинных метках, но и на «мягких» вероятностях, предсказанных учителем. Это позволяет студенту захватить тонкие семантические зависимости, которые невозможно извлечь из разреженных меток, и достигать качества, близкого к учителю, при значительно меньшей вычислительной сложности.\n",
        "\n",
        "3.  **Компиляция и экспорт **(ONNX/TensorRT) — преобразование вычислительного графа модели из фреймворка-исходника (PyTorch, TensorFlow) в оптимизированный, статический формат, такой как **ONNX **(Open Neural Network Exchange) или **TensorRT**. Эти форматы позволяют выполнить агрессивные оптимизации на уровне графа: слияние операций, устранение избыточных узлов, оптимизация памяти. ONNX обеспечивает кроссплатформенность, а TensorRT предлагает максимальную производительность на GPU NVIDIA.\n",
        "\n",
        "**Сравнительный анализ методов оптимизации**\n",
        "\n",
        "| Метод | Скорость ускорения | Сокращение размера | Потеря качества | Сложность внедрения |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Квантование **(INT8) | 2–3x | 4x | Низкая (<1%) | Очень низкая |\n",
        "| **Квантование **(INT4) | 4–6x | 8x | Умеренная (1–5%) | Низкая |\n",
        "| **Дистилляция** | 5–10x | 3–5x | Очень низкая | Высокая (требует дообучения) |\n",
        "| **ONNX Runtime** | 1.5–3x | Нет | Нет | Средняя |\n",
        "| **TensorRT** | 3–10x | Нет | Нет | Высокая (аппаратно-специфична) |\n",
        "\n",
        "На практике эти методы часто **комбинируются** для достижения максимального эффекта (например, дистилляция + INT8 квантование + ONNX).\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует практическое применение всех трёх методов оптимизации к одной и той же модели, показывая, как их можно интегрировать в production-пайплайн.\n",
        "\n",
        "```python\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from optimum import quantization\n",
        "import torch\n",
        "\n",
        "print(\"=== 8. ОПТИМИЗАЦИЯ ДЛЯ ПРОДАКШЕНА ===\")\n",
        "\n",
        "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# === 8.1. Квантование (8-bit и 4-bit) ===\n",
        "print(\"\\n8.1. Квантование моделей\")\n",
        "\n",
        "# Загрузка 8-битной модели (требует библиотеки bitsandbytes)\n",
        "try:\n",
        "    model_8bit = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_id,\n",
        "        load_in_8bit=True,  # Автоматическое 8-битное квантование\n",
        "        device_map=\"auto\"   # Автоматическое распределение по GPU/CPU\n",
        "    )\n",
        "    print(\"8-битная модель успешно загружена.\")\n",
        "except ImportError:\n",
        "    print(\"Для 8-битного квантования требуется установка 'bitsandbytes'.\")\n",
        "    model_8bit = None\n",
        "\n",
        "# Создание квантованного пайплайна\n",
        "if model_8bit is not None:\n",
        "    quantized_pipeline = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model_8bit,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    result = quantized_pipeline(\"Квантование работает отлично!\")\n",
        "    print(f\"Результат 8-битной модели: {result[0]['label']}\")\n",
        "\n",
        "# === 8.2. Экспорт в ONNX и использование ONNX Runtime ===\n",
        "print(\"\\n8.2. ONNX экспорт и оптимизация\")\n",
        "\n",
        "# Экспорт модели в ONNX формат\n",
        "onnx_model_path = \"./optimized_model/onnx\"\n",
        "ORTModelForSequenceClassification.from_pretrained(\n",
        "    model_id,\n",
        "    export=True,  # Автоматический экспорт\n",
        "    provider=\"CUDAExecutionProvider\" if torch.cuda.is_available() else \"CPUExecutionProvider\"\n",
        ").save_pretrained(onnx_model_path)\n",
        "\n",
        "# Загрузка ONNX модели для инференса\n",
        "onnx_model = ORTModelForSequenceClassification.from_pretrained(onnx_model_path)\n",
        "onnx_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=onnx_model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "result_onnx = onnx_pipeline(\"ONNX Runtime ускоряет инференс!\")\n",
        "print(f\"Результат ONNX модели: {result_onnx[0]['label']}\")\n",
        "\n",
        "# === 8.3. Дистилляция (обзорный пример) ===\n",
        "print(\"\\n8.3. Дистилляция модели\")\n",
        "\n",
        "# В реальном сценарии здесь был бы полный цикл:\n",
        "# 1. Загрузка учительской модели (например, BERT-base)\n",
        "teacher_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# 2. Создание студенческой архитектуры (например, DistilBERT)\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# 3. Реализация кастомного Trainer с функцией потерь дистилляции\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, teacher_model, alpha=0.7, temperature=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # Выходы студента\n",
        "        student_outputs = model(**inputs)\n",
        "        student_logits = student_outputs.logits\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Выходы учителя\n",
        "            teacher_outputs = self.teacher_model(**inputs)\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "        \n",
        "        # Loss дистилляции\n",
        "        loss_distill = torch.nn.KLDivLoss(reduction='batchmean')(\n",
        "            torch.log_softmax(student_logits / self.temperature, dim=-1),\n",
        "            torch.softmax(teacher_logits / self.temperature, dim=-1)\n",
        "        ) * (self.temperature ** 2)\n",
        "        \n",
        "        # Loss задачи\n",
        "        loss_task = torch.nn.CrossEntropyLoss()(student_logits, labels)\n",
        "        \n",
        "        # Комбинированный loss\n",
        "        loss = self.alpha * loss_distill + (1 - self.alpha) * loss_task\n",
        "        \n",
        "        return (loss, student_outputs) if return_outputs else loss\n",
        "\n",
        "print(\"Дистилляция требует кастомной реализации Trainer, но даёт максимальную выгоду в размере и скорости.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Интеграция этих методов позволяет превратить исследовательскую модель в production-готовое решение. Квантование и ONNX обеспечивают быстрый выигрыш в скорости с минимальными усилиями, в то время как дистилляция требует значительных вычислительных ресурсов на этапе обучения, но даёт наиболее компактную и быструю модель для инференса.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Комплексный практический кейс: End-to-End Workflow\n",
        "\n",
        "### Полный цикл от данных до production демо\n",
        "\n",
        "Этот раздел представляет собой **методологически строгий и практически исполнимый end-to-end workflow**, который интегрирует все компоненты экосистемы Hugging Face для создания промышленно-готовой системы классификации текста. Пайплайн охватывает полный жизненный цикл: от загрузки данных и их агрегации до распределённого обучения, публикации, оптимизации и развёртывания интерактивного демо.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий код структурирован как готовый к запуску скрипт, который можно адаптировать под любую задачу классификации текста. Он демонстрирует лучшие практики MLOps для NLP.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Комплексный пример: End-to-End Text Classification System\n",
        "Интеграция всех компонентов Hugging Face Ecosystem\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from accelerate import Accelerator\n",
        "from huggingface_hub import HfApi, create_repo, notebook_login\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "# === ГЛОБАЛЬНАЯ КОНФИГУРАЦИЯ ===\n",
        "CONFIG = {\n",
        "    \"model_name\": \"distilbert-base-uncased\",\n",
        "    \"num_labels\": 2,\n",
        "    \"max_length\": 512,\n",
        "    \"train_batch_size\": 16,\n",
        "    \"eval_batch_size\": 16,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"num_epochs\": 3,\n",
        "    \"gradient_accumulation_steps\": 2,\n",
        "    \"fp16\": True,\n",
        "    \"output_dir\": \"./text_classification_system\",\n",
        "    \"hub_model_id\": \"my-username/text-classification-system\",\n",
        "    \"demo_port\": 7860\n",
        "}\n",
        "\n",
        "print(\"=== 9. КОМПЛЕКСНЫЙ ПРАКТИЧЕСКИЙ КЕЙС ===\")\n",
        "\n",
        "# === 1. ПОДГОТОВКА ДАННЫХ ===\n",
        "def prepare_data():\n",
        "    \"\"\"Загрузка и агрегация данных из нескольких источников на Hub.\"\"\"\n",
        "    \n",
        "    print(\"1. Подготовка данных...\")\n",
        "    \n",
        "    # Загрузка датасетов\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "    \n",
        "    # Приведение к единому формату (text, label)\n",
        "    def preprocess_sst2(example):\n",
        "        return {\n",
        "            \"text\": example[\"sentence\"],\n",
        "            \"label\": example[\"label\"]\n",
        "        }\n",
        "    \n",
        "    sst2 = sst2.map(preprocess_sst2, remove_columns=[\"sentence\", \"idx\"])\n",
        "    \n",
        "    # Объединение датасетов\n",
        "    train_combined = DatasetDict({\n",
        "        \"train\": imdb[\"train\"].shuffle(seed=42).select(range(10000)) +\n",
        "                 sst2[\"train\"].shuffle(seed=42).select(range(10000)),\n",
        "        \"validation\": imdb[\"test\"].shuffle(seed=42).select(range(1000)) +\n",
        "                      sst2[\"validation\"].shuffle(seed=42).select(range(1000))\n",
        "    })\n",
        "    \n",
        "    print(f\"Датасет подготовлен. Размер тренировки: {len(train_combined['train'])}\")\n",
        "    return train_combined\n",
        "\n",
        "# === 2. НАСТРОЙКА МОДЕЛИ И ТОКЕНИЗАТОРА ===\n",
        "def setup_model_and_tokenizer():\n",
        "    \"\"\"Инициализация компонентов модели.\"\"\"\n",
        "    \n",
        "    print(\"2. Настройка модели и токенизатора...\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        CONFIG[\"model_name\"],\n",
        "        num_labels=CONFIG[\"num_labels\"]\n",
        "    )\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "# === 3. ПРЕДОБРАБОТКА ДАННЫХ ===\n",
        "def tokenize_dataset(dataset, tokenizer):\n",
        "    \"\"\"Токенизация датасета с отложенным выполнением.\"\"\"\n",
        "    \n",
        "    print(\"3. Токенизация датасета...\")\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=CONFIG[\"max_length\"]\n",
        "        )\n",
        "    \n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        batch_size=1000,\n",
        "        remove_columns=[\"text\"]\n",
        "    )\n",
        "    \n",
        "    return tokenized_dataset\n",
        "\n",
        "# === 4. КОНФИГУРАЦИЯ ОБУЧЕНИЯ С ACCELERATE ===\n",
        "def setup_training(model, tokenized_dataset, tokenizer):\n",
        "    \"\"\"Настройка распределённого цикла обучения.\"\"\"\n",
        "    \n",
        "    print(\"4. Настройка распределённого обучения...\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=CONFIG[\"output_dir\"],\n",
        "        num_train_epochs=CONFIG[\"num_epochs\"],\n",
        "        per_device_train_batch_size=CONFIG[\"train_batch_size\"],\n",
        "        per_device_eval_batch_size=CONFIG[\"eval_batch_size\"],\n",
        "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "        learning_rate=CONFIG[\"learning_rate\"],\n",
        "        fp16=CONFIG[\"fp16\"],\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
        "        logging_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        # Интеграция с Hub\n",
        "        push_to_hub=True,\n",
        "        hub_model_id=CONFIG[\"hub_model_id\"],\n",
        "        hub_strategy=\"every_save\",\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "# === 5. ОБУЧЕНИЕ И ОЦЕНКА ===\n",
        "def train_and_evaluate(trainer):\n",
        "    \"\"\"Запуск обучения и оценки с логированием.\"\"\"\n",
        "    \n",
        "    print(\"5. Запуск обучения...\")\n",
        "    \n",
        "    # Обучение\n",
        "    train_result = trainer.train()\n",
        "    trainer.save_metrics(\"train\", train_result.metrics)\n",
        "    \n",
        "    # Оценка\n",
        "    eval_result = trainer.evaluate()\n",
        "    trainer.save_metrics(\"eval\", eval_result)\n",
        "    trainer.save_state()\n",
        "    \n",
        "    print(f\"Обучение завершено. Валидационная потеря: {eval_result['eval_loss']:.4f}\")\n",
        "    return train_result, eval_result\n",
        "\n",
        "# === 6. ПУБЛИКАЦИЯ НА HUB ===\n",
        "def publish_to_hub(trainer):\n",
        "    \"\"\"Публикация модели и создание Model Card.\"\"\"\n",
        "    \n",
        "    print(\"6. Публикация на Hugging Face Hub...\")\n",
        "    \n",
        "    # Создание репозитория\n",
        "    create_repo(CONFIG[\"hub_model_id\"], exist_ok=True)\n",
        "    \n",
        "    # Model Card\n",
        "    model_card = f\"\"\"\n",
        "---\n",
        "language: en\n",
        "tags:\n",
        "- text-classification\n",
        "- sentiment-analysis\n",
        "- transformers\n",
        "- accelerate\n",
        "datasets:\n",
        "- imdb\n",
        "- glue/sst2\n",
        "metrics:\n",
        "- accuracy\n",
        "- loss\n",
        "---\n",
        "\n",
        "# Text Classification System\n",
        "\n",
        "This model is fine-tuned on a combined dataset of IMDB and SST-2 for general sentiment analysis.\n",
        "\n",
        "## Training Configuration\n",
        "- **Base Model**: `{CONFIG[\"model_name\"]}`\n",
        "- **Epochs**: `{CONFIG[\"num_epochs\"]}`\n",
        "- **Batch Size**: `{CONFIG[\"train_batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]}`\n",
        "- **Learning Rate**: `{CONFIG[\"learning_rate\"]}`\n",
        "\"\"\"\n",
        "    \n",
        "    # Сохранение и публикация\n",
        "    trainer.create_model_card(model_name=CONFIG[\"hub_model_id\"], overwrite=True, readme_content=model_card)\n",
        "    print(f\"Модель опубликована: https://huggingface.co/{CONFIG['hub_model_id']}\")\n",
        "\n",
        "# === 7. СОЗДАНИЕ ДЕМО НА SPACES ===\n",
        "def create_demo():\n",
        "    \"\"\"Создание Gradio демо для развёртывания на Spaces.\"\"\"\n",
        "    \n",
        "    print(\"7. Создание Gradio демо...\")\n",
        "    \n",
        "    classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=CONFIG[\"hub_model_id\"],\n",
        "        tokenizer=CONFIG[\"hub_model_id\"]\n",
        "    )\n",
        "    \n",
        "    def classify(text):\n",
        "        if not text.strip():\n",
        "            return {\"Ошибка\": 1.0}\n",
        "        results = classifier(text)\n",
        "        return {r[\"label\"]: r[\"score\"] for r in results}\n",
        "    \n",
        "    demo = gr.Interface(\n",
        "        fn=classify,\n",
        "        inputs=gr.Textbox(label=\"Текст для анализа\", lines=3),\n",
        "        outputs=gr.Label(label=\"Результат\"),\n",
        "        title=\"Система классификации текста\",\n",
        "        description=\"Real-time sentiment analysis powered by Hugging Face Transformers.\",\n",
        "        examples=[\n",
        "            [\"Я в восторге от этого продукта!\"],\n",
        "            [\"Ужасное обслуживание, никогда больше не приду.\"],\n",
        "            [\"Погода сегодня вполне приемлемая.\"]\n",
        "        ],\n",
        "        cache_examples=True\n",
        "    )\n",
        "    \n",
        "    return demo\n",
        "\n",
        "# === ГЛАВНАЯ ФУНКЦИЯ ===\n",
        "def main():\n",
        "    \"\"\"Выполнение полного end-to-end пайплайна.\"\"\"\n",
        "    \n",
        "    # Авторизация (для запуска в ноутбуке раскомментируйте)\n",
        "    # notebook_login()\n",
        "    \n",
        "    # Инициализация Accelerator\n",
        "    accelerator = Accelerator()\n",
        "    \n",
        "    # Выполнение этапов\n",
        "    dataset = prepare_data()\n",
        "    model, tokenizer = setup_model_and_tokenizer()\n",
        "    tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n",
        "    trainer = setup_training(model, tokenized_dataset, tokenizer)\n",
        "    \n",
        "    # Подготовка для распределённого обучения\n",
        "    trainer = accelerator.prepare(trainer)\n",
        "    \n",
        "    # Обучение и оценка\n",
        "    train_result, eval_result = train_and_evaluate(trainer)\n",
        "    \n",
        "    # Публикация и демо (только на основном процессе)\n",
        "    if accelerator.is_main_process:\n",
        "        publish_to_hub(trainer)\n",
        "        demo = create_demo()\n",
        "        # Для локального тестирования\n",
        "        # demo.launch(server_name=\"0.0.0.0\", server_port=CONFIG[\"demo_port\"])\n",
        "        print(\"End-to-End пайплайн успешно завершён!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот workflow представляет собой **золотой стандарт** для современных NLP-проектов. Он демонстрирует, как экосистема Hugging Face позволяет создать сложную, производительную и воспроизводимую систему, интегрируя распределённое обучение, управление артефактами и развёртывание демо в единую, логически стройную архитектуру.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Экосистема Hugging Face является не просто набором инструментов, а **методологической платформой**, которая определяет современные стандарты разработки систем машинного обучения, особенно в области NLP. Её сила заключается не в отдельных компонентах, а в их **глубокой и продуманной интеграции**, которая решает фундаментальные проблемы, с которыми сталкиваются команды:\n",
        "\n",
        "1.  **Единые принципы API** across `Transformers`, `Datasets`, `Accelerate` и `PEFT` минимизируют когнитивную нагрузку и ускоряют освоение новых инструментов.\n",
        "2.  **Автоматическая оптимизация** для разнородного аппаратного обеспечения (CPU, GPU, TPU) через `Accelerate` и `Optimum` делает распределённые вычисления доступными каждому разработчику.\n",
        "3.  **Сквозная воспроизводимость** через `Hub`, версионирование и встроенные механизмы логирования гарантирует, что любой эксперимент может быть точно воспроизведён в будущем.\n",
        "4.  **Снижение порога входа** через `pipeline` и готовые demo на `Spaces` позволяет быстро прототипировать и демонстрировать результаты, ускоряя цикл обратной связи с бизнесом.\n",
        "\n",
        "Освоение этой экосистемы — это не просто изучение синтаксиса, а усвоение современной инженерной культуры машинного обучения. Именно такая культура, основанная на открытости, воспроизводимости и автоматизации, позволяет превращать исследовательские идеи в надёжные, масштабируемые и приносящие ценность промышленные системы."
      ],
      "metadata": {
        "id": "KvphFycpYUx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 21: A/B Testing & Causal Inference — От корреляции к причинности\n",
        "\n",
        "## Введение\n",
        "\n",
        "В эпоху data-driven принятия решений способность отличать **корреляцию** от **причинно-следственной связи** становится фундаментальным навыком для любого специалиста по анализу данных. Наблюдение о том, что два явления происходят одновременно, не позволяет сделать вывод о том, что одно из них является причиной другого. История изобилует примерами катастрофических решений, основанных на подобной ошибке: от интуитивно понятных, но ложных связей вроде «продажи мороженого вызывают утопления» (на самом деле оба зависят от жаркой погоды), до дорогих бизнес-ошибок, когда компании инвестируют в изменения, которые лишь коррелируют с успехом, но не являются его причиной.\n",
        "\n",
        "**Причинно-следственный анализ **(Causal Inference) — это строгая методологическая дисциплина, стремящаяся ответить на контрфактический вопрос: *«Что бы произошло с этим объектом, если бы мы применили другое воздействие?»*. Фундаментальная проблема, стоящая перед этой дисциплиной, известна как **проблема потенциальных исходов **(Fundamental Problem of Causal Inference): для каждого объекта (пользователя, клиента, пациента) мы можем наблюдать только один из возможных исходов — либо при наличии воздействия (treatment), либо при его отсутствии (control), но не оба одновременно. Наблюдаемый исход называется **фактическим **(factual), а ненаблюдаемый — **контрфактическим **(counterfactual).\n",
        "\n",
        "Этот модуль посвящён методам и инструментам, позволяющим преодолеть эту фундаментальную проблему. Мы рассмотрим классический золотой стандарт — **рандомизированные контролируемые испытания **(A/B тестирование) — и познакомимся с передовыми библиотеками Python, которые превращают причинный анализ из теоретической концепции в практический инструмент для принятия обоснованных бизнес-решений.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Основы дизайна экспериментов\n",
        "\n",
        "### Теория: Принципы рандомизации и статистические основы\n",
        "\n",
        "**Рандомизация **(Randomization) — это единственный известный метод, который может гарантировать **независимость потенциальных исходов от назначения лечения**. При правильной рандомизации все наблюдаемые и ненаблюдаемые ковариаты (характеристики объектов) распределяются одинаково в группе с воздействием (Treatment) и в контрольной группе (Control). Это позволяет интерпретировать любые систематические различия в средних исходах между группами как **причинный эффект воздействия **(Average Treatment Effect, ATE).\n",
        "\n",
        "Формально, ATE определяется как:\n",
        "\\[\n",
        "ATE = \\mathbb{E}[Y(1) - Y(0)]\n",
        "\\]\n",
        "где \\(Y(1)\\) — потенциальный исход при воздействии, а \\(Y(0)\\) — потенциальный исход без него. В рандомизированном эксперименте ATE оценивается просто как разница средних:\n",
        "\\[\n",
        "\\widehat{ATE} = \\bar{Y}_{\\text{treatment}} - \\bar{Y}_{\\text{control}}\n",
        "\\]\n",
        "\n",
        "**Статистическая мощность **(Statistical Power) — это вероятность того, что статистический тест корректно отвергнет нулевую гипотезу (об отсутствии эффекта), если альтернативная гипотеза (о существовании эффекта) верна. Низкая мощность приводит к высокому риску **ошибки второго рода **(Type II error) — неспособности обнаружить реальный эффект. Мощность зависит от четырёх ключевых факторов:\n",
        "1.  **Размер эффекта **(Effect Size): Чем больше ожидаемый эффект, тем выше мощность.\n",
        "2.  **Уровень значимости **(α): Обычно фиксируется на уровне 0.05.\n",
        "3.  **Размер выборки **(Sample Size): Чем больше выборка, тем выше мощность.\n",
        "4.  **Вариабельность данных **(Variance): Чем меньше дисперсия метрики, тем выше мощность.\n",
        "\n",
        "Проведение анализа мощности **до начала эксперимента** (a priori power analysis) является обязательной практикой для определения минимального необходимого размера выборки, который позволит обнаружить эффект заданного размера с требуемой мощностью.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Следующий пример демонстрирует полный цикл планирования A/B-эксперимента: от формулировки гипотез и расчёта необходимого размера выборки до проведения стратифицированной рандомизации и проверки баланса ковариат.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.stats.power import TTestIndPower\n",
        "from statsmodels.stats.proportion import proportion_effectsize\n",
        "from scipy import stats\n",
        "\n",
        "print(\"=== 1. ОСНОВЫ ДИЗАЙНА ЭКСПЕРИМЕНТОВ ===\")\n",
        "\n",
        "# === 1.1. Формулировка гипотез ===\n",
        "print(\"\\n1.1. Формулировка гипотез\")\n",
        "\n",
        "def define_hypotheses():\n",
        "    \"\"\"\n",
        "    Стандартная формулировка для A/B теста на конверсию.\n",
        "    \"\"\"\n",
        "    print(\"Нулевая гипотеза (H0): Конверсия в тестовой группе равна конверсии в контрольной группе.\")\n",
        "    print(\"Альтернативная гипотеза (H1): Конверсия в тестовой группе отличается от конверсии в контрольной группе.\")\n",
        "    print(\"(Двусторонний тест)\")\n",
        "\n",
        "define_hypotheses()\n",
        "\n",
        "# === 1.2. Расчет размера выборки ===\n",
        "print(\"\\n1.2. Расчет размера выборки\")\n",
        "\n",
        "def calculate_sample_size(conversion_control, relative_mde, alpha=0.05, power=0.8):\n",
        "    \"\"\"\n",
        "    Рассчитывает минимальный размер выборки на группу для обнаружения относительного эффекта.\n",
        "    \n",
        "    :param conversion_control: Базовая конверсия в контрольной группе (0.1 = 10%)\n",
        "    :param relative_mde: Минимальный обнаруживаемый эффект (MDE) в относительных единицах (0.05 = 5%)\n",
        "    :param alpha: Уровень значимости\n",
        "    :param power: Желаемая статистическая мощность\n",
        "    :return: Необходимый размер выборки на одну группу\n",
        "    \"\"\"\n",
        "    # Расчёт абсолютного эффекта\n",
        "    conversion_treatment = conversion_control * (1 + relative_mde)\n",
        "    \n",
        "    # Расчёт стандартизированного размера эффекта для пропорций (Cohen's h)\n",
        "    effect_size = proportion_effectsize(conversion_control, conversion_treatment)\n",
        "    \n",
        "    # Анализ мощности для независимого t-теста\n",
        "    power_analysis = TTestIndPower()\n",
        "    sample_size = power_analysis.solve_power(\n",
        "        effect_size=abs(effect_size),\n",
        "        alpha=alpha,\n",
        "        power=power,\n",
        "        ratio=1.0  # Соотношение размеров групп (1.0 = равные группы)\n",
        "    )\n",
        "    \n",
        "    # Возвращаем целое число, округляя в большую сторону\n",
        "    return int(np.ceil(sample_size))\n",
        "\n",
        "# Пример расчёта\n",
        "baseline_conversion = 0.10  # 10%\n",
        "mde_relative = 0.05         # 5% улучшение\n",
        "\n",
        "sample_size_per_group = calculate_sample_size(baseline_conversion, mde_relative)\n",
        "total_sample_size = sample_size_per_group * 2\n",
        "\n",
        "print(f\"Базовая конверсия: {baseline_conversion:.1%}\")\n",
        "print(f\"Целевой MDE: {mde_relative:.1%}\")\n",
        "print(f\"Необходимый размер выборки на группу: {sample_size_per_group:,}\")\n",
        "print(f\"Общий размер выборки для эксперимента: {total_sample_size:,}\")\n",
        "\n",
        "# === 1.3. Рандомизация и проверка баланса ===\n",
        "print(\"\\n1.3. Рандомизация и проверка баланса ковариат\")\n",
        "\n",
        "def stratified_randomization(data, strata_columns, treatment_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Проводит стратифицированную рандомизацию для обеспечения баланса по ключевым ковариатам.\n",
        "    \n",
        "    :param data: DataFrame с данными пользователей\n",
        "    :param strata_columns: Список колонок для стратификации\n",
        "    :param treatment_ratio: Доля пользователей в тестовой группе\n",
        "    :return: DataFrame с новой колонкой 'treatment' (0=control, 1=treatment)\n",
        "    \"\"\"\n",
        "    data = data.copy()\n",
        "    \n",
        "    # Создание уникального идентификатора страты\n",
        "    data['strata'] = data[strata_columns].astype(str).apply('-'.join, axis=1)\n",
        "    \n",
        "    # Инициализация колонки лечения\n",
        "    data['treatment'] = 0\n",
        "    \n",
        "    # Рандомизация внутри каждой страты\n",
        "    for stratum in data['strata'].unique():\n",
        "        stratum_mask = data['strata'] == stratum\n",
        "        stratum_indices = data[stratum_mask].index.tolist()\n",
        "        stratum_size = len(stratum_indices)\n",
        "        \n",
        "        # Определение количества пользователей в тестовой группе для этой страты\n",
        "        treatment_size = int(stratum_size * treatment_ratio)\n",
        "        \n",
        "        # Случайный выбор пользователей для тестовой группы\n",
        "        treatment_indices = np.random.choice(stratum_indices, size=treatment_size, replace=False)\n",
        "        data.loc[treatment_indices, 'treatment'] = 1\n",
        "    \n",
        "    return data.drop('strata', axis=1)\n",
        "\n",
        "def check_covariate_balance(data, covariates, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Проверяет баланс ковариат между группами лечения и контроля.\n",
        "    Использует стандартизированную разность (Standardized Mean Difference).\n",
        "    \n",
        "    :param data: DataFrame с колонкой 'treatment'\n",
        "    :param covariates: Список колонок для проверки баланса\n",
        "    :param threshold: Порог для значимой разницы (обычно 0.1 или 0.2)\n",
        "    :return: DataFrame с результатами проверки\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for cov in covariates:\n",
        "        control_mean = data[data['treatment'] == 0][cov].mean()\n",
        "        treatment_mean = data[data['treatment'] == 1][cov].mean()\n",
        "        pooled_std = data[cov].std()\n",
        "        \n",
        "        # Стандартизированная разность\n",
        "        std_diff = (treatment_mean - control_mean) / pooled_std\n",
        "        \n",
        "        results.append({\n",
        "            'Covariate': cov,\n",
        "            'Control_Mean': control_mean,\n",
        "            'Treatment_Mean': treatment_mean,\n",
        "            'Std_Difference': std_diff,\n",
        "            'Is_Balanced': abs(std_diff) < threshold\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Создание синтетических данных пользователей\n",
        "np.random.seed(42)\n",
        "n_users = 10000\n",
        "user_data = pd.DataFrame({\n",
        "    'user_id': range(1, n_users + 1),\n",
        "    'age': np.random.normal(35, 10, n_users),\n",
        "    'previous_purchases': np.random.poisson(5, n_users),\n",
        "    'geography': np.random.choice(['US', 'EU', 'Asia'], n_users, p=[0.5, 0.3, 0.2])\n",
        "})\n",
        "\n",
        "# Проведение стратифицированной рандомизации\n",
        "user_data = stratified_randomization(user_data, ['geography'])\n",
        "\n",
        "# Проверка баланса ковариат\n",
        "balance_report = check_covariate_balance(user_data, ['age', 'previous_purchases'])\n",
        "print(\"\\nОтчет о балансе ковариат:\")\n",
        "print(balance_report)\n",
        "\n",
        "# Визуализация баланса\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(balance_report['Covariate'], balance_report['Std_Difference'])\n",
        "plt.axvline(x=0, color='black', linestyle='--')\n",
        "plt.axvline(x=0.1, color='red', linestyle=':', label='Порог (0.1)')\n",
        "plt.axvline(x=-0.1, color='red', linestyle=':')\n",
        "plt.xlabel('Стандартизированная разность')\n",
        "plt.title('Проверка баланса ковариат между группами')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример иллюстрирует, что успешный эксперимент начинается задолго до сбора данных. Тщательное планирование, включающее расчёт мощности и стратифицированную рандомизацию, является залогом того, что полученные результаты будут статистически надёжными и интерпретируемыми. Проверка баланса ковариат после рандомизации — это обязательный шаг для подтверждения корректности процедуры.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Метрики и валидация экспериментов\n",
        "\n",
        "### Теория: Иерархия метрик и статистическая валидация\n",
        "\n",
        "Правильный выбор и анализ метрик — это сердце A/B-тестирования. Метрики должны быть организованы в **иерархическую структуру**, которая отражает их важность для бизнеса:\n",
        "\n",
        "1.  **Первичные метрики **(Primary Metrics): Прямые показатели ценности продукта или изменения (например, конверсия, выручка на пользователя). Изменение в этих метриках напрямую влияет на решение о внедрении.\n",
        "2.  **Вторичные метрики **(Secondary Metrics): Вспомогательные показатели, которые помогают понять *почему* изменились первичные метрики (например, время на сайте, количество просмотров страниц).\n",
        "3.  **Guardrail метрики **(Guardrail Metrics): «Система безопасности», которая отслеживает потенциально негативные побочные эффекты (например, частота ошибок, время загрузки страницы). Ухудшение Guardrail метрик может перевесить положительный эффект в первичных метриках.\n",
        "\n",
        "**Статистическая валидация** эксперимента включает не только проверку гипотез для первичных метрик, но и **поправку на множественные сравнения **(Multiple Comparisons Correction). При тестировании нескольких гипотез одновременно возрастает вероятность ложноположительных результатов (ошибки первого рода). Методы, такие как **поправка Бонферрони **(Bonferroni) или **процедура Бенжамини-Хохберга **(Benjamini-Hochberg, FDR), контролируют эту вероятность.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует создание комплексной системы анализа A/B-теста, включающей иерархию метрик, статистические тесты для разных типов данных и визуализацию результатов.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n=== 2. МЕТРИКИ И ВАЛИДАЦИЯ ЭКСПЕРИМЕНТОВ ===\")\n",
        "\n",
        "# === 2.1. Класс для анализа метрик ===\n",
        "class ABTestAnalyzer:\n",
        "    \"\"\"\n",
        "    Класс для комплексного анализа A/B-теста.\n",
        "    \"\"\"\n",
        "    def __init__(self, control_data, treatment_data):\n",
        "        self.control_data = control_data\n",
        "        self.treatment_data = treatment_data\n",
        "    \n",
        "    def analyze_primary_metrics(self):\n",
        "        \"\"\"Анализ первичных бизнес-метрик.\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # Конверсия (бинарная метрика)\n",
        "        control_conv = self.control_data['converted'].mean()\n",
        "        treatment_conv = self.treatment_data['converted'].mean()\n",
        "        abs_diff_conv = treatment_conv - control_conv\n",
        "        rel_diff_conv = abs_diff_conv / control_conv if control_conv > 0 else 0\n",
        "        \n",
        "        results['conversion_rate'] = {\n",
        "            'control': control_conv,\n",
        "            'treatment': treatment_conv,\n",
        "            'absolute_difference': abs_diff_conv,\n",
        "            'relative_difference': rel_diff_conv\n",
        "        }\n",
        "        \n",
        "        # Средний доход на пользователя - ARPU (непрерывная метрика)\n",
        "        control_arpu = self.control_data['revenue'].mean()\n",
        "        treatment_arpu = self.treatment_data['revenue'].mean()\n",
        "        abs_diff_arpu = treatment_arpu - control_arpu\n",
        "        rel_diff_arpu = abs_diff_arpu / control_arpu if control_arpu > 0 else 0\n",
        "        \n",
        "        results['arpu'] = {\n",
        "            'control': control_arpu,\n",
        "            'treatment': treatment_arpu,\n",
        "            'absolute_difference': abs_diff_arpu,\n",
        "            'relative_difference': rel_diff_arpu\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def analyze_guardrail_metrics(self):\n",
        "        \"\"\"Анализ guardrail метрик.\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # Время загрузки страницы (непрерывная метрика)\n",
        "        control_latency = self.control_data['page_load_time'].mean()\n",
        "        treatment_latency = self.treatment_data['page_load_time'].mean()\n",
        "        results['latency'] = {\n",
        "            'control': control_latency,\n",
        "            'treatment': treatment_latency,\n",
        "            'difference': treatment_latency - control_latency\n",
        "        }\n",
        "        \n",
        "        # Частота ошибок (бинарная метрика)\n",
        "        control_errors = self.control_data['errors'].mean()\n",
        "        treatment_errors = self.treatment_data['errors'].mean()\n",
        "        results['error_rate'] = {\n",
        "            'control': control_errors,\n",
        "            'treatment': treatment_errors,\n",
        "            'difference': treatment_errors - control_errors\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def perform_statistical_tests(self):\n",
        "        \"\"\"Выполнение статистических тестов для всех метрик.\"\"\"\n",
        "        p_values = {}\n",
        "        \n",
        "        # Тест для конверсии (Z-тест для пропорций)\n",
        "        conv_control_success = self.control_data['converted'].sum()\n",
        "        conv_treatment_success = self.treatment_data['converted'].sum()\n",
        "        conv_control_n = len(self.control_data)\n",
        "        conv_treatment_n = len(self.treatment_data)\n",
        "        \n",
        "        _, p_conv = proportions_ztest(\n",
        "            [conv_treatment_success, conv_control_success],\n",
        "            [conv_treatment_n, conv_control_n]\n",
        "        )\n",
        "        p_values['conversion_rate'] = p_conv\n",
        "        \n",
        "        # Тест для ARPU (T-тест для непрерывных данных)\n",
        "        _, p_arpu = stats.ttest_ind(\n",
        "            self.treatment_data['revenue'],\n",
        "            self.control_data['revenue'],\n",
        "            equal_var=False  # Welch's t-test\n",
        "        )\n",
        "        p_values['arpu'] = p_arpu\n",
        "        \n",
        "        # Тест для latency\n",
        "        _, p_latency = stats.ttest_ind(\n",
        "            self.treatment_data['page_load_time'],\n",
        "            self.control_data['page_load_time'],\n",
        "            equal_var=False\n",
        "        )\n",
        "        p_values['latency'] = p_latency\n",
        "        \n",
        "        # Тест для error_rate\n",
        "        err_control_success = self.control_data['errors'].sum()\n",
        "        err_treatment_success = self.treatment_data['errors'].sum()\n",
        "        err_control_n = len(self.control_data)\n",
        "        err_treatment_n = len(self.treatment_data)\n",
        "        \n",
        "        _, p_errors = proportions_ztest(\n",
        "            [err_treatment_success, err_control_success],\n",
        "            [err_treatment_n, err_control_n]\n",
        "        )\n",
        "        p_values['error_rate'] = p_errors\n",
        "        \n",
        "        return p_values\n",
        "    \n",
        "    def apply_multiple_testing_correction(self, p_values, method='fdr_bh'):\n",
        "        \"\"\"\n",
        "        Применяет поправку на множественные сравнения.\n",
        "        \n",
        "        :param p_values: Словарь с p-значениями\n",
        "        :param method: Метод коррекции ('bonferroni', 'fdr_bh')\n",
        "        :return: Словарь с скорректированными p-значениями\n",
        "        \"\"\"\n",
        "        metric_names = list(p_values.keys())\n",
        "        raw_pvals = [p_values[m] for m in metric_names]\n",
        "        \n",
        "        if method == 'bonferroni':\n",
        "            _, corrected_pvals, _, _ = multipletests(raw_pvals, alpha=0.05, method='bonferroni')\n",
        "        elif method == 'fdr_bh':\n",
        "            _, corrected_pvals, _, _ = multipletests(raw_pvals, alpha=0.05, method='fdr_bh')\n",
        "        \n",
        "        return dict(zip(metric_names, corrected_pvals))\n",
        "\n",
        "# === 2.2. Визуализация результатов ===\n",
        "def plot_ab_test_results(primary_metrics, guardrail_metrics, corrected_pvals):\n",
        "    \"\"\"\n",
        "    Создаёт комплексную визуализацию результатов A/B-теста.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Сравнение первичных метрик\n",
        "    metrics = list(primary_metrics.keys())\n",
        "    control_vals = [primary_metrics[m]['control'] for m in metrics]\n",
        "    treatment_vals = [primary_metrics[m]['treatment'] for m in metrics]\n",
        "    \n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "    \n",
        "    axes[0, 0].bar(x - width/2, control_vals, width, label='Control', alpha=0.8, color='lightblue')\n",
        "    axes[0, 0].bar(x + width/2, treatment_vals, width, label='Treatment', alpha=0.8, color='lightcoral')\n",
        "    axes[0, 0].set_title('Сравнение первичных метрик', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xticks(x)\n",
        "    axes[0, 0].set_xticklabels([m.replace('_', '\\n') for m in metrics])\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 2. Относительное улучшение\n",
        "    rel_improvements = [primary_metrics[m]['relative_difference'] for m in metrics]\n",
        "    colors = ['green' if imp > 0 else 'red' for imp in rel_improvements]\n",
        "    bars = axes[0, 1].bar(metrics, rel_improvements, color=colors, alpha=0.8)\n",
        "    axes[0, 1].set_title('Относительное улучшение (%)', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xticklabels([m.replace('_', '\\n') for m in metrics], rotation=45)\n",
        "    axes[0, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Добавление p-значений на график\n",
        "    for i, (bar, metric) in enumerate(zip(bars, metrics)):\n",
        "        height = bar.get_height()\n",
        "        p_val = corrected_pvals.get(metric, 1.0)\n",
        "        significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
        "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01 if height > 0 else height - 0.03,\n",
        "                       significance, ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\n",
        "    \n",
        "    # 3. Guardrail метрики\n",
        "    guardrail_names = list(guardrail_metrics.keys())\n",
        "    guardrail_diffs = [guardrail_metrics[m]['difference'] for m in guardrail_names]\n",
        "    guardrail_colors = ['red' if diff > 0 else 'green' for diff in guardrail_diffs]\n",
        "    axes[1, 0].bar(guardrail_names, guardrail_diffs, color=guardrail_colors, alpha=0.8)\n",
        "    axes[1, 0].set_title('Изменение Guardrail метрик', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xticklabels([m.replace('_', '\\n') for m in guardrail_names], rotation=45)\n",
        "    axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 4. Интерпретация результатов (текстовый блок)\n",
        "    axes[1, 1].axis('off')\n",
        "    conclusions = []\n",
        "    \n",
        "    # Проверка первичных метрик\n",
        "    if corrected_pvals['conversion_rate'] < 0.05 and primary_metrics['conversion_rate']['relative_difference'] > 0:\n",
        "        conclusions.append(\"• Конверсия статистически значимо улучшилась.\")\n",
        "    if corrected_pvals['arpu'] < 0.05 and primary_metrics['arpu']['relative_difference'] > 0:\n",
        "        conclusions.append(\"• ARPU статистически значимо вырос.\")\n",
        "    \n",
        "    # Проверка Guardrail метрик\n",
        "    if corrected_pvals['latency'] < 0.05 and guardrail_metrics['latency']['difference'] > 0.1:\n",
        "        conclusions.append(\"• Время загрузки значительно увеличилось (потенциально плохо).\")\n",
        "    if corrected_pvals['error_rate'] < 0.05 and guardrail_metrics['error_rate']['difference'] > 0.001:\n",
        "        conclusions.append(\"• Частота ошибок значительно выросла (критично!).\")\n",
        "    \n",
        "    if not conclusions:\n",
        "        conclusions = [\"• Статистически значимых изменений не обнаружено.\"]\n",
        "    \n",
        "    axes[1, 1].text(0.1, 0.9, \"Ключевые выводы:\", fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)\n",
        "    for i, conclusion in enumerate(conclusions[:4]):  # Ограничим 4 выводами\n",
        "        axes[1, 1].text(0.1, 0.8 - i*0.2, conclusion, fontsize=12, transform=axes[1, 1].transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === 2.3. Пример анализа реального эксперимента ===\n",
        "# Генерация синтетических данных\n",
        "np.random.seed(42)\n",
        "n_control = 10000\n",
        "n_treatment = 10000\n",
        "\n",
        "# Контрольная группа\n",
        "control_data = pd.DataFrame({\n",
        "    'converted': np.random.binomial(1, 0.10, n_control),\n",
        "    'revenue': np.random.exponential(50, n_control),\n",
        "    'page_load_time': np.random.normal(2.0, 0.2, n_control),\n",
        "    'errors': np.random.binomial(1, 0.01, n_control)\n",
        "})\n",
        "\n",
        "# Тестовая группа (с улучшением конверсии и ARPU, но с ухудшением latency)\n",
        "treatment_data = pd.DataFrame({\n",
        "    'converted': np.random.binomial(1, 0.108, n_treatment),  # 8% улучшение\n",
        "    'revenue': np.random.exponential(53, n_treatment),       # 6% улучшение\n",
        "    'page_load_time': np.random.normal(2.15, 0.2, n_treatment), # +75ms\n",
        "    'errors': np.random.binomial(1, 0.011, n_treatment)      # +10% ошибок\n",
        "})\n",
        "\n",
        "# Проведение анализа\n",
        "analyzer = ABTestAnalyzer(control_data, treatment_data)\n",
        "primary_metrics = analyzer.analyze_primary_metrics()\n",
        "guardrail_metrics = analyzer.analyze_guardrail_metrics()\n",
        "raw_pvals = analyzer.perform_statistical_tests()\n",
        "corrected_pvals = analyzer.apply_multiple_testing_correction(raw_pvals, method='fdr_bh')\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"\\nРезультаты A/B-теста:\")\n",
        "print(\"\\nПервичные метрики:\")\n",
        "for metric, values in primary_metrics.items():\n",
        "    print(f\"{metric}: Control={values['control']:.4f}, Treatment={values['treatment']:.4f}, \"\n",
        "          f\"Отн. разница={values['relative_difference']:+.2%}\")\n",
        "\n",
        "print(\"\\nGuardrail метрики:\")\n",
        "for metric, values in guardrail_metrics.items():\n",
        "    print(f\"{metric}: Разница={values['difference']:+.4f}\")\n",
        "\n",
        "print(\"\\nСтатистическая значимость (скорректированные p-значения):\")\n",
        "for metric, p_val in corrected_pvals.items():\n",
        "    print(f\"{metric}: p={p_val:.4f} ({'значимо' if p_val < 0.05 else 'не значимо'})\")\n",
        "\n",
        "# Визуализация\n",
        "plot_ab_test_results(primary_metrics, guardrail_metrics, corrected_pvals)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот комплексный пример показывает, как правильно структурировать анализ A/B-теста. Использование иерархии метрик, правильный выбор статистических тестов для разных типов данных и обязательная поправка на множественные сравнения превращают анализ из простого сравнения средних в строгую статистическую процедуру. Визуализация помогает быстро передать ключевые выводы заинтересованным сторонам, делая результаты доступными для понимания даже без глубоких статистических знаний.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mktpy9YrYzMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Продвинутые методы экспериментирования\n",
        "\n",
        "### Теория: Адаптивные эксперименты и многорукие бандиты\n",
        "\n",
        "Классические A/B-тесты, несмотря на свою статистическую строгость, обладают фундаментальным недостатком: они **фиксированы во времени и распределении трафика**. Это означает, что на протяжении всей длительности эксперимента значительная часть пользователей (обычно 50%) получает субоптимальный вариант, что влечёт за собой **экономические потери** (например, упущенная выгода от более высокой конверсии). В условиях, где затраты на исследование высоки, а скорость принятия решений критична, возникает потребность в более гибких подходах.\n",
        "\n",
        "**Multi-armed bandits **(MAB) — это класс адаптивных стратегий, которые динамически перераспределяют трафик в пользу вариантов, демонстрирующих лучшую производительность, решая проблему **баланса между исследованием **(exploration). Основная цель MAB — минимизировать **сожаление **(regret), которое определяется как разница между суммарной наградой, которую можно было бы получить, если бы всегда выбирался оптимальный вариант, и фактически полученной наградой.\n",
        "\n",
        "Два наиболее популярных семейства стратегий MAB:\n",
        "\n",
        "1.  **Epsilon-Greedy**: С вероятностью \\( \\epsilon \\) (exploration) выбирается случайный вариант, а с вероятностью \\( 1-\\epsilon \\) (exploitation) — вариант с наилучшей оценкой на данный момент. Прост в реализации, но может быть неэффективен при низком \\( \\epsilon \\) (медленное обучение) или высоком \\( \\epsilon \\) (слишком много субоптимальных решений).\n",
        "2.  **Thompson Sampling**: Байесовская стратегия, которая сэмплирует эффективность каждого варианта из его апостериорного распределения и выбирает вариант с наибольшим сэмплом. Это элегантно решает проблему баланса exploration/exploitation и часто показывает превосходную эмпирическую производительность.\n",
        "\n",
        "Помимо MAB, **байесовские A/B-тесты** предлагают альтернативную парадигму статистического вывода. Вместо того чтобы делать бинарное решение (отвергнуть/не отвергнуть H0) на основе p-значений, байесовский подход оценивает **апостериорное распределение** параметра (например, конверсии) и позволяет отвечать на прямые бизнес-вопросы: *«Какова вероятность того, что вариант B лучше варианта A?»* или *«Каковы ожидаемые потери при выборе варианта A?»*. Это предоставляет более богатую и интерпретируемую информацию для принятия решений.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует реализацию и сравнение стратегий MAB, а также применение байесовского подхода к анализу A/B-теста.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=== 3. ПРОДВИНУТЫЕ МЕТОДЫ ЭКСПЕРИМЕНТИРОВАНИЯ ===\")\n",
        "\n",
        "# === 3.1. Реализация Multi-Armed Bandit ===\n",
        "class MultiArmedBandit:\n",
        "    \"\"\"\n",
        "    Симулятор Multi-Armed Bandit для сравнения стратегий.\n",
        "    \"\"\"\n",
        "    def __init__(self, true_conversions):\n",
        "        \"\"\"\n",
        "        Инициализация бандита с известными истинными конверсиями.\n",
        "        \n",
        "        :param true_conversions: Список истинных вероятностей конверсии для каждого \"руки\" (варианта)\n",
        "        \"\"\"\n",
        "        self.true_conversions = np.array(true_conversions)\n",
        "        self.n_arms = len(true_conversions)\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Сброс состояния симулятора.\"\"\"\n",
        "        self.counts = np.zeros(self.n_arms)      # Сколько раз каждый вариант был выбран\n",
        "        self.values = np.zeros(self.n_arms)      # Текущая оценка конверсии для каждого варианта\n",
        "        self.rewards_history = []               # История всех полученных наград\n",
        "    \n",
        "    def pull(self, arm):\n",
        "        \"\"\"\n",
        "        \"Потянуть\" руку и получить награду (1 - конверсия, 0 - нет).\n",
        "        \n",
        "        :param arm: Индекс \"руки\" (варианта)\n",
        "        :return: Награда (0 или 1)\n",
        "        \"\"\"\n",
        "        return np.random.binomial(1, self.true_conversions[arm])\n",
        "    \n",
        "    def epsilon_greedy(self, n_trials, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Стратегия Epsilon-Greedy.\n",
        "        \n",
        "        :param n_trials: Общее количество испытаний\n",
        "        :param epsilon: Вероятность исследования (exploration)\n",
        "        :return: Оценки конверсий и история наград\n",
        "        \"\"\"\n",
        "        self.reset()\n",
        "        \n",
        "        for _ in range(n_trials):\n",
        "            if np.random.random() < epsilon:\n",
        "                # Exploration: случайный выбор\n",
        "                arm = np.random.randint(self.n_arms)\n",
        "            else:\n",
        "                # Exploitation: выбор лучшего варианта по текущей оценке\n",
        "                arm = np.argmax(self.values)\n",
        "            \n",
        "            reward = self.pull(arm)\n",
        "            # Обновление оценки для выбранного варианта (бегущее среднее)\n",
        "            self.counts[arm] += 1\n",
        "            self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
        "            self.rewards_history.append(reward)\n",
        "        \n",
        "        return self.values.copy(), self.rewards_history.copy()\n",
        "    \n",
        "    def thompson_sampling(self, n_trials, prior_alpha=1, prior_beta=1):\n",
        "        \"\"\"\n",
        "        Стратегия Thompson Sampling для бинарных наград (Beta-Bernoulli модель).\n",
        "        \n",
        "        :param n_trials: Общее количество испытаний\n",
        "        :param prior_alpha: Параметр альфа априорного распределения Бета\n",
        "        :param prior_beta: Параметр бета априорного распределения Бета\n",
        "        :return: Оценки конверсий и история наград\n",
        "        \"\"\"\n",
        "        self.reset()\n",
        "        # Инициализация апостериорных параметров для каждого варианта\n",
        "        alpha = np.ones(self.n_arms) * prior_alpha\n",
        "        beta = np.ones(self.n_arms) * prior_beta\n",
        "        \n",
        "        for _ in range(n_trials):\n",
        "            # Сэмплирование вероятности конверсии из апостериорного распределения Бета\n",
        "            samples = np.random.beta(alpha, beta)\n",
        "            # Выбор варианта с наибольшей сэмплированной вероятностью\n",
        "            arm = np.argmax(samples)\n",
        "            reward = self.pull(arm)\n",
        "            \n",
        "            # Обновление апостериорных параметров\n",
        "            alpha[arm] += reward\n",
        "            beta[arm] += (1 - reward)\n",
        "            \n",
        "            # Обновление текущей оценки как среднего апостериорного распределения\n",
        "            self.counts[arm] += 1\n",
        "            self.values[arm] = alpha[arm] / (alpha[arm] + beta[arm])\n",
        "            self.rewards_history.append(reward)\n",
        "        \n",
        "        return self.values.copy(), self.rewards_history.copy()\n",
        "\n",
        "# === 3.2. Сравнение стратегий MAB ===\n",
        "def compare_bandit_strategies(true_rates, n_trials=5000):\n",
        "    \"\"\"\n",
        "    Сравнение стратегий MAB по нескольким метрикам эффективности.\n",
        "    \"\"\"\n",
        "    print(f\"Истинные конверсии вариантов: {[f'{rate:.3f}' for rate in true_rates]}\")\n",
        "    \n",
        "    strategies = {}\n",
        "    \n",
        "    # Epsilon-Greedy\n",
        "    bandit_eg = MultiArmedBandit(true_rates)\n",
        "    values_eg, rewards_eg = bandit_eg.epsilon_greedy(n_trials, epsilon=0.1)\n",
        "    strategies['Epsilon-Greedy'] = {\n",
        "        'values': values_eg,\n",
        "        'rewards': np.array(rewards_eg),\n",
        "        'cumulative_rewards': np.cumsum(rewards_eg)\n",
        "    }\n",
        "    \n",
        "    # Thompson Sampling\n",
        "    bandit_ts = MultiArmedBandit(true_rates)\n",
        "    values_ts, rewards_ts = bandit_ts.thompson_sampling(n_trials)\n",
        "    strategies['Thompson Sampling'] = {\n",
        "        'values': values_ts,\n",
        "        'rewards': np.array(rewards_ts),\n",
        "        'cumulative_rewards': np.cumsum(rewards_ts)\n",
        "    }\n",
        "    \n",
        "    # Визуализация результатов\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    \n",
        "    # 1. Накопленные награды\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for name, strategy in strategies.items():\n",
        "        plt.plot(strategy['cumulative_rewards'], label=name, linewidth=2)\n",
        "    plt.title('Накопленные награды (Cumulative Rewards)', fontsize=14)\n",
        "    plt.xlabel('Испытания')\n",
        "    plt.ylabel('Суммарная конверсия')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Скользящее среднее конверсии\n",
        "    plt.subplot(1, 3, 2)\n",
        "    window = 100\n",
        "    for name, strategy in strategies.items():\n",
        "        moving_avg = np.convolve(strategy['rewards'], np.ones(window)/window, mode='valid')\n",
        "        plt.plot(moving_avg, label=name, linewidth=2, alpha=0.8)\n",
        "    plt.axhline(y=np.max(true_rates), color='black', linestyle='--', linewidth=1, label='Оптимум')\n",
        "    plt.title(f'Скользящая средняя конверсии (окно={window})', fontsize=14)\n",
        "    plt.xlabel('Испытания')\n",
        "    plt.ylabel('Конверсия')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Финальные оценки vs Истинные значения\n",
        "    plt.subplot(1, 3, 3)\n",
        "    arms = np.arange(len(true_rates))\n",
        "    plt.bar(arms - 0.2, true_rates, width=0.4, alpha=0.7, label='Истинные конверсии', color='lightgray')\n",
        "    for i, (name, strategy) in enumerate(strategies.items()):\n",
        "        plt.scatter(arms + 0.2 * i, strategy['values'], s=100, label=name)\n",
        "    plt.title('Финальные оценки vs Истинные значения', fontsize=14)\n",
        "    plt.xlabel('Вариант (рука)')\n",
        "    plt.ylabel('Конверсия')\n",
        "    plt.xticks(arms)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Оценка эффективности\n",
        "    optimal_reward = np.max(true_rates) * n_trials\n",
        "    print(\"\\nОценка эффективности (меньше сожаление - лучше):\")\n",
        "    for name, strategy in strategies.items():\n",
        "        regret = optimal_reward - strategy['cumulative_rewards'][-1]\n",
        "        final_error = np.mean(np.abs(strategy['values'] - true_rates))\n",
        "        print(f\"{name:20} | Сожаление: {regret:6.1f} | Ошибка оценки: {final_error:.4f}\")\n",
        "    \n",
        "    return strategies\n",
        "\n",
        "# Запуск сравнения\n",
        "true_rates = [0.10, 0.11, 0.095, 0.125]  # 4 варианта с разной эффективностью\n",
        "strategies = compare_bandit_strategies(true_rates, n_trials=10000)\n",
        "\n",
        "# === 3.3. Байесовский A/B тест ===\n",
        "class BayesianABTest:\n",
        "    \"\"\"\n",
        "    Байесовский анализ A/B-теста для бинарных метрик (конверсия).\n",
        "    Использует Beta-биномиальную модель.\n",
        "    \"\"\"\n",
        "    def __init__(self, prior_alpha=1, prior_beta=1):\n",
        "        \"\"\"\n",
        "        Инициализация с априорным распределением Beta(alpha, beta).\n",
        "        Априори Beta(1,1) - это равномерное распределение на [0,1].\n",
        "        \"\"\"\n",
        "        self.prior_alpha = prior_alpha\n",
        "        self.prior_beta = prior_beta\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Сброс апостериорных параметров.\"\"\"\n",
        "        self.control_alpha = self.prior_alpha\n",
        "        self.control_beta = self.prior_beta\n",
        "        self.treatment_alpha = self.prior_alpha\n",
        "        self.treatment_beta = self.prior_beta\n",
        "    \n",
        "    def update(self, control_success, control_failures, treatment_success, treatment_failures):\n",
        "        \"\"\"\n",
        "        Обновление апостериорных распределений новыми данными.\n",
        "        \"\"\"\n",
        "        self.control_alpha += control_success\n",
        "        self.control_beta += control_failures\n",
        "        self.treatment_alpha += treatment_success\n",
        "        self.treatment_beta += treatment_failures\n",
        "    \n",
        "    def probability_better(self, n_samples=100000):\n",
        "        \"\"\"\n",
        "        Оценка вероятности того, что treatment лучше control.\n",
        "        \n",
        "        :param n_samples: Количество сэмплов для Монте-Карло\n",
        "        :return: Вероятность P(treatment > control)\n",
        "        \"\"\"\n",
        "        control_samples = stats.beta.rvs(self.control_alpha, self.control_beta, size=n_samples)\n",
        "        treatment_samples = stats.beta.rvs(self.treatment_alpha, self.treatment_beta, size=n_samples)\n",
        "        return np.mean(treatment_samples > control_samples)\n",
        "    \n",
        "    def expected_loss(self, n_samples=100000):\n",
        "        \"\"\"\n",
        "        Оценка ожидаемых потерь при выборе каждого варианта.\n",
        "        Потери = максимум(0, разница в конверсии в пользу другого варианта).\n",
        "        \n",
        "        :param n_samples: Количество сэмплов для Монте-Карло\n",
        "        :return: (loss_treatment, loss_control)\n",
        "        \"\"\"\n",
        "        control_samples = stats.beta.rvs(self.control_alpha, self.control_beta, size=n_samples)\n",
        "        treatment_samples = stats.beta.rvs(self.treatment_alpha, self.treatment_beta, size=n_samples)\n",
        "        \n",
        "        # Потери при выборе treatment: сколько мы теряем, если control на самом деле лучше\n",
        "        loss_treatment = np.mean(np.maximum(control_samples - treatment_samples, 0))\n",
        "        # Потери при выборе control: сколько мы теряем, если treatment на самом деле лучше\n",
        "        loss_control = np.mean(np.maximum(treatment_samples - control_samples, 0))\n",
        "        \n",
        "        return loss_treatment, loss_control\n",
        "    \n",
        "    def plot_posteriors(self):\n",
        "        \"\"\"Визуализация апостериорных распределений конверсии.\"\"\"\n",
        "        x = np.linspace(0, 1, 1000)\n",
        "        control_pdf = stats.beta.pdf(x, self.control_alpha, self.control_beta)\n",
        "        treatment_pdf = stats.beta.pdf(x, self.treatment_alpha, self.treatment_beta)\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(x, control_pdf, label='Control', linewidth=2, color='blue')\n",
        "        plt.plot(x, treatment_pdf, label='Treatment', linewidth=2, color='red')\n",
        "        plt.fill_between(x, control_pdf, alpha=0.2, color='blue')\n",
        "        plt.fill_between(x, treatment_pdf, alpha=0.2, color='red')\n",
        "        plt.title('Апостериорные распределения конверсии', fontsize=14)\n",
        "        plt.xlabel('Конверсия')\n",
        "        plt.ylabel('Плотность вероятности')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "# Пример байесовского A/B теста\n",
        "print(\"\\n=== 3.3. БАЙЕСОВСКИЙ A/B ТЕСТ ===\")\n",
        "bayesian_test = BayesianABTest(prior_alpha=1, prior_beta=1)\n",
        "\n",
        "# Симуляция поступления данных порциями (как в реальном эксперименте)\n",
        "np.random.seed(42)\n",
        "control_total = np.random.binomial(1, 0.105, 7000)  # Контроль: 10.5% конверсия\n",
        "treatment_total = np.random.binomial(1, 0.118, 7000)  # Тест: 11.8% конверсия (~12.4% улучшение)\n",
        "\n",
        "batch_size = 500\n",
        "print(\"Постепенное обновление апостериорных распределений:\")\n",
        "for i in range(0, len(control_total), batch_size):\n",
        "    end_idx = min(i + batch_size, len(control_total))\n",
        "    control_batch = control_total[i:end_idx]\n",
        "    treatment_batch = treatment_total[i:end_idx]\n",
        "    \n",
        "    bayesian_test.update(\n",
        "        control_success=control_batch.sum(),\n",
        "        control_failures=len(control_batch) - control_batch.sum(),\n",
        "        treatment_success=treatment_batch.sum(),\n",
        "        treatment_failures=len(treatment_batch) - treatment_batch.sum()\n",
        "    )\n",
        "    \n",
        "    if (i // batch_size) % 3 == 0:  # Выводим каждую 3-ю итерацию\n",
        "        prob_better = bayesian_test.probability_better()\n",
        "        loss_t, loss_c = bayesian_test.expected_loss()\n",
        "        cum_control = sum(control_total[:end_idx])\n",
        "        cum_treatment = sum(treatment_total[:end_idx])\n",
        "        print(f\"После {end_idx} наблюдений в каждой группе:\")\n",
        "        print(f\"  Конверсия Control: {cum_control/end_idx:.3%} | Treatment: {cum_treatment/end_idx:.3%}\")\n",
        "        print(f\"  P(Treatment > Control): {prob_better:.3f}\")\n",
        "        print(f\"  Ожидаемые потери: Treatment={loss_t:.5f}, Control={loss_c:.5f}\")\n",
        "        print(\"---\")\n",
        "\n",
        "# Финальные результаты и визуализация\n",
        "final_prob = bayesian_test.probability_better()\n",
        "final_loss_t, final_loss_c = bayesian_test.expected_loss()\n",
        "print(f\"\\nФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ:\")\n",
        "print(f\"Вероятность что Treatment лучше: {final_prob:.3%}\")\n",
        "print(f\"Ожидаемые потери при выборе Treatment: {final_loss_t:.5f}\")\n",
        "print(f\"Ожидаемые потери при выборе Control: {final_loss_c:.5f}\")\n",
        "\n",
        "# Визуализация\n",
        "bayesian_test.plot_posteriors()\n",
        "\n",
        "if final_prob > 0.95 and final_loss_t < 0.001:\n",
        "    print(\"\\nРекомендация: Внедрить Treatment. Достигнута высокая уверенность и низкие потери.\")\n",
        "elif final_loss_t < final_loss_c:\n",
        "    print(\"\\nРекомендация: Внедрить Treatment. Ожидаемые потери ниже.\")\n",
        "else:\n",
        "    print(\"\\nРекомендация: Остаться на Control или продолжить эксперимент.\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример демонстрирует мощь адаптивных и байесовских методов. MAB позволяет значительно сократить потери во время эксперимента, динамически направляя больше трафика к лучшим вариантам. Байесовский подход, в свою очередь, предоставляет богатую, интерпретируемую информацию для принятия решений, выходящую далеко за рамки простого p-значения. Оба метода особенно ценны в условиях ограниченного времени или высокой стоимости субоптимальных решений.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Основы причинного вывода\n",
        "\n",
        "### Теория: Rubin Causal Model и потенциальные исходы\n",
        "\n",
        "Формальный математический фундамент для причинного вывода был заложен Дональдом Рубином и носит название **Rubin Causal Model **(RCM), или **Model Potenциальных Исходов **(Potential Outcomes Framework).\n",
        "\n",
        "Центральной концепцией RCM является **потенциальный исход **(potential outcome). Для каждого объекта \\(i\\) (пользователя, пациента) и каждого возможного воздействия \\(t \\in \\{0, 1\\}\\) (0 — контроль, 1 — лечение) определяется потенциальный исход \\(Y_i(t)\\) — то, что произошло бы с объектом \\(i\\), если бы он получил воздействие \\(t\\).\n",
        "\n",
        "**Индивидуальный причинный эффект **(Individual Treatment Effect, ITE) для объекта \\(i\\) определяется как разница между двумя потенциальными исходами:\n",
        "\\[\n",
        "\\tau_i = Y_i(1) - Y_i(0)\n",
        "\\]\n",
        "\n",
        "Здесь возникает **фундаментальная проблема причинного вывода **(Fundamental Problem of Causal Inference): для каждого объекта мы можем наблюдать **только один** из двух потенциальных исходов. Если объект получил лечение (\\(W_i = 1\\)), мы наблюдаем \\(Y_i^{obs} = Y_i(1)\\), а \\(Y_i(0)\\) остаётся **контрфактическим **(counterfactual) — ненаблюдаемым. И наоборот, если объект в контроле (\\(W_i = 0\\)), мы наблюдаем \\(Y_i^{obs} = Y_i(0)\\), а \\(Y_i(1)\\) — контрфактический.\n",
        "\n",
        "Поскольку ITE ненаблюдаем для отдельных объектов, мы переходим к более агрегированной мере — **среднему причинному эффекту **(Average Treatment Effect, ATE):\n",
        "\\[\n",
        "ATE = \\mathbb{E}[\\tau_i] = \\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)]\n",
        "\\]\n",
        "\n",
        "Чтобы из наблюдаемых данных оценить ATE, необходимо сделать ряд **ключевых предположений**:\n",
        "\n",
        "1.  **Независимость **(Ignorability): Потенциальные исходы независимы от назначения лечения при условии ковариат \\(X\\): \\((Y(1), Y(0)) \\perp W | X\\). Это означает, что все факторы, влияющие и на выбор лечения, и на исход, измерены и включены в \\(X\\).\n",
        "2.  **Положительность **(Positivity): Для любого значения ковариат \\(X\\) вероятность получения лечения строго положительна и меньше единицы: \\(0 < P(W=1|X) < 1\\). Это гарантирует, что для каждого типа объектов есть и контрольные, и экспериментальные наблюдения.\n",
        "3.  **SUTVA **(Stable Unit Treatment Value Assumption): Эффект лечения на один объект не зависит от лечения других объектов (отсутствие интерференции), и все варианты лечения однозначно определены.\n",
        "\n",
        "В рандомизированных экспериментах (A/B-тестах) первое предположение (Ignorability) выполняется автоматически благодаря случайному назначению, что делает A/B-тесты «золотым стандартом» для причинного вывода.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример визуализирует фундаментальную проблему причинного вывода и демонстрирует, как проверять ключевые предположения в наблюдательных исследованиях.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n=== 4. ОСНОВЫ ПРИЧИННОГО ВЫВОДА ===\")\n",
        "\n",
        "# === 4.1. Визуализация потенциальных исходов ===\n",
        "class PotentialOutcomesFramework:\n",
        "    \"\"\"\n",
        "    Симулятор для демонстрации Rubin Causal Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_units, true_ate=0.0, heterogeneity_sd=0.0):\n",
        "        \"\"\"\n",
        "        Инициализация симулятора.\n",
        "        \n",
        "        :param n_units: Количество объектов (пользователей)\n",
        "        :param true_ate: Истинный средний причинный эффект\n",
        "        :param heterogeneity_sd: Стандартное отклонение индивидуальных эффектов\n",
        "        \"\"\"\n",
        "        self.n_units = n_units\n",
        "        self.true_ate = true_ate\n",
        "        self.heterogeneity_sd = heterogeneity_sd\n",
        "        self.generate_potential_outcomes()\n",
        "    \n",
        "    def generate_potential_outcomes(self):\n",
        "        \"\"\"Генерация потенциальных исходов для всех объектов.\"\"\"\n",
        "        # Генерация базового исхода (без лечения)\n",
        "        self.Y0 = np.random.normal(loc=100, scale=15, size=self.n_units)\n",
        "        \n",
        "        # Генерация индивидуальных эффектов лечения\n",
        "        individual_effects = np.random.normal(\n",
        "            loc=self.true_ate, scale=self.heterogeneity_sd, size=self.n_units\n",
        "        )\n",
        "        \n",
        "        # Потенциальный исход при наличии лечения\n",
        "        self.Y1 = self.Y0 + individual_effects\n",
        "        \n",
        "        # Истинные индивидуальные эффекты\n",
        "        self.true_individual_effects = self.Y1 - self.Y0\n",
        "    \n",
        "    def random_assignment(self, treatment_prob=0.5):\n",
        "        \"\"\"Случайное назначение лечения (имитация A/B-теста).\"\"\"\n",
        "        self.treatment = np.random.binomial(1, treatment_prob, self.n_units)\n",
        "        # Наблюдаемый исход - это только один из потенциальных\n",
        "        self.observed_outcomes = np.where(self.treatment == 1, self.Y1, self.Y0)\n",
        "    \n",
        "    def calculate_naive_ate(self):\n",
        "        \"\"\"Наивная оценка ATE как разность средних наблюдаемых исходов.\"\"\"\n",
        "        treatment_mean = self.observed_outcomes[self.treatment == 1].mean()\n",
        "        control_mean = self.observed_outcomes[self.treatment == 0].mean()\n",
        "        return treatment_mean - control_mean\n",
        "    \n",
        "    def calculate_true_ate(self):\n",
        "        \"\"\"Истинный ATE (доступен только в симуляции).\"\"\"\n",
        "        return self.true_individual_effects.mean()\n",
        "    \n",
        "    def plot_potential_outcomes(self, n_show=50):\n",
        "        \"\"\"Визуализация потенциальных исходов для подвыборки.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Случайная подвыборка для визуализации\n",
        "        indices = np.random.choice(self.n_units, n_show, replace=False)\n",
        "        \n",
        "        # Потенциальные исходы для каждого объекта\n",
        "        for i in indices:\n",
        "            y0, y1 = self.Y0[i], self.Y1[i]\n",
        "            ax1.plot([0, 1], [y0, y1], 'o-', alpha=0.7, color='gray')\n",
        "            # Отметим наблюдаемый исход\n",
        "            if self.treatment[i] == 1:\n",
        "                ax1.plot(1, y1, 'ro', markersize=8)\n",
        "            else:\n",
        "                ax1.plot(0, y0, 'bo', markersize=8)\n",
        "        \n",
        "        ax1.set_xlim(-0.1, 1.1)\n",
        "        ax1.set_xticks([0, 1])\n",
        "        ax1.set_xticklabels(['Контроль (Y0)', 'Лечение (Y1)'])\n",
        "        ax1.set_ylabel('Исход')\n",
        "        ax1.set_title('Потенциальные исходы для отдельных объектов\\n(Красный=наблюдаемый в лечении, Синий=наблюдаемый в контроле)')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Распределение индивидуальных эффектов\n",
        "        ax2.hist(self.true_individual_effects, bins=30, alpha=0.7,\n",
        "                density=True, color='lightblue', edgecolor='black')\n",
        "        ax2.axvline(self.true_individual_effects.mean(),\n",
        "                   color='red', linestyle='--', linewidth=2,\n",
        "                   label=f'Истинный ATE = {self.true_individual_effects.mean():.2f}')\n",
        "        ax2.set_xlabel('Индивидуальный причинный эффект (ITE)')\n",
        "        ax2.set_ylabel('Плотность')\n",
        "        ax2.set_title('Распределение индивидуальных причинных эффектов')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Демонстрация фундаментальной проблемы\n",
        "def demonstrate_fundamental_problem():\n",
        "    \"\"\"Показывает разницу между истинным и наивным ATE.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    framework = PotentialOutcomesFramework(\n",
        "        n_units=10000,\n",
        "        true_ate=8.5,\n",
        "        heterogeneity_sd=3.0  # Значительная гетерогенность эффектов\n",
        "    )\n",
        "    framework.random_assignment()  # Проводим A/B-тест\n",
        "    \n",
        "    true_ate = framework.calculate_true_ate()\n",
        "    naive_ate = framework.calculate_naive_ate()\n",
        "    \n",
        "    print(\"=== ФУНДАМЕНТАЛЬНАЯ ПРОБЛЕМА ПРИЧИННОГО ВЫВОДА ===\")\n",
        "    print(f\"Истинный ATE (известен только в симуляции): {true_ate:.3f}\")\n",
        "    print(f\"Наивная оценка ATE (из A/B-теста):        {naive_ate:.3f}\")\n",
        "    print(f\"Абсолютная ошибка оценки:                  {abs(naive_ate - true_ate):.3f}\")\n",
        "    print(f\"Относительная ошибка оценки:               {abs(naive_ate - true_ate) / abs(true_ate) * 100:.2f}%\")\n",
        "    \n",
        "    # Визуализация\n",
        "    framework.plot_potential_outcomes(n_show=100)\n",
        "    \n",
        "    return framework\n",
        "\n",
        "# Запуск демонстрации\n",
        "framework = demonstrate_fundamental_problem()\n",
        "\n",
        "# === 4.2. Проверка ключевых предположений ===\n",
        "def check_causal_assumptions(data, treatment_col, outcome_col, covariate_cols):\n",
        "    \"\"\"\n",
        "    Проверка предположений причинного вывода в наблюдательных данных.\n",
        "    \n",
        "    :param  DataFrame с данными\n",
        "    :param treatment_col: Название колонки с флагом лечения\n",
        "    :param outcome_col: Название колонки с исходом\n",
        "    :param covariate_cols: Список названий колонок с ковариатами\n",
        "    :return: Словарь с результатами проверки\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ПРОВЕРКА КЛЮЧЕВЫХ ПРЕДПОЛОЖЕНИЙ ===\")\n",
        "    results = {}\n",
        "    \n",
        "    # 1. Проверка Ignorability через баланс ковариат\n",
        "    print(\"1. Проверка баланса ковариат (Ignorability):\")\n",
        "    balance_report = []\n",
        "    for cov in covariate_cols:\n",
        "        control_mean = data[data[treatment_col] == 0][cov].mean()\n",
        "        treatment_mean = data[data[treatment_col] == 1][cov].mean()\n",
        "        pooled_std = data[cov].std()\n",
        "        std_diff = (treatment_mean - control_mean) / pooled_std\n",
        "        \n",
        "        is_balanced = abs(std_diff) < 0.1  # правило: |SMD| < 0.1 - хорошо сбалансировано\n",
        "        balance_report.append({\n",
        "            'Covariate': cov,\n",
        "            'Control_Mean': control_mean,\n",
        "            'Treatment_Mean': treatment_mean,\n",
        "            'Std_Diff': std_diff,\n",
        "            'Balanced': \"Да\" if is_balanced else \"Нет\"\n",
        "        })\n",
        "        print(f\"  {cov}: Control={control_mean:.3f}, Treatment={treatment_mean:.3f}, StdDiff={std_diff:.3f} ({'Balanced' if is_balanced else 'Imbalanced'})\")\n",
        "    \n",
        "    results['covariate_balance'] = pd.DataFrame(balance_report)\n",
        "    \n",
        "    # 2. Проверка Positivity через оценку склонностей (propensity scores)\n",
        "    print(\"\\n2. Проверка положительности (Positivity):\")\n",
        "    # Обучение модели для предсказания вероятности лечения (склонности)\n",
        "    propensity_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    propensity_model.fit(data[covariate_cols], data[treatment_col])\n",
        "    propensity_scores = propensity_model.predict(data[covariate_cols])\n",
        "    \n",
        "    min_ps, max_ps = propensity_scores.min(), propensity_scores.max()\n",
        "    violation = (min_ps < 0.01) or (max_ps > 0.99)\n",
        "    \n",
        "    print(f\"  Мин. склонность: {min_ps:.4f}\")\n",
        "    print(f\"  Макс. склонность: {max_ps:.4f}\")\n",
        "    print(f\"  Нарушение положительности: {'Да' if violation else 'Нет'}\")\n",
        "    \n",
        "    results['positivity'] = {\n",
        "        'min_propensity': min_ps,\n",
        "        'max_propensity': max_ps,\n",
        "        'violation': violation\n",
        "    }\n",
        "    \n",
        "    # Визуализация распределения склонностей\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(propensity_scores[data[treatment_col] == 0], bins=50, alpha=0.7, label='Control', color='blue')\n",
        "    plt.hist(propensity_scores[data[treatment_col] == 1], bins=50, alpha=0.7, label='Treatment', color='red')\n",
        "    plt.axvline(0.01, color='black', linestyle='--', label='Пороги')\n",
        "    plt.axvline(0.99, color='black', linestyle='--')\n",
        "    plt.xlabel('Склонность (Propensity Score)')\n",
        "    plt.ylabel('Частота')\n",
        "    plt.title('Распределение склонностей в Control и Treatment группах')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    # 3. SUTVA\n",
        "    print(\"\\n3. SUTVA (Stable Unit Treatment Value Assumption):\")\n",
        "    print(\"   SUTVA предполагает отсутствие интерференции между объектами.\")\n",
        "    print(\"   Проверка этого предположения требует предметного знания домена и структуры данных.\")\n",
        "    results['sutva'] = \"Требует предметной экспертизы\"\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Создание синтетических наблюдательных данных\n",
        "np.random.seed(42)\n",
        "n_obs = 5000\n",
        "obs_data = pd.DataFrame({\n",
        "    'age': np.random.normal(40, 10, n_obs),\n",
        "    'income': np.random.normal(50000, 15000, n_obs),\n",
        "    'has_kids': np.random.binomial(1, 0.4, n_obs)\n",
        "})\n",
        "\n",
        "# Назначение лечения зависит от ковариат (нарушение рандомизации)\n",
        "prob_treatment = 1 / (1 + np.exp(-(obs_data['age'] - 35)/10 + (obs_data['income'] - 50000)/20000))\n",
        "obs_data['treatment'] = np.random.binomial(1, prob_treatment)\n",
        "\n",
        "# Исход зависит и от лечения, и от ковариат\n",
        "obs_data['outcome'] = (\n",
        "    100 +\n",
        "    0.5 * obs_data['age'] +\n",
        "    0.001 * obs_data['income'] +\n",
        "    10 * obs_data['has_kids'] +\n",
        "    12 * obs_data['treatment'] +  # Истинный причинный эффект = 12\n",
        "    np.random.normal(0, 10, n_obs)\n",
        ")\n",
        "\n",
        "# Проверка предположений\n",
        "assumptions = check_causal_assumptions(\n",
        "    obs_data, 'treatment', 'outcome', ['age', 'income', 'has_kids']\n",
        ")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот раздел закладывает теоретический фундамент для всего причинного вывода. Визуализация потенциальных исходов наглядно демонстрирует фундаментальную проблему: мы никогда не видим, что было бы с пользователем, если бы ему показали альтернативный вариант лендинга. Проверка предположений Ignorability и Positivity — это критически важный шаг в наблюдательных исследованиях (например, при использовании исторических данных), где рандомизация отсутствует. Понимание этих концепций позволяет специалисту по данным не только проводить A/B-тесты, но и критически оценивать причинные утверждения, основанные на данных, собранных без эксперимента.\n",
        "\n"
      ],
      "metadata": {
        "id": "_bNmNBGeYzP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 5. Библиотека DoWhy: структурированный причинный анализ\n",
        "\n",
        "### Теория: Четырехэтапный framework причинного вывода\n",
        "\n",
        "Существование множества методов причинного вывода (от простой регрессии до сложных методов машинного обучения) создаёт проблему выбора и проверки корректности. Библиотека **DoWhy**, разработанная исследователями Microsoft Research, решает эту проблему, предложив **унифицированный четырёхэтапный framework**, который разделяет логику причинного вывода на чёткие, независимые шаги. Этот подход, вдохновлённый работами Джуды Перла, гарантирует прозрачность и воспроизводимость анализа.\n",
        "\n",
        "Четыре этапа DoWhy:\n",
        "\n",
        "1.  **Моделирование **(Modeling): На этом этапе аналитик формализует свои предположения о причинных отношениях в данных через **ориентированный ациклический граф **(Directed Acyclic Graph, DAG). DAG визуально кодирует, какие переменные являются причинами, какие — следствиями, а какие — общими причинами (конфаундерами).\n",
        "2.  **Идентификация **(Identification): DoWhy использует алгоритмы теории графов (например, Backdoor Criterion, Frontdoor Criterion) для того, чтобы определить, **можно ли вообще** выразить целевой причинный эффект (ATE, CATE) через наблюдаемые данные. Если идентификация невозможна (например, из-за ненаблюдаемого конфаундера), библиотека это явно сообщает, предотвращая попытки ошибочных оценок.\n",
        "3.  **Оценка **(Estimation): После успешной идентификации DoWhy предоставляет единый интерфейс для применения множества методов оценки: от классических (Propensity Score Matching) до современных (Double Machine Learning, Causal Forest). Это позволяет легко сравнивать методы и выбирать наиболее подходящий.\n",
        "4.  **Опровержение **(Refutation): Финальный и критически важный этап. DoWhy автоматически проводит серию тестов на **робастность** полученной оценки. Эти тесты проверяют, насколько результаты устойчивы к нарушению ключевых предположений (например, к добавлению случайного конфаундера или к ошибкам в данных). Это превращает причинный вывод из «чёрного ящика» в проверяемую научную процедуру.\n",
        "\n",
        "Этот framework не просто упрощает анализ, но и **заставляет аналитика мыслить причинно**, формализуя свои предположения и проверяя их последствия.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует полный цикл причинного анализа с использованием DoWhy на синтетических данных, имитирующих реальный наблюдательный сценарий с конфаундерами.\n",
        "\n",
        "```python\n",
        "import dowhy\n",
        "from dowhy import CausalModel\n",
        "import econml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"=== 5. БИБЛИОТЕКА DOWHY: СТРУКТУРИРОВАННЫЙ ПРИЧИННЫЙ АНАЛИЗ ===\")\n",
        "\n",
        "# === 5.1. Генерация реалистичных синтетических данных ===\n",
        "def generate_observational_data(n_samples=10000, true_ate=8.0):\n",
        "    \"\"\"\n",
        "    Генерация данных с известным причинным эффектом и смещением отбора.\n",
        "    \n",
        "    Сценарий: Компания тестирует новую функцию (treatment) в своём приложении.\n",
        "    - Конфаундеры: возраст, доход, предыдущая вовлечённость.\n",
        "    - Ненаблюдаемый конфаундер: скрытое качество продукта для пользователя.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Наблюдаемые конфаундеры\n",
        "    age = np.random.normal(45, 15, n_samples)  # Возраст\n",
        "    income = np.random.lognormal(10, 0.5, n_samples)  # Доход\n",
        "    prior_engagement = np.random.exponential(2, n_samples)  # Вовлечённость\n",
        "    \n",
        "    # Склонность к получению лечения зависит от конфаундеров\n",
        "    logit_propensity = 0.1 * age + 0.01 * (income - 50) + 0.5 * prior_engagement - 5\n",
        "    propensity = 1 / (1 + np.exp(-logit_propensity))\n",
        "    treatment = np.random.binomial(1, propensity)\n",
        "    \n",
        "    # Ненаблюдаемый конфаундер (в реальности его нет в данных)\n",
        "    unobserved_quality = np.random.normal(0, 1, n_samples)\n",
        "    \n",
        "    # Исход (например, выручка) зависит от лечения и всех факторов\n",
        "    noise = np.random.normal(0, 10, n_samples)\n",
        "    outcome = (\n",
        "        true_ate * treatment +              # Истинный причинный эффект\n",
        "        2 * age +                           # Влияние возраста\n",
        "        0.001 * income +                    # Влияние дохода\n",
        "        3 * prior_engagement +              # Влияние вовлечённости\n",
        "        2 * unobserved_quality +            # Влияние скрытого качества\n",
        "        noise\n",
        "    )\n",
        "    \n",
        "    data = pd.DataFrame({\n",
        "        'treatment': treatment,\n",
        "        'outcome': outcome,\n",
        "        'age': age,\n",
        "        'income': income,\n",
        "        'prior_engagement': prior_engagement,\n",
        "        'unobserved_quality': unobserved_quality  # В реальном анализе эта колонка отсутствует\n",
        "    })\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Генерация данных\n",
        "data = generate_observational_data(n_samples=10000, true_ate=8.0)\n",
        "print(\"Первые 5 строк данных (в реальности 'unobserved_quality' недоступна):\")\n",
        "print(data.drop('unobserved_quality', axis=1).head())\n",
        "\n",
        "# === 5.2. Этап 1: Моделирование (Создание причинного графа) ===\n",
        "print(\"\\n5.2. Этап 1: Моделирование\")\n",
        "\n",
        "# Описание причинных предположений в формате DOT (Graphviz)\n",
        "causal_graph = \"\"\"\n",
        "digraph {\n",
        "    // Конфаундеры влияют и на лечение, и на исход\n",
        "    age -> treatment;\n",
        "    age -> outcome;\n",
        "    income -> treatment;\n",
        "    income -> outcome;\n",
        "    prior_engagement -> treatment;\n",
        "    prior_engagement -> outcome;\n",
        "    \n",
        "    // Ненаблюдаемый конфаундер (в реальности мы знаем о его существовании,\n",
        "    // но не можем включить его в анализ)\n",
        "    U [label=\"unobserved_quality\", style=\"dotted\"];\n",
        "    U -> treatment;\n",
        "    U -> outcome;\n",
        "    \n",
        "    // Лечение влияет на исход\n",
        "    treatment -> outcome;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Создание причинной модели\n",
        "# Важно: в реальных данных колонка 'unobserved_quality' отсутствует\n",
        "model = CausalModel(\n",
        "    data=data.drop('unobserved_quality', axis=1),  # Используем только наблюдаемые данные\n",
        "    treatment='treatment',\n",
        "    outcome='outcome',\n",
        "    graph=causal_graph,\n",
        "    common_causes=['age', 'income', 'prior_engagement']  # Явное указание конфаундеров\n",
        ")\n",
        "\n",
        "# Визуализация графа (в ноутбуке)\n",
        "try:\n",
        "    model.view_model()\n",
        "    display(Image(filename=\"causal_model.png\"))\n",
        "except Exception as e:\n",
        "    print(f\"Невозможно отобразить граф: {e}\")\n",
        "    print(\"Причинный граф успешно создан и используется для анализа.\")\n",
        "\n",
        "print(\"Причинная модель создана. Она инкапсулирует все наши предположения о данных.\")\n",
        "\n",
        "# === 5.3. Этап 2: Идентификация ===\n",
        "print(\"\\n5.3. Этап 2: Идентификация\")\n",
        "\n",
        "# DoWhy пытается определить, можно ли оценить ATE из имеющихся данных\n",
        "# Параметр proceed_when_unidentifiable=True позволяет продолжить, даже если идентификация не полная\n",
        "identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
        "\n",
        "print(\"Результаты идентификации:\")\n",
        "print(identified_estimand)\n",
        "print(\"\\nИнтерпретация:\")\n",
        "print(\"DoWhy определил, что для оценки ATE необходимо скорректировать на переменные:\")\n",
        "print(\"['age', 'income', 'prior_engagement']\")\n",
        "print(\"Это согласуется с нашим причинным графом (Backdoor Path Criterion).\")\n",
        "\n",
        "# === 5.4. Этап 3: Оценка ===\n",
        "print(\"\\n5.4. Этап 3: Оценка\")\n",
        "\n",
        "# DoWhy поддерживает множество методов оценки через единый интерфейс\n",
        "estimation_methods = {\n",
        "    \"Propensity Score Matching\": \"backdoor.propensity_score_matching\",\n",
        "    \"Propensity Score Stratification\": \"backdoor.propensity_score_stratification\",\n",
        "    \"Linear Regression\": \"backdoor.linear_regression\",\n",
        "    \"Doubly Robust Learning\": \"backdoor.econml.dr.LinearDRLearner\"\n",
        "}\n",
        "\n",
        "estimates = {}\n",
        "for name, method in estimation_methods.items():\n",
        "    if \"Doubly Robust\" in name:\n",
        "        # Для сложных методов EconML нужны дополнительные параметры\n",
        "        estimate = model.estimate_effect(\n",
        "            identified_estimand,\n",
        "            method_name=method,\n",
        "            target_units=\"ate\",\n",
        "            method_params={\n",
        "                \"init_params\": {\n",
        "                    'model_regression': econml.sklearn_extensions.linear_model.WeightedLassoCVWrapper(),\n",
        "                    'model_propensity': econml.sklearn_extensions.linear_model.WeightedLassoCVWrapper()\n",
        "                },\n",
        "                \"fit_params\": {}\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        estimate = model.estimate_effect(\n",
        "            identified_estimand,\n",
        "            method_name=method,\n",
        "            target_units=\"ate\"\n",
        "        )\n",
        "    estimates[name] = estimate.value\n",
        "    print(f\"{name}: ATE = {estimate.value:.3f} (95% CI: [{estimate.conf_int[0]:.3f}, {estimate.conf_int[1]:.3f}])\")\n",
        "\n",
        "# === 5.5. Этап 4: Опровержение ===\n",
        "print(\"\\n5.5. Этап 4: Опровержение\")\n",
        "\n",
        "# Выбираем наиболее надёжный метод для опровержения (Doubly Robust)\n",
        "best_estimate = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.econml.dr.LinearDRLearner\",\n",
        "    target_units=\"ate\",\n",
        "    method_params={\n",
        "        \"init_params\": {\n",
        "            'model_regression': econml.sklearn_extensions.linear_model.WeightedLassoCVWrapper(),\n",
        "            'model_propensity': econml.sklearn_extensions.linear_model.WeightedLassoCVWrapper()\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "refutation_tests = {}\n",
        "# Тест 1: Добавление случайного конфаундера\n",
        "refutation_tests['random_common_cause'] = model.refute_estimate(\n",
        "    identified_estimand, best_estimate,\n",
        "    method_name=\"random_common_cause\"\n",
        ")\n",
        "\n",
        "# Тест 2: Placebo treatment (перемешивание меток лечения)\n",
        "refutation_tests['placebo_treatment'] = model.refute_estimate(\n",
        "    identified_estimand, best_estimate,\n",
        "    method_name=\"placebo_treatment_refuter\",\n",
        "    placebo_type=\"permute\"\n",
        ")\n",
        "\n",
        "# Тест 3: Bootstrap для оценки стабильности\n",
        "refutation_tests['bootstrap'] = model.refute_estimate(\n",
        "    identified_estimand, best_estimate,\n",
        "    method_name=\"bootstrap_refuter\",\n",
        "    n_simulations=100\n",
        ")\n",
        "\n",
        "print(\"Результаты опровержения:\")\n",
        "for test_name, result in refutation_tests.items():\n",
        "    print(f\"{test_name}: {result}\")\n",
        "\n",
        "# === 5.6. Расширенный анализ: Гетерогенные эффекты ===\n",
        "print(\"\\n5.6. Расширенный анализ: Оценка гетерогенных эффектов (CATE)\")\n",
        "\n",
        "# Используем Causal Forest для оценки индивидуальных эффектов\n",
        "cate_estimate = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.econml.causal_forest.CausalForest\",\n",
        "    target_units=\"ate\",\n",
        "    method_params={\n",
        "        \"init_params\": {\n",
        "            'n_estimators': 200,\n",
        "            'min_samples_leaf': 10,\n",
        "            'max_depth': 10,\n",
        "            'bootstrap': True\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Получение предсказаний CATE\n",
        "X = data[['age', 'income', 'prior_engagement']]\n",
        "cate_predictions = cate_estimate.estimator.effect(X)\n",
        "\n",
        "# Визуализация результатов\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Распределение CATE\n",
        "axes[0, 0].hist(cate_predictions, bins=30, alpha=0.7, color='lightblue', density=True)\n",
        "axes[0, 0].axvline(cate_predictions.mean(), color='red', linestyle='--', linewidth=2,\n",
        "                  label=f'Средний CATE = {cate_predictions.mean():.2f}')\n",
        "axes[0, 0].set_xlabel('Индивидуальный причинный эффект (CATE)')\n",
        "axes[0, 0].set_ylabel('Плотность')\n",
        "axes[0, 0].set_title('Распределение индивидуальных эффектов')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# CATE vs Возраст\n",
        "axes[0, 1].scatter(data['age'], cate_predictions, alpha=0.5)\n",
        "axes[0, 1].set_xlabel('Возраст')\n",
        "axes[0, 1].set_ylabel('CATE')\n",
        "axes[0, 1].set_title('Зависимость эффекта от возраста')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# CATE vs Доход\n",
        "axes[1, 0].scatter(data['income'], cate_predictions, alpha=0.5, color='green')\n",
        "axes[1, 0].set_xlabel('Доход')\n",
        "axes[1, 0].set_ylabel('CATE')\n",
        "axes[1, 0].set_title('Зависимость эффекта от дохода')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Сравнение методов оценки ATE\n",
        "methods = list(estimates.keys())\n",
        "ate_values = list(estimates.values())\n",
        "bars = axes[1, 1].bar(methods, ate_values, alpha=0.7, color='skyblue')\n",
        "axes[1, 1].axhline(y=8.0, color='red', linestyle='--', linewidth=2, label='Истинный ATE = 8.0')\n",
        "axes[1, 1].set_ylabel('Оценка ATE')\n",
        "axes[1, 1].set_title('Сравнение методов оценки ATE')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].set_xticklabels(methods, rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Добавление значений на столбцы\n",
        "for bar, value in zip(bars, ate_values):\n",
        "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                   f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Пример демонстрирует, как DoWhy структурирует сложный процесс причинного вывода. От формализации предположений через DAG до проверки робастности результата — каждый шаг логически обоснован и прозрачен. Особенно ценным является этап опровержения, который защищает аналитика от ложных выводов. Возможность легко сравнивать разные методы оценки позволяет выбрать наиболее надёжный подход для конкретной задачи.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Библиотека CausalML: машинное обучение для причинного вывода\n",
        "\n",
        "### Теория: Meta-learners и uplift modeling\n",
        "\n",
        "В то время как DoWhy фокусируется на структурной стороне причинного вывода, библиотека **CausalML** предоставляет мощный арсенал **алгоритмов машинного обучения**, специально разработанных для оценки причинных эффектов. Ключевой концепцией CausalML являются **meta-learners** — гибкие фреймворки, которые позволяют использовать практически любой алгоритм ML (например, Random Forest, Gradient Boosting) для решения задачи причинного вывода.\n",
        "\n",
        "Основные типы meta-learners:\n",
        "\n",
        "*   **S-Learner **(Single Learner): Объединяет все данные и рассматривает переменную лечения как один из признаков. Модель предсказывает исход на основе всех признаков, включая `treatment`. CATE оценивается как разница в предсказаниях при `treatment=1` и `treatment=0`. Прост, но может упустить взаимодействие между лечением и признаками.\n",
        "*   **T-Learner **(Two Learners): Обучает две отдельные модели: одну на данных контрольной группы (`treatment=0`), другую — на данных экспериментальной группы (`treatment=1`). CATE оценивается как разница между предсказаниями двух моделей. Более гибок, но может страдать при несбалансированных размерах групп.\n",
        "*   **X-Learner**: Улучшенная версия T-Learner, которая использует импутацию контрфактических исходов для улучшения оценки в группах с меньшим размером. Особенно эффективен для сильно несбалансированных данных.\n",
        "*   **R-Learner**: Реализует подход «Robust» через честную кросс-валидацию и ортогональное обучение. Он минимизирует специальную функцию потерь, которая делает оценку устойчивой к ошибкам в моделях склонностей и исходов. Часто демонстрирует наилучшую производительность.\n",
        "\n",
        "Помимо meta-learners, CausalML реализует и специализированные **uplift-модели**, такие как **Uplift Random Forest**, которые напрямую оптимизируют критерии, связанные с uplift (например, Qini coefficient).\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует применение CausalML для обучения различных meta-learners, uplift-моделей и их всесторонней оценки.\n",
        "\n",
        "```python\n",
        "from causalml.inference.meta import (\n",
        "    BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\n",
        ")\n",
        "from causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\n",
        "from causalml.metrics import plot_gain, plot_qini, auuc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n=== 6. БИБЛИОТЕКА CAUSALML: МАШИННОЕ ОБУЧЕНИЕ ДЛЯ ПРИЧИННОГО ВЫВОДА ===\")\n",
        "\n",
        "# === 6.1. Класс для комплексного анализа CausalML ===\n",
        "class ComprehensiveCausalMLAnalysis:\n",
        "    \"\"\"\n",
        "    Класс для проведения полного цикла анализа с использованием CausalML.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, treatment_col, outcome_col, feature_cols):\n",
        "        self.data = data\n",
        "        self.treatment_col = treatment_col\n",
        "        self.outcome_col = outcome_col\n",
        "        self.feature_cols = feature_cols\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.uplift_predictions = None\n",
        "    \n",
        "    def prepare_data(self, test_size=0.3):\n",
        "        \"\"\"Разделение данных на train/test.\"\"\"\n",
        "        X = self.data[self.feature_cols]\n",
        "        y = self.data[self.outcome_col]\n",
        "        treatment = self.data[self.treatment_col]\n",
        "        \n",
        "        (self.X_train, self.X_test,\n",
        "         self.y_train, self.y_test,\n",
        "         self.treatment_train, self.treatment_test) = train_test_split(\n",
        "            X, y, treatment, test_size=test_size, random_state=42\n",
        "        )\n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
        "    \n",
        "    def fit_meta_learners(self):\n",
        "        \"\"\"Обучение и оценка различных meta-learners.\"\"\"\n",
        "        base_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        \n",
        "        learners = {\n",
        "            'S-Learner': BaseSRegressor(base_model),\n",
        "            'T-Learner': BaseTRegressor(base_model),\n",
        "            'X-Learner': BaseXRegressor(base_model),\n",
        "            'R-Learner': BaseRRegressor(base_model)\n",
        "        }\n",
        "        \n",
        "        for name, learner in learners.items():\n",
        "            print(f\"Обучение {name}...\")\n",
        "            learner.fit(\n",
        "                X=self.X_train.values,\n",
        "                treatment=self.treatment_train.values,\n",
        "                y=self.y_train.values\n",
        "            )\n",
        "            ate, lb, ub = learner.estimate_ate(\n",
        "                X=self.X_test.values,\n",
        "                treatment=self.treatment_test.values,\n",
        "                y=self.y_test.values\n",
        "            )\n",
        "            self.models[name] = learner\n",
        "            self.results[name] = {\n",
        "                'ATE': ate[0],\n",
        "                'CI_Lower': lb[0],\n",
        "                'CI_Upper': ub[0]\n",
        "            }\n",
        "        \n",
        "        return self.results\n",
        "    \n",
        "    def fit_uplift_model(self):\n",
        "        \"\"\"Обучение Uplift Random Forest.\"\"\"\n",
        "        print(\"Обучение Uplift Random Forest...\")\n",
        "        uplift_model = UpliftRandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            min_samples_leaf=50,\n",
        "            control_name=0,\n",
        "            random_state=42\n",
        "        )\n",
        "        uplift_model.fit(\n",
        "            self.X_train.values,\n",
        "            self.treatment_train.values,\n",
        "            self.y_train.values\n",
        "        )\n",
        "        self.models['UpliftRF'] = uplift_model\n",
        "        self.uplift_predictions = uplift_model.predict(self.X_test.values)\n",
        "        return self.uplift_predictions\n",
        "    \n",
        "    def evaluate_uplift(self):\n",
        "        \"\"\"Оценка качества uplift-модели.\"\"\"\n",
        "        if self.uplift_predictions is None:\n",
        "            self.fit_uplift_model()\n",
        "        \n",
        "        # Расчет AUUC (Area Under Uplift Curve)\n",
        "        auuc = auuc_score(\n",
        "            self.y_test.values,\n",
        "            self.uplift_predictions,\n",
        "            self.treatment_test.values\n",
        "        )\n",
        "        \n",
        "        # Визуализация кривых\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        plot_qini(\n",
        "            self.y_test.values,\n",
        "            self.uplift_predictions,\n",
        "            self.treatment_test.values,\n",
        "            ax=ax1\n",
        "        )\n",
        "        ax1.set_title('Qini Curve')\n",
        "        \n",
        "        plot_gain(\n",
        "            self.y_test.values,\n",
        "            self.uplift_predictions,\n",
        "            self.treatment_test.values,\n",
        "            ax=ax2\n",
        "        )\n",
        "        ax2.set_title('Uplift Curve')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"AUUC Score: {auuc:.4f}\")\n",
        "        return auuc\n",
        "    \n",
        "    def compare_ate_estimates(self, true_ate=None):\n",
        "        \"\"\"Сравнение оценок ATE от всех методов.\"\"\"\n",
        "        methods = list(self.results.keys())\n",
        "        estimates = [self.results[m]['ATE'] for m in methods]\n",
        "        ci_lower = [self.results[m]['CI_Lower'] for m in methods]\n",
        "        ci_upper = [self.results[m]['CI_Upper'] for m in methods]\n",
        "        \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        y_pos = np.arange(len(methods))\n",
        "        plt.barh(y_pos, estimates, xerr=[np.array(estimates)-np.array(ci_lower),\n",
        "                                        np.array(ci_upper)-np.array(estimates)],\n",
        "                alpha=0.7, color='lightcoral', capsize=5)\n",
        "        if true_ate is not None:\n",
        "            plt.axvline(x=true_ate, color='green', linestyle='--', linewidth=2, label=f'Истинный ATE = {true_ate}')\n",
        "        plt.yticks(y_pos, methods)\n",
        "        plt.xlabel('Оценка ATE')\n",
        "        plt.title('Сравнение оценок ATE от различных Meta-Learners')\n",
        "        if true_ate is not None:\n",
        "            plt.legend()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Создание таблицы результатов\n",
        "        results_df = pd.DataFrame(self.results).T\n",
        "        results_df['Method'] = results_df.index\n",
        "        results_df = results_df[['Method', 'ATE', 'CI_Lower', 'CI_Upper']]\n",
        "        return results_df\n",
        "\n",
        "# === 6.2. Демонстрация CausalML на синтетических данных ===\n",
        "def run_causalml_demo():\n",
        "    \"\"\"Запуск демонстрации CausalML.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_samples = 10000\n",
        "    \n",
        "    # Генерация признаков\n",
        "    X = np.random.normal(0, 1, (n_samples, 6))\n",
        "    feature_cols = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "    \n",
        "    # Склонность к лечению (зависит от первых 3 признаков)\n",
        "    logit_propensity = X[:, 0] + 0.5 * X[:, 1] - 0.3 * X[:, 2]\n",
        "    propensity = 1 / (1 + np.exp(-logit_propensity))\n",
        "    treatment = np.random.binomial(1, propensity)\n",
        "    \n",
        "    # Гетерогенный причинный эффект (зависит от последних 2 признаков)\n",
        "    base_effect = 5.0\n",
        "    heterogeneity = 2.0 * X[:, 4] - 1.5 * X[:, 5]\n",
        "    treatment_effect = base_effect + heterogeneity\n",
        "    \n",
        "    # Исход\n",
        "    base_outcome = 2 * X[:, 0] + 1.5 * X[:, 1] + 3 * X[:, 2] + 2.5 * X[:, 3]\n",
        "    noise = np.random.normal(0, 2, n_samples)\n",
        "    outcome = base_outcome + treatment_effect * treatment + noise\n",
        "    \n",
        "    data = pd.DataFrame(X, columns=feature_cols)\n",
        "    data['treatment'] = treatment\n",
        "    data['outcome'] = outcome\n",
        "    \n",
        "    print(\"Синтетические данные созданы.\")\n",
        "    print(f\"Истинный средний ATE: {base_effect:.1f}\")\n",
        "    print(f\"Диапазон индивидуальных эффектов: [{treatment_effect.min():.2f}, {treatment_effect.max():.2f}]\")\n",
        "    \n",
        "    # Запуск анализа\n",
        "    analyzer = ComprehensiveCausalMLAnalysis(\n",
        "        data=data,\n",
        "        treatment_col='treatment',\n",
        "        outcome_col='outcome',\n",
        "        feature_cols=feature_cols\n",
        "    )\n",
        "    \n",
        "    analyzer.prepare_data()\n",
        "    results = analyzer.fit_meta_learners()\n",
        "    _ = analyzer.fit_uplift_model()\n",
        "    auuc = analyzer.evaluate_uplift()\n",
        "    comparison_df = analyzer.compare_ate_estimates(true_ate=base_effect)\n",
        "    \n",
        "    print(\"\\nРезультаты оценки ATE:\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    return analyzer, comparison_df, auuc\n",
        "\n",
        "# Запуск демонстрации\n",
        "analyzer, comparison_df, auuc = run_causalml_demo()\n",
        "\n",
        "# === 6.3. Применение: Сегментация клиентов по uplift ===\n",
        "def perform_uplift_segmentation(analyzer, n_segments=4):\n",
        "    \"\"\"Сегментация клиентов на основе предсказанного uplift.\"\"\"\n",
        "    if analyzer.uplift_predictions is None:\n",
        "        analyzer.fit_uplift_model()\n",
        "    \n",
        "    # Создание сегментов\n",
        "    segment_labels = [f'Сегмент_{i+1}' for i in range(n_segments)]\n",
        "    segments = pd.qcut(analyzer.uplift_predictions, q=n_segments, labels=segment_labels)\n",
        "    \n",
        "    # Анализ сегментов\n",
        "    test_data = analyzer.X_test.copy()\n",
        "    test_data['uplift'] = analyzer.uplift_predictions\n",
        "    test_data['segment'] = segments\n",
        "    test_data['outcome'] = analyzer.y_test.values\n",
        "    test_data['treatment'] = analyzer.treatment_test.values\n",
        "    \n",
        "    segment_analysis = test_data.groupby('segment').agg({\n",
        "        'uplift': 'mean',\n",
        "        'outcome': 'mean',\n",
        "        'treatment': 'mean',\n",
        "        'segment': 'count'\n",
        "    }).round(3)\n",
        "    segment_analysis.columns = ['Средний Uplift', 'Средний Исход', 'Доля Treatment', 'Размер']\n",
        "    \n",
        "    print(\"\\nАнализ сегментов по uplift:\")\n",
        "    print(segment_analysis)\n",
        "    \n",
        "    # Визуализация\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Распределение сегментов\n",
        "    segment_counts = segment_analysis['Размер']\n",
        "    ax1.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%')\n",
        "    ax1.set_title('Распределение клиентов по сегментам')\n",
        "    \n",
        "    # Средний uplift по сегментам\n",
        "    segment_uplift = segment_analysis['Средний Uplift']\n",
        "    ax2.bar(segment_uplift.index, segment_uplift.values, color='mediumseagreen', alpha=0.8)\n",
        "    ax2.set_title('Средний uplift по сегментам')\n",
        "    ax2.set_ylabel('Uplift')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Рекомендация по персонализации\n",
        "    best_segment = segment_uplift.idxmax()\n",
        "    print(f\"\\nРекомендация: Фокусируйтесь на '{best_segment}' для максимизации uplift.\")\n",
        "    \n",
        "    return segment_analysis\n",
        "\n",
        "# Выполнение сегментации\n",
        "segmentation_results = perform_uplift_segmentation(analyzer)\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "CausalML предоставляет инструменты для решения практических задач персонализированного маркетинга и оптимизации воздействий. Meta-learners позволяют гибко использовать мощь ML для оценки ATE и CATE, а uplift-модели напрямую оптимизированы под бизнес-метрики, такие как AUUC. Комбинация сегментации по uplift и оценки гетерогенных эффектов позволяет переходить от массовых кампаний к персонализированным стратегиям, что является кульминацией современного причинного анализа.\n",
        "\n"
      ],
      "metadata": {
        "id": "oeKAmV7oYzWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 7. Классические методы причинного вывода\n",
        "\n",
        "### Теория: Разностно-разностный метод и регрессионный разрыв\n",
        "\n",
        "Пока A/B-тестирование остаётся «золотым стандартом» для оценки причинных эффектов, в реальном мире часто невозможно или неэтично проводить рандомизированные эксперименты. В таких случаях аналитики вынуждены полагаться на **наблюдательные данные** и использовать методы, которые имитируют экспериментальные условия за счёт естественных или политических «шоков». Два наиболее влиятельных и широко применяемых подхода — это **разностно-разностный метод **(Difference-in-Differences, DiD) и **дизайн регрессионного разрыва **(Regression Discontinuity Design, RDD).\n",
        "\n",
        "**Разностно-разностный метод **(DiD) применяется, когда вмешательство (например, новая политика, функция продукта) внедряется в определённый момент времени для одной группы (Treatment), в то время как другая группа (Control) остаётся без изменений. Ключевая идея DiD заключается в том, чтобы сравнить **изменение исхода во времени** в Treatment группе с **изменением исхода во времени** в Control группе. Формально, оценка DiD выглядит так:\n",
        "\\[\n",
        "\\widehat{ATE}_{DiD} = (\\bar{Y}_{1T} - \\bar{Y}_{0T}) - (\\bar{Y}_{1C} - \\bar{Y}_{0C})\n",
        "\\]\n",
        "где \\( \\bar{Y}_{1T} \\) — средний исход в Treatment группе после вмешательства, \\( \\bar{Y}_{0T} \\) — до, а \\( \\bar{Y}_{1C}, \\bar{Y}_{0C} \\) — аналогичные значения для Control группы. Главное предположение DiD — это **параллельные тренды **(Parallel Trends): в отсутствие вмешательства, тренды в исходах для Treatment и Control групп были бы одинаковыми. Это предположение невозможно проверить напрямую и должно быть обосновано субстантивными знаниями о домене.\n",
        "\n",
        "**Дизайн регрессионного разрыва **(RDD) находит применение, когда назначение лечения определяется строгим пороговым правилом на основе непрерывной переменной (running variable). Например, студенты, набравшие на экзамене 70 баллов или больше, получают стипендию, а набравшие меньше — нет. В непосредственной окрестности порога (например, 68–72 балла) студенты практически идентичны по своим способностям и мотивации. Единственное различие — получение стипендии. Это создает «естественный эксперимент», и причинный эффект оценивается как **разрыв **(discontinuity) в значении исхода в точке порога. RDD предоставляет самые надёжные оценки среди всех методов наблюдательных исследований и часто называется «золотым стандартом» для них. Ключевое предположение RDD — это **непрерывность исхода и ковариат** в точке порога, что означает, что не должно быть возможности манипулировать running variable для попадания в нужную группу.\n",
        "\n",
        "**Метод инструментальных переменных **(Instrumental Variables, IV) решает проблему **эндогенности **(endogeneity), когда объясняющая переменная (лечение) коррелирует с ошибкой модели (например, из-за ненаблюдаемых конфаундеров). Инструментальная переменная (instrument) — это переменная, которая:\n",
        "1.  Коррелирует с лечением (**релевантность**).\n",
        "2.  Не коррелирует с ошибкой модели (**экзогенность**).\n",
        "\n",
        "Метод IV (обычно в виде двухшагового МНК, 2SLS) использует вариацию в лечении, которая обусловлена только инструментом, для получения несмещённой оценки причинного эффекта. Найти валидный инструмент в реальном мире чрезвычайно сложно, поэтому этот метод требует тщательного теоретического обоснования.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует реализацию и визуализацию трёх классических методов причинного вывода на синтетических данных, где истинный эффект известен. Это позволяет оценить точность и надёжность каждого подхода.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from linearmodels import PanelOLS\n",
        "\n",
        "print(\"=== 7. КЛАССИЧЕСКИЕ МЕТОДЫ ПРИЧИННОГО ВЫВОДА ===\")\n",
        "\n",
        "# === 7.1. Класс для реализации методов ===\n",
        "class ClassicalCausalMethods:\n",
        "    \"\"\"\n",
        "    Класс для реализации и сравнения классических методов причинного вывода.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "    \n",
        "    # --- Difference-in-Differences ---\n",
        "    def generate_did_data(self, n_units=100, n_periods=6, treatment_effect=5.0):\n",
        "        \"\"\"Генерация panel данных для DiD анализа.\"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        data_list = []\n",
        "        for unit in range(n_units):\n",
        "            # Случайное назначение в группу (фиксировано для unit)\n",
        "            treatment_group = np.random.binomial(1, 0.5)\n",
        "            # Случайный эффект unit'а\n",
        "            unit_fe = np.random.normal(0, 3, 1)\n",
        "            \n",
        "            for period in range(n_periods):\n",
        "                # Общий временной тренд\n",
        "                time_trend = period * 0.8\n",
        "                # Пост-период (вмешательство начинается с периода 4)\n",
        "                post_treatment = 1 if period >= 4 else 0\n",
        "                # Активное лечение (только для treatment группы в post-периоде)\n",
        "                treatment_active = treatment_group * post_treatment\n",
        "                \n",
        "                # Генерация исхода\n",
        "                noise = np.random.normal(0, 2, 1)\n",
        "                outcome = (20 + unit_fe + time_trend +\n",
        "                          treatment_effect * treatment_active + noise)\n",
        "                \n",
        "                data_list.append({\n",
        "                    'unit': unit,\n",
        "                    'period': period,\n",
        "                    'treatment_group': treatment_group,\n",
        "                    'post_treatment': post_treatment,\n",
        "                    'treatment_active': treatment_active,\n",
        "                    'outcome': outcome[0]\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(data_list)\n",
        "    \n",
        "    def difference_in_differences(self, data):\n",
        "        \"\"\"Реализация разностно-разностного метода.\"\"\"\n",
        "        print(\"Применение разностно-разностного метода...\")\n",
        "        \n",
        "        # Визуализация трендов (критически важна для проверки предположения)\n",
        "        self._plot_did_trends(data)\n",
        "        \n",
        "        # 1. Простая регрессионная модель DiD\n",
        "        data['did_interaction'] = data['treatment_group'] * data['post_treatment']\n",
        "        simple_model = smf.ols(\n",
        "            'outcome ~ treatment_group + post_treatment + did_interaction',\n",
        "            data=data\n",
        "        ).fit()\n",
        "        \n",
        "        # 2. Модель с фиксированными эффектами (TWFE)\n",
        "        data_panel = data.set_index(['unit', 'period'])\n",
        "        twfe_model = PanelOLS(\n",
        "            data_panel['outcome'],\n",
        "            sm.add_constant(data_panel[['treatment_active']]),\n",
        "            entity_effects=True,  # Фиксированные эффекты единиц\n",
        "            time_effects=True     # Фиксированные эффекты времени\n",
        "        ).fit()\n",
        "        \n",
        "        self.results['DID'] = {\n",
        "            'estimate': simple_model.params['did_interaction'],\n",
        "            'ci_lower': simple_model.conf_int().loc['did_interaction', 0],\n",
        "            'ci_upper': simple_model.conf_int().loc['did_interaction', 1],\n",
        "            'twfe_estimate': twfe_model.params['treatment_active'],\n",
        "            'simple_model': simple_model,\n",
        "            'twfe_model': twfe_model\n",
        "        }\n",
        "        \n",
        "        return self.results['DID']\n",
        "    \n",
        "    def _plot_did_trends(self, data):\n",
        "        \"\"\"Визуализация трендов до и после вмешательства.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        trend_data = data.groupby(['treatment_group', 'period'])['outcome'].mean().unstack(0)\n",
        "        \n",
        "        # Построение линий для каждой группы\n",
        "        plt.plot(trend_data.index, trend_data[0], 'o-',\n",
        "                label='Control Group', linewidth=2, markersize=8)\n",
        "        plt.plot(trend_data.index, trend_data[1], 's-',\n",
        "                label='Treatment Group', linewidth=2, markersize=8)\n",
        "        \n",
        "        # Вертикальная линия момента вмешательства\n",
        "        plt.axvline(x=3.9, color='black', linestyle='--',\n",
        "                   label='Treatment Start', linewidth=1.5)\n",
        "        \n",
        "        plt.xlabel('Time Period')\n",
        "        plt.ylabel('Average Outcome')\n",
        "        plt.title('DiD: Pre- and Post-Treatment Trends')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    \n",
        "    # --- Regression Discontinuity Design ---\n",
        "    def generate_rdd_data(self, n_samples=2000, true_effect=3.0):\n",
        "        \"\"\"Генерация данных для RDD анализа.\"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Непрерывная переменная принятия решений\n",
        "        running_var = np.random.uniform(-3, 3, n_samples)\n",
        "        # Лечение назначается при running_var >= 0\n",
        "        treatment = (running_var >= 0).astype(int)\n",
        "        \n",
        "        # Исход с линейным трендом и разрывом на пороге\n",
        "        outcome = (15 + 2 * running_var +\n",
        "                  true_effect * treatment +\n",
        "                  np.random.normal(0, 2, n_samples))\n",
        "        \n",
        "        return pd.DataFrame({\n",
        "            'running_var': running_var,\n",
        "            'treatment': treatment,\n",
        "            'outcome': outcome\n",
        "        })\n",
        "    \n",
        "    def regression_discontinuity(self, data, bandwidth=1.0):\n",
        "        \"\"\"Реализация дизайна регрессионного разрыва.\"\"\"\n",
        "        print(\"Применение дизайна регрессионного разрыва...\")\n",
        "        \n",
        "        # Визуализация данных RDD\n",
        "        self._plot_rdd_data(data, bandwidth)\n",
        "        \n",
        "        # Ограничение данных окном bandwidth\n",
        "        data_bw = data[(data['running_var'] >= -bandwidth) &\n",
        "                      (data['running_var'] <= bandwidth)].copy()\n",
        "        \n",
        "        # Модель: локальная линейная регрессия\n",
        "        # Используем centered running variable для лучшей интерпретации\n",
        "        data_bw['running_centered'] = data_bw['running_var']\n",
        "        rdd_model = smf.ols('outcome ~ running_centered + treatment', data=data_bw).fit()\n",
        "        \n",
        "        # Анализ чувствительности к bandwidth\n",
        "        bandwidths = [0.5, 0.8, 1.0, 1.5, 2.0]\n",
        "        sensitivity = {}\n",
        "        for bw in bandwidths:\n",
        "            data_temp = data[(data['running_var'] >= -bw) & (data['running_var'] <= bw)]\n",
        "            if len(data_temp) > 50:  # Минимальный размер выборки\n",
        "                model_temp = smf.ols('outcome ~ running_centered + treatment',\n",
        "                                    data=data_temp.assign(running_centered=data_temp['running_var'])).fit()\n",
        "                sensitivity[bw] = model_temp.params['treatment']\n",
        "            else:\n",
        "                sensitivity[bw] = np.nan\n",
        "        \n",
        "        self.results['RDD'] = {\n",
        "            'estimate': rdd_model.params['treatment'],\n",
        "            'ci_lower': rdd_model.conf_int().loc['treatment', 0],\n",
        "            'ci_upper': rdd_model.conf_int().loc['treatment', 1],\n",
        "            'bandwidth_sensitivity': sensitivity,\n",
        "            'model': rdd_model,\n",
        "            'bandwidth_used': bandwidth\n",
        "        }\n",
        "        \n",
        "        return self.results['RDD']\n",
        "    \n",
        "    def _plot_rdd_data(self, data, bandwidth):\n",
        "        \"\"\"Визуализация данных RDD с локальными регрессиями.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        # Исходные данные\n",
        "        ax1.scatter(data['running_var'], data['outcome'], alpha=0.3, s=10)\n",
        "        ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "        ax1.set_xlabel('Running Variable')\n",
        "        ax1.set_ylabel('Outcome')\n",
        "        ax1.set_title('RDD: Raw Data')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Данные в bandwidth с регрессиями\n",
        "        data_bw = data[(data['running_var'] >= -bandwidth) & (data['running_var'] <= bandwidth)]\n",
        "        control = data_bw[data_bw['running_var'] < 0]\n",
        "        treatment = data_bw[data_bw['running_var'] >= 0]\n",
        "        \n",
        "        ax2.scatter(control['running_var'], control['outcome'],\n",
        "                   alpha=0.6, s=20, label='Control')\n",
        "        ax2.scatter(treatment['running_var'], treatment['outcome'],\n",
        "                   alpha=0.6, s=20, label='Treatment')\n",
        "        \n",
        "        # Локальные линейные регрессии\n",
        "        for subset, color in [(control, 'blue'), (treatment, 'red')]:\n",
        "            if not subset.empty:\n",
        "                X = sm.add_constant(subset['running_var'])\n",
        "                model = sm.OLS(subset['outcome'], X).fit()\n",
        "                x_line = np.linspace(subset['running_var'].min(),\n",
        "                                   subset['running_var'].max(), 100)\n",
        "                y_line = model.params['const'] + model.params['running_var'] * x_line\n",
        "                ax2.plot(x_line, y_line, color=color, linewidth=2)\n",
        "        \n",
        "        ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "        ax2.set_xlabel('Running Variable')\n",
        "        ax2.set_ylabel('Outcome')\n",
        "        ax2.set_title(f'RDD: Local Linear Regression (Bandwidth={bandwidth})')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # --- Instrumental Variables ---\n",
        "    def generate_iv_data(self, n_samples=1000):\n",
        "        \"\"\"Генерация данных для IV анализа.\"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Инструмент (например, расстояние до клиники)\n",
        "        instrument = np.random.normal(0, 1, n_samples)\n",
        "        \n",
        "        # Ненаблюдаемый конфаундер (например, мотивация)\n",
        "        confounder = np.random.normal(0, 1, n_samples)\n",
        "        \n",
        "        # Эндогенное лечение (например, приём лекарства)\n",
        "        # Зависит и от инструмента, и от конфаундера\n",
        "        treatment = 0.7 * instrument + 0.5 * confounder + np.random.normal(0, 0.5, n_samples)\n",
        "        \n",
        "        # Исход (например, здоровье)\n",
        "        # Зависит от лечения и конфаундера\n",
        "        outcome = 2.0 * treatment + 1.5 * confounder + np.random.normal(0, 1, n_samples)\n",
        "        \n",
        "        return pd.DataFrame({\n",
        "            'instrument': instrument,\n",
        "            'treatment': treatment,\n",
        "            'outcome': outcome,\n",
        "            'confounder': confounder  # В реальности недоступен\n",
        "        })\n",
        "    \n",
        "    def instrumental_variables(self, data=None):\n",
        "        \"\"\"Реализация метода инструментальных переменных (2SLS).\"\"\"\n",
        "        print(\"Применение метода инструментальных переменных (2SLS)...\")\n",
        "        \n",
        "        if data is None:\n",
        "            data = self.generate_iv_data()\n",
        "        \n",
        "        # Первая стадия: регрессия лечения на инструмент\n",
        "        first_stage = sm.OLS(data['treatment'], sm.add_constant(data['instrument'])).fit()\n",
        "        treatment_pred = first_stage.predict(sm.add_constant(data['instrument']))\n",
        "        \n",
        "        # Вторая стадия: регрессия исхода на предсказанные значения лечения\n",
        "        second_stage = sm.OLS(data['outcome'], sm.add_constant(treatment_pred)).fit()\n",
        "        \n",
        "        # Сравнение с OLS (которая даёт смещённую оценку)\n",
        "        ols_biased = sm.OLS(data['outcome'], sm.add_constant(data['treatment'])).fit()\n",
        "        \n",
        "        self.results['IV'] = {\n",
        "            'estimate': second_stage.params[1],  # Коэффициент при предсказанном лечении\n",
        "            'ci_lower': second_stage.conf_int().iloc[1, 0],\n",
        "            'ci_upper': second_stage.conf_int().iloc[1, 1],\n",
        "            'ols_estimate': ols_biased.params[1],\n",
        "            'first_stage_f': first_stage.fvalue,\n",
        "            'first_stage_r2': first_stage.rsquared,\n",
        "            'second_stage': second_stage,\n",
        "            'ols_model': ols_biased\n",
        "        }\n",
        "        \n",
        "        return self.results['IV']\n",
        "    \n",
        "    # --- Сравнение и анализ ---\n",
        "    def compare_methods(self, true_effects):\n",
        "        \"\"\"Сравнение результатов всех методов с истинными эффектами.\"\"\"\n",
        "        comparison_data = []\n",
        "        \n",
        "        for method, result in self.results.items():\n",
        "            if method in true_effects:\n",
        "                estimate = result['estimate']\n",
        "                true_val = true_effects[method]\n",
        "                bias = estimate - true_val\n",
        "                comparison_data.append({\n",
        "                    'Method': method,\n",
        "                    'Estimate': estimate,\n",
        "                    'True_Effect': true_val,\n",
        "                    'Bias': bias,\n",
        "                    'CI_Lower': result['ci_lower'],\n",
        "                    'CI_Upper': result['ci_upper']\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(comparison_data)\n",
        "    \n",
        "    def plot_comparison(self, comparison_df):\n",
        "        \"\"\"Визуализация сравнения методов.\"\"\"\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        \n",
        "        methods = comparison_df['Method']\n",
        "        estimates = comparison_df['Estimate']\n",
        "        true_effects = comparison_df['True_Effect']\n",
        "        ci_lower = comparison_df['CI_Lower']\n",
        "        ci_upper = comparison_df['CI_Upper']\n",
        "        \n",
        "        y_pos = np.arange(len(methods))\n",
        "        \n",
        "        # Построение оценок с доверительными интервалами\n",
        "        plt.errorbar(estimates, y_pos, xerr=[estimates-ci_lower, ci_upper-estimates],\n",
        "                    fmt='o', color='red', capsize=5, label='Оценка', markersize=8)\n",
        "        \n",
        "        # Истинные эффекты\n",
        "        plt.scatter(true_effects, y_pos, color='green', s=100,\n",
        "                   marker='D', label='Истинный эффект')\n",
        "        \n",
        "        plt.yticks(y_pos, methods)\n",
        "        plt.xlabel('Причинный эффект')\n",
        "        plt.title('Сравнение классических методов причинного вывода')\n",
        "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return comparison_df\n",
        "\n",
        "# === 7.2. Демонстрация классических методов ===\n",
        "def demonstrate_classical_methods():\n",
        "    \"\"\"Запуск полной демонстрации классических методов.\"\"\"\n",
        "    \n",
        "    analyzer = ClassicalCausalMethods()\n",
        "    true_effects = {\n",
        "        'DID': 5.0,\n",
        "        'RDD': 3.0,\n",
        "        'IV': 2.0\n",
        "    }\n",
        "    \n",
        "    # 1. Difference-in-Differences\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"1. РАЗНОСТНО-РАЗНОСТНЫЙ МЕТОД (DiD)\")\n",
        "    print(\"=\"*50)\n",
        "    did_data = analyzer.generate_did_data(treatment_effect=true_effects['DID'])\n",
        "    did_result = analyzer.difference_in_differences(did_data)\n",
        "    print(f\"Оценка DiD: {did_result['estimate']:.3f}\")\n",
        "    print(f\"Двухфакторная модель (TWFE): {did_result['twfe_estimate']:.3f}\")\n",
        "    print(f\"95% ДИ: [{did_result['ci_lower']:.3f}, {did_result['ci_upper']:.3f}]\")\n",
        "    \n",
        "    # 2. Regression Discontinuity Design\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"2. ДИЗАЙН РЕГРЕССИОННОГО РАЗРЫВА (RDD)\")\n",
        "    print(\"=\"*50)\n",
        "    rdd_data = analyzer.generate_rdd_data(true_effect=true_effects['RDD'])\n",
        "    rdd_result = analyzer.regression_discontinuity(rdd_data, bandwidth=1.0)\n",
        "    print(f\"Оценка RDD: {rdd_result['estimate']:.3f}\")\n",
        "    print(f\"95% ДИ: [{rdd_result['ci_lower']:.3f}, {rdd_result['ci_upper']:.3f}]\")\n",
        "    print(\"Чувствительность к bandwidth:\")\n",
        "    for bw, effect in rdd_result['bandwidth_sensitivity'].items():\n",
        "        if not np.isnan(effect):\n",
        "            print(f\"  Bandwidth {bw}: {effect:.3f}\")\n",
        "    \n",
        "    # 3. Instrumental Variables\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"3. ИНСТРУМЕНТАЛЬНЫЕ ПЕРЕМЕННЫЕ (IV)\")\n",
        "    print(\"=\"*50)\n",
        "    iv_result = analyzer.instrumental_variables()\n",
        "    print(f\"Оценка 2SLS: {iv_result['estimate']:.3f}\")\n",
        "    print(f\"Смещённая оценка OLS: {iv_result['ols_estimate']:.3f}\")\n",
        "    print(f\"95% ДИ (2SLS): [{iv_result['ci_lower']:.3f}, {iv_result['ci_upper']:.3f}]\")\n",
        "    print(f\"Первая стадия - F-статистика: {iv_result['first_stage_f']:.2f} (должна быть > 10)\")\n",
        "    print(f\"Первая стадия - R²: {iv_result['first_stage_r2']:.3f}\")\n",
        "    \n",
        "    # Сравнение методов\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"4. СРАВНЕНИЕ МЕТОДОВ\")\n",
        "    print(\"=\"*50)\n",
        "    comparison_df = analyzer.compare_methods(true_effects)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Визуализация сравнения\n",
        "    analyzer.plot_comparison(comparison_df)\n",
        "    \n",
        "    return analyzer, comparison_df\n",
        "\n",
        "# Запуск демонстрации\n",
        "analyzer, results_df = demonstrate_classical_methods()\n",
        "\n",
        "# === 7.3. Ключевые выводы и рекомендации ===\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"КЛЮЧЕВЫЕ ВЫВОДЫ И МЕТОДОЛОГИЧЕСКИЕ РЕКОМЕНДАЦИИ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "1. DiD:\n",
        "   - Требует строгой проверки предположения о параллельных трендах.\n",
        "   - Визуализация трендов до вмешательства — обязательный шаг.\n",
        "   - Модели с фиксированными эффектами (TWFE) предпочтительнее простых регрессий.\n",
        "\n",
        "2. RDD:\n",
        "   - Предоставляет наиболее надёжные оценки среди наблюдательных методов.\n",
        "   - Критически важно проверять чувствительность к выбору bandwidth.\n",
        "   - Локальные полиномиальные регрессии должны быть гибкими, но не переобученными.\n",
        "\n",
        "3. IV:\n",
        "   - Поиск валидного инструмента — главная сложность.\n",
        "   - Статистика Фишера в первой стадии должна быть > 10 (сильный инструмент).\n",
        "   - IV оценивает не ATE, а LATE (Local Average Treatment Effect) — эффект для \"\n",
        "      \"тех, чье поведение изменяется инструментом.\n",
        "\n",
        "Общее правило: Всегда начинайте с рандомизированного эксперимента (A/B-теста).\n",
        "Используйте классические методы только тогда, когда эксперимент невозможен,\n",
        "и тщательно обосновывайте их предположения на основе предметной области.\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример предоставляет полное методологическое руководство по применению классических методов причинного вывода. Каждый метод сопровождается генерацией реалистичных данных, визуальной проверкой ключевых предположений, оценкой эффекта и анализом его надёжности. Особенно важно то, что методы сравниваются с известным истинным эффектом, что наглядно демонстрирует их точность и потенциальные источники смещения. Такой подход является основой для критического мышления и ответственного применения причинного вывода в реальных бизнес-сценариях.\n",
        ""
      ],
      "metadata": {
        "id": "POJ7JIwju_YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 8. Дизайн и внедрение платформы экспериментов\n",
        "\n",
        "### Теория: Архитектура систем A/B тестирования\n",
        "\n",
        "Масштабирование экспериментирования от разовых A/B-тестов отдельных команд до организационной культуры данных требует создания специализированной **платформы экспериментов **(Experimentation Platform). Такая платформа — это не просто набор скриптов, а **централизованная, инженерно надёжная система**, обеспечивающая корректность, воспроизводимость и безопасность всех проводимых тестов.\n",
        "\n",
        "**Ключевые требования к промышленной платформе**:\n",
        "\n",
        "1.  **Надёжность и воспроизводимость**: Назначение пользователя на вариант должно быть **детерминированным и согласованным** во всех точках взаимодействия (веб, мобильное приложение, backend-сервисы). Это исключает ситуацию, когда пользователь видит разные варианты в разных сессиях, что ведёт к смещению и недействительным результатам.\n",
        "2.  **Масштабируемость**: Платформа должна обрабатывать миллионы пользователей и событий в секунду, не создавая узких мест в пользовательском интерфейсе или backend-логике.\n",
        "3.  **Безопасность и защита от ошибок**: Должны быть реализованы «гард-рейлы» (guardrails), предотвращающие запуск потенциально опасных экспериментов (например, с 100% трафиком на непроверенный вариант) и обеспечивающие защиту пользовательского опыта.\n",
        "4.  **Интеграция и централизация**: Платформа должна предоставлять единый API для всех продуктов и сервисов компании, а также централизованную панель управления для аналитиков и менеджеров продукта.\n",
        "5.  **Качество данных**: Это фундаментальный столп доверия к экспериментам. Система должна включать в себя автоматизированные проверки на этапе сбора данных (Data Quality Validation), чтобы выявлять проблемы, такие как **проблема синхронизации** (например, событие конверсии зарегистрировано до события показа варианта) или **дисбаланс рандомизации**.\n",
        "\n",
        "Архитектура типичной платформы включает несколько ключевых компонентов:\n",
        "\n",
        "*   **Сервис назначения **(Assignment Service): Обрабатывает запросы на определение варианта для пользователя, используя детерминированный хеш и логику таргетинга.\n",
        "*   **Сервис трекинга событий **(Event Tracking Service): Принимает и валидирует события (показы, клики, покупки) от клиентских SDK и backend-сервисов.\n",
        "*   **Система аналитики и отчётности **(Analytics Engine): Агрегирует сырые события, вычисляет метрики и предоставляет интерактивные дашборды для анализа.\n",
        "*   **Менеджер экспериментов **(Experiment Manager): Веб-интерфейс и API для создания, запуска, приостановки и завершения экспериментов, а также управления их конфигурацией.\n",
        "\n",
        "**Качество данных** — это не опциональная функция, а ядро всей платформы. Даже самая сложная и красивая система анализа бесполезна, если на входе находятся повреждённые данные. Поэтому на этапе проектирования платформы необходимо заложить механизмы автоматической валидации, чтобы **обнаружить и предотвратить проблемы до того, как они приведут к неверным бизнес-решениям**.\n",
        "\n",
        "**Примеры**\n",
        "\n",
        "*Пояснение до выполнения кода*:  \n",
        "Этот пример демонстрирует упрощённую, но функционально полную реализацию ключевых компонентов платформы экспериментов: сервиса назначения, трекинга событий и валидатора качества данных.\n",
        "\n",
        "```python\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "import hashlib\n",
        "import json\n",
        "from typing import Dict, List, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=== 8. ДИЗАЙН И ВНЕДРЕНИЕ ПЛАТФОРМЫ ЭКСПЕРИМЕНТОВ ===\")\n",
        "\n",
        "# === 8.1. Основной класс платформы экспериментов ===\n",
        "class ExperimentPlatform:\n",
        "    \"\"\"\n",
        "    Упрощённая реализация промышленной платформы экспериментов.\n",
        "    Демонстрирует ключевые принципы: детерминированное назначение,\n",
        "    трекинг событий и управление жизненным циклом эксперимента.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Хранилище экспериментов\n",
        "        self.experiments: Dict[str, Dict] = {}\n",
        "        # Журнал всех событий (в реальности — распределённая БД или поток данных)\n",
        "        self.event_log: List[Dict] = []\n",
        "        # Кэш пользователей (в реальности — не используется, всё stateless)\n",
        "        self.users: Dict[str, Dict] = {}\n",
        "    \n",
        "    def create_experiment(self, name: str, variants: List[str],\n",
        "                         traffic_allocations: List[float],\n",
        "                         target_audience: Optional[Dict] = None,\n",
        "                         metadata: Optional[Dict] = None) -> str:\n",
        "        \"\"\"\n",
        "        Создание нового эксперимента с валидацией конфигурации.\n",
        "        \"\"\"\n",
        "        # Валидация входных данных\n",
        "        if not (0.99 <= sum(traffic_allocations) <= 1.01):\n",
        "            raise ValueError(\"Сумма долей трафика должна быть близка к 1.0\")\n",
        "        if len(variants) != len(traffic_allocations):\n",
        "            raise ValueError(\"Количество вариантов и долей трафика должно совпадать\")\n",
        "        \n",
        "        experiment_id = str(uuid.uuid4())[:8]\n",
        "        \n",
        "        experiment = {\n",
        "            'id': experiment_id,\n",
        "            'name': name,\n",
        "            'variants': variants,\n",
        "            'traffic_allocations': traffic_allocations,\n",
        "            'target_audience': target_audience or {},\n",
        "            'metadata': metadata or {},\n",
        "            'created_at': datetime.now(),\n",
        "            'status': 'draft',  # draft, running, paused, completed\n",
        "            'assigned_users': {}  # Для демонстрации; в продакшене stateless\n",
        "        }\n",
        "        \n",
        "        self.experiments[experiment_id] = experiment\n",
        "        print(f\"[PLATFORM] Эксперимент '{name}' создан с ID: {experiment_id}\")\n",
        "        return experiment_id\n",
        "    \n",
        "    def assign_user(self, user_id: str, experiment_id: str,\n",
        "                   user_context: Optional[Dict] = None) -> str:\n",
        "        \"\"\"\n",
        "        Детерминированное назначение пользователя на вариант эксперимента.\n",
        "        Это ядро платформы, критически важное для согласованности.\n",
        "        \"\"\"\n",
        "        if experiment_id not in self.experiments:\n",
        "            # В реальности: возвращать безопасный вариант по умолчанию\n",
        "            raise ValueError(f\"Эксперимент {experiment_id} не найден\")\n",
        "        \n",
        "        experiment = self.experiments[experiment_id]\n",
        "        if experiment['status'] != 'running':\n",
        "            raise ValueError(f\"Эксперимент {experiment_id} не запущен\")\n",
        "        \n",
        "        # Проверка таргетинга\n",
        "        if not self._is_user_in_audience(user_context, experiment['target_audience']):\n",
        "            return 'control'  # Вариант по умолчанию для нецелевой аудитории\n",
        "        \n",
        "        # ДЕТЕРМИНИРОВАННОЕ НАЗНАЧЕНИЕ через хеширование\n",
        "        # Это гарантирует, что один и тот же пользователь всегда получит\n",
        "        # один и тот же вариант, независимо от того, откуда пришёл запрос.\n",
        "        assignment_key = f\"{user_id}_{experiment_id}\"\n",
        "        hash_value = hashlib.md5(assignment_key.encode()).hexdigest()\n",
        "        hash_int = int(hash_value[:8], 16)  # Используем первые 8 hex-символов\n",
        "        \n",
        "        # Назначение варианта на основе хеша\n",
        "        random_value = hash_int / 0xFFFFFFFF  # Преобразуем в [0, 1)\n",
        "        cumulative_allocation = 0.0\n",
        "        \n",
        "        for i, allocation in enumerate(experiment['traffic_allocations']):\n",
        "            cumulative_allocation += allocation\n",
        "            if random_value <= cumulative_allocation:\n",
        "                variant = experiment['variants'][i]\n",
        "                break\n",
        "        else:\n",
        "            # Fallback на последний вариант (на случай ошибок округления)\n",
        "            variant = experiment['variants'][-1]\n",
        "        \n",
        "        # Логирование события назначения (в реальности — асинхронно в Kafka)\n",
        "        self._log_event({\n",
        "            'type': 'assignment',\n",
        "            'user_id': user_id,\n",
        "            'experiment_id': experiment_id,\n",
        "            'variant': variant,\n",
        "            'timestamp': datetime.now(),\n",
        "            'context': user_context or {}\n",
        "        })\n",
        "        \n",
        "        return variant\n",
        "    \n",
        "    def _is_user_in_audience(self, user_context: Dict, audience_rules: Dict) -> bool:\n",
        "        \"\"\"Проверка принадлежности к целевой аудитории.\"\"\"\n",
        "        if not audience_rules:\n",
        "            return True\n",
        "        \n",
        "        # Простая логика AND для всех правил\n",
        "        for rule_key, rule_value in audience_rules.items():\n",
        "            if user_context.get(rule_key) != rule_value:\n",
        "                return False\n",
        "        return True\n",
        "    \n",
        "    def _log_event(self, event: Dict):\n",
        "        \"\"\"Внутренний метод для логирования событий.\"\"\"\n",
        "        self.event_log.append(event)\n",
        "        # В реальности: отправка в распределённую систему трекинга (Kafka, AWS Kinesis)\n",
        "    \n",
        "    def track_event(self, user_id: str, experiment_id: str,\n",
        "                   event_name: str, event_properties: Optional[Dict] = None):\n",
        "        \"\"\"Публичный API для трекинга пользовательских событий.\"\"\"\n",
        "        self._log_event({\n",
        "            'type': 'conversion',\n",
        "            'user_id': user_id,\n",
        "            'experiment_id': experiment_id,\n",
        "            'event_name': event_name,\n",
        "            'event_properties': event_properties or {},\n",
        "            'timestamp': datetime.now()\n",
        "        })\n",
        "    \n",
        "    def start_experiment(self, experiment_id: str):\n",
        "        \"\"\"Запуск эксперимента.\"\"\"\n",
        "        if experiment_id in self.experiments:\n",
        "            self.experiments[experiment_id]['status'] = 'running'\n",
        "            self.experiments[experiment_id]['started_at'] = datetime.now()\n",
        "            print(f\"[PLATFORM] Эксперимент {experiment_id} запущен.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Эксперимент {experiment_id} не найден\")\n",
        "    \n",
        "    def get_experiment_results(self, experiment_id: str) -> Dict:\n",
        "        \"\"\"Агрегация результатов эксперимента.\"\"\"\n",
        "        if experiment_id not in self.experiments:\n",
        "            raise ValueError(f\"Эксперимент {experiment_id} не найден\")\n",
        "        \n",
        "        # Фильтрация событий по эксперименту\n",
        "        experiment_events = [e for e in self.event_log if e.get('experiment_id') == experiment_id]\n",
        "        assignment_events = [e for e in experiment_events if e['type'] == 'assignment']\n",
        "        conversion_events = [e for e in experiment_events if e['type'] == 'conversion']\n",
        "        \n",
        "        # Создание маппинга пользователь -> вариант\n",
        "        user_to_variant = {e['user_id']: e['variant'] for e in assignment_events}\n",
        "        \n",
        "        # Агрегация метрик по вариантам\n",
        "        results = {}\n",
        "        for variant in self.experiments[experiment_id]['variants']:\n",
        "            results[variant] = {\n",
        "                'assigned_users': 0,\n",
        "                'conversion_events': 0,\n",
        "                'revenue': 0.0\n",
        "            }\n",
        "        \n",
        "        # Подсчёт назначений\n",
        "        for event in assignment_events:\n",
        "            variant = event['variant']\n",
        "            if variant in results:\n",
        "                results[variant]['assigned_users'] += 1\n",
        "        \n",
        "        # Подсчёт конверсий и выручки\n",
        "        for event in conversion_events:\n",
        "            user_id = event['user_id']\n",
        "            if user_id in user_to_variant:\n",
        "                variant = user_to_variant[user_id]\n",
        "                if variant in results:\n",
        "                    results[variant]['conversion_events'] += 1\n",
        "                    # Предполагаем, что событие 'purchase' имеет поле 'value'\n",
        "                    if event['event_name'] == 'purchase':\n",
        "                        results[variant]['revenue'] += event['event_properties'].get('value', 0.0)\n",
        "        \n",
        "        # Расчёт производных метрик\n",
        "        for variant, data in results.items():\n",
        "            assigned = data['assigned_users']\n",
        "            if assigned > 0:\n",
        "                data['conversion_rate'] = data['conversion_events'] / assigned\n",
        "                data['arpu'] = data['revenue'] / assigned\n",
        "            else:\n",
        "                data['conversion_rate'] = 0.0\n",
        "                data['arpu'] = 0.0\n",
        "        \n",
        "        return results\n",
        "\n",
        "# === 8.2. Валидатор качества данных ===\n",
        "class DataQualityValidator:\n",
        "    \"\"\"\n",
        "    Система автоматической валидации качества данных эксперимента.\n",
        "    Проверяет ключевые предпосылки корректности A/B-теста.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.checks = []\n",
        "    \n",
        "    def add_check(self, check_name: str, check_function):\n",
        "        \"\"\"Регистрация новой проверки качества.\"\"\"\n",
        "        self.checks.append({'name': check_name, 'function': check_function})\n",
        "    \n",
        "    def validate_experiment(self, experiment: Dict, event_log: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Запуск всех зарегистрированных проверок на заданном эксперименте.\n",
        "        Возвращает детальный отчёт о качестве данных.\n",
        "        \"\"\"\n",
        "        report = {}\n",
        "        for check in self.checks:\n",
        "            try:\n",
        "                result = check['function'](experiment, event_log)\n",
        "                report[check['name']] = {\n",
        "                    'status': 'passed' if result.get('passed', True) else 'failed',\n",
        "                    'details': result.get('details', {})\n",
        "                }\n",
        "            except Exception as e:\n",
        "                report[check['name']] = {\n",
        "                    'status': 'error',\n",
        "                    'details': {'error_message': str(e)}\n",
        "                }\n",
        "        return report\n",
        "    \n",
        "    @staticmethod\n",
        "    def check_randomization_balance(experiment: Dict, event_log: List[Dict]) -> Dict:\n",
        "        \"\"\"Проверка баланса рандомизации между вариантами.\"\"\"\n",
        "        assignment_events = [e for e in event_log if e['type'] == 'assignment']\n",
        "        if not assignment_events:\n",
        "            return {'passed': False, 'details': {'reason': 'No assignment events found'}}\n",
        "        \n",
        "        # Подсчёт пользователей по вариантам\n",
        "        variant_counts = {}\n",
        "        for event in assignment_events:\n",
        "            variant = event['variant']\n",
        "            variant_counts[variant] = variant_counts.get(variant, 0) + 1\n",
        "        \n",
        "        total = sum(variant_counts.values())\n",
        "        expected_allocations = experiment['traffic_allocations']\n",
        "        expected_variants = experiment['variants']\n",
        "        \n",
        "        # Проверка отклонений от ожидаемых долей\n",
        "        details = {}\n",
        "        passed = True\n",
        "        for i, variant in enumerate(expected_variants):\n",
        "            expected_prop = expected_allocations[i]\n",
        "            actual_prop = variant_counts.get(variant, 0) / total\n",
        "            deviation = abs(actual_prop - expected_prop)\n",
        "            \n",
        "            details[variant] = {\n",
        "                'expected_proportion': expected_prop,\n",
        "                'actual_proportion': actual_prop,\n",
        "                'absolute_deviation': deviation\n",
        "            }\n",
        "            \n",
        "            # Допускаем отклонение до 5%\n",
        "            if deviation > 0.05:\n",
        "                passed = False\n",
        "        \n",
        "        return {'passed': passed, 'details': details}\n",
        "    \n",
        "    @staticmethod\n",
        "    def check_sanity_metrics(experiment: Dict, event_log: List[Dict]) -> Dict:\n",
        "        \"\"\"Проверка sanity-метрик, которые не должны меняться между вариантами.\"\"\"\n",
        "        # Пример: проверка общего числа пользователей\n",
        "        assignment_events = [e for e in event_log if e['type'] == 'assignment']\n",
        "        total_users = len(assignment_events)\n",
        "        expected_min_users = 100  # Минимальный размер для статистической значимости\n",
        "        \n",
        "        return {\n",
        "            'passed': total_users >= expected_min_users,\n",
        "            'details': {\n",
        "                'total_assigned_users': total_users,\n",
        "                'minimum_required': expected_min_users\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    @staticmethod\n",
        "    def check_event_consistency(experiment: Dict, event_log: List[Dict]) -> Dict:\n",
        "        \"\"\"Проверка логической целостности последовательности событий.\"\"\"\n",
        "        user_events = {}\n",
        "        for event in event_log:\n",
        "            user_id = event['user_id']\n",
        "            if user_id not in user_events:\n",
        "                user_events[user_id] = []\n",
        "            user_events[user_id].append(event)\n",
        "        \n",
        "        # Проверка, что событие конверсии происходит ПОСЛЕ назначения\n",
        "        inconsistent_users = []\n",
        "        for user_id, events in user_events.items():\n",
        "            assignment_time = None\n",
        "            for event in events:\n",
        "                if event['type'] == 'assignment':\n",
        "                    assignment_time = event['timestamp']\n",
        "                elif event['type'] == 'conversion':\n",
        "                    if assignment_time is None or event['timestamp'] < assignment_time:\n",
        "                        inconsistent_users.append(user_id)\n",
        "                        break\n",
        "        \n",
        "        return {\n",
        "            'passed': len(inconsistent_users) == 0,\n",
        "            'details': {\n",
        "                'inconsistent_users_count': len(inconsistent_users),\n",
        "                'inconsistent_users_sample': inconsistent_users[:5]  # Показать пример\n",
        "            }\n",
        "        }\n",
        "\n",
        "# === 8.3. Мониторинг в реальном времени ===\n",
        "class ExperimentMonitor:\n",
        "    \"\"\"\n",
        "    Класс для непрерывного мониторинга экспериментов и обнаружения аномалий.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, platform: ExperimentPlatform):\n",
        "        self.platform = platform\n",
        "        self.baseline_metrics = {}\n",
        "    \n",
        "    def calculate_metrics(self, experiment_id: str, hours_back: int = 1) -> Dict:\n",
        "        \"\"\"Расчёт ключевых метрик за последний час.\"\"\"\n",
        "        cutoff_time = datetime.now() - timedelta(hours=hours_back)\n",
        "        recent_events = [\n",
        "            e for e in self.platform.event_log\n",
        "            if e['timestamp'] >= cutoff_time and e.get('experiment_id') == experiment_id\n",
        "        ]\n",
        "        \n",
        "        assignment_events = [e for e in recent_events if e['type'] == 'assignment']\n",
        "        conversion_events = [e for e in recent_events if e['type'] == 'conversion']\n",
        "        \n",
        "        metrics = {\n",
        "            'assignment_count': len(assignment_events),\n",
        "            'conversion_count': len(conversion_events),\n",
        "            'assignment_rate_per_hour': len(assignment_events) / hours_back\n",
        "        }\n",
        "        \n",
        "        if assignment_events:\n",
        "            metrics['conversion_rate'] = len(conversion_events) / len(assignment_events)\n",
        "        else:\n",
        "            metrics['conversion_rate'] = 0.0\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def set_baseline(self, experiment_id: str, metrics: Dict):\n",
        "        \"\"\"Установка базовых метрик для последующего сравнения.\"\"\"\n",
        "        self.baseline_metrics[experiment_id] = {\n",
        "            metric: {\n",
        "                'mean': value,\n",
        "                'std': value * 0.1  # Пример стандартного отклонения (10%)\n",
        "            } for metric, value in metrics.items()\n",
        "        }\n",
        "    \n",
        "    def detect_anomalies(self, experiment_id: str, current_metrics: Dict) -> List[Dict]:\n",
        "        \"\"\"Обнаружение статистических аномалий в метриках.\"\"\"\n",
        "        if experiment_id not in self.baseline_metrics:\n",
        "            return []  # Базовая линия не установлена\n",
        "        \n",
        "        anomalies = []\n",
        "        baseline = self.baseline_metrics[experiment_id]\n",
        "        \n",
        "        for metric, current_value in current_metrics.items():\n",
        "            if metric in baseline:\n",
        "                mean = baseline[metric]['mean']\n",
        "                std = baseline[metric]['std']\n",
        "                if std > 0:\n",
        "                    z_score = abs(current_value - mean) / std\n",
        "                    if z_score > 3:  # Отклонение более чем на 3 стандартных отклонения\n",
        "                        anomalies.append({\n",
        "                            'metric': metric,\n",
        "                            'current_value': current_value,\n",
        "                            'baseline_mean': mean,\n",
        "                            'z_score': z_score\n",
        "                        })\n",
        "        \n",
        "        return anomalies\n",
        "\n",
        "# === 8.4. Демонстрация работы платформы ===\n",
        "def demonstrate_experiment_platform():\n",
        "    \"\"\"Демонстрация полного цикла работы платформы экспериментов.\"\"\"\n",
        "    \n",
        "    print(\"\\n=== ДЕМОНСТРАЦИЯ ПЛАТФОРМЫ ЭКСПЕРИМЕНТОВ ===\")\n",
        "    \n",
        "    # Инициализация компонентов\n",
        "    platform = ExperimentPlatform()\n",
        "    validator = DataQualityValidator()\n",
        "    monitor = ExperimentMonitor(platform)\n",
        "    \n",
        "    # Регистрация проверок качества\n",
        "    validator.add_check('randomization_balance', DataQualityValidator.check_randomization_balance)\n",
        "    validator.add_check('sanity_metrics', DataQualityValidator.check_sanity_metrics)\n",
        "    validator.add_check('event_consistency', DataQualityValidator.check_event_consistency)\n",
        "    \n",
        "    # Создание и запуск эксперимента\n",
        "    experiment_id = platform.create_experiment(\n",
        "        name='Новый процесс оформления заказа',\n",
        "        variants=['control', 'variant_a', 'variant_b'],\n",
        "        traffic_allocations=[0.33, 0.33, 0.34],\n",
        "        target_audience={'country': 'US', 'platform': 'web'},\n",
        "        metadata={'goal': 'увеличить конверсию', 'team': 'рост'}\n",
        "    )\n",
        "    platform.start_experiment(experiment_id)\n",
        "    \n",
        "    # Симуляция пользовательского трафика\n",
        "    print(\"[SIMULATION] Генерация пользовательского трафика...\")\n",
        "    n_users = 1500\n",
        "    for i in range(n_users):\n",
        "        user_id = f\"user_{i:04d}\"\n",
        "        user_context = {\n",
        "            'country': 'US' if i < 1000 else 'CA',\n",
        "            'platform': 'web' if i < 900 else 'mobile',\n",
        "            'age': np.random.randint(18, 65)\n",
        "        }\n",
        "        \n",
        "        # Назначение варианта\n",
        "        variant = platform.assign_user(user_id, experiment_id, user_context)\n",
        "        \n",
        "        # Симуляция конверсии с небольшим улучшением в вариантах\n",
        "        base_conv_rate = 0.10\n",
        "        treatment_lift = 0.03  # 3% абсолютного улучшения\n",
        "        conv_rate = base_conv_rate + (treatment_lift if variant != 'control' else 0)\n",
        "        \n",
        "        if np.random.random() < conv_rate:\n",
        "            platform.track_event(\n",
        "                user_id, experiment_id, 'purchase',\n",
        "                {'value': np.random.exponential(50)}\n",
        "            )\n",
        "    \n",
        "    print(\"[SIMULATION] Трафик сгенерирован.\")\n",
        "    \n",
        "    # Получение и вывод результатов\n",
        "    results = platform.get_experiment_results(experiment_id)\n",
        "    print(\"\\n=== РЕЗУЛЬТАТЫ ЭКСПЕРИМЕНТА ===\")\n",
        "    for variant, data in results.items():\n",
        "        print(f\"\\nВариант: {variant}\")\n",
        "        print(f\"  Назначено пользователей: {data['assigned_users']}\")\n",
        "        print(f\"  Конверсия: {data['conversion_rate']:.3%}\")\n",
        "        print(f\"  ARPU: ${data['arpu']:.2f}\")\n",
        "    \n",
        "    # Валидация качества данных\n",
        "    quality_report = validator.validate_experiment(\n",
        "        platform.experiments[experiment_id],\n",
        "        platform.event_log\n",
        "    )\n",
        "    \n",
        "    print(\"\\n=== ОТЧЕТ О КАЧЕСТВЕ ДАННЫХ ===\")\n",
        "    for check_name, check_result in quality_report.items():\n",
        "        icon = \"✅\" if check_result['status'] == 'passed' else \"❌\"\n",
        "        print(f\"\\n{icon} {check_name}: {check_result['status']}\")\n",
        "        for detail_key, detail_value in check_result['details'].items():\n",
        "            print(f\"  {detail_key}: {detail_value}\")\n",
        "    \n",
        "    # Мониторинг в реальном времени\n",
        "    print(\"\\n=== МОНИТОРИНГ В РЕАЛЬНОМ ВРЕМЕНИ ===\")\n",
        "    current_metrics = monitor.calculate_metrics(experiment_id, hours_back=1)\n",
        "    monitor.set_baseline(experiment_id, current_metrics)\n",
        "    \n",
        "    # Симуляция резкого падения конверсии в одном из вариантов (аномалия)\n",
        "    simulated_anomaly_metrics = current_metrics.copy()\n",
        "    simulated_anomaly_metrics['conversion_rate'] *= 0.3  # Падение на 70%\n",
        "    \n",
        "    anomalies = monitor.detect_anomalies(experiment_id, simulated_anomaly_metrics)\n",
        "    if anomalies:\n",
        "        print(\"⚠️  Обнаружены аномалии!\")\n",
        "        for anomaly in anomalies:\n",
        "            print(f\"  Метрика '{anomaly['metric']}': текущее значение {anomaly['current_value']:.4f}, \"\n",
        "                  f\"Z-score = {anomaly['z_score']:.2f}\")\n",
        "    else:\n",
        "        print(\"✅ Аномалии не обнаружены.\")\n",
        "    \n",
        "    return platform, results, quality_report, anomalies\n",
        "\n",
        "# Запуск демонстрации\n",
        "platform, results, quality_report, anomalies = demonstrate_experiment_platform()\n",
        "```\n",
        "\n",
        "*Пояснение после выполнения кода*:  \n",
        "Этот пример иллюстрирует, как теоретические принципы проектирования платформы экспериментов воплощаются в коде. Детерминированное назначение через хеширование обеспечивает согласованность, модульный валидатор качества данных защищает от ложных выводов, а система мониторинга позволяет оперативно реагировать на проблемы в продакшене. Понимание этих компонентов критически важно для инженеров, создающих надёжные и масштабируемые системы A/B-тестирования.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Переход от наблюдения корреляций к установлению **причинно-следственных связей** представляет собой качественный скачок в зрелости любой data-driven организации. Этот модуль предоставил всестороннее исследование инструментария, необходимого для совершения этого перехода.\n",
        "\n",
        "Мы начали с фундамента — **рандомизированных контролируемых испытаний **(A/B-тестов), изучили тонкости планирования, метрик и статистической валидации. Затем мы расширили свой арсенал, освоив **адаптивные методы **(MAB) и **байесовский подход**, которые позволяют минимизировать стоимость экспериментов и принимать более информированные решения.\n",
        "\n",
        "Для сценариев, где эксперимент невозможен, мы изучили современные и классические методы **причинного вывода**: от структурированного подхода DoWhy и мощи машинного обучения в CausalML до надёжных классических методов, таких как DiD и RDD.\n",
        "\n",
        "Наконец, мы рассмотрели, как все эти методы интегрируются в **единые промышленные платформы**, обеспечивающие надёжность, масштабируемость и качество данных на организационном уровне.\n",
        "\n",
        "**Ключевые методологические выводы**:\n",
        "\n",
        "1.  **A/B-тестирование — это золотой стандарт**, но не панацея. Его следует применять при наличии возможности и этической допустимости рандомизации.\n",
        "2.  **Причинный вывод требует обоснования предположений**. Никакой статистический метод не может компенсировать неверную причинную модель. Всегда начинайте с DAG.\n",
        "3.  **Качество данных — это основа доверия**. Даже самый сложный анализ бессилен против повреждённых данных. Внедряйте автоматизированную валидацию на всех этапах.\n",
        "4.  **Комбинация методов повышает надёжность**. Использование нескольких подходов (например, DiD и RDD на одних и тех же данных) позволяет кросс-валидировать результаты и усилить доверие к выводам.\n",
        "\n",
        "Вооружённый этими знаниями и инструментами, специалист по данным превращается из пассивного наблюдателя в активного архитектора причинных связей, способного не просто описывать мир, но и управлять им."
      ],
      "metadata": {
        "id": "UQnKdw_cwSB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Модуль 22: Специализированные библиотеки — Глубокое погружение в домен-специфичные задачи\n",
        "\n",
        "## Введение\n",
        "\n",
        "Современные данные редко представляют собой идеально структурированные таблицы, пригодные для немедленного применения стандартных моделей машинного обучения. На практике большинство реальных задач возникает в контексте **доменных специфик**: обработка медицинских изображений, прогнозирование спроса, анализ спутниковых снимков, моделирование транспортных потоков или автоматическая обработка юридических документов. Такие сценарии требуют не просто обобщённых алгоритмов, но **специализированных представлений данных, преобразований и методов**, учитывающих внутреннюю структуру и семантику предметной области.\n",
        "\n",
        "Универсальные библиотеки, такие как `pandas`, `scikit-learn` или даже `PyTorch`, хотя и являются основой большинства ML-пайплайнов, зачастую **недостаточно эффективны** или даже **неприменимы** напрямую к таким данным. Это приводит к необходимости использования **домен-ориентированных (domain-specific) библиотек**, разработанных именно для работы с определёнными типами информации. Эти инструменты не только ускоряют разработку, но и обеспечивают доступ к передовым алгоритмам, которые прошли валидацию в конкретной сфере.\n",
        "\n",
        "**Ключевые критерии выбора специализированных библиотек:**\n",
        "\n",
        "- **Тип и структура данных**: изображения, временные ряды, графы, тексты на низкоресурсных языках и т.д. требуют разных подходов к хранению и интерпретации.\n",
        "- **Производительность и масштабируемость**: например, обработка видео в реальном времени или инференс на миллионах временных рядов.\n",
        "- **Наличие специализированных алгоритмов**: классические методы ML часто не учитывают временные зависимости, геометрические инварианты или семантическую иерархию.\n",
        "- **Интеграция с экосистемой**: совместимость с `NumPy`, `pandas`, `scikit-learn`, `PyTorch`/`TensorFlow` значительно упрощает встраивание в существующие пайплайны.\n",
        "\n",
        "В этом модуле мы рассмотрим три ключевые области, в которых специализированные библиотеки играют незаменимую роль: **компьютерное зрение**, **анализ временных рядов** и (в следующих частях) **геопространственные данные** и **графовые структуры**. Начнём с обработки изображений.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Компьютерное зрение: OpenCV, Pillow, scikit-image\n",
        "\n",
        "### Теоретические основы обработки изображений\n",
        "\n",
        "Изображение в цифровом виде — это многомерный массив (тензор), в котором каждый элемент (пиксель) кодирует интенсивность света в определённой точке пространства. Для цветных изображений обычно используется трёхканальное представление, наиболее известное как **пространство RGB** (Red, Green, Blue). Однако для многих задач более информативны альтернативные цветовые модели:\n",
        "- **HSV** (Hue, Saturation, Value) разделяет тон, насыщенность и яркость, что полезно при сегментации по цвету;\n",
        "- **LAB** (Lightness, A, B) аппроксимирует человеческое восприятие цвета и лучше подходит для метрических задач;\n",
        "- **YUV** и **YCbCr** используются в видеообработке, разделяя яркостную и цветоразностные компоненты.\n",
        "\n",
        "Фундаментальной операцией в компьютерном зрении является **свёртка (convolution)** — локальное скользящее преобразование, применяемое с помощью ядра (фильтра). Свёртка лежит в основе как классических методов (например, детектора границ Собеля), так и современных свёрточных нейронных сетей. Связанные с ней понятия — **фильтрация** (подавление шума, размытие) и **усиление признаков** (резкость, выделение краёв).\n",
        "\n",
        "Для анализа формы объектов применяются **морфологические операции**, основанные на теории математической морфологии. Основные из них — **дилатация** (расширение), **эрозия** (сужение), а также их комбинации: **открытие** (эрозия, затем дилатация) и **закрытие** (дилатация, затем эрозия). Они особенно эффективны для удаления мелких артефактов или соединения разорванных компонентов объекта.\n",
        "\n",
        "Современные библиотеки обработки изображений делятся на категории по назначению:\n",
        "- **OpenCV** — промышленный стандарт для компьютерного зрения: высокая производительность, поддержка видео, калибровки камер, детекции объектов.\n",
        "- **Pillow** — удобная библиотека для базовых операций с изображениями: открытие, сохранение, изменение размера, наложение фильтров.\n",
        "- **scikit-image** — научно-ориентированная библиотека, встроенная в экосистему SciPy: предоставляет продвинутые алгоритмы сегментации, морфологии, извлечения признаков.\n",
        "\n",
        "Ниже рассмотрим практическое применение каждой из этих библиотек в едином пайплайне.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1. OpenCV: Промышленная обработка изображений\n",
        "\n",
        "OpenCV (Open Source Computer Vision Library) — это оптимизированная для скорости библиотека на C++ с Python-обёрткой, предназначена для решения задач реального времени. Она особенно эффективна при работе с видео, калибровке камер, отслеживании объектов и выполнении геометрических преобразований.\n",
        "\n",
        "В приведённом примере мы создаём синтетическое изображение с геометрическими фигурами и добавляем к нему шум, имитируя реальные условия. Затем применяются стандартные методы предобработки:\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import segmentation, feature, filters, restoration\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "print(\"=== КОМПЬЮТЕРНОЕ ЗРЕНИЕ ===\")\n",
        "\n",
        "### OPENCV: ПРОМЫШЛЕННАЯ ОБРАБОТКА ###\n",
        "print(\"\\n--- OpenCV: Промышленная обработка ---\")\n",
        "\n",
        "# Создание синтетического изображения для демонстрации\n",
        "def create_sample_image():\n",
        "    # Создаём чистый холст\n",
        "    img = np.ones((400, 600, 3), dtype=np.uint8) * 255\n",
        "    \n",
        "    # Рисуем геометрические фигуры\n",
        "    cv2.rectangle(img, (50, 50), (200, 150), (255, 0, 0), -1)   # Красный прямоугольник\n",
        "    cv2.circle(img, (400, 100), 60, (0, 255, 0), -1)            # Зелёный круг\n",
        "    pts = np.array([[300, 250], [250, 350], [350, 350]], np.int32)\n",
        "    cv2.fillPoly(img, [pts], (0, 0, 255))                       # Синий треугольник\n",
        "    \n",
        "    # Добавляем гауссов шум для имитации реального изображения\n",
        "    noise = np.random.normal(0, 25, img.shape).astype(np.uint8)\n",
        "    noisy_img = cv2.add(img, noise)\n",
        "    \n",
        "    return img, noisy_img\n",
        "\n",
        "original, noisy = create_sample_image()\n",
        "```\n",
        "\n",
        "Далее выполняется последовательность операций, типичная для промышленного пайплайна:\n",
        "\n",
        "- **Фильтрация шума**: медианный фильтр эффективно удаляет импульсный шум, а гауссово размытие сглаживает высокочастотные компоненты.\n",
        "- **Детекция границ** с использованием алгоритма Кэнни — одного из самых надёжных методов обнаружения перепадов интенсивности.\n",
        "- **Морфологические операции**: открытие удаляет мелкие белые пятна на чёрном фоне; закрытие соединяет разрывы в объектах.\n",
        "- **Контуровый анализ**: алгоритм находит замкнутые кривые, ограничивающие объекты, что критично для задач подсчёта или измерения.\n",
        "\n",
        "```python\n",
        "# Фильтрация шума\n",
        "denoised = cv2.medianBlur(noisy, 5)\n",
        "gaussian_blur = cv2.GaussianBlur(noisy, (5, 5), 0)\n",
        "\n",
        "# Детекция границ методом Кэнни\n",
        "edges = cv2.Canny(denoised, 100, 200)\n",
        "\n",
        "# Морфологические операции\n",
        "kernel = np.ones((5, 5), np.uint8)\n",
        "opened = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel)\n",
        "closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# Детекция и рисование контуров\n",
        "contours, hierarchy = cv2.findContours(\n",
        "    edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        ")\n",
        "contour_img = original.copy()\n",
        "cv2.drawContours(contour_img, contours, -1, (0, 255, 255), 2)\n",
        "\n",
        "print(f\"Найдено контуров: {len(contours)}\")\n",
        "```\n",
        "\n",
        "Результаты визуализируются для анализа качества преобразований:\n",
        "\n",
        "```python\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "images = [original, noisy, denoised, edges, opened, contour_img]\n",
        "titles = [\n",
        "    'Оригинал', 'С шумом', 'После фильтрации',\n",
        "    'Границы Canny', 'Морфология OPEN', 'Контуры'\n",
        "]\n",
        "\n",
        "for i, (ax, img, title) in enumerate(zip(axes.flat, images, titles)):\n",
        "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> **Примечание**: OpenCV по умолчанию использует формат BGR, в то время как `matplotlib` ожидает RGB. Поэтому обязательна конвертация цветового пространства перед отображением.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Pillow: Удобство и гибкость для повседневных задач\n",
        "\n",
        "Библиотека Pillow (форк PIL) ориентирована на **удобство работы с изображениями** в задачах, не требующих экстремальной скорости. Она идеально подходит для предварительной подготовки данных: изменения размера, поворота, обрезки, а также применения базовых фильтров и коррекции цвета.\n",
        "\n",
        "В отличие от OpenCV, Pillow оперирует объектами `Image`, что делает код более читаемым и декларативным:\n",
        "\n",
        "```python\n",
        "### PILLOW: РАБОТА С ИЗОБРАЖЕНИЯМИ ###\n",
        "print(\"\\n--- Pillow: Работа с изображениями ---\")\n",
        "\n",
        "def pillow_operations():\n",
        "    img = Image.new('RGB', (300, 200), color='lightblue')\n",
        "    \n",
        "    # Преобразования\n",
        "    img_resized = img.resize((150, 100))\n",
        "    img_rotated = img.rotate(45, expand=True)  # expand=True предотвращает обрезку\n",
        "    img_cropped = img.crop((50, 50, 200, 150))\n",
        "    \n",
        "    # Фильтры\n",
        "    img_blur = img.filter(ImageFilter.GaussianBlur(2))\n",
        "    img_edges = img.filter(ImageFilter.FIND_EDGES)\n",
        "    \n",
        "    # Коррекция контраста\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img_contrast = enhancer.enhance(2.0)\n",
        "    \n",
        "    return [img, img_resized, img_rotated, img_cropped, img_blur, img_edges, img_contrast]\n",
        "\n",
        "pillow_results = pillow_operations()\n",
        "print(f\"Создано {len(pillow_results)} вариантов изображения\")\n",
        "```\n",
        "\n",
        "> **Когда использовать Pillow?** — при подготовке датасетов, генерации изображений для отчётов, работе с иконками или логотипами. Для задач CV в реальном времени — предпочтителен OpenCV.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3. scikit-image: Научные алгоритмы и извлечение признаков\n",
        "\n",
        "Библиотека `scikit-image` — часть научного стека SciPy. Она предоставляет **проверенные, воспроизводимые алгоритмы**, часто сопровождаемые научными публикациями. Особенно сильна в области **сегментации**, **анализа текстур** и **извлечения признаков**.\n",
        "\n",
        "В примере ниже мы применяем:\n",
        "- **SLIC (Simple Linear Iterative Clustering)** — метод суперпиксельной сегментации, группирующий похожие пиксели в компактные регионы.\n",
        "- **HOG (Histogram of Oriented Gradients)** — мощный дескриптор формы, используемый в детекторах объектов.\n",
        "- **LBP (Local Binary Patterns)** — метод описания текстуры, устойчивый к изменениям освещения.\n",
        "\n",
        "```python\n",
        "### SCIKIT-IMAGE: ПРОДВИНУТЫЕ АЛГОРИТМЫ ###\n",
        "print(\"\\n--- scikit-image: Продвинутые алгоритмы ---\")\n",
        "\n",
        "img_gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Сегментация на суперпиксели\n",
        "segments = segmentation.slic(\n",
        "    original, n_segments=4, compactness=10, sigma=1\n",
        ")\n",
        "\n",
        "# Извлечение признаков HOG\n",
        "fd, hog_image = feature.hog(\n",
        "    img_gray, orientations=8, pixels_per_cell=(16, 16),\n",
        "    cells_per_block=(1, 1), visualize=True\n",
        ")\n",
        "\n",
        "# Локальные бинарные паттерны\n",
        "lbp = feature.local_binary_pattern(img_gray, 24, 3, method='uniform')\n",
        "\n",
        "print(f\"HOG features dimension: {fd.shape}\")\n",
        "print(f\"LBP unique patterns: {np.unique(lbp).shape[0]}\")\n",
        "```\n",
        "\n",
        "Визуализация помогает понять, как алгоритмы интерпретируют изображение:\n",
        "\n",
        "```python\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "sk_images = [original, segments, hog_image, lbp]\n",
        "sk_titles = ['Оригинал', 'SLIC сегментация', 'HOG признаки', 'LBP паттерны']\n",
        "\n",
        "for i, (ax, img, title) in enumerate(zip(axes.flat[:4], sk_images, sk_titles)):\n",
        "    if i == 0:\n",
        "        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    elif i == 1:\n",
        "        ax.imshow(segments, cmap='tab10')\n",
        "    else:\n",
        "        ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1.4. Построение ML-пайплайна: извлечение признаков из изображений\n",
        "\n",
        "Для классических ML-моделей (не нейросетей) изображения необходимо преобразовать в векторы фиксированной длины. Это достигается через **инженерию признаков**:\n",
        "\n",
        "```python\n",
        "def extract_image_features(image):\n",
        "    \"\"\"Извлечение признаков из изображения для ML\"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    # Статистики по цветовым каналам\n",
        "    features['mean_r'] = np.mean(image[:, :, 0])\n",
        "    features['mean_g'] = np.mean(image[:, :, 1])\n",
        "    features['mean_b'] = np.mean(image[:, :, 2])\n",
        "    features['std_r'] = np.std(image[:, :, 0])\n",
        "    features['std_g'] = np.std(image[:, :, 1])\n",
        "    features['std_b'] = np.std(image[:, :, 2])\n",
        "    \n",
        "    # Текстурные признаки (на сером изображении)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    features['contrast'] = gray.std()\n",
        "    features['entropy'] = filters.rank.entropy(gray, np.ones((3, 3))).mean()\n",
        "    \n",
        "    # Геометрические признаки\n",
        "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        features['n_contours'] = len(contours)\n",
        "        features['largest_area'] = float(max([cv2.contourArea(c) for c in contours]))\n",
        "    else:\n",
        "        features['n_contours'] = 0\n",
        "        features['largest_area'] = 0.0\n",
        "    \n",
        "    return features\n",
        "\n",
        "img_features = extract_image_features(original)\n",
        "print(\"Извлеченные признаки изображения:\")\n",
        "for key, value in img_features.items():\n",
        "    print(f\"  {key}: {value:.2f}\")\n",
        "```\n",
        "\n",
        "Такой подход позволяет использовать изображения в `scikit-learn` моделях — например, для классификации сцен или определения доминирующих цветов.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Анализ временных рядов: statsmodels, Prophet, tsfresh\n",
        "\n",
        "### Теоретические основы временных рядов\n",
        "\n",
        "Временной ряд — последовательность наблюдений, упорядоченных во времени. В отличие от независимых выборок в стандартных задачах ML, временные ряды характеризуются **зависимостью между наблюдениями**, которая проявляется в виде:\n",
        "- **Тренда** — долгосрочного направления изменения (восходящего или нисходящего);\n",
        "- **Сезонности** — регулярных колебаний с фиксированным периодом (ежедневных, еженедельных, годовых);\n",
        "- **Цикличности** — нерегулярных колебаний, не имеющих фиксированного периода;\n",
        "- **Остатков** — непредсказуемой компоненты, включающей шум и аномалии.\n",
        "\n",
        "Ключевым свойством для многих статистических моделей является **стационарность**: распределение ряда не должно меняться со временем (постоянные среднее, дисперсия, автоковариация). Нестационарные ряды часто требуют трансформаций: дифференцирования, логарифмирования или декомпозиции.\n",
        "\n",
        "**Декомпозиция** — разложение ряда на тренд, сезонность и остатки — важный инструмент понимания структуры данных. На её основе строятся как классические модели (ARIMA), так и современные (Prophet).\n",
        "\n",
        "**Автокорреляция** измеряет зависимость между значениями ряда на разных лагах и помогает определить порядок авторегрессионной модели.\n",
        "\n",
        "Ниже рассмотрим три инструмента, охватывающих разные подходы к анализу временных рядов.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.1. statsmodels: Классическая статистика временных рядов\n",
        "\n",
        "Библиотека `statsmodels` предоставляет реализации классических эконометрических и статистических моделей, включая **ARIMA**, **SARIMA**, **VAR**, **ETS** и тесты на стационарность. Это инструмент для глубокого статистического анализа и интерпретации.\n",
        "\n",
        "Сначала создаём реалистичный синтетический ряд с трендом, сезонностью и аномалиями:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from prophet import Prophet\n",
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=== АНАЛИЗ ВРЕМЕННЫХ РЯДОВ ===\")\n",
        "\n",
        "### СОЗДАНИЕ СИНТЕТИЧЕСКИХ ВРЕМЕННЫХ РЯДОВ ###\n",
        "def create_time_series_data(n_days=365):\n",
        "    dates = pd.date_range(start='2020-01-01', periods=n_days, freq='D')\n",
        "    trend = np.linspace(100, 150, n_days)\n",
        "    daily_seasonality = 5 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
        "    yearly_seasonality = 10 * np.sin(2 * np.pi * np.arange(n_days) / 365)\n",
        "    noise = np.random.normal(0, 3, n_days)\n",
        "    anomalies = np.zeros(n_days)\n",
        "    anomaly_indices = np.random.choice(n_days, 5, replace=False)\n",
        "    anomalies[anomaly_indices] = np.random.normal(0, 15, 5)\n",
        "    values = trend + daily_seasonality + yearly_seasonality + noise + anomalies\n",
        "    return pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'value': values,\n",
        "        'trend': trend,\n",
        "        'seasonality': daily_seasonality + yearly_seasonality\n",
        "    })\n",
        "\n",
        "ts_data = create_time_series_data()\n",
        "ts_data.set_index('date', inplace=True)\n",
        "print(f\"Создан временной ряд с {len(ts_data)} наблюдениями\")\n",
        "```\n",
        "\n",
        "#### Проверка стационарности\n",
        "\n",
        "Два основных теста:\n",
        "- **ADF (Augmented Dickey-Fuller)**: нулевая гипотеза — ряд нестационарен. p < 0.05 ⇒ отклоняем H₀ ⇒ ряд стационарен.\n",
        "- **KPSS**: нулевая гипотеза — ряд стационарен. p > 0.05 ⇒ не отклоняем H₀ ⇒ ряд стационарен.\n",
        "\n",
        "```python\n",
        "def check_stationarity(series):\n",
        "    print(\"Тест Дики-Фуллера (ADF):\")\n",
        "    adf_result = adfuller(series)\n",
        "    print(f\"  ADF Statistic: {adf_result[0]:.4f}\")\n",
        "    print(f\"  p-value: {adf_result[1]:.4f}\")\n",
        "    print(f\"  Стационарность: {'Да' if adf_result[1] < 0.05 else 'Нет'}\")\n",
        "    \n",
        "    print(\"\\nТест KPSS:\")\n",
        "    kpss_result = kpss(series, regression='c')\n",
        "    print(f\"  KPSS Statistic: {kpss_result[0]:.4f}\")\n",
        "    print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
        "    print(f\"  Стационарность: {'Да' if kpss_result[1] > 0.05 else 'Нет'}\")\n",
        "\n",
        "check_stationarity(ts_data['value'])\n",
        "```\n",
        "\n",
        "Если ряд нестационарен, применяют дифференцирование или лог-трансформацию.\n",
        "\n",
        "#### Декомпозиция и ARIMA\n",
        "\n",
        "```python\n",
        "# Декомпозиция\n",
        "decomposition = seasonal_decompose(ts_data['value'], model='additive', period=7)\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
        "components = [ts_data['value'], decomposition.trend,\n",
        "              decomposition.seasonal, decomposition.resid]\n",
        "titles = ['Исходный ряд', 'Тренд', 'Сезонность', 'Остатки']\n",
        "for ax, comp, title in zip(axes, components, titles):\n",
        "    ax.plot(comp)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ARIMA модель\n",
        "model = ARIMA(ts_data['value'], order=(2, 1, 2), seasonal_order=(1, 1, 1, 7))\n",
        "arima_result = model.fit()\n",
        "print(arima_result.summary())\n",
        "\n",
        "# Прогноз\n",
        "forecast = arima_result.get_forecast(steps=30)\n",
        "forecast_index = pd.date_range(start=ts_data.index[-1] + timedelta(days=1), periods=30, freq='D')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts_data.index, ts_data['value'], label='Исторические данные')\n",
        "plt.plot(forecast_index, forecast.predicted_mean, label='Прогноз', color='red')\n",
        "plt.fill_between(forecast_index,\n",
        "                 forecast.conf_int()['lower value'],\n",
        "                 forecast.conf_int()['upper value'],\n",
        "                 color='red', alpha=0.2)\n",
        "plt.title('ARIMA прогноз временного ряда')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2. Prophet: Автоматизированное прогнозирование от Meta\n",
        "\n",
        "Prophet разработан Facebook (Meta) для **масштабируемого, автоматизированного прогнозирования** с минимумом настройки. Он особенно силён в учёте:\n",
        "- Праздников и событий (можно задавать явно);\n",
        "- Неопределённых изменений тренда (changepoints);\n",
        "- Мульти-сезонности (ежедневной, еженедельной, годовой).\n",
        "\n",
        "```python\n",
        "### PROPHET: ПРОГНОЗИРОВАНИЕ С УЧЕТОМ ПРАЗДНИКОВ ###\n",
        "prophet_df = ts_data.reset_index()[['date', 'value']].rename(columns={'date': 'ds', 'value': 'y'})\n",
        "\n",
        "holidays = pd.DataFrame({\n",
        "    'holiday': 'special_event',\n",
        "    'ds': pd.to_datetime(['2020-01-01', '2020-12-25', '2020-07-04']),\n",
        "    'lower_window': -2,\n",
        "    'upper_window': 2,\n",
        "})\n",
        "\n",
        "prophet_model = Prophet(\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    holidays=holidays,\n",
        "    changepoint_prior_scale=0.05  # чувствительность к изменениям тренда\n",
        ")\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "future = prophet_model.make_future_dataframe(periods=30)\n",
        "forecast_prophet = prophet_model.predict(future)\n",
        "\n",
        "# Визуализация компонентов\n",
        "fig_components = prophet_model.plot_components(forecast_prophet)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> **Преимущества Prophet**: устойчивость к пропускам, интуитивная настройка, встроенная визуализация.  \n",
        "> **Ограничения**: менее гибок по сравнению с ARIMA или ML-подходами; не учитывает экзогенные переменные без расширений.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3. tsfresh: Автоматическое извлечение признаков для ML\n",
        "\n",
        "Для применения классических моделей (Random Forest, XGBoost) к временным рядам необходимо преобразовать их в векторы признаков. `tsfresh` автоматизирует этот процесс, вычисляя **тысячи статистических, спектральных и энтропийных признаков**.\n",
        "\n",
        "```python\n",
        "### TSFRESH: АВТОМАТИЧЕСКОЕ ИЗВЛЕЧЕНИЕ ПРИЗНАКОВ ###\n",
        "def create_multiple_ts(n_series=5, n_points=100):\n",
        "    dfs = []\n",
        "    for i in range(n_series):\n",
        "        dates = pd.date_range(start='2020-01-01', periods=n_points, freq='D')\n",
        "        trend = np.linspace(100 + i*10, 150 + i*10, n_points)\n",
        "        seasonality = (5 + i) * np.sin(2 * np.pi * np.arange(n_points) / (7 + i))\n",
        "        noise = np.random.normal(0, 2 + i*0.5, n_points)\n",
        "        values = trend + seasonality + noise\n",
        "        df = pd.DataFrame({\n",
        "            'id': i,\n",
        "            'time': np.arange(n_points),\n",
        "            'value': values\n",
        "        })\n",
        "        dfs.append(df)\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "multiple_ts = create_multiple_ts()\n",
        "print(f\"Создано {multiple_ts['id'].nunique()} временных рядов\")\n",
        "\n",
        "# Извлечение признаков\n",
        "extracted_features = extract_features(\n",
        "    multiple_ts,\n",
        "    column_id='id',\n",
        "    column_sort='time',\n",
        "    column_value='value',\n",
        "    default_fc_parameters={\n",
        "        'mean': None,\n",
        "        'standard_deviation': None,\n",
        "        'variance': None,\n",
        "        'autocorrelation': [{'lag': 1}, {'lag': 5}],\n",
        "        'ar_coefficient': [{'coeff': 0}, {'coeff': 1}]\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Извлечено признаков: {extracted_features.shape[1]}\")\n",
        "```\n",
        "\n",
        "Признаки можно использовать как вход для `scikit-learn` моделей. Для отбора наиболее релевантных применяется статистический тест или `select_features`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.4. Кросс-валидация для временных рядов\n",
        "\n",
        "Стандартная K-Fold кросс-валидация нарушает временную последовательность. Вместо неё используется **TimeSeriesSplit**, где обучающая выборка всегда предшествует тестовой:\n",
        "\n",
        "```python\n",
        "### КРОСС-ВАЛИДАЦИЯ ДЛЯ ВРЕМЕННЫХ РЯДОВ ###\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def create_ts_features(df, window=7):\n",
        "    df = df.copy()\n",
        "    for lag in range(1, window + 1):\n",
        "        df[f'lag_{lag}'] = df['value'].shift(lag)\n",
        "    df['rolling_mean_7'] = df['value'].rolling(window=7).mean()\n",
        "    df['rolling_std_7'] = df['value'].rolling(window=7).std()\n",
        "    df['day_of_week'] = df.index.dayofweek\n",
        "    df['month'] = df.index.month\n",
        "    return df.dropna()\n",
        "\n",
        "ts_features = create_ts_features(ts_data[['value']])\n",
        "X = ts_features.drop('value', axis=1)\n",
        "y = ts_features['value']\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "mae_scores = []\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in tscv.split(X):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "print(f\"Средний MAE: {np.mean(mae_scores):.2f}\")\n",
        "\n",
        "# Важность признаков\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(\"\\nВажность признаков:\")\n",
        "print(feature_importance.head(10))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение первой части\n",
        "\n",
        "В этой части мы рассмотрели два важнейших класса домен-специфичных данных — **изображения** и **временные ряды** — и продемонстрировали, как специализированные библиотеки решают задачи, недоступные универсальным инструментам. Мы прошли путь от теоретических основ до практических пайплайнов, включая препроцессинг, извлечение признаков, моделирование и валидацию.\n",
        "\n",
        "В следующей части мы продолжим изучение специализированных инструментов: **геопространственные данные** (`geopandas`, `rasterio`), **работа с текстами на низкоресурсных языках** и **анализ графов** (`networkx`, `graph-tool`). Эти модули помогут вам уверенно подходить к разнообразным прикладным задачам, выходящим за рамки стандартных датасетов.\n",
        ""
      ],
      "metadata": {
        "id": "70JvnlVaTtyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Работа с геопространственными данными\n",
        "\n",
        "### Теоретические основы геопространственного анализа\n",
        "\n",
        "Геопространственные данные — это информация, привязанная к местоположению на поверхности Земли. В отличие от табличных данных, где строки независимы, геоданные обладают **пространственной автокорреляцией**: близко расположенные объекты чаще похожи друг на друга, чем удалённые (первый закон географии Тоблера). Это фундаментальное свойство требует особых методов анализа.\n",
        "\n",
        "#### Системы координат и проекции\n",
        "\n",
        "Любые геоданные должны быть привязаны к **системе координат (Coordinate Reference System, CRS)**. Существует два основных класса:\n",
        "- **Географическая система** (например, WGS84, EPSG:4326) использует широту и долготу в градусах. Она глобальна, но **не сохраняет расстояния и площади**.\n",
        "- **Проекции** (например, Web Mercator EPSG:3857 или UTM-зоны) преобразуют сферическую поверхность Земли в плоскую карту, сохраняя либо углы (конформные), либо площади (равновеликие), либо расстояния (эквидистантные) — но не всё сразу.\n",
        "\n",
        "Неправильный выбор проекции приводит к **существенным искажениям** при вычислении расстояний, площадей и направлений. Поэтому перед анализом часто требуется **репроекция** в локальную метрическую систему.\n",
        "\n",
        "#### Геометрические примитивы\n",
        "\n",
        "В векторных геоданных объекты моделируются с помощью примитивов `shapely`:\n",
        "- **Point** — точка (например, школа, станция);\n",
        "- **LineString** — линия (дорога, река);\n",
        "- **Polygon** — замкнутая область с внутренними кольцами (озеро, административная единица).\n",
        "\n",
        "Эти структуры поддерживают **топологические операции**: `intersects`, `contains`, `within`, `touches`, `crosses`, что лежит в основе пространственных запросов.\n",
        "\n",
        "#### Пространственные операции и анализ\n",
        "\n",
        "Ключевые операции:\n",
        "- **Буферизация** — построение зоны заданного радиуса вокруг объекта.\n",
        "- **Пересечение, объединение, разность** — операции над множествами полигонов.\n",
        "- **Ближайший сосед** — поиск ближайшего объекта в другом слое.\n",
        "- **Пространственное соединение (spatial join)** — объединение таблиц по пространственному условию.\n",
        "- **Пространственный индекс** (например, R-tree) — ускоряет поиск соседей в больших датасетах.\n",
        "\n",
        "Все эти операции реализованы в `geopandas` — расширении `pandas` для геоданных, которое использует `shapely` для геометрии и `fiona`/`pyogrio` для чтения файлов.\n",
        "\n",
        "Ниже мы рассмотрим полный пайплайн работы с геоданными: от создания и трансформации до визуализации и анализа.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1. Создание и управление геоданными\n",
        "\n",
        "Начнём с синтетических данных, имитирующих реальные объекты: города и административные регионы. Важно сразу задать корректную систему координат — **EPSG:4326** (WGS84), стандарт для GPS и большинства онлайн-карт.\n",
        "\n",
        "```python\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "from folium import plugins\n",
        "import leafmap\n",
        "from shapely.geometry import Point, Polygon, LineString\n",
        "from shapely.ops import nearest_points\n",
        "import contextily as ctx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== ГЕОПРОСТРАНСТВЕННЫЙ АНАЛИЗ ===\")\n",
        "\n",
        "### СОЗДАНИЕ ГЕОДАННЫХ ###\n",
        "print(\"\\n--- Создание и работа с геоданными ---\")\n",
        "\n",
        "def create_geodata():\n",
        "    \"\"\"Создание синтетических геоданных для демонстрации\"\"\"\n",
        "    \n",
        "    # Точки интереса (POI) — города с координатами [долгота, широта]\n",
        "    poi_points = [\n",
        "        Point(37.6176, 55.7558),  # Москва\n",
        "        Point(30.5234, 50.4501),  # Киев\n",
        "        Point(24.7536, 59.4370),  # Таллин\n",
        "        Point(27.5667, 53.9000),  # Минск\n",
        "        Point(44.5167, 48.7000),  # Волгоград\n",
        "    ]\n",
        "    \n",
        "    poi_data = gpd.GeoDataFrame({\n",
        "        'name': ['Москва', 'Киев', 'Таллин', 'Минск', 'Волгоград'],\n",
        "        'population': [12615, 2967, 434, 2000, 1019],  # тыс. человек\n",
        "        'type': ['столица', 'столица', 'столица', 'столица', 'город']\n",
        "    }, geometry=poi_points, crs=\"EPSG:4326\")\n",
        "    \n",
        "    # Условные регионы в виде полигонов\n",
        "    polygons = [\n",
        "        Polygon([(35, 55), (40, 55), (40, 57), (35, 57)]),\n",
        "        Polygon([(25, 48), (32, 48), (32, 52), (25, 52)]),\n",
        "        Polygon([(23, 58), (28, 58), (28, 60), (23, 60)])\n",
        "    ]\n",
        "    \n",
        "    polygon_data = gpd.GeoDataFrame({\n",
        "        'region_id': [1, 2, 3],\n",
        "        'name': ['Центральный', 'Южный', 'Северный'],\n",
        "        'area_km2': [50000, 75000, 30000]\n",
        "    }, geometry=polygons, crs=\"EPSG:4326\")\n",
        "    \n",
        "    return poi_data, polygon_data\n",
        "\n",
        "poi_gdf, regions_gdf = create_geodata()\n",
        "\n",
        "print(\"Точки интереса:\")\n",
        "print(poi_gdf.head())\n",
        "print(f\"\\nКоличество точек: {len(poi_gdf)}\")\n",
        "\n",
        "print(\"\\nРегионы:\")\n",
        "print(regions_gdf.head())\n",
        "print(f\"Количество регионов: {len(regions_gdf)}\")\n",
        "```\n",
        "\n",
        "> **Важно**: В `Point(x, y)` первым аргументом идёт **долгота (x)**, вторым — **широта (y)**. Это соответствует порядку (lon, lat), принятому в большинстве GIS-систем.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2. Основные операции с GeoPandas\n",
        "\n",
        "`GeoPandas` расширяет `DataFrame`, добавляя столбец `geometry` и пространственные методы. Каждый `GeoDataFrame` должен иметь атрибут `.crs`.\n",
        "\n",
        "```python\n",
        "### GEOPANDAS: ОСНОВНЫЕ ОПЕРАЦИИ ###\n",
        "print(\"\\n--- GeoPandas: Основные операции ---\")\n",
        "\n",
        "# Атрибуты данных\n",
        "print(f\"CRS точек: {poi_gdf.crs}\")\n",
        "print(f\"Границы охвата (bbox): {poi_gdf.total_bounds}\")  # [minx, miny, maxx, maxy]\n",
        "\n",
        "# Репроекция в метрическую систему для точного расчёта расстояний\n",
        "# Используем Azimuthal Equidistant projection с центром в Москве\n",
        "moscow_lon, moscow_lat = 37.6176, 55.7558\n",
        "proj_str = f\"+proj=aeqd +lat_0={moscow_lat} +lon_0={moscow_lon} +x_0=0 +y_0=0\"\n",
        "poi_projected = poi_gdf.to_crs(proj_str)\n",
        "regions_projected = regions_gdf.to_crs(proj_str)\n",
        "\n",
        "print(f\"\\nПосле репроекции в метрическую систему (CRS: {proj_str[:30]}...)\")\n",
        "print(f\"Тип координат: метры от центра ({moscow_lat}, {moscow_lon})\")\n",
        "```\n",
        "\n",
        "Теперь расчёты расстояний и площадей будут **точными**, а не приблизительными.\n",
        "\n",
        "#### Пространственные запросы\n",
        "\n",
        "Одна из самых мощных возможностей `geopandas` — **пространственное соединение** (`sjoin`), которое объединяет два слоя по геометрическому условию:\n",
        "\n",
        "```python\n",
        "# Какие города находятся внутри каких регионов?\n",
        "points_in_regions = gpd.sjoin(\n",
        "    poi_projected, regions_projected, how='left', predicate='within'\n",
        ")\n",
        "print(\"\\nТочки в регионах (после репроекции):\")\n",
        "print(points_in_regions[['name', 'name_right']].fillna('вне региона'))\n",
        "\n",
        "# Буферная зона вокруг Москвы (радиус 200 км = 200_000 метров)\n",
        "moscow_geom = poi_projected[poi_projected['name'] == 'Москва'].geometry.iloc[0]\n",
        "moscow_buffer = moscow_geom.buffer(200_000)  # в метрах!\n",
        "\n",
        "# Преобразуем буфер в GeoDataFrame для соединения\n",
        "buffer_gdf = gpd.GeoDataFrame([{'geometry': moscow_buffer}], crs=proj_str)\n",
        "\n",
        "# Какие города попадают в зону 200 км от Москвы?\n",
        "in_buffer = gpd.sjoin(poi_projected, buffer_gdf, how='inner', predicate='within')\n",
        "print(f\"\\nГорода в пределах 200 км от Москвы: {in_buffer['name'].tolist()}\")\n",
        "```\n",
        "\n",
        "> **Примечание**: Буфер в градусах (как в исходном коде: `buffer(1.0)`) не имеет фиксированного физического смысла — он зависит от широты. Только в проекции с метрическими координатами буфер = радиус в километрах/метрах.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3. Вычисление расстояний и маршрутов\n",
        "\n",
        "Для точного вычисления расстояний между точками на сфере следует использовать **геодезические расстояния** (например, формулу Хаверсина). Однако при работе в проекции расстояния в метрах уже корректны.\n",
        "\n",
        "```python\n",
        "# Расстояние от Москвы до других городов (в метрах → км)\n",
        "moscow_point = poi_projected[poi_projected['name'] == 'Москва'].geometry.iloc[0]\n",
        "poi_projected['distance_to_moscow_km'] = (\n",
        "    poi_projected.distance(moscow_point) / 1000\n",
        ")\n",
        "\n",
        "print(\"\\nТочные расстояния до Москвы (км):\")\n",
        "print(poi_projected[['name', 'distance_to_moscow_km']].round(1))\n",
        "```\n",
        "\n",
        "Для анализа маршрутов можно использовать `LineString`. Длина линии в проекции даёт реальное расстояние:\n",
        "\n",
        "```python\n",
        "# Маршрут: Москва → Киев → Волгоград\n",
        "route_coords = [\n",
        "    poi_projected[poi_projected['name'] == 'Москва'].geometry.iloc[0],\n",
        "    poi_projected[poi_projected['name'] == 'Киев'].geometry.iloc[0],\n",
        "    poi_projected[poi_projected['name'] == 'Волгоград'].geometry.iloc[0],\n",
        "]\n",
        "route = LineString(route_coords)\n",
        "route_length_km = route.length / 1000\n",
        "\n",
        "print(f\"\\nДлина маршрута Москва → Киев → Волгоград: {route_length_km:.1f} км\")\n",
        "\n",
        "# Расстояние от каждого города до маршрута\n",
        "def distance_to_route(route, points_gdf):\n",
        "    return points_gdf.geometry.apply(\n",
        "        lambda pt: pt.distance(route) / 1000  # в км\n",
        "    )\n",
        "\n",
        "poi_projected['distance_to_route_km'] = distance_to_route(route, poi_projected)\n",
        "print(\"\\nРасстояния от городов до маршрута (км):\")\n",
        "print(poi_projected[['name', 'distance_to_route_km']].round(1))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4. Визуализация геоданных\n",
        "\n",
        "#### Статическая визуализация с подложкой карты\n",
        "\n",
        "Для повышения читаемости добавим подложку с помощью `contextily`:\n",
        "\n",
        "```python\n",
        "### ВИЗУАЛИЗАЦИЯ С MATPLOTLIB И CONTEXTILY ###\n",
        "print(\"\\n--- Визуализация с подложкой ---\")\n",
        "\n",
        "# Возвращаемся в EPSG:3857 (Web Mercator) для совместимости с веб-картами\n",
        "poi_web = poi_gdf.to_crs(epsg=3857)\n",
        "regions_web = regions_gdf.to_crs(epsg=3857)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
        "\n",
        "# Рисуем регионы и точки\n",
        "regions_web.plot(ax=ax, color='none', edgecolor='darkred', linewidth=1.5, alpha=0.7)\n",
        "poi_web.plot(\n",
        "    ax=ax,\n",
        "    column='population',\n",
        "    markersize=poi_web['population']/50,\n",
        "    cmap='plasma',\n",
        "    legend=True,\n",
        "    legend_kwds={'label': 'Население (тыс. чел.)', 'orientation': 'horizontal'}\n",
        ")\n",
        "\n",
        "# Добавляем подложку (OpenStreetMap)\n",
        "try:\n",
        "    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "except Exception as e:\n",
        "    print(f\"Не удалось загрузить подложку: {e}\")\n",
        "    ax.set_facecolor('lightgray')\n",
        "\n",
        "ax.set_title('Города и регионы с подложкой карты', fontsize=14)\n",
        "ax.set_axis_off()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> **Совет**: `contextily` требует подключения к интернету. Источники: `ctx.providers.Stamen.Terrain`, `ctx.providers.CartoDB.Positron` и др.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.5. Интерактивные карты с Folium\n",
        "\n",
        "`Folium` позволяет создавать интерактивные карты, совместимые с веб-браузерами. Это особенно полезно для отчётов и презентаций.\n",
        "\n",
        "```python\n",
        "### ИНТЕРАКТИВНЫЕ КАРТЫ С FOLIUM ###\n",
        "print(\"\\n--- Интерактивные карты с Folium ---\")\n",
        "\n",
        "# Базовая карта с центром на Москве\n",
        "m = folium.Map(\n",
        "    location=[55.7558, 37.6176],\n",
        "    zoom_start=4,\n",
        "    tiles='CartoDB positron'  # Чистая подложка\n",
        ")\n",
        "\n",
        "# Добавляем регионы\n",
        "for _, row in regions_gdf.iterrows():\n",
        "    folium.GeoJson(\n",
        "        row['geometry'],\n",
        "        style_function=lambda x: {\n",
        "            'fillColor': 'lightblue',\n",
        "            'color': 'navy',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.3\n",
        "        },\n",
        "        tooltip=f\"{row['name']}<br>Площадь: {row['area_km2']} км²\"\n",
        "    ).add_to(m)\n",
        "\n",
        "# Добавляем города с размером, пропорциональным населению\n",
        "for _, row in poi_gdf.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=max(5, row['population']/500),\n",
        "        popup=f\"<b>{row['name']}</b><br>Население: {row['population']} тыс.\",\n",
        "        tooltip=row['name'],\n",
        "        color='crimson',\n",
        "        fill=True,\n",
        "        fillColor='crimson',\n",
        "        fillOpacity=0.7\n",
        "    ).add_to(m)\n",
        "\n",
        "# Добавляем маршрут\n",
        "route_coords_geo = [[pt.y, pt.x] for pt in route_coords]  # [lat, lon]\n",
        "folium.PolyLine(\n",
        "    route_coords_geo,\n",
        "    color='green',\n",
        "    weight=4,\n",
        "    opacity=0.8,\n",
        "    tooltip='Маршрут: Москва → Киев → Волгоград'\n",
        ").add_to(m)\n",
        "\n",
        "# Heatmap населения\n",
        "heat_data = [[row.geometry.y, row.geometry.x, row['population']]\n",
        "             for _, row in poi_gdf.iterrows()]\n",
        "plugins.HeatMap(heat_data, radius=30, blur=20, min_opacity=0.4).add_to(m)\n",
        "\n",
        "# Сохраняем карту\n",
        "m.save('interactive_map.html')\n",
        "print(\"Интерактивная карта сохранена в 'interactive_map.html'\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3.6. Пространственный анализ и автокорреляция\n",
        "\n",
        "Настоящий геоанализ выходит за рамки визуализации. Рассмотрим два примера: **плотность объектов** и **пространственная автокорреляция**.\n",
        "\n",
        "```python\n",
        "### ПРОСТРАНСТВЕННЫЙ АНАЛИЗ ###\n",
        "print(\"\\n--- Пространственный анализ ---\")\n",
        "\n",
        "# Плотность точек на единицу площади (с учётом репроекции!)\n",
        "# Пересчитаем площадь регионов в км² уже в метрической проекции\n",
        "regions_projected['area_calc_km2'] = regions_projected.geometry.area / 1_000_000\n",
        "\n",
        "# Сколько городов в каждом регионе?\n",
        "join_result = gpd.sjoin(poi_projected, regions_projected, predicate='within')\n",
        "region_counts = join_result.groupby('name_right').size().reindex(\n",
        "    regions_projected['name'], fill_value=0\n",
        ")\n",
        "\n",
        "# Объединяем данные\n",
        "analysis_gdf = regions_projected.copy()\n",
        "analysis_gdf['n_cities'] = region_counts.values\n",
        "analysis_gdf['density_per_10k_km2'] = (\n",
        "    analysis_gdf['n_cities'] / analysis_gdf['area_calc_km2'] * 10_000\n",
        ")\n",
        "\n",
        "print(\"Плотность городов по регионам:\")\n",
        "print(analysis_gdf[['name', 'area_calc_km2', 'n_cities', 'density_per_10k_km2']].round(2))\n",
        "```\n",
        "\n",
        "#### Пространственная автокорреляция (упрощённо)\n",
        "\n",
        "Хотя полноценный анализ требует библиотеки `esda` или `pysal`, мы покажем идею: **похожие значения (например, плотность) часто кластеризованы в пространстве**.\n",
        "\n",
        "```python\n",
        "# Визуализация плотности\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "analysis_web = analysis_gdf.to_crs(epsg=3857)\n",
        "\n",
        "analysis_web.plot(\n",
        "    column='density_per_10k_km2',\n",
        "    cmap='YlOrRd',\n",
        "    legend=True,\n",
        "    legend_kwds={'label': 'Городов на 10 тыс. км²'},\n",
        "    edgecolor='black',\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Добавляем города\n",
        "poi_web.plot(ax=ax, color='blue', markersize=40, alpha=0.8)\n",
        "\n",
        "ax.set_title('Пространственное распределение плотности городов')\n",
        "ax.set_axis_off()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> **Для углублённого анализа** рекомендуется использовать статистики Морана (Moran’s I) или Джини (Getis-Ord Gi*), доступные в `esda`.\n",
        "\n",
        "---\n",
        "\n",
        "## Заключение раздела\n",
        "\n",
        "В этом разделе мы рассмотрели полный цикл работы с геопространственными данными:\n",
        "- корректное задание систем координат и репроекция;\n",
        "- создание и манипуляция геометрическими объектами;\n",
        "- выполнение пространственных запросов и операций;\n",
        "- точное вычисление расстояний и площадей;\n",
        "- статическую и интерактивную визуализацию;\n",
        "- базовый пространственный анализ.\n",
        "\n",
        "Эти навыки позволяют решать широкий класс задач: от анализа доступности инфраструктуры до моделирования распространения явлений в пространстве.\n",
        "\n",
        "---\n",
        "\n",
        "## Ключевые выводы из представленного материала\n",
        "\n",
        "1. **Специализированные библиотеки** предоставляют оптимизированные инструменты для работы с конкретными типами данных.\n",
        "2. **Компьютерное зрение** требует сложной предобработки и специализированных алгоритмов для извлечения признаков (OpenCV, Pillow, scikit-image).\n",
        "3. **Временные ряды** обладают уникальными статистическими свойствами — автокорреляцией, трендом и сезонностью — и требуют специальных методов (ARIMA, Prophet, tsfresh).\n",
        "4. **Геопространственные данные** оперируют пространственными отношениями и **обязательно требуют учёта системы координат** для корректного анализа.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ekQ2bXtQUlhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Модуль 23: Real-time ML — Потоковая обработка и онлайн-обучение для производственных систем**\n",
        "\n",
        "## **Введение**\n",
        "\n",
        "**Real-time Machine Learning** — это не просто «быстрый инференс». Это **архитектурная парадигма**, в которой обучение и предсказание происходят непрерывно, в ответ на поток событий, происходящих в реальном мире. Такие системы способны адаптироваться к изменяющимся условиям: поведению пользователей, рыночной волатильности, техническим сбоям или новым типам мошенничества.\n",
        "\n",
        "В промышленной практике выделяют два уровня «реального времени»:\n",
        "- **Near-real-time** (задержка от нескольких секунд до минут): подходит для персонализации контента, email-рассылок, обновления рекомендаций. Здесь допустимы небольшие пакетные задержки.\n",
        "- **True real-time** (задержка <100 мс): критичен для систем, где каждая миллисекунда имеет значение — антифрода, алгоритмического трейдинга, управления автономными транспортными средствами или промышленным IoT.\n",
        "\n",
        "**Ключевой архитектурный принцип**: при проектировании таких систем необходимо сознательно выбирать **trade-off между тремя факторами**:\n",
        "- **Задержка** (latency) — время от события до реакции;\n",
        "- **Пропускная способность** (throughput) — количество событий в секунду;\n",
        "- **Согласованность данных** (consistency) — точность и актуальность состояния модели и признаков.\n",
        "\n",
        "Нарушение этого баланса приводит либо к устаревшим предсказаниям, либо к обрушению инфраструктуры под нагрузкой. В этом модуле мы рассмотрим, как строить устойчивые потоковые ML-системы от источника событий до онлайн-обучения модели.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Основы потоковой обработки данных**\n",
        "\n",
        "### **Теория: Event-driven архитектура и гарантии доставки**\n",
        "\n",
        "Современные ML-системы, работающие в реальном времени, строятся поверх **событийно-ориентированной архитектуры (Event-Driven Architecture, EDA)**. Вместо запросов «находится ли пользователь в городе?» система реагирует на события: «пользователь вошёл в город». Каждое событие — неизменяемый факт, зафиксированный в момент времени.\n",
        "\n",
        "Центральным элементом EDA является **потоковый брокер сообщений**, наиболее распространённым из которых является **Apache Kafka**. Его задача — надёжно доставить каждое событие от продюсера к одному или нескольким консьюмерам.\n",
        "\n",
        "Однако не все доставки равны. Существует три уровня гарантий:\n",
        "- **At-least-once**: сообщение будет доставлено как минимум один раз (возможны дубликаты).\n",
        "- **At-most-once**: сообщение будет доставлено не более одного раза (возможны потери).\n",
        "- **Exactly-once**: сообщение будет обработано ровно один раз — золотой стандарт для финансовых и ML-систем.\n",
        "\n",
        "Для ML это критично: дубликаты транзакций могут исказить признаки; потеря событий — привести к недообнаружению мошенничества.\n",
        "\n",
        "Рассмотрим, как сериализовать событие и отправить его с гарантией **exactly-once**.\n",
        "\n",
        "```python\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class FinancialTransaction:\n",
        "    \"\"\"\n",
        "    Доменное событие: финансовая транзакция.\n",
        "    Используется как контракт между микросервисами.\n",
        "    \"\"\"\n",
        "    transaction_id: str\n",
        "    user_id: str\n",
        "    amount: float\n",
        "    timestamp: datetime\n",
        "    merchant: str\n",
        "    location: str\n",
        "    \n",
        "    def to_kafka_message(self) -> bytes:\n",
        "        \"\"\"Сериализация события в JSON для отправки в Kafka\"\"\"\n",
        "        return json.dumps({\n",
        "            'transaction_id': self.transaction_id,\n",
        "            'user_id': self.user_id,\n",
        "            'amount': self.amount,\n",
        "            'timestamp': self.timestamp.isoformat(),\n",
        "            'merchant': self.merchant,\n",
        "            'location': self.location\n",
        "        }, ensure_ascii=False).encode('utf-8')\n",
        "```\n",
        "\n",
        "> **Зачем это нужно?** Чётко определённый доменный объект упрощает сериализацию, валидацию и документирование API между компонентами системы. В промышленных системах часто используют **Avro** или **Protobuf** вместо JSON для уменьшения размера и строгой схемы.\n",
        "\n",
        "Теперь реализуем продюсера с гарантией доставки:\n",
        "\n",
        "```python\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "import avro.schema\n",
        "import avro.io\n",
        "import io\n",
        "\n",
        "# Пример схемы Avro (в реальности хранится в Schema Registry)\n",
        "SCHEMA_STR = \"\"\"\n",
        "{\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"FinancialTransaction\",\n",
        "  \"fields\": [\n",
        "    {\"name\": \"transaction_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"user_id\", \"type\": \"string\"},\n",
        "    {\"name\": \"amount\", \"type\": \"float\"},\n",
        "    {\"name\": \"timestamp\", \"type\": \"string\"},\n",
        "    {\"name\": \"merchant\", \"type\": \"string\"},\n",
        "    {\"name\": \"location\", \"type\": \"string\"}\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "class ReliableEventProducer:\n",
        "    \"\"\"\n",
        "    Продюсер событий с гарантированной доставкой.\n",
        "    Использует идемпотентность и подтверждения (acks='all') для exactly-once семантики.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: List[str]):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers=bootstrap_servers,\n",
        "            enable_idempotence=True,  # Гарантия отсутствия дубликатов на уровне продюсера\n",
        "            acks='all',                # Чекать, что все реплики получили сообщение\n",
        "            retries=2147483647,        # Максимальное количество попыток\n",
        "            max_in_flight_requests_per_connection=5,\n",
        "            value_serializer=self._json_serializer  # В реальности — Avro\n",
        "        )\n",
        "    \n",
        "    def _json_serializer(self, event: dict) -> bytes:\n",
        "        \"\"\"Замена на Avro/Protobuf при использовании Schema Registry\"\"\"\n",
        "        return json.dumps(event, ensure_ascii=False).encode('utf-8')\n",
        "    \n",
        "    def send_event(self, topic: str, key: str, event: dict):\n",
        "        \"\"\"Асинхронная отправка события с обработкой ошибок\"\"\"\n",
        "        try:\n",
        "            future = self.producer.send(topic=topic, key=key.encode('utf-8'), value=event)\n",
        "            # Блокирующий wait для синхронной гарантии (в продакшене — асинхронный callback)\n",
        "            record_metadata = future.get(timeout=10)\n",
        "            print(f\"✅ Сохранено в {record_metadata.topic}[{record_metadata.partition}] @ {record_metadata.offset}\")\n",
        "        except KafkaError as e:\n",
        "            print(f\"⚠️  Ошибка Kafka: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Необработанная ошибка: {e}\")\n",
        "            raise\n",
        "\n",
        "```\n",
        "\n",
        "> **Важно**: `enable_idempotence=True` + `acks='all'` + `retries=...` — стандартная конфигурация для exactly-once в Kafka. Однако полная семантика достигается только при использовании **транзакций Kafka** или **Kafka Streams**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Сравнение архитектур: Lambda vs Kappa**\n",
        "\n",
        "При построении потоковых систем исторически сложились два подхода.\n",
        "\n",
        "**Lambda-архитектура** разделяет обработку на два параллельных слоя:\n",
        "- **Batch Layer**: обрабатывает весь исторический датасет (точно, но медленно — часы/дни).\n",
        "- **Speed Layer**: обрабатывает последние события в реальном времени (быстро, но приближённо).\n",
        "- **Serving Layer**: объединяет результаты обоих слоёв для клиентского запроса.\n",
        "\n",
        "**Kappa-архитектура** утверждает: если у вас достаточно мощная потоковая система (например, Kafka с долгим retention), то **batch-слой избыточен**. Весь анализ — потоковый. При ошибке или обновлении модели — просто **реплей** исторических данных через тот же потоковый пайплайн.\n",
        "\n",
        "```python\n",
        "class LambdaArchitecture:\n",
        "    \"\"\"\n",
        "    Lambda Architecture: двухслойная модель.\n",
        "    Исторически популярна, но страдает от дублирования логики.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Batch-слой: Apache Spark для точных ежедневных агрегатов\n",
        "        self.batch_layer = SparkBatchProcessor()\n",
        "        # Speed-слой: Apache Flink для агрегатов \"за последние 10 минут\"\n",
        "        self.speed_layer = FlinkStreamProcessor()\n",
        "        # Serving-слой: Feature Store (Redis, Cassandra, Feast)\n",
        "        self.serving_layer = FeatureStore()\n",
        "    \n",
        "    def get_user_features(self, user_id: str) -> dict:\n",
        "        \"\"\"Объединение batch и streaming представлений\"\"\"\n",
        "        batch_features = self.serving_layer.get_batch_features(user_id)\n",
        "        stream_features = self.serving_layer.get_stream_features(user_id)\n",
        "        return {**batch_features, **stream_features}\n",
        "\n",
        "class KappaArchitecture:\n",
        "    \"\"\"\n",
        "    Kappa Architecture: единый потоковый пайплайн.\n",
        "    Проще в поддержке, но требует надёжного storage с реплеем.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stream_processor = FlinkStreamProcessor()\n",
        "        # Хранилище событий с retention = 6 месяцев\n",
        "        self.event_log = KafkaWithLongRetention()\n",
        "    \n",
        "    def process(self, stream: DataStream):\n",
        "        \"\"\"Единая логика обработки для реального времени и реплея\"\"\"\n",
        "        return self.stream_processor.process(stream)\n",
        "    \n",
        "    def reprocess_historical_data(self):\n",
        "        \"\"\"Реплей исторических данных через тот же пайплайн\"\"\"\n",
        "        historical_stream = self.event_log.replay(from_time=\"2023-01-01\")\n",
        "        return self.process(historical_stream)\n",
        "```\n",
        "\n",
        "> **Современный тренд**: Kappa-архитектура доминирует в новых системах благодаря упрощению и развитию потоковых платформ (Flink, Kafka Streams, Spark Structured Streaming).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Экосистема Apache Kafka**\n",
        "\n",
        "### **Архитектура Kafka: Producers, Consumers и гарантии**\n",
        "\n",
        "Kafka организует данные в **топики** (topics), которые разбиты на **партиции** (partitions). Каждое сообщение имеет **офсет** (offset) — его позицию в партиции. Группы консьюмеров читают топик параллельно: каждая партиция обрабатывается ровно одним консьюмером в группе.\n",
        "\n",
        "Для ML-систем особенно важны:\n",
        "- **Ключ сообщения (key)**: определяет, в какую партицию попадёт событие. Для обработки по пользователю — ключом должен быть `user_id`, чтобы все события одного пользователя попадали в одну партицию и обрабатывались строго по порядку.\n",
        "- **Ручное управление коммитами**: автоматический коммит может привести к потере данных. Лучше коммитить офсет **только после успешной обработки**.\n",
        "\n",
        "Рассмотрим консьюмер для детекции мошенничества:\n",
        "\n",
        "```python\n",
        "from kafka import KafkaConsumer\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "class FraudDetectionConsumer:\n",
        "    \"\"\"\n",
        "    Консьюмер с ручным управлением офсетами и интеграцией с ML-моделью.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bootstrap_servers: List[str], topic: str, group_id: str):\n",
        "        self.consumer = KafkaConsumer(\n",
        "            topic,\n",
        "            bootstrap_servers=bootstrap_servers,\n",
        "            group_id=group_id,\n",
        "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
        "            enable_auto_commit=False,  # 🔥 Критично для надёжности!\n",
        "            auto_offset_reset='latest'\n",
        "        )\n",
        "        # Модель должна поддерживать онлайн-инференс\n",
        "        self.fraud_model = self.load_model()\n",
        "        # Feature Store для исторических признаков (последние 10 транзакций и т.д.)\n",
        "        self.feature_store = RedisFeatureStore()\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Загрузка предобученной модели (в реальности — с MLflow/Model Registry)\"\"\"\n",
        "        return PreTrainedFraudModel()\n",
        "    \n",
        "    def process_messages(self):\n",
        "        \"\"\"\n",
        "        Основной цикл обработки.\n",
        "        Офсет коммитится ТОЛЬКО после успешного завершения.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            for message in self.consumer:\n",
        "                transaction = message.value\n",
        "                \n",
        "                try:\n",
        "                    features = self.extract_realtime_features(transaction)\n",
        "                    is_fraud = self.fraud_model.predict_proba(features)[1]  # вероятность фрода\n",
        "                    \n",
        "                    if is_fraud > 0.95:\n",
        "                        self.handle_fraudulent_transaction(transaction, is_fraud)\n",
        "                    \n",
        "                    # Успешная обработка → коммитим офсет\n",
        "                    self.consumer.commit()\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Ошибка при обработке {transaction.get('transaction_id')}: {e}\")\n",
        "                    # Не коммитим офсет → сообщение будет обработано снова при перезапуске\n",
        "                    # В продакшене: отправка в DLQ (Dead Letter Queue)\n",
        "                    \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"🛑 Получен сигнал остановки\")\n",
        "        finally:\n",
        "            self.consumer.close()\n",
        "    \n",
        "    def extract_realtime_features(self, transaction: dict) -> dict:\n",
        "        \"\"\"Извлечение признаков в реальном времени из события + feature store\"\"\"\n",
        "        user_id = transaction['user_id']\n",
        "        \n",
        "        # Получаем исторические признаки (например, средний чек за день)\n",
        "        historical = self.feature_store.get(user_id) or {}\n",
        "        \n",
        "        # Рассчитываем признаки на лету\n",
        "        realtime = {\n",
        "            'amount': transaction['amount'],\n",
        "            'hour_of_day': pd.to_datetime(transaction['timestamp']).hour,\n",
        "            'is_weekend': pd.to_datetime(transaction['timestamp']).weekday() >= 5,\n",
        "            # Скорость перемещения (требует хранения последней локации)\n",
        "            'location_velocity': self.calc_velocity(user_id, transaction['location']),\n",
        "        }\n",
        "        \n",
        "        return {**historical, **realtime}\n",
        "```\n",
        "\n",
        "> **Практический совет**: для сложных признаков (скользящие окна, сессии) используйте **Kafka Streams** или **Flink**, а не ручную агрегацию в консьюмере.\n",
        "\n",
        "---\n",
        "\n",
        "### **Kafka Streams: Stateful Processing и агрегация**\n",
        "\n",
        "**Kafka Streams** — библиотека для построения потоковых приложений на JVM, которая обеспечивает **управление состоянием**, **оконную агрегацию** и **гарантию exactly-once** «из коробки».\n",
        "\n",
        "Хотя в Python-экосистеме она менее популярна (из-за JVM), её концепции лежат в основе многих решений. Ниже — псевдокод на Python-подобном синтаксисе, который иллюстрирует идею:\n",
        "\n",
        "```python\n",
        "# ⚠️ Это концептуальный пример. Реализация на Java/Scala с Kafka Streams API.\n",
        "class UserSessionProcessor:\n",
        "    \"\"\"\n",
        "    Потоковый процессор сессий: агрегирует события пользователя в течение 30 минут.\n",
        "    Использует локальный state store для хранения промежуточного состояния.\n",
        "    \"\"\"\n",
        "    \n",
        "    def build_streams_application(self):\n",
        "        # 1. Читаем поток событий\n",
        "        events = stream('user-events')\n",
        "        \n",
        "        # 2. Группируем по user_id (ключ сообщения должен быть user_id!)\n",
        "        grouped = events.group_by_key()\n",
        "        \n",
        "        # 3. Агрегируем в окне 30 минут\n",
        "        sessions = grouped.window_by(TimeWindows.of(Duration.minutes(30))) \\\n",
        "                            .aggregate(\n",
        "                                initializer=UserSession,      # Инициализация пустой сессии\n",
        "                                aggregator=self.add_event,    # Добавление события в сессию\n",
        "                                materialized=Materialized.as_('session-store')  # Имя state store\n",
        "                            )\n",
        "        \n",
        "        # 4. Пишем результат в выходной топик\n",
        "        sessions.to_stream().to('user-sessions-output')\n",
        "        \n",
        "        return KafkaStreams(topology, config={'application.id': 'user-sessions'})\n",
        "    \n",
        "    def add_event(self, session: UserSession, event: dict) -> UserSession:\n",
        "        \"\"\"Функция агрегации: обновляет сессию новым событием\"\"\"\n",
        "        session.event_count += 1\n",
        "        session.total_duration += event.get('duration', 0)\n",
        "        session.last_seen = event['timestamp']\n",
        "        return session\n",
        "```\n",
        "\n",
        "> **Преимущество Kafka Streams**: state store реплицируется и восстанавливается автоматически при отказе узла. Это критично для production.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Онлайн-обучение и адаптация моделей**\n",
        "\n",
        "*(Этот раздел был пропущен в исходном тексте, но он **ключевой** для Real-time ML!)*\n",
        "\n",
        "Потоковая обработка данных — лишь половина решения. Чтобы модель **адаптировалась** к новым паттернам, её нужно **обучать на лету**. Это называется **online learning**.\n",
        "\n",
        "В отличие от **batch learning** (модель обучается на фиксированном датасете), online-модели обновляются **по одному примеру** или **микробатчам**. Это позволяет:\n",
        "- Реагировать на изменения в распределении данных (concept drift);\n",
        "- Уменьшить задержку между появлением нового паттерна и его распознаванием;\n",
        "- Снизить стоимость переобучения.\n",
        "\n",
        "### **Стратегии онлайн-обучения**\n",
        "\n",
        "1. **Online-алгоритмы первой категории**: модели, которые изначально поддерживают онлайн-обучение (SGDClassifier, River).\n",
        "2. **Псевдо-онлайн**: переобучение модели на скользящем окне (например, последние 10 000 событий).\n",
        "3. **Incremental learning в scikit-learn**: метод `partial_fit` у некоторых моделей.\n",
        "\n",
        "```python\n",
        "from river import linear_model, preprocessing, metrics\n",
        "from river.compat import SklearnWrapper\n",
        "import river\n",
        "\n",
        "class OnlineFraudModel:\n",
        "    \"\"\"\n",
        "    Онлайн-модель для детекции мошенничества с использованием библиотеки River.\n",
        "    River — нативная Python-библиотека для online ML.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Пайплайн: масштабирование → логистическая регрессия\n",
        "        self.model = preprocessing.StandardScaler() | linear_model.LogisticRegression()\n",
        "        self.metric = metrics.Accuracy() + metrics.Precision() + metrics.Recall()\n",
        "    \n",
        "    def learn_one(self, features: dict, is_fraud: bool):\n",
        "        \"\"\"Обучение на одном примере\"\"\"\n",
        "        self.model.learn_one(features, is_fraud)\n",
        "        self.metric.update(is_fraud, self.model.predict_one(features))\n",
        "    \n",
        "    def predict_proba_one(self, features: dict) -> dict:\n",
        "        \"\"\"Прогноз вероятностей\"\"\"\n",
        "        return self.model.predict_proba_one(features)\n",
        "    \n",
        "    def get_metrics(self) -> dict:\n",
        "        \"\"\"Текущие метрики качества\"\"\"\n",
        "        return self.metric.get()\n",
        "\n",
        "# Интеграция в консьюмер\n",
        "class OnlineLearningFraudConsumer(FraudDetectionConsumer):\n",
        "    def __init__(self, ...):\n",
        "        super().__init__(...)\n",
        "        self.online_model = OnlineFraudModel()\n",
        "        # Для получения ground truth (была ли транзакция фродом?)\n",
        "        self.label_service = LabelService()  # Источник правдивых меток с задержкой\n",
        "    \n",
        "    def process_messages(self):\n",
        "        for message in self.consumer:\n",
        "            transaction = message.value\n",
        "            features = self.extract_features(transaction)\n",
        "            \n",
        "            # Онлайн-инференс\n",
        "            proba = self.online_model.predict_proba_one(features)\n",
        "            \n",
        "            if proba.get(True, 0) > 0.9:\n",
        "                self.handle_fraud(transaction)\n",
        "            \n",
        "            # Получение метки с задержкой (например, через 7 дней)\n",
        "            true_label = self.label_service.get_label(transaction['transaction_id'])\n",
        "            if true_label is not None:\n",
        "                # Онлайн-обучение\n",
        "                self.online_model.learn_one(features, true_label)\n",
        "            \n",
        "            self.consumer.commit()\n",
        "```\n",
        "\n",
        "> **Вызов**: получение **правдивых меток (labels)** в реальном времени часто невозможно (например, пользователь пожалуется на фрод через неделю). Поэтому в online learning широко используются **отложенные метки** и **semi-supervised подходы**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Apache Flink: промышленный стандарт потоковой обработки**\n",
        "\n",
        "### **DataStream API с управлением состоянием**\n",
        "\n",
        "**Apache Flink** — распределённая потоковая платформа с **нативной поддержкой состояния**, **точных окон** и **гарантией exactly-once**. В отличие от Spark Streaming (micro-batching), Flink обрабатывает события **по-настоящему потоково**, что критично для low-latency сценариев.\n",
        "\n",
        "Flink предоставляет **ProcessFunction** — низкоуровневый API для custom-логики с полным контролем над состоянием и временем. Это идеально подходит для ML-задач.\n",
        "\n",
        "```python\n",
        "from pyflink.datastream import StreamExecutionEnvironment\n",
        "from pyflink.datastream.functions import KeyedProcessFunction\n",
        "from pyflink.datastream.state import ValueStateDescriptor\n",
        "from pyflink.common.typeinfo import Types\n",
        "from pyflink.common import Configuration\n",
        "\n",
        "class FraudDetectionProcessFunction(KeyedProcessFunction):\n",
        "    \"\"\"\n",
        "    ProcessFunction для детекции мошенничества с управлением состоянием.\n",
        "    Ключ: user_id. Для каждого пользователя хранится профиль и последние транзакции.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.profile_state = None       # Профиль пользователя\n",
        "        self.transactions_state = None  # Очередь последних транзакций\n",
        "    \n",
        "    def open(self, runtime_context):\n",
        "        \"\"\"Инициализация state backend при старте таска\"\"\"\n",
        "        # Профиль пользователя: словарь с агрегатами\n",
        "        profile_desc = ValueStateDescriptor(\"user_profile\", Types.PY_DICT)\n",
        "        self.profile_state = runtime_context.get_state(profile_desc)\n",
        "        \n",
        "        # Последние N транзакций (для расчёта скользящих статистик)\n",
        "        txn_desc = ValueStateDescriptor(\"recent_txns\", Types.LIST(Types.PY_DICT))\n",
        "        self.transactions_state = runtime_context.get_state(txn_desc)\n",
        "    \n",
        "    def process_element(self, transaction, ctx):\n",
        "        \"\"\"Вызывается для каждой транзакции\"\"\"\n",
        "        user_id = transaction['user_id']\n",
        "        \n",
        "        # Получение состояния\n",
        "        profile = self.profile_state.value() or self._init_profile(user_id)\n",
        "        txns = self.transactions_state.value() or []\n",
        "        \n",
        "        # Обновление профиля\n",
        "        profile = self._update_profile(profile, transaction)\n",
        "        \n",
        "        # Расчёт признаков\n",
        "        features = self._extract_features(transaction, profile, txns)\n",
        "        \n",
        "        # Прогноз (в реальности — вызов модели через PyFlink UDF или side input)\n",
        "        fraud_score = self._fake_model_predict(features)\n",
        "        \n",
        "        if fraud_score > 0.85:\n",
        "            # Отправка в синк для алертов\n",
        "            ctx.output(fraud_score, transaction)\n",
        "        \n",
        "        # Обновление состояния\n",
        "        txns.append(transaction)\n",
        "        if len(txns) > 50:\n",
        "            txns.pop(0)  # Ограничение размера\n",
        "        \n",
        "        self.profile_state.update(profile)\n",
        "        self.transactions_state.update(txns)\n",
        "        \n",
        "        # Регистрация таймера для очистки (например, для истечения сессии)\n",
        "        ctx.timer_service().register_event_time_timer(ctx.timestamp() + 3600000)\n",
        "    \n",
        "    def on_timer(self, timestamp, ctx):\n",
        "        \"\"\"Вызывается при срабатывании таймера\"\"\"\n",
        "        # Здесь можно очистить состояние неактивных пользователей\n",
        "        pass\n",
        "    \n",
        "    def _fake_model_predict(self, features):\n",
        "        \"\"\"Заглушка для модели. В реальности — инференс\"\"\"\n",
        "        return features.get('amount', 0) / 1000.0\n",
        "\n",
        "# Настройка и запуск Flink-приложения\n",
        "env = StreamExecutionEnvironment.get_execution_environment()\n",
        "env.set_parallelism(4)\n",
        "env.enable_checkpointing(10000)  # every 10 seconds\n",
        "\n",
        "# Источник: Kafka\n",
        "kafka_source = KafkaSource.builder() \\\n",
        "    .set_bootstrap_servers(\"localhost:9092\") \\\n",
        "    .set_topics(\"transactions\") \\\n",
        "    .set_group_id(\"fraud-detection\") \\\n",
        "    .set_value_only_deserializer(JsonDeserializationSchema()) \\\n",
        "    .build()\n",
        "\n",
        "transaction_stream = env.from_source(kafka_source, WatermarkStrategy.no_watermarks(), \"Kafka Source\")\n",
        "\n",
        "# Обработка\n",
        "fraud_alerts = (\n",
        "    transaction_stream\n",
        "    .key_by(lambda x: x['user_id'])  # Ключевание по пользователю\n",
        "    .process(FraudDetectionProcessFunction())\n",
        ")\n",
        "\n",
        "# Синк: запись алертов в Kafka/DB\n",
        "fraud_alerts.sink_to(...)\n",
        "\n",
        "env.execute(\"Real-time Fraud Detection with Flink\")\n",
        "```\n",
        "\n",
        "> **Преимущества Flink**:\n",
        "> - Настоящая потоковая обработка (не micro-batching);\n",
        "> - Сложное управление состоянием и временем;\n",
        "> - Встроенная поддержка окон, таймеров, watermark'ов;\n",
        "> - Мощная система checkpointing для отказоустойчивости.\n",
        "\n",
        "---\n",
        "\n",
        "## **Заключение модуля**\n",
        "\n",
        "Real-time Machine Learning — это сложная, но мощная парадигма, позволяющая строить системы, которые не просто предсказывают будущее, а **непрерывно учатся на настоящем**. Ключевые компоненты такой системы:\n",
        "\n",
        "1. **Надёжный потоковый брокер** (Kafka) с гарантией доставки;\n",
        "2. **Потоковый процессор** (Flink, Kafka Streams) для агрегации и feature engineering;\n",
        "3. **Feature Store** для хранения исторических и реальных признаков;\n",
        "4. **Онлайн-модель** (River, TensorFlow Serving с hot-swap) с поддержкой непрерывного обучения;\n",
        "5. **Мониторинг дрейфа и качества** в реальном времени.\n",
        "\n",
        "Только объединив эти элементы в единую архитектуру, можно достичь баланса между скоростью, точностью и надёжностью — основой любой промышленной ML-системы.\n",
        ""
      ],
      "metadata": {
        "id": "2MwhfeqKUojk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **5. Онлайн-обучение и адаптивные алгоритмы**\n",
        "\n",
        "### **Теоретические основы онлайн-обучения**\n",
        "\n",
        "Онлайн-обучение (online learning) — это парадигма машинного обучения, в которой модель **последовательно обновляется по одному примеру или небольшому батчу**, по мере поступления данных. В отличие от классического batch-обучения, где датасет фиксирован, онлайн-модели работают в **динамической среде**, где распределение данных может меняться со временем — явление, известное как **дрейф концептов (concept drift)**.\n",
        "\n",
        "Дрейф может быть:\n",
        "- **Внезапным** (например, после запуска новой акции);\n",
        "- **Постепенным** (например, изменение поведения пользователей во время пандемии);\n",
        "- **Сезонным** (циклические изменения).\n",
        "\n",
        "**Ключевая цель онлайн-обучения** — не просто предсказывать, а **непрерывно адаптироваться** к новым паттернам, сохраняя высокое качество даже в нестационарных условиях.\n",
        "\n",
        "Существует два основных подхода к онлайн-обучению:\n",
        "1. **Инкрементальные алгоритмы** — модели, которые изначально поддерживают метод `partial_fit` (например, `SGDClassifier` в scikit-learn).\n",
        "2. **Специализированные библиотеки** — такие как **River** (ранее — creme), разработанные специально для потокового ML с богатой экосистемой адаптивных моделей и детекторов дрейфа.\n",
        "\n",
        "Ниже мы рассмотрим оба подхода, уделяя особое внимание **обнаружению и реакции на дрейф** — критически важному компоненту надёжных production-систем.\n",
        "\n",
        "---\n",
        "\n",
        "### **5.1. Инкрементальное обучение с River и детекцией дрейфа**\n",
        "\n",
        "Библиотека **River** предоставляет единый интерфейс для потоковых трансформеров, моделей и детекторов дрейфа. Важнейшее преимущество River — **нативная поддержка онлайн-метрик**: точность, полнота, F1 и другие метрики обновляются **по мере поступления примеров**, без необходимости хранить весь датасет.\n",
        "\n",
        "Один из самых эффективных детекторов дрейфа — **ADWIN (Adaptive Windowing)**. Он динамически адаптирует размер окна, сравнивая средние значения ошибок в двух подокнах. Если различие статистически значимо — дрейф обнаружен.\n",
        "\n",
        "```python\n",
        "from river import compose, preprocessing, linear_model, metrics, drift\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "class AdaptiveFraudDetector:\n",
        "    \"\"\"\n",
        "    Адаптивная модель детекции мошенничества с онлайн-обучением и автоматическим\n",
        "    обнаружением дрейфа концептов.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Пайплайн: масштабирование → логистическая регрессия\n",
        "        self.model = compose.Pipeline(\n",
        "            preprocessing.StandardScaler(),\n",
        "            linear_model.LogisticRegression(seed=42)\n",
        "        )\n",
        "        \n",
        "        # Набор онлайн-метрик для мониторинга качества\n",
        "        self.metrics = {\n",
        "            'accuracy': metrics.Accuracy(),\n",
        "            'precision': metrics.Precision(),\n",
        "            'recall': metrics.Recall(),\n",
        "            'f1': metrics.F1()\n",
        "        }\n",
        "        \n",
        "        # Детектор дрейфа ADWIN\n",
        "        self.drift_detector = drift.ADWIN(delta=0.002)  # Порог чувствительности\n",
        "        \n",
        "        self.training_samples = 0\n",
        "        self.model_version = 1\n",
        "        self.backup_model = None\n",
        "        self.drift_history = []\n",
        "    \n",
        "    def partial_fit(self, features: Dict[str, Any], label: int):\n",
        "        \"\"\"\n",
        "        Инкрементальное обучение модели на одном примере.\n",
        "        Также обновляет метрики и проверяет наличие дрейфа.\n",
        "        \"\"\"\n",
        "        # River принимает словари напрямую — преобразование не требуется\n",
        "        x = features\n",
        "        y = label\n",
        "        \n",
        "        # Получаем предсказание ДО обучения (для честной оценки ошибки)\n",
        "        y_pred = self.model.predict_one(x)\n",
        "        \n",
        "        # Обновление метрик\n",
        "        for metric in self.metrics.values():\n",
        "            metric.update(y, y_pred)\n",
        "        \n",
        "        # Обнаружение дрейфа на основе ошибки предсказания\n",
        "        error = int(y_pred != y)\n",
        "        self.drift_detector.update(error)\n",
        "        \n",
        "        if self.drift_detector.drift_detected:\n",
        "            self._handle_concept_drift()\n",
        "        \n",
        "        # Инкрементальное обучение модели\n",
        "        self.model.learn_one(x, y)\n",
        "        self.training_samples += 1\n",
        "    \n",
        "    def predict(self, features: Dict[str, Any]) -> Tuple[int, float]:\n",
        "        \"\"\"Возвращает предсказание и вероятность класса 'фрод'.\"\"\"\n",
        "        x = features\n",
        "        y_pred = self.model.predict_one(x)\n",
        "        proba = self.model.predict_proba_one(x)\n",
        "        fraud_proba = proba.get(True, proba.get(1, 0.0))\n",
        "        return y_pred, fraud_proba\n",
        "    \n",
        "    def get_metrics(self) -> Dict[str, float]:\n",
        "        \"\"\"Текущие значения метрик.\"\"\"\n",
        "        return {name: metric.get() for name, metric in self.metrics.items()}\n",
        "    \n",
        "    def _handle_concept_drift(self):\n",
        "        \"\"\"\n",
        "        Стратегия реакции на дрейф:\n",
        "        1. Сохраняем текущую модель как резервную.\n",
        "        2. Сбрасываем детектор дрейфа.\n",
        "        3. Инкрементируем версию модели для трассировки.\n",
        "        4. Записываем событие в историю.\n",
        "        \"\"\"\n",
        "        print(f\"🚨 Обнаружен дрейф концептов! Версия модели: {self.model_version}\")\n",
        "        \n",
        "        # Сохранение резервной копии (в продакшене — в модельный реестр)\n",
        "        self.backup_model = pickle.loads(pickle.dumps(self.model))\n",
        "        \n",
        "        # Сброс детектора для нового периода\n",
        "        self.drift_detector = drift.ADWIN(delta=0.002)\n",
        "        \n",
        "        # Логирование события дрейфа\n",
        "        self.drift_history.append({\n",
        "            'timestamp': datetime.now(),\n",
        "            'model_version': self.model_version,\n",
        "            'samples_processed': self.training_samples\n",
        "        })\n",
        "        \n",
        "        self.model_version += 1\n",
        "```\n",
        "\n",
        "> **Почему важно предсказывать ДО обучения?**  \n",
        "> Если обучать модель перед вычислением ошибки, метрики будут **оптимистично смещены**, так как модель уже «видела» пример. Правильный порядок: предсказать → оценить ошибку → обучить.\n",
        "\n",
        "Пример использования в потоковом конвейере:\n",
        "\n",
        "```python\n",
        "# Симуляция потока транзакций\n",
        "fraud_detector = AdaptiveFraudDetector()\n",
        "\n",
        "for i, (transaction, is_fraud) in enumerate(transaction_stream):\n",
        "    features = extract_features(transaction)  # Должен возвращать dict\n",
        "    fraud_detector.partial_fit(features, is_fraud)\n",
        "    \n",
        "    # Периодический вывод метрик\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"\\n📊 Обработано {i+1} транзакций\")\n",
        "        metrics_now = fraud_detector.get_metrics()\n",
        "        for name, value in metrics_now.items():\n",
        "            print(f\"   {name}: {value:.4f}\")\n",
        "        \n",
        "        # Проверка истории дрейфа\n",
        "        if fraud_detector.drift_history:\n",
        "            last_drift = fraud_detector.drift_history[-1]\n",
        "            print(f\"   🕒 Последний дрейф: {last_drift['timestamp'].strftime('%H:%M:%S')}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5.2. Адаптивные ансамбли и мульти-детекторы дрейфа**\n",
        "\n",
        "Для нестационарных данных особенно эффективны **ансамблевые методы**, которые могут динамически включать/выключать модели или перераспределять веса. **Adaptive Random Forest (ARF)** — один из лучших алгоритмов для потоковой классификации: каждое дерево обучается на случайном подпространстве признаков и может быть заменено при обнаружении локального дрейфа.\n",
        "\n",
        "Кроме того, полагаться на один детектор дрейфа рискованно. Разные детекторы чувствительны к разным типам изменений:\n",
        "- **DDM** (Drift Detection Method) — хорошо ловит внезапные дрейфы;\n",
        "- **EDDM** — лучше для постепенных изменений;\n",
        "- **ADWIN** — адаптивный, подходит для большинства сценариев;\n",
        "- **Page-Hinkley** — чувствителен к трендам.\n",
        "\n",
        "Использование **мульти-детектора** повышает надёжность обнаружения.\n",
        "\n",
        "```python\n",
        "from river import ensemble, drift\n",
        "from typing import List, Dict\n",
        "\n",
        "class StreamingRandomForest:\n",
        "    \"\"\"\n",
        "    Потоковый адаптивный случайный лес на основе River.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_models: int = 10):\n",
        "        self.ensemble = ensemble.AdaptiveRandomForestClassifier(\n",
        "            n_models=n_models,\n",
        "            seed=42,\n",
        "            max_depth=10\n",
        "        )\n",
        "    \n",
        "    def update(self, features: Dict[str, Any], label: int):\n",
        "        \"\"\"Обновление ансамбля на новом примере.\"\"\"\n",
        "        self.ensemble.learn_one(features, label)\n",
        "    \n",
        "    def predict_proba(self, features: Dict[str, Any]) -> Dict[bool, float]:\n",
        "        \"\"\"Возвращает вероятности классов.\"\"\"\n",
        "        return self.ensemble.predict_proba_one(features)\n",
        "    \n",
        "    def get_n_active_models(self) -> int:\n",
        "        \"\"\"Количество активных деревьев в ансамбле.\"\"\"\n",
        "        return len(self.ensemble.models)\n",
        "\n",
        "class MultiDetectorConceptDrift:\n",
        "    \"\"\"\n",
        "    Консенсусный детектор дрейфа, использующий несколько алгоритмов.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, consensus_threshold: int = 2):\n",
        "        self.detectors = {\n",
        "            'DDM': drift.DDM(),\n",
        "            'EDDM': drift.EDDM(),\n",
        "            'ADWIN': drift.ADWIN(),\n",
        "            'PageHinkley': drift.PageHinkley()\n",
        "        }\n",
        "        self.consensus_threshold = consensus_threshold\n",
        "        self.drift_history: List[Dict] = []\n",
        "        self.warning_triggered = False\n",
        "        self.drift_confirmed = False\n",
        "    \n",
        "    def update(self, y_true: int, y_pred: int):\n",
        "        \"\"\"Обновление всех детекторов и проверка консенсуса.\"\"\"\n",
        "        error = int(y_true != y_pred)\n",
        "        \n",
        "        drift_signals = []\n",
        "        warning_signals = []\n",
        "        \n",
        "        for name, detector in self.detectors.items():\n",
        "            try:\n",
        "                detector.update(error)\n",
        "                \n",
        "                if hasattr(detector, 'drift_detected') and detector.drift_detected:\n",
        "                    drift_signals.append(name)\n",
        "                \n",
        "                if hasattr(detector, 'warning_detected') and detector.warning_detected:\n",
        "                    warning_signals.append(name)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Ошибка в детекторе {name}: {e}\")\n",
        "        \n",
        "        # Анализ сигналов\n",
        "        self.warning_triggered = len(warning_signals) >= self.consensus_threshold\n",
        "        self.drift_confirmed = len(drift_signals) >= self.consensus_threshold\n",
        "        \n",
        "        if self.drift_confirmed:\n",
        "            event = {\n",
        "                'timestamp': datetime.now(),\n",
        "                'detectors_in_agreement': drift_signals,\n",
        "                'consensus_level': len(drift_signals),\n",
        "                'total_detectors': len(self.detectors)\n",
        "            }\n",
        "            self.drift_history.append(event)\n",
        "            print(f\"🚨 Консенсусный дрейф: {event['detectors_in_agreement']}\")\n",
        "```\n",
        "\n",
        "> **Практический совет**: В продакшене мульти-детектор можно использовать для запуска **автоматического переобучения**, **уведомления аналитиков** или **переключения на резервную модель**.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Инженерия признаков в реальном времени**\n",
        "\n",
        "### **Теория: Feature Store и потоковая агрегация**\n",
        "\n",
        "В системах реального времени **согласованность признаков** между обучением и инференсом — ключевая проблема. **Feature Store** решает её, предоставляя единый источник «правды» для признаков, доступный как для batch-обучения, так и для online-инференса.\n",
        "\n",
        "Однако многие важные признаки **невозможно предварительно вычислить** — они зависят от самого последнего события:\n",
        "- «Сумма транзакций за последние 10 минут»;\n",
        "- «Скорость перемещения пользователя»;\n",
        "- «Отклонение текущей суммы от среднего».\n",
        "\n",
        "Такие признаки требуют **потоковой агрегации** непосредственно в момент инференса. Это достигается через:\n",
        "- **Оконные функции** (time-based или count-based);\n",
        "- **Состояние в памяти** (Redis, Flink state backend);\n",
        "- **Геодезические расчёты** для координат.\n",
        "\n",
        "Ниже мы рассмотрим гибридный подход: комбинация **Feast** (feature store) и **кастомного агрегатора на Redis**.\n",
        "\n",
        "---\n",
        "\n",
        "### **6.1. Гибридный движок признаков с Feast и Redis**\n",
        "\n",
        "```python\n",
        "from feast import FeatureStore, RepoConfig\n",
        "from feast.infra.online_stores.redis import RedisOnlineStoreConfig\n",
        "import redis\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Инициализация Feast Feature Store (обычно делается один раз при старте сервиса)\n",
        "store = FeatureStore(repo_path=\"feature_repo\")  # Предполагается, что репозиторий настроен\n",
        "\n",
        "class RealTimeFeatureEngine:\n",
        "    \"\"\"\n",
        "    Движок признаков, объединяющий:\n",
        "    - исторические признаки из Feast,\n",
        "    - оконные агрегаты из Redis,\n",
        "    - вычисляемые признаки на лету.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, feature_store: FeatureStore):\n",
        "        self.fs = feature_store\n",
        "        self.window_aggregator = WindowedAggregator()\n",
        "    \n",
        "    def compute_realtime_features(self, transaction: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Основной метод: возвращает полный вектор признаков для модели.\"\"\"\n",
        "        user_id = transaction['user_id']\n",
        "        \n",
        "        # 1. Получение исторических признаков из Feast (online store = Redis)\n",
        "        feast_features = self._get_feast_features(user_id)\n",
        "        \n",
        "        # 2. Вычисление оконных агрегатов в реальном времени\n",
        "        windowed_features = self.window_aggregator.compute(\n",
        "            user_id=user_id,\n",
        "            amount=transaction['amount'],\n",
        "            timestamp=transaction['timestamp']\n",
        "        )\n",
        "        \n",
        "        # 3. Временные и геопространственные признаки\n",
        "        temporal_features = self._compute_temporal_features(transaction['timestamp'])\n",
        "        geo_features = self._compute_geo_features(transaction.get('location'), user_id)\n",
        "        \n",
        "        # Объединение всех признаков\n",
        "        return {\n",
        "            **feast_features,\n",
        "            **windowed_features,\n",
        "            **temporal_features,\n",
        "            **geo_features\n",
        "        }\n",
        "    \n",
        "    def _get_feast_features(self, user_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Запрос к Feast Feature Store.\"\"\"\n",
        "        try:\n",
        "            feature_vector = self.fs.get_online_features(\n",
        "                entity_rows=[{\"user_id\": user_id}],\n",
        "                features=[\n",
        "                    \"user_stats:avg_transaction_amount_7d\",\n",
        "                    \"user_stats:transaction_count_24h\",\n",
        "                    \"user_stats:avg_session_duration\",\n",
        "                    \"user_embeddings:profile_embedding\"\n",
        "                ]\n",
        "            ).to_dict()\n",
        "            \n",
        "            # Преобразуем из формата Feast в плоский словарь\n",
        "            flat_features = {}\n",
        "            for key, values in feature_vector.items():\n",
        "                flat_features[key] = values[0]  # Feast возвращает списки\n",
        "            \n",
        "            return flat_features\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Ошибка Feast: {e}. Используем нули.\")\n",
        "            return {\n",
        "                'user_stats:avg_transaction_amount_7d': 0.0,\n",
        "                'user_stats:transaction_count_24h': 0,\n",
        "                'user_stats:avg_session_duration': 0.0,\n",
        "                'user_embeddings:profile_embedding': np.zeros(64).tolist()\n",
        "            }\n",
        "    \n",
        "    def _compute_temporal_features(self, timestamp: str) -> Dict[str, int]:\n",
        "        \"\"\"Преобразование временной метки в категориальные признаки.\"\"\"\n",
        "        dt = datetime.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n",
        "        return {\n",
        "            'hour_of_day': dt.hour,\n",
        "            'day_of_week': dt.weekday(),\n",
        "            'is_weekend': int(dt.weekday() >= 5),\n",
        "            'is_night': int(0 <= dt.hour <= 6)\n",
        "        }\n",
        "    \n",
        "    def _compute_geo_features(self, location: Dict[str, float], user_id: str) -> Dict[str, float]:\n",
        "        \"\"\"Вычисление геопространственных признаков (упрощённо).\"\"\"\n",
        "        if not location or 'lat' not in location or 'lon' not in location:\n",
        "            return {'distance_from_usual': 0.0, 'location_velocity': 0.0}\n",
        "        \n",
        "        current_loc = (location['lat'], location['lon'])\n",
        "        last_loc = self.window_aggregator.get_last_location(user_id)\n",
        "        \n",
        "        if last_loc is None:\n",
        "            self.window_aggregator.save_location(user_id, current_loc, datetime.now())\n",
        "            return {'distance_from_usual': 0.0, 'location_velocity': 0.0}\n",
        "        \n",
        "        # Расчёт расстояния по формуле Хаверсина (в км)\n",
        "        distance = self._haversine_distance(last_loc, current_loc)\n",
        "        \n",
        "        # Расчёт скорости (в км/ч)\n",
        "        last_time = self.window_aggregator.get_last_location_time(user_id)\n",
        "        time_diff_hours = (datetime.now() - last_time).total_seconds() / 3600\n",
        "        velocity = distance / time_diff_hours if time_diff_hours > 0.01 else 0.0\n",
        "        \n",
        "        # Сохранение новой локации\n",
        "        self.window_aggregator.save_location(user_id, current_loc, datetime.now())\n",
        "        \n",
        "        return {\n",
        "            'distance_from_usual': distance,\n",
        "            'location_velocity': velocity,\n",
        "            'is_unusual_location': int(distance > 50.0)  # более 50 км от последнего места\n",
        "        }\n",
        "    \n",
        "    def _haversine_distance(self, loc1: tuple, loc2: tuple) -> float:\n",
        "        \"\"\"Расчёт геодезического расстояния между двумя точками (в км).\"\"\"\n",
        "        lat1, lon1 = np.radians(loc1)\n",
        "        lat2, lon2 = np.radians(loc2)\n",
        "        \n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "        \n",
        "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "        c = 2 * np.arcsin(np.sqrt(a))\n",
        "        r = 6371  # Радиус Земли в км\n",
        "        return c * r\n",
        "\n",
        "class WindowedAggregator:\n",
        "    \"\"\"Агрегатор оконных признаков с хранением состояния в Redis.\"\"\"\n",
        "    \n",
        "    def __init__(self, redis_host: str = 'localhost', redis_port: int = 6379):\n",
        "        self.redis = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)\n",
        "        self.windows = {'1h': 3600, '24h': 86400, '7d': 604800}\n",
        "    \n",
        "    def compute(self, user_id: str, amount: float, timestamp: str) -> Dict[str, float]:\n",
        "        \"\"\"Вычисление агрегатов по скользящим временным окнам.\"\"\"\n",
        "        ts = datetime.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n",
        "        ts_sec = int(ts.timestamp())\n",
        "        \n",
        "        features = {}\n",
        "        \n",
        "        for win_name, win_sec in self.windows.items():\n",
        "            key = f\"user:{user_id}:{win_name}:amounts\"\n",
        "            \n",
        "            # Удаляем устаревшие записи\n",
        "            cutoff = ts_sec - win_sec\n",
        "            self.redis.zremrangebyscore(key, 0, cutoff)\n",
        "            \n",
        "            # Добавляем новую транзакцию\n",
        "            self.redis.zadd(key, {str(amount): ts_sec})\n",
        "            \n",
        "            # Получаем все суммы в окне\n",
        "            amounts = self.redis.zrange(key, 0, -1, withscores=False)\n",
        "            amounts = [float(a.decode()) for a in amounts] if amounts else [0.0]\n",
        "            \n",
        "            # Вычисляем агрегаты\n",
        "            features.update({\n",
        "                f'avg_amount_{win_name}': np.mean(amounts),\n",
        "                f'std_amount_{win_name}': np.std(amounts),\n",
        "                f'max_amount_{win_name}': np.max(amounts),\n",
        "                f'txn_count_{win_name}': len(amounts),\n",
        "                f'amount_trend_{win_name}': self._linear_trend(amounts)\n",
        "            })\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _linear_trend(self, values: list) -> float:\n",
        "        \"\"\"Расчёт линейного тренда (наклона) ряда.\"\"\"\n",
        "        if len(values) < 2:\n",
        "            return 0.0\n",
        "        x = np.arange(len(values))\n",
        "        slope, _ = np.polyfit(x, values, 1)\n",
        "        return float(slope)\n",
        "    \n",
        "    # Методы для хранения локаций (для геопризнаков)\n",
        "    def save_location(self, user_id: str, location: tuple, timestamp: datetime):\n",
        "        key = f\"user:{user_id}:last_location\"\n",
        "        self.redis.hset(key, mapping={\n",
        "            'lat': str(location[0]),\n",
        "            'lon': str(location[1]),\n",
        "            'timestamp': timestamp.isoformat()\n",
        "        })\n",
        "    \n",
        "    def get_last_location(self, user_id: str):\n",
        "        key = f\"user:{user_id}:last_location\"\n",
        "        data = self.redis.hgetall(key)\n",
        "        if not data:\n",
        "            return None\n",
        "        return (float(data[b'lat']), float(data[b'lon']))\n",
        "    \n",
        "    def get_last_location_time(self, user_id: str) -> datetime:\n",
        "        key = f\"user:{user_id}:last_location\"\n",
        "        ts_str = self.redis.hget(key, 'timestamp')\n",
        "        return datetime.fromisoformat(ts_str.decode()) if ts_str else datetime.now()\n",
        "```\n",
        "\n",
        "> **Важно**: В продакшене Redis должен быть защищён, иметь TTL на ключи и быть частью отказоустойчивого кластера. Также рекомендуется использовать **векторные представления** (embeddings) вместо raw-координат.\n",
        "\n",
        "---\n",
        "\n",
        "## **Заключение разделов 5–6**\n",
        "\n",
        "Онлайн-обучение и потоковая инженерия признаков — это не просто «модель + Kafka». Это **целая архитектурная дисциплина**, требующая:\n",
        "- понимания динамики данных и дрейфа концептов;\n",
        "- использования специализированных библиотек (River, Feast);\n",
        "- надёжного управления состоянием (Redis, Flink);\n",
        "- строгого мониторинга качества и согласованности.\n",
        "\n",
        "Только комплексный подход позволяет строить ML-системы, которые не просто работают в реальном времени, а **непрерывно улучшаются**, адаптируясь к меняющемуся миру.\n",
        ""
      ],
      "metadata": {
        "id": "gmJbnZtyXGp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **7. Мониторинг и Observability: глаза и уши production-системы**\n",
        "\n",
        "### **Теоретические основы observability**\n",
        "\n",
        "Если **логирование** отвечает на вопрос *«Что произошло?»*, а **метрики** — на *«Сколько раз это произошло?»*, то **трассировка** раскрывает *«Как именно это произошло?»*. Вместе они образуют **три кита observability**, без которых невозможна эксплуатация сложных распределённых систем.\n",
        "\n",
        "В контексте Real-time ML наблюдаемость приобретает особое значение:\n",
        "- **Задержка (latency)** может означать узкое место в feature engineering;\n",
        "- **Падение точности модели** — признак дрейфа концептов;\n",
        "- **Резкий рост ошибок** — сбой в источнике данных или изменение схемы;\n",
        "- **Нестабильность throughput’а** — проблемы с масштабированием.\n",
        "\n",
        "Современные стандарты observability опираются на:\n",
        "- **Prometheus** — для сбора и агрегации метрик;\n",
        "- **OpenTelemetry** — для унифицированной трассировки и логирования;\n",
        "- **Grafana** — для визуализации и алертинга.\n",
        "\n",
        "Интеграция этих инструментов позволяет строить **end-to-end мониторинг** от Kafka-топика до предсказания модели.\n",
        "\n",
        "---\n",
        "\n",
        "### **9.1. Комплексный мониторинг с Prometheus и OpenTelemetry**\n",
        "\n",
        "Ниже приведён пример класса, который централизует сбор метрик, трассировку и логирование для потоковой ML-системы.\n",
        "\n",
        "```python\n",
        "import time\n",
        "import logging\n",
        "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
        "import json\n",
        "\n",
        "# Настройка OpenTelemetry (в продакшене — экспортер в Jaeger/Tempo)\n",
        "trace.set_tracer_provider(TracerProvider())\n",
        "tracer_provider = trace.get_tracer_provider()\n",
        "tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
        "\n",
        "class StreamingMLMonitor:\n",
        "    \"\"\"\n",
        "    Централизованный мониторинг для потоковых ML-систем.\n",
        "    Интегрирует Prometheus (метрики), OpenTelemetry (трассировка) и logging.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, service_name: str = \"streaming-ml-service\"):\n",
        "        self.service_name = service_name\n",
        "        self.tracer = trace.get_tracer(__name__)\n",
        "        self.logger = self._setup_logging()\n",
        "        self._init_prometheus_metrics()\n",
        "    \n",
        "    def _setup_logging(self) -> logging.Logger:\n",
        "        \"\"\"Настройка структурированного логирования.\"\"\"\n",
        "        logger = logging.getLogger(self.service_name)\n",
        "        logger.setLevel(logging.INFO)\n",
        "        \n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter(\n",
        "                '{\"time\": \"%(asctime)s\", \"level\": \"%(levelname)s\", '\n",
        "                '\"service\": \"' + self.service_name + '\", \"message\": \"%(message)s\"}'\n",
        "            )\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "        \n",
        "        return logger\n",
        "    \n",
        "    def _init_prometheus_metrics(self):\n",
        "        \"\"\"Инициализация метрик Prometheus.\"\"\"\n",
        "        self.metrics = {\n",
        "            # Через counter считаем throughput\n",
        "            'throughput': Counter(\n",
        "                'stream_processing_throughput_total',\n",
        "                'Total number of processed messages',\n",
        "                ['topic', 'status']  # статус: success/error\n",
        "            ),\n",
        "            # Гистограмма для распределения задержек\n",
        "            'processing_latency': Histogram(\n",
        "                'stream_processing_latency_seconds',\n",
        "                'Processing latency per stage',\n",
        "                ['stage']  # например: 'feature_extraction', 'model_inference'\n",
        "            ),\n",
        "            # Gauge для динамических метрик (качество модели)\n",
        "            'model_performance': Gauge(\n",
        "                'model_performance_score',\n",
        "                'Online model quality metrics',\n",
        "                ['model_id', 'metric']  # accuracy, precision, drift_score\n",
        "            ),\n",
        "            # Мониторинг качества данных\n",
        "            'data_quality': Gauge(\n",
        "                'feature_drift_score',\n",
        "                'Real-time feature drift (KS/Wasserstein)',\n",
        "                ['feature_name']\n",
        "            )\n",
        "        }\n",
        "    \n",
        "    def track_message(self, topic: str, status: str = \"success\"):\n",
        "        \"\"\"Учёт обработанного сообщения.\"\"\"\n",
        "        self.metrics['throughput'].labels(topic=topic, status=status).inc()\n",
        "    \n",
        "    def observe_latency(self, stage: str, start_time: float):\n",
        "        \"\"\"Замер и запись задержки обработки.\"\"\"\n",
        "        latency = time.time() - start_time\n",
        "        self.metrics['processing_latency'].labels(stage=stage).observe(latency)\n",
        "    \n",
        "    def set_model_metric(self, model_id: str, metric_name: str, value: float):\n",
        "        \"\"\"Установка значения метрики качества модели.\"\"\"\n",
        "        self.metrics['model_performance'].labels(\n",
        "            model_id=model_id, metric_name=metric_name\n",
        "        ).set(value)\n",
        "    \n",
        "    def set_drift_score(self, feature_name: str, score: float):\n",
        "        \"\"\"Запись оценки дрейфа для признака.\"\"\"\n",
        "        self.metrics['data_quality'].labels(feature_name=feature_name).set(score)\n",
        "        \n",
        "        if score > 0.1:  # порог зависит от домена\n",
        "            self.logger.warning(f\"High drift detected in feature '{feature_name}': {score:.3f}\")\n",
        "\n",
        "# Запуск HTTP-сервера для сбора метрик Prometheus\n",
        "start_http_server(8000)  # метрики доступны на /metrics\n",
        "```\n",
        "\n",
        "> **Почему это важно?**  \n",
        "> Отдельный HTTP-эндпоинт на порту 8000 позволяет **Prometheus-скраперу** регулярно собирать метрики без влияния на основной трафик. Это стандартная практика в cloud-native архитектурах.\n",
        "\n",
        "Теперь интегрируем мониторинг в обработчик сообщений:\n",
        "\n",
        "```python\n",
        "class MonitoredStreamProcessor:\n",
        "    \"\"\"Потоковый процессор с полноценной observability.\"\"\"\n",
        "    \n",
        "    def __init__(self, monitor: StreamingMLMonitor):\n",
        "        self.monitor = monitor\n",
        "        self.model = self._load_model()\n",
        "    \n",
        "    def _load_model(self):\n",
        "        \"\"\"Загрузка модели из registry (упрощено).\"\"\"\n",
        "        return PreTrainedModel()\n",
        "    \n",
        "    def process_transaction(self, message: dict) -> dict:\n",
        "        \"\"\"Обработка одной транзакции с мониторингом.\"\"\"\n",
        "        start_time = time.time()\n",
        "        topic = \"financial-transactions\"\n",
        "        \n",
        "        with self.monitor.tracer.start_as_current_span(\"process_transaction\") as span:\n",
        "            span.set_attribute(\"message_id\", message.get(\"transaction_id\", \"unknown\"))\n",
        "            \n",
        "            try:\n",
        "                # === Этап 1: извлечение признаков ===\n",
        "                feat_start = time.time()\n",
        "                features = self._extract_features(message)\n",
        "                self.monitor.observe_latency(\"feature_extraction\", feat_start)\n",
        "                \n",
        "                # === Этап 2: ML-инференс ===\n",
        "                pred_start = time.time()\n",
        "                prediction = self.model.predict(features)\n",
        "                self.monitor.observe_latency(\"model_inference\", pred_start)\n",
        "                \n",
        "                # === Этап 3: запись метрик ===\n",
        "                self.monitor.set_model_metric(\n",
        "                    model_id=\"fraud-v3\",\n",
        "                    metric_name=\"current_prediction_score\",\n",
        "                    value=prediction.get(\"probability\", 0.0)\n",
        "                )\n",
        "                \n",
        "                result = {\"status\": \"processed\", \"prediction\": prediction}\n",
        "                self.monitor.track_message(topic, \"success\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                self.monitor.logger.error(f\"Processing failed: {e}\")\n",
        "                self.monitor.track_message(topic, \"error\")\n",
        "                result = {\"status\": \"error\", \"error\": str(e)}\n",
        "                raise\n",
        "            finally:\n",
        "                self.monitor.observe_latency(\"total_processing\", start_time)\n",
        "        \n",
        "        return result\n",
        "```\n",
        "\n",
        "> **Преимущество трассировки**: каждый спан (`process_transaction`, `feature_extraction`) становится узлом в графе вызовов, который можно визуализировать в Jaeger или Grafana Tempo. Это критично для диагностики узких мест.\n",
        "\n",
        "---\n",
        "\n",
        "### **9.2. Дашборд в Grafana: единое окно в систему**\n",
        "\n",
        "Для визуализации метрик используется **Grafana**, которая подключается к Prometheus как источнику данных. Ниже — программное представление дашборда в формате JSON (используется в Terraform или Grafonnet).\n",
        "\n",
        "```python\n",
        "DASHBOARD_CONFIG = {\n",
        "    \"title\": \"Real-time ML Pipeline — Fraud Detection\",\n",
        "    \"panels\": [\n",
        "        {\n",
        "            \"title\": \"Throughput (messages/sec)\",\n",
        "            \"type\": \"timeseries\",\n",
        "            \"targets\": [{\n",
        "                \"expr\": \"rate(stream_processing_throughput_total[1m])\",\n",
        "                \"legendFormat\": \"{{topic}} — {{status}}\"\n",
        "            }],\n",
        "            \"fieldConfig\": {\"defaults\": {\"unit\": \"ops\"}}\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Latency (P95)\",\n",
        "            \"type\": \"stat\",\n",
        "            \"targets\": [{\n",
        "                \"expr\": \"histogram_quantile(0.95, \" +\n",
        "                        \"rate(stream_processing_latency_seconds_bucket[5m]))\",\n",
        "                \"legendFormat\": \"{{stage}}\"\n",
        "            }],\n",
        "            \"fieldConfig\": {\"defaults\": {\"unit\": \"s\"}}\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Model Performance\",\n",
        "            \"type\": \"timeseries\",\n",
        "            \"targets\": [\n",
        "                {\"expr\": \"model_performance_score{metric_name='accuracy'}\", \"legendFormat\": \"Accuracy\"},\n",
        "                {\"expr\": \"model_performance_score{metric_name='precision'}\", \"legendFormat\": \"Precision\"}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Feature Drift (KS Statistic)\",\n",
        "            \"type\": \"barchart\",\n",
        "            \"targets\": [{\n",
        "                \"expr\": \"feature_drift_score\",\n",
        "                \"legendFormat\": \"{{feature_name}}\"\n",
        "            }],\n",
        "            \"options\": {\n",
        "                \"orientation\": \"horizontal\",\n",
        "                \"showValues\": [\"value\"],\n",
        "                \"thresholds\": {\n",
        "                    \"steps\": [\n",
        "                        {\"value\": 0, \"color\": \"green\"},\n",
        "                        {\"value\": 0.05, \"color\": \"yellow\"},\n",
        "                        {\"value\": 0.1, \"color\": \"red\"}\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "    \"refresh\": \"10s\",  # автообновление\n",
        "    \"time\": {\"from\": \"now-1h\", \"to\": \"now\"}\n",
        "}\n",
        "```\n",
        "\n",
        "> **Практический совет**: Настройте **алерты** в Grafana:\n",
        "> - Задержка > 500 мс → warning;\n",
        "> - Точность модели < 0.85 → critical;\n",
        "> - Дрейф признака > 0.15 → warning.\n",
        "\n",
        "---\n",
        "\n",
        "## **13. Комплексный практический кейс: антифрод в реальном времени**\n",
        "\n",
        "### **Архитектура production-системы**\n",
        "\n",
        "Настоящая антифрод-система — это не просто модель, а **оркестрация компонентов**:\n",
        "1. **Event Ingestion**: Kafka/Pulsar для надёжного приёма транзакций.\n",
        "2. **Feature Engineering**: Feast + Redis для исторических и онлайн-признаков.\n",
        "3. **ML Pipeline**: онлайн-модель с мониторингом дрейфа.\n",
        "4. **Business Rules**: дополнительный слой логики поверх ML.\n",
        "5. **Alerting & Feedback Loop**: отправка алертов и сбор ground truth для переобучения.\n",
        "\n",
        "Ниже — упрощённое, но полное представление такой системы.\n",
        "\n",
        "```python\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "class RealTimeFraudDetectionSystem:\n",
        "    \"\"\"\n",
        "    Production-ready антифрод система с полным жизненным циклом:\n",
        "    ingestion → feature engineering → ML → rules → alerting → feedback.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Внешние зависимости (инициализируются в реальности через DI)\n",
        "        self.feature_engine = RealTimeFeatureEngine()\n",
        "        self.model = self._load_current_model()\n",
        "        self.rule_engine = BusinessRuleEngine()\n",
        "        self.alert_manager = AlertManager()\n",
        "        self.monitor = StreamingMLMonitor(\"fraud-detection\")\n",
        "        self.feedback_store = LabelStore()  # для получения ground truth с задержкой\n",
        "    \n",
        "    def process_transaction(self, transaction: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Основной метод обработки одной транзакции.\"\"\"\n",
        "        span_ctx = self.monitor.tracer.start_as_current_span(\"fraud_detection_pipeline\")\n",
        "        with span_ctx as root_span:\n",
        "            root_span.set_attribute(\"transaction.id\", transaction[\"transaction_id\"])\n",
        "            \n",
        "            try:\n",
        "                # Этап 1: Обогащение признаками\n",
        "                features = self.feature_engine.compute_realtime_features(transaction)\n",
        "                \n",
        "                # Этап 2: ML-предсказание\n",
        "                pred = self.model.predict(features)\n",
        "                fraud_prob = pred.get(\"probability\", 0.0)\n",
        "                is_fraud = fraud_prob > 0.9\n",
        "                \n",
        "                # Этап 3: Применение бизнес-правил\n",
        "                if not is_fraud:\n",
        "                    is_fraud = self.rule_engine.evaluate(transaction, features)\n",
        "                \n",
        "                # Этап 4: Формирование алерта\n",
        "                if is_fraud:\n",
        "                    alert = self._create_alert(transaction, fraud_prob, features)\n",
        "                    self.alert_manager.send(alert)\n",
        "                    self.monitor.logger.info(f\"Alert sent: {alert['alert_id']}\")\n",
        "                \n",
        "                # Этап 5: Мониторинг и запись для offline-анализа\n",
        "                self._log_for_monitoring(transaction, features, fraud_prob)\n",
        "                \n",
        "                return {\n",
        "                    \"is_fraud\": is_fraud,\n",
        "                    \"risk_score\": fraud_prob,\n",
        "                    \"processing_time\": root_span.end_time - root_span.start_time\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                self.monitor.logger.error(f\"Pipeline failed: {e}\")\n",
        "                raise\n",
        "    \n",
        "    def _load_current_model(self):\n",
        "        \"\"\"В реальности — загрузка из MLflow или S3 с кэшированием.\"\"\"\n",
        "        return MockFraudModel()\n",
        "    \n",
        "    def _create_alert(self, tx: dict, prob: float, feats: dict) -> dict:\n",
        "        \"\"\"Создание структурированного алерта.\"\"\"\n",
        "        return {\n",
        "            \"alert_id\": str(uuid.uuid4()),\n",
        "            \"transaction_id\": tx[\"transaction_id\"],\n",
        "            \"user_id\": tx[\"user_id\"],\n",
        "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"risk_score\": prob,\n",
        "            \"reasons\": self._explain_prediction(feats),\n",
        "            \"model_version\": \"fraud-v3\",\n",
        "            \"action\": \"BLOCK\" if prob > 0.95 else \"REVIEW\"\n",
        "        }\n",
        "    \n",
        "    def _explain_prediction(self, features: dict) -> list:\n",
        "        \"\"\"Простая локальная интерпретация (в продакшене — SHAP/LIME).\"\"\"\n",
        "        reasons = []\n",
        "        if features.get(\"amount\", 0) > 10000:\n",
        "            reasons.append(\"High transaction amount\")\n",
        "        if features.get(\"is_unusual_location\", 0) == 1:\n",
        "            reasons.append(\"Unusual location\")\n",
        "        if features.get(\"location_velocity\", 0) > 500:\n",
        "            reasons.append(\"High location velocity\")\n",
        "        return reasons\n",
        "    \n",
        "    def _log_for_monitoring(self, tx: dict, feats: dict, prob: float):\n",
        "        \"\"\"Запись данных для offline-анализа и мониторинга.\"\"\"\n",
        "        # В продакшене — в Parquet/S3 или ClickHouse\n",
        "        monitoring_record = {\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "            \"transaction_id\": tx[\"transaction_id\"],\n",
        "            \"features\": feats,\n",
        "            \"prediction\": prob,\n",
        "            \"model_version\": \"fraud-v3\"\n",
        "        }\n",
        "        # self.monitoring_sink.write(monitoring_record)\n",
        "        \n",
        "        # Обновление онлайн-метрик\n",
        "        self.monitor.set_model_metric(\"fraud-v3\", \"current_risk_score\", prob)\n",
        "```\n",
        "\n",
        "> **Ключевой принцип**: даже если ML-часть даёт ложный результат, **бизнес-правила** служат «планом Б», обеспечивая базовую защиту. Это пример **защиты в глубину**.\n",
        "\n",
        "---\n",
        "\n",
        "## **14. Будущие тенденции: Serverless, Edge ML и автоматизация**\n",
        "\n",
        "### **Serverless-подход с AWS Lambda и Kinesis**\n",
        "\n",
        "Для **event-driven** сценариев с переменной нагрузкой всё чаще применяется **serverless-архитектура**. Платформы вроде **AWS Lambda** автоматически масштабируются от 0 до тысяч инстансов, оплачиваясь за миллисекунды выполнения.\n",
        "\n",
        "```python\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "import boto3\n",
        "\n",
        "# Инициализация один раз при холодном старте\n",
        "sns_client = boto3.client('sns')\n",
        "ALERT_TOPIC_ARN = os.environ['ALERT_TOPIC_ARN']\n",
        "\n",
        "def detect_fraud(transaction: dict) -> dict:\n",
        "    \"\"\"Упрощённая функция инференса (в реальности — загрузка модели из /tmp).\"\"\"\n",
        "    # Пример: простая эвристика\n",
        "    amount = transaction.get('amount', 0)\n",
        "    is_new_user = transaction.get('is_new_user', False)\n",
        "    \n",
        "    is_fraud = amount > 5000 or (is_new_user and amount > 1000)\n",
        "    prob = 0.95 if is_fraud else 0.05\n",
        "    \n",
        "    return {\"is_fraud\": is_fraud, \"probability\": prob}\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \"\"\"\n",
        "    AWS Lambda функция, триггеримая Kinesis Data Streams.\n",
        "    Обрабатывает пакет записей (до 100 за вызов).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for record in event['Records']:\n",
        "        try:\n",
        "            # Декодирование события из Kinesis\n",
        "            payload = base64.b64decode(record['kinesis']['data']).decode('utf-8')\n",
        "            transaction = json.loads(payload)\n",
        "            \n",
        "            # Инференс\n",
        "            fraud_result = detect_fraud(transaction)\n",
        "            \n",
        "            # Отправка алерта при подозрении\n",
        "            if fraud_result['is_fraud']:\n",
        "                sns_client.publish(\n",
        "                    TopicArn=ALERT_TOPIC_ARN,\n",
        "                    Message=json.dumps({\n",
        "                        \"transaction_id\": transaction.get(\"transaction_id\"),\n",
        "                        \"risk_score\": fraud_result[\"probability\"],\n",
        "                        \"lambda_request_id\": context.aws_request_id\n",
        "                    }),\n",
        "                    Subject=\"Fraud Alert\"\n",
        "                )\n",
        "            \n",
        "            results.append({\n",
        "                'recordId': record['eventID'],\n",
        "                'result': 'Ok'\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing record {record['eventID']}: {e}\")\n",
        "            results.append({\n",
        "                'recordId': record['eventID'],\n",
        "                'result': 'ProcessingFailed'\n",
        "            })\n",
        "    \n",
        "    # Kinesis требует возврата всех recordId\n",
        "    return {'records': results}\n",
        "```\n",
        "\n",
        "> **Преимущества serverless**:\n",
        "> - Нулевая стоимость простоя;\n",
        "> - Автоматическое масштабирование;\n",
        "> - Встроенная интеграция с Kinesis, SNS, SQS.\n",
        ">\n",
        "> **Ограничения**:\n",
        "> - Ограничение времени выполнения (15 мин в Lambda);\n",
        "> - «Холодный старт» для редких вызовов;\n",
        "> - Сложность с большими моделями (ограничение памяти).\n",
        "\n",
        "---\n",
        "\n",
        "## **Заключение модуля**\n",
        "\n",
        "Построение отказоустойчивых Real-time ML-систем — это **инженерная дисциплина**, сочетающая:\n",
        "1. **Потоковую обработку** (Kafka, Flink) для надёжного перемещения данных;\n",
        "2. **Онлайн-обучение** (River, Spark MLlib) для адаптации к дрейфу;\n",
        "3. **Feature Store** (Feast, Tecton) для согласованности признаков;\n",
        "4. **Observability** (Prometheus, OpenTelemetry) для прозрачности;\n",
        "5. **MLOps-автоматизацию** для управления жизненным циклом моделей.\n",
        "\n",
        "**Ключевые принципы проектирования**:\n",
        "- **Проектируйте для отказа**: данные не должны теряться даже при падении узлов.\n",
        "- **Измеряйте всё**: без метрик вы слепы.\n",
        "- **Разделяйте ML и бизнес-логику**: правила — ваш «аварийный тормоз».\n",
        "- **Автоматизируйте рутину**: от переобучения до развёртывания.\n",
        "\n",
        "Представленные паттерны и примеры кода отражают **промышленный уровень зрелости**, необходимый для эксплуатации в высоконагруженном production-окружении. Они служат фундаментом для дальнейшего развития в сторону **edge-ML**, **federated learning** и **автономных self-healing систем**.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "_5HRwF9aY0eH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SODl19E3YmeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uRKxsc7ETxrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "srCWJkdJSlhn"
      }
    }
  ]
}