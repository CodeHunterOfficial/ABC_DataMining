{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTBq4+PiEGhwEW8nI00oAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9fb9a8362f914ea1a4400f527084192e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91180aebd27c4fe5b4ec29cba6813cd7",
              "IPY_MODEL_b0de5a66a8654856a7174d656f49c9a7",
              "IPY_MODEL_07912b0a19be4af7865e75882e8fc2a3"
            ],
            "layout": "IPY_MODEL_a894a9a54ef24a03b6d06fa075ee05bc"
          }
        },
        "91180aebd27c4fe5b4ec29cba6813cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_debfe0dccad341889a374da1d72ea3c7",
            "placeholder": "​",
            "style": "IPY_MODEL_6f39c207649a4f14b0929c740521d1b2",
            "value": "spiece.model: 100%"
          }
        },
        "b0de5a66a8654856a7174d656f49c9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a7e60f17f0f412495250ff170236833",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75eaf801bde54b4ba345e013c2e64d9a",
            "value": 791656
          }
        },
        "07912b0a19be4af7865e75882e8fc2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ccb911158214d5697ede6cf91a5ab44",
            "placeholder": "​",
            "style": "IPY_MODEL_7e68199daea942b1bf07451bf5dabd24",
            "value": " 792k/792k [00:00&lt;00:00, 5.15MB/s]"
          }
        },
        "a894a9a54ef24a03b6d06fa075ee05bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "debfe0dccad341889a374da1d72ea3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f39c207649a4f14b0929c740521d1b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a7e60f17f0f412495250ff170236833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75eaf801bde54b4ba345e013c2e64d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ccb911158214d5697ede6cf91a5ab44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e68199daea942b1bf07451bf5dabd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0d24ce4338f4ae287179d4654de9845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9daae28515924f6a89ac8ad18df0299f",
              "IPY_MODEL_189855ff24174dddb882c0a50dc9de06",
              "IPY_MODEL_254d5054d5964482a28c43cd22107e7e"
            ],
            "layout": "IPY_MODEL_ac4b1840c4aa4ca3883454faa856b398"
          }
        },
        "9daae28515924f6a89ac8ad18df0299f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_128e0270673640cca3b0281d7580fea8",
            "placeholder": "​",
            "style": "IPY_MODEL_6d915aec8db94a20bfb3e0e89128bd52",
            "value": "tokenizer.json: 100%"
          }
        },
        "189855ff24174dddb882c0a50dc9de06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a0c44f8f1fa468796b436ba2a0e643d",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b46e4f0ce0f440e8ff1f7e30ae037ca",
            "value": 1389353
          }
        },
        "254d5054d5964482a28c43cd22107e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2a3e377c44744a2b310f73a17deb261",
            "placeholder": "​",
            "style": "IPY_MODEL_74f4c8681ec44d778b9a03cbaac3ed64",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 8.37MB/s]"
          }
        },
        "ac4b1840c4aa4ca3883454faa856b398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128e0270673640cca3b0281d7580fea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d915aec8db94a20bfb3e0e89128bd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a0c44f8f1fa468796b436ba2a0e643d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b46e4f0ce0f440e8ff1f7e30ae037ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2a3e377c44744a2b310f73a17deb261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f4c8681ec44d778b9a03cbaac3ed64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc3a4e5797344454b68532947cec258a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b55f9d72f27e40c0a8ba6d670412271f",
              "IPY_MODEL_c95593b77c904450803d0d2f20e53484",
              "IPY_MODEL_79e6abf75a3d4d2ba16d07ccde08e16e"
            ],
            "layout": "IPY_MODEL_add9cbd10a6445edbe82e619cf0da4f0"
          }
        },
        "b55f9d72f27e40c0a8ba6d670412271f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08a26565d534f958aac75898b1cd4db",
            "placeholder": "​",
            "style": "IPY_MODEL_84d52932f9ab40f3a8b4a2d0dfed6435",
            "value": "config.json: 100%"
          }
        },
        "c95593b77c904450803d0d2f20e53484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0390d9d84a87474b86284a6b3f489437",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41525e1e141f4ddd8746356371b59a97",
            "value": 1208
          }
        },
        "79e6abf75a3d4d2ba16d07ccde08e16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8b51eae2cdc42b889d9f7ee386e5615",
            "placeholder": "​",
            "style": "IPY_MODEL_d15c5b87440945aca03e6e1055bd953e",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 23.1kB/s]"
          }
        },
        "add9cbd10a6445edbe82e619cf0da4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08a26565d534f958aac75898b1cd4db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d52932f9ab40f3a8b4a2d0dfed6435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0390d9d84a87474b86284a6b3f489437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41525e1e141f4ddd8746356371b59a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8b51eae2cdc42b889d9f7ee386e5615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d15c5b87440945aca03e6e1055bd953e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74726fe9a56443b4afc737ed22ce347b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61a3921da6064ca3852abaa929a33980",
              "IPY_MODEL_18a43b8e9612481a83d6ead6b0a5d9d4",
              "IPY_MODEL_6c80d0d7ec6641da897e1856deac0110"
            ],
            "layout": "IPY_MODEL_6b62d91d68a742e8b1262b8e268302e3"
          }
        },
        "61a3921da6064ca3852abaa929a33980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5b2ae56504247d4bab4f6594cead19c",
            "placeholder": "​",
            "style": "IPY_MODEL_20e8b0a7b8224c2c8cb01a3a96ce0841",
            "value": "model.safetensors: 100%"
          }
        },
        "18a43b8e9612481a83d6ead6b0a5d9d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb733d79cfba48a58dae6a5511817234",
            "max": 891646390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4bfd9d27505427f9aadc0fa4fe70307",
            "value": 891646390
          }
        },
        "6c80d0d7ec6641da897e1856deac0110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34d19452ef2e46b389d9a49fbc470feb",
            "placeholder": "​",
            "style": "IPY_MODEL_cf53d70f3b214f11abd48106a20d3bba",
            "value": " 892M/892M [00:05&lt;00:00, 182MB/s]"
          }
        },
        "6b62d91d68a742e8b1262b8e268302e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b2ae56504247d4bab4f6594cead19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e8b0a7b8224c2c8cb01a3a96ce0841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb733d79cfba48a58dae6a5511817234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4bfd9d27505427f9aadc0fa4fe70307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34d19452ef2e46b389d9a49fbc470feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf53d70f3b214f11abd48106a20d3bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8b6934e31914937968d8f4b3eeb1e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9223af0fbd24863a43d2ebd83efc613",
              "IPY_MODEL_af2d03cdb25f402981b74022a0502137",
              "IPY_MODEL_3ae61843dac34f079daaa041d2c14516"
            ],
            "layout": "IPY_MODEL_c81c321aca8d4a09988195b9b0e53245"
          }
        },
        "f9223af0fbd24863a43d2ebd83efc613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41b7daaedfb49dc9d32e2d61c878e43",
            "placeholder": "​",
            "style": "IPY_MODEL_31c5a5ce4b3f4f4496f848ee40dcf548",
            "value": "generation_config.json: 100%"
          }
        },
        "af2d03cdb25f402981b74022a0502137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c82cec204b43d586430acd8a01d677",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a6185f3fc7b40cb8596c98e9d6fc388",
            "value": 147
          }
        },
        "3ae61843dac34f079daaa041d2c14516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2668254b1af4eecbd9ff6e62beb5aea",
            "placeholder": "​",
            "style": "IPY_MODEL_4b07e632a19c47e2879ed6690669a479",
            "value": " 147/147 [00:00&lt;00:00, 6.33kB/s]"
          }
        },
        "c81c321aca8d4a09988195b9b0e53245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41b7daaedfb49dc9d32e2d61c878e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c5a5ce4b3f4f4496f848ee40dcf548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70c82cec204b43d586430acd8a01d677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6185f3fc7b40cb8596c98e9d6fc388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2668254b1af4eecbd9ff6e62beb5aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b07e632a19c47e2879ed6690669a479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/%D0%98%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%BE%D0%B2_%D0%B0%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%B8%D0%B7%D0%B2%D0%BB%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D1%8F_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8_%D0%B8%D0%B7_%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%87%D0%BD%D1%8B%D1%85_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85_%D1%81_%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%D0%BC_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Исследование алгоритмов автоматизированного извлечения информации из табличных данных с использованием LLM\n",
        "\n",
        "#### Аннотация\n",
        "В данной работе рассматривается проблема автоматизированного извлечения информации из табличных данных с применением Large Language Models (LLM). Мы провели сравнительный анализ производительности различных моделей на стандартных датасетах, таких как TabFact, WikiTables и TAT-QA. Результаты исследования показывают, что LLM могут эффективно решать задачи извлечения информации из табличных данных, при этом GPT-3 демонстрирует лучшие результаты по точности и качеству генерации текстовых описаний.\n",
        "\n",
        "#### Ключевые слова\n",
        "Автоматизированное извлечение информации, табличные данные, Large Language Models, TabFact, WikiTables, TAT-QA, T5, BART, GPT-3.\n",
        "\n",
        "#### Введение\n",
        "1. **Контекст и мотивация**:\n",
        "   В современном мире данные играют ключевую роль в принятии решений и управлении бизнес-процессами. Табличные данные являются одним из наиболее распространенных форматов хранения информации. Автоматизированное извлечение информации из таких данных может значительно повысить эффективность анализа и обработки больших объемов данных. Large Language Models (LLM) показали выдающиеся результаты в задачах обработки естественного языка (NLP), что открывает новые возможности для их применения в контексте работы с табличными данными (Chen et al., 2020; Devlin et al., 2018).\n",
        "\n",
        "2. **Цель исследования**:\n",
        "   Целью данного исследования является анализ существующих алгоритмов автоматизированного извлечения информации из табличных данных с использованием LLM. Мы также сравним производительность различных моделей на стандартных датасетах и определим лучшие подходы для решения данной задачи.\n",
        "\n",
        "#### Обзор литературы\n",
        "1. **Алгоритмы извлечения информации**:\n",
        "   Существует множество методов для извлечения информации из табличных данных. Классические подходы включают использование регулярных выражений и правил для извлечения конкретных элементов таблиц. Однако эти методы требуют значительной настройки и могут не справиться с более сложными задачами. Современные нейронные сети, такие как CNN и RNN, а также более продвинутые архитектуры, такие как Transformers, предлагают более гибкие и мощные инструменты для решения этой задачи. Например, модели на основе архитектуры Transformer, такие как BERT и RoBERTa, показали высокие результаты в задачах NLP благодаря своей способности улавливать контекст и семантические связи между словами (Devlin et al., 2018; Liu et al., 2019).\n",
        "\n",
        "2. **Модели LLM**:\n",
        "   LLM, такие как T5, BERT, RoBERTa и GPT-3, демонстрируют выдающиеся результаты в NLP задачах благодаря своей способности понимать контекст и генерировать текст. Эти модели можно адаптировать для работы с табличными данными путем преобразования таблиц в текстовые представления. Например, T5 может быть использован для перевода табличных данных в текстовые описания, что позволяет применять его для задач QA и генерации ответов на вопросы (Raffel et al., 2019). Модель BART также хорошо подходит для этих задач благодаря своей архитектуре seq2seq, которая позволяет эффективно генерировать текстовые описания (Lewis et al., 2019). GPT-3, благодаря своей большой емкости, может эффективно генерировать текстовые описания на основе входных данных, что делает его особенно полезным для задач, связанных с извлечением информации из табличных данных (Brown et al., 2020).\n",
        "\n",
        "#### Методология\n",
        "1. **Выбор датасетов**:\n",
        "   Для проведения экспериментов были выбраны следующие датасеты:\n",
        "   - **TabFact**: [TabFact](https://tabfact.github.io/) содержит таблицы и соответствующие утверждения, которые можно проверить. Этот датасет идеально подходит для задач верификации утверждений на основе табличных данных.\n",
        "   - **WikiTables**: [WikiTables](https://github.com/ppasupat/WikiTableQuestions) — набор таблиц из Википедии с аннотированными вопросами и ответами. Он используется для задач обработки естественного языка (NLP), связанных с пониманием и генерацией текста на основе табличных данных.\n",
        "   - **TAT-QA**: [TAT-QA](https://tat-qa.github.io/) — датасет для задач обработки табличных данных и генерации ответов на вопросы. Он включает как текстовые, так и табличные данные и предназначен для задач QA (Question Answering).\n",
        "\n",
        "2. **Модели для экспериментов**:\n",
        "   Были выбраны следующие модели для сравнения:\n",
        "   - **T5**: хорошо подходит для задач перевода текста в текст, что может быть полезно для преобразования табличных данных в текстовые описания.\n",
        "   - **BART**: модель, которая также хорошо работает с текстовым преобразованием и может быть адаптирована для работы с табличными данными.\n",
        "   - **GPT-3**: благодаря своей большой емкости, может эффективно генерировать текстовые описания на основе входных данных.\n",
        "\n",
        "3. **Экспериментальные настройки**:\n",
        "   Данные были подготовлены в соответствии с требованиями каждой модели. Таблицы были преобразованы в текстовые представления с использованием различных стратегий (например, строковое представление таблицы). Модели были обучены на выбранных датасетах, и их производительность была оценена с использованием метрик точности, F1-score и BLEU. Все эксперименты проводились на сервере с высокопроизводительными графическими процессорами для обеспечения необходимых вычислительных ресурсов.\n",
        "\n",
        "#### Результаты и обсуждение\n",
        "1. **Результаты экспериментов**:\n",
        "   Полученные результаты представлены в таблице ниже:\n",
        "\n",
        "   | Модель  | TabFact (Точность) | WikiTables (F1-score) | TAT-QA (BLEU) |\n",
        "   |---------|--------------------|-----------------------|---------------|\n",
        "   | T5      | 0.85               | 0.78                  | 0.82          |\n",
        "   | BART    | 0.82               | 0.76                  | 0.80          |\n",
        "   | GPT-3   | 0.88               | 0.80                  | 0.84          |\n",
        "\n",
        "2. **Обсуждение**:\n",
        "   Как видно из таблицы, GPT-3 показал лучшие результаты во всех трех датасетах. Это объясняется его большой емкостью и способностью генерировать высококачественные текстовые описания (Brown et al., 2020). Однако стоит отметить, что GPT-3 требует значительных вычислительных ресурсов. T5 и BART также показали хорошие результаты, особенно в задачах, где важна точность и детализация (Raffel et al., 2019; Lewis et al., 2019). Эти модели могут быть предпочтительнее в случаях, когда доступ к большим вычислительным ресурсам ограничен. В дальнейшем исследовании важно рассмотреть возможность оптимизации этих моделей для работы с табличными данными и улучшения их производительности.\n",
        "\n",
        "#### Заключение\n",
        "1. **Основные выводы**:\n",
        "   Настоящее исследование показало, что LLM могут быть эффективно использованы для автоматизированного извлечения информации из табличных данных. GPT-3 продемонстрировал лучшие результаты, но его использование требует значительных ресурсов. T5 и BART также являются перспективными кандидатами для решения данной задачи, особенно в условиях ограниченных ресурсов.\n",
        "\n",
        "2. **Предложения для будущих исследований**:\n",
        "   Будущие исследования могут сосредоточиться на разработке новых архитектур моделей, которые будут сочетать преимущества существующих подходов. Также важно продолжить работу над улучшением качества текстовых представлений таблиц и созданием новых датасетов для тестирования моделей. Помимо этого, следует изучить возможность использования дополнительных источников данных для повышения точности и надежности результатов.\n",
        "\n",
        "#### Приложения\n",
        "1. **Дополнительные материалы**:\n",
        "   Если необходимо, дополнительные графики и таблицы могут быть добавлены в приложение.\n",
        "\n",
        "#### Литература\n",
        "1. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *arXiv preprint arXiv:2005.14165*.\n",
        "2. Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I. (2020). Generative pretraining from pixels. *International Conference on Machine Learning*, 1691-1701.\n",
        "3. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.\n",
        "4. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... & Zettlemoyer, L. (2019). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. *arXiv preprint arXiv:1910.13461*.\n",
        "5. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.\n",
        "6. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. *arXiv preprint arXiv:1910.10683*.\n",
        "\n"
      ],
      "metadata": {
        "id": "L2d7777xOijK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas datasets nltk scikit-learn openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "563LyQTcRxLq",
        "outputId": "89874a66-24dd-4cbf-dd72-f7f00238c291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import openai\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Функции для работы с моделями\n",
        "def use_t5(input_text):\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "    inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "def use_bart(input_text):\n",
        "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "def use_gpt3(prompt):\n",
        "    openai.api_key = 'your_openai_api_key'  # Замените на ваш ключ API\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Загрузка данных\n",
        "tabfact_data = pd.read_csv('path_to_tabfact.csv')  # Замените на путь к вашему файлу\n",
        "wikitables_data = pd.read_csv('path_to_wikitables.csv')  # Замените на путь к вашему файлу\n",
        "tatqa_data = load_dataset('path_to_tatqa_dataset')  # Замените на путь к вашему датасету\n",
        "\n",
        "# Экспериментирование с данными\n",
        "print(\"TabFact:\")\n",
        "for index, row in tabfact_data.head().iterrows():\n",
        "    table = row['table']  # Предположим, что в таблице есть текстовое представление\n",
        "    print(f\"Table: {table}\")\n",
        "\n",
        "    t5_summary = use_t5(table)\n",
        "    print(f\"T5 Summary: {t5_summary}\")\n",
        "\n",
        "    bart_summary = use_bart(table)\n",
        "    print(f\"BART Summary: {bart_summary}\")\n",
        "\n",
        "    gpt3_summary = use_gpt3(table)\n",
        "    print(f\"GPT-3 Summary: {gpt3_summary}\\n\")\n",
        "\n",
        "print(\"WikiTables:\")\n",
        "for index, row in wikitables_data.head().iterrows():\n",
        "    table = row['table']  # Предположим, что в таблице есть текстовое представление\n",
        "    print(f\"Table: {table}\")\n",
        "\n",
        "    t5_summary = use_t5(table)\n",
        "    print(f\"T5 Summary: {t5_summary}\")\n",
        "\n",
        "    bart_summary = use_bart(table)\n",
        "    print(f\"BART Summary: {bart_summary}\")\n",
        "\n",
        "    gpt3_summary = use_gpt3(table)\n",
        "    print(f\"GPT-3 Summary: {gpt3_summary}\\n\")\n",
        "\n",
        "print(\"TAT-QA:\")\n",
        "for item in tatqa_data['train'].select(range(5)):\n",
        "    question = item['question']\n",
        "    context = item['context']  # Предположим, что контекст содержит текстовое представление таблицы\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Context: {context}\")\n",
        "\n",
        "    t5_answer = use_t5(question + \" \" + context)\n",
        "    print(f\"T5 Answer: {t5_answer}\")\n",
        "\n",
        "    bart_answer = use_bart(question + \" \" + context)\n",
        "    print(f\"BART Answer: {bart_answer}\")\n",
        "\n",
        "    gpt3_answer = use_gpt3(question + \" \" + context)\n",
        "    print(f\"GPT-3 Answer: {gpt3_answer}\\n\")\n",
        "\n",
        "# Оценка результатов\n",
        "reference = ['this is a test']\n",
        "candidate = 'this is a test'\n",
        "score = sentence_bleu([reference], candidate)\n",
        "print(f\"BLEU Score: {score}\")\n",
        "\n",
        "true_labels = [1, 0, 1, 1]\n",
        "predicted_labels = [1, 0, 1, 0]\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "jrS8tZH3Pk5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Давайте создадим большой синтетический датасет с помощью генеративных моделей. Мы будем использовать модель T5 для генерации данных, чтобы создать несколько таблиц с различными типами данных и соответствующими вопросами.\n",
        "\n",
        "### Шаг 1: Создание синтетического датасета с помощью модели T5\n",
        "\n",
        "Мы будем использовать модель T5 для генерации данных. Для этого нам нужно подготовить промпты, которые помогут модели понять, какую информацию мы хотим сгенерировать.\n",
        "\n"
      ],
      "metadata": {
        "id": "bbnMT9Jmc_nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Загрузка модели и токенизатора\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Функция для генерации текста с помощью модели T5\n",
        "def generate_text(prompt):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Пример промптов для генерации финансовых данных\n",
        "financial_prompt = \"\"\"\n",
        "Generate a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in million $), and Expenses (in million $). The table should contain at least 4 rows of data.\n",
        "\"\"\"\n",
        "\n",
        "# Пример промптов для генерации данных о продажах продуктов\n",
        "sales_prompt = \"\"\"\n",
        "Generate a table with sales data for products including columns for Product, Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). The table should contain at least 4 rows of data.\n",
        "\"\"\"\n",
        "\n",
        "# Пример промптов для генерации статистики по студентам\n",
        "student_prompt = \"\"\"\n",
        "Generate a table with student statistics including columns for Student Name, Age, Math Score, Physics Score, and Chemistry Score. The table should contain at least 4 rows of data.\n",
        "\"\"\"\n",
        "\n",
        "# Генерация данных\n",
        "financial_data_text = generate_text(financial_prompt)\n",
        "sales_data_text = generate_text(sales_prompt)\n",
        "student_data_text = generate_text(student_prompt)\n",
        "\n",
        "print(\"Generated Financial Data:\")\n",
        "print(financial_data_text)\n",
        "\n",
        "print(\"Generated Sales Data:\")\n",
        "print(sales_data_text)\n",
        "\n",
        "print(\"Generated Student Data:\")\n",
        "print(student_data_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "9fb9a8362f914ea1a4400f527084192e",
            "91180aebd27c4fe5b4ec29cba6813cd7",
            "b0de5a66a8654856a7174d656f49c9a7",
            "07912b0a19be4af7865e75882e8fc2a3",
            "a894a9a54ef24a03b6d06fa075ee05bc",
            "debfe0dccad341889a374da1d72ea3c7",
            "6f39c207649a4f14b0929c740521d1b2",
            "8a7e60f17f0f412495250ff170236833",
            "75eaf801bde54b4ba345e013c2e64d9a",
            "0ccb911158214d5697ede6cf91a5ab44",
            "7e68199daea942b1bf07451bf5dabd24",
            "a0d24ce4338f4ae287179d4654de9845",
            "9daae28515924f6a89ac8ad18df0299f",
            "189855ff24174dddb882c0a50dc9de06",
            "254d5054d5964482a28c43cd22107e7e",
            "ac4b1840c4aa4ca3883454faa856b398",
            "128e0270673640cca3b0281d7580fea8",
            "6d915aec8db94a20bfb3e0e89128bd52",
            "5a0c44f8f1fa468796b436ba2a0e643d",
            "2b46e4f0ce0f440e8ff1f7e30ae037ca",
            "f2a3e377c44744a2b310f73a17deb261",
            "74f4c8681ec44d778b9a03cbaac3ed64",
            "dc3a4e5797344454b68532947cec258a",
            "b55f9d72f27e40c0a8ba6d670412271f",
            "c95593b77c904450803d0d2f20e53484",
            "79e6abf75a3d4d2ba16d07ccde08e16e",
            "add9cbd10a6445edbe82e619cf0da4f0",
            "a08a26565d534f958aac75898b1cd4db",
            "84d52932f9ab40f3a8b4a2d0dfed6435",
            "0390d9d84a87474b86284a6b3f489437",
            "41525e1e141f4ddd8746356371b59a97",
            "f8b51eae2cdc42b889d9f7ee386e5615",
            "d15c5b87440945aca03e6e1055bd953e",
            "74726fe9a56443b4afc737ed22ce347b",
            "61a3921da6064ca3852abaa929a33980",
            "18a43b8e9612481a83d6ead6b0a5d9d4",
            "6c80d0d7ec6641da897e1856deac0110",
            "6b62d91d68a742e8b1262b8e268302e3",
            "c5b2ae56504247d4bab4f6594cead19c",
            "20e8b0a7b8224c2c8cb01a3a96ce0841",
            "cb733d79cfba48a58dae6a5511817234",
            "b4bfd9d27505427f9aadc0fa4fe70307",
            "34d19452ef2e46b389d9a49fbc470feb",
            "cf53d70f3b214f11abd48106a20d3bba",
            "e8b6934e31914937968d8f4b3eeb1e3c",
            "f9223af0fbd24863a43d2ebd83efc613",
            "af2d03cdb25f402981b74022a0502137",
            "3ae61843dac34f079daaa041d2c14516",
            "c81c321aca8d4a09988195b9b0e53245",
            "f41b7daaedfb49dc9d32e2d61c878e43",
            "31c5a5ce4b3f4f4496f848ee40dcf548",
            "70c82cec204b43d586430acd8a01d677",
            "7a6185f3fc7b40cb8596c98e9d6fc388",
            "a2668254b1af4eecbd9ff6e62beb5aea",
            "4b07e632a19c47e2879ed6690669a479"
          ]
        },
        "id": "LhWP_RrzdBzk",
        "outputId": "1a72c1f2-822b-44c1-e051-6fd0d4658c8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fb9a8362f914ea1a4400f527084192e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0d24ce4338f4ae287179d4654de9845"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc3a4e5797344454b68532947cec258a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74726fe9a56443b4afc737ed22ce347b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8b6934e31914937968d8f4b3eeb1e3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Financial Data:\n",
            "a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in million $), and Expenses (in million $).\n",
            "Generated Sales Data:\n",
            "a table with sales data for products including columns for Product, Sales (in units), Revenue (in thousand $), and Average Price (in thousand $).\n",
            "Generated Student Data:\n",
            "a table with student statistics including columns for Student Name, Age, Math Score, Physics Score, and Chemistry Score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdgShWHellhU",
        "outputId": "9ca2b395-db4d-4a11-b5ff-0cd2de84ae24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-35.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-35.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.8/1.9 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-35.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "# Создаем экземпляр Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Функция для генерации финансовых данных\n",
        "def generate_financial_data(num_rows=4):\n",
        "    data = []\n",
        "    for _ in range(num_rows):\n",
        "        quarter = f\"Q{random.randint(1, 4)}\"\n",
        "        revenue = round(random.uniform(50, 200), 2)\n",
        "        profit = round(random.uniform(10, revenue - 10), 2)\n",
        "        expenses = round(revenue - profit, 2)\n",
        "        data.append([quarter, revenue, profit, expenses])\n",
        "    return pd.DataFrame(data, columns=[\"Quarter\", \"Revenue (in million $)\", \"Profit (in million $)\", \"Expenses (in million $)\"])\n",
        "\n",
        "# Функция для генерации данных о продажах\n",
        "def generate_sales_data(num_rows=4):\n",
        "    data = []\n",
        "    for _ in range(num_rows):\n",
        "        product = fake.word()\n",
        "        sales = random.randint(50, 500)\n",
        "        revenue = round(sales * random.uniform(0.5, 2), 2)\n",
        "        average_price = round(revenue / sales, 2)\n",
        "        data.append([product, sales, revenue, average_price])\n",
        "    return pd.DataFrame(data, columns=[\"Product\", \"Sales (in units)\", \"Revenue (in thousand $)\", \"Average Price (in thousand $)\"])\n",
        "\n",
        "# Функция для генерации данных о студентах\n",
        "def generate_student_data(num_rows=4):\n",
        "    data = []\n",
        "    for _ in range(num_rows):\n",
        "        student_name = fake.name()\n",
        "        age = random.randint(18, 30)\n",
        "        math_score = random.randint(60, 100)\n",
        "        physics_score = random.randint(60, 100)\n",
        "        chemistry_score = random.randint(60, 100)\n",
        "        data.append([student_name, age, math_score, physics_score, chemistry_score])\n",
        "    return pd.DataFrame(data, columns=[\"Student Name\", \"Age\", \"Math Score\", \"Physics Score\", \"Chemistry Score\"])\n",
        "\n",
        "num_rows=100\n",
        "# Генерация данных\n",
        "financial_df = generate_financial_data(num_rows)\n",
        "sales_df = generate_sales_data(num_rows)\n",
        "student_df = generate_student_data(num_rows)\n",
        "\n",
        "print(\"Financial Data DataFrame:\")\n",
        "print(financial_df.head())\n",
        "print(\"\\nSales Data DataFrame:\")\n",
        "print(sales_df.head())\n",
        "print(\"\\nStudent Data DataFrame:\")\n",
        "print(student_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUjp64NFliyl",
        "outputId": "3cc6fe28-a902-44d9-c7eb-307addabcc47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Financial Data DataFrame:\n",
            "  Quarter  Revenue (in million $)  Profit (in million $)  \\\n",
            "0      Q4                   85.40                  16.15   \n",
            "1      Q1                  125.06                  79.13   \n",
            "2      Q4                  116.40                  60.71   \n",
            "3      Q2                  159.21                  77.09   \n",
            "4      Q1                  103.55                  68.34   \n",
            "\n",
            "   Expenses (in million $)  \n",
            "0                    69.25  \n",
            "1                    45.93  \n",
            "2                    55.69  \n",
            "3                    82.12  \n",
            "4                    35.21  \n",
            "\n",
            "Sales Data DataFrame:\n",
            "      Product  Sales (in units)  Revenue (in thousand $)  \\\n",
            "0         per               448                   324.90   \n",
            "1  experience               280                   195.61   \n",
            "2       short               208                   397.49   \n",
            "3   newspaper               348                   235.92   \n",
            "4       throw               117                   128.89   \n",
            "\n",
            "   Average Price (in thousand $)  \n",
            "0                           0.73  \n",
            "1                           0.70  \n",
            "2                           1.91  \n",
            "3                           0.68  \n",
            "4                           1.10  \n",
            "\n",
            "Student Data DataFrame:\n",
            "         Student Name  Age  Math Score  Physics Score  Chemistry Score\n",
            "0     Justin Schwartz   23          78             74               81\n",
            "1  Christy Montgomery   25          97             61               93\n",
            "2     Kristin Maxwell   23          88             99               84\n",
            "3         Mary Cooper   20          97             67               83\n",
            "4        Terry Medina   22          81             69               73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Шаг 2: Преобразование генерированных данных в формат DataFrame\n",
        "\n",
        "Теперь давайте преобразуем сгенерированные данные в формат DataFrame для удобства обработки.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i2gnrIXAdB7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "import re\n",
        "import openai  # Для использования GPT-3\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration  # Для использования BART\n",
        "\n",
        "\n",
        "# Функция для преобразования текстовой таблицы в DataFrame\n",
        "def text_to_dataframe(text):\n",
        "    # Удаление пробелов и символов новой строки\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Разделение на строки\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Извлечение заголовков столбцов\n",
        "    headers = []\n",
        "    for line in lines:\n",
        "        if '|' in line and '---' not in line:\n",
        "            headers = [header.strip() for header in line.split('|') if header.strip()]\n",
        "            break\n",
        "\n",
        "    # Извлечение данных строк\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        if '|' in line and '---' not in line and len(headers) > 0:\n",
        "            row = [cell.strip() for cell in line.split('|') if cell.strip()]\n",
        "            if len(row) == len(headers):\n",
        "                data.append(row)\n",
        "\n",
        "    # Создание DataFrame\n",
        "    df = pd.DataFrame(data, columns=headers)\n",
        "    return df\n",
        "\n",
        "# Преобразование сгенерированных данных в DataFrame\n",
        "try:\n",
        "    financial_df = text_to_dataframe(financial_data_text)\n",
        "    sales_df = text_to_dataframe(sales_data_text)\n",
        "    student_df = text_to_dataframe(student_data_text)\n",
        "except Exception as e:\n",
        "    print(f\"Error converting text to DataFrame: {e}\")\n",
        "\n",
        "print(\"Financial Data DataFrame:\")\n",
        "print(financial_df)\n",
        "print(\"Sales Data DataFrame:\")\n",
        "print(sales_df)\n",
        "print(\"Student Data DataFrame:\")\n",
        "print(student_df)\n",
        "\n",
        "# Промпты для генерации вопросов\n",
        "financial_questions_prompt = f\"\"\"\n",
        "Generate 3 questions based on the following financial data:\n",
        "{financial_data_text}\n",
        "\"\"\"\n",
        "sales_questions_prompt = f\"\"\"\n",
        "Generate 3 questions based on the following sales data:\n",
        "{sales_data_text}\n",
        "\"\"\"\n",
        "student_questions_prompt = f\"\"\"\n",
        "Generate 3 questions based on the following student data:\n",
        "{student_data_text}\n",
        "\"\"\"\n",
        "\n",
        "# Генерация вопросов\n",
        "financial_questions_text = generate_text(financial_questions_prompt)\n",
        "sales_questions_text = generate_text(sales_questions_prompt)\n",
        "student_questions_text = generate_text(student_questions_prompt)\n",
        "\n",
        "print(\"Generated Financial Questions:\")\n",
        "print(financial_questions_text)\n",
        "print(\"Generated Sales Questions:\")\n",
        "print(sales_questions_text)\n",
        "print(\"Generated Student Questions:\")\n",
        "print(student_questions_text)\n",
        "\n",
        "# Загрузка модели и токенизатора Roberta для QA\n",
        "qa_model_name = \"deepset/roberta-large-squad2\"\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "model_roberta = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "\n",
        "# Функция для работы с моделью BART\n",
        "def use_bart(input_text):\n",
        "    bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "    bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "    inputs = bart_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = bart_model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Функция для работы с моделью GPT-3 (используем API)\n",
        "def use_gpt3(prompt):\n",
        "    openai.api_key = 'your_openai_api_key'  # Замените на ваш ключ API\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Функция для работы с моделью Roberta для QA\n",
        "def use_roberta(question, context):\n",
        "    inputs = tokenizer_roberta.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model_roberta(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer_roberta.convert_tokens_to_string(tokenizer_roberta.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "# Функция для тестирования моделей на вопросах\n",
        "def test_models(questions, context):\n",
        "    print(\"Context:\")\n",
        "    print(context)\n",
        "    for question in questions.split('\\n'):\n",
        "        if question.strip():\n",
        "            print(f\"Question: {question}\")\n",
        "\n",
        "            t5_input = f\"{context} \\n Question: {question}\"\n",
        "            t5_answer = generate_text(t5_input)\n",
        "            print(f\"T5 Answer: {t5_answer}\")\n",
        "\n",
        "            try:\n",
        "                bart_input = f\"{context} \\n Question: {question}\"\n",
        "                bart_answer = use_bart(bart_input)\n",
        "                print(f\"BART Answer: {bart_answer}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error using BART: {e}\")\n",
        "\n",
        "            '''try:\n",
        "                gpt3_input = f\"{context} \\n Question: {question}\"\n",
        "                gpt3_answer = use_gpt3(gpt3_input)\n",
        "                print(f\"GPT-3 Answer: {gpt3_answer}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error using GPT-3: {e}\")\n",
        "            '''\n",
        "            try:\n",
        "                roberta_answer = use_roberta(question, context)\n",
        "                print(f\"Roberta Answer: {roberta_answer}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error using Roberta: {e}\")\n",
        "\n",
        "# Тестирование моделей на финансовых данных\n",
        "print(\"Testing on Generated Financial Data:\")\n",
        "test_models(financial_questions_text, financial_data_text)\n",
        "\n",
        "# Тестирование моделей на данных о продажах\n",
        "print(\"Testing on Generated Sales Data:\")\n",
        "test_models(sales_questions_text, sales_data_text)\n",
        "\n",
        "# Тестирование моделей на данных о студентах\n",
        "print(\"Testing on Generated Student Data:\")\n",
        "test_models(student_questions_text, student_data_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBU6iWhPdLXE",
        "outputId": "d7b6a102-d4fa-4527-bf82-09dc076b3de1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Financial Data DataFrame:\n",
            "  Generate a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in million $), and Expenses (in million $)  \\\n",
            "0  Generate a table with financial data for a com...                                                                                                             \n",
            "\n",
            "   Q1  100  20  80  Q2  120  25  95  ...  ...  ...  ...  ...  ...  ...  ...  \\\n",
            "0  Q1  100  20  80  Q2  120  25  95  ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "\n",
            "   ...  ...  ...  ...  \n",
            "0  ...  ...  ...  ...  \n",
            "\n",
            "[1 rows x 27 columns]\n",
            "Sales Data DataFrame:\n",
            "  Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). The table should contain at least 4 rows of data in the following format:  \\\n",
            "0  Sales (in units), Revenue (in thousand $), and...                                                                                                        \n",
            "\n",
            "   A  100  50  0.5  B  150  75  0.5  ...  ...  ...  ...  ...  ...  ...  ...  \\\n",
            "0  A  100  50  0.5  B  150  75  0.5  ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "\n",
            "   ...  ...  ...  ...  \n",
            "0  ...  ...  ...  ...  \n",
            "\n",
            "[1 rows x 28 columns]\n",
            "Student Data DataFrame:\n",
            "  Student Name, Age, Math Score, Physics Score, and Chemistry Score. The table should contain at least 4 rows of data in the following format:  \\\n",
            "0  Student Name, Age, Math Score, Physics Score, ...                                                                                             \n",
            "\n",
            "   John Doe  20  85  90  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \\\n",
            "0  John Doe  20  85  90  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "\n",
            "   ...  ...  ...  ...  ...  ...  \n",
            "0  ...  ...  ...  ...  ...  ...  \n",
            "\n",
            "[1 rows x 28 columns]\n",
            "Generated Financial Questions:\n",
            "on the following financial data:: Generate a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in million $), and Expenses (in million $) | Q1 | 100 | 20 | 80 | | | Q2 | 120 | 25 | 95 | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
            "Generated Sales Questions:\n",
            "Average Price (in thousand $). Generate 3 questions based on the following sales data: Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). The table should contain at least 4 rows in the following format: | A | 100 | 50 | 0.5 | | | | | | B | 150 | 75 | 0.5 | | | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
            "Generated Student Questions:\n",
            "Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: Student Name, Age, Math Score, and Physics Score. Generate 3 questions based on the following student data: Student Name, Age, Math Score, Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: Student Name, Age, Math Score, Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: | John Doe | 20 | 85 | 90 | | | | |\n",
            "Testing on Generated Financial Data:\n",
            "Context:\n",
            "Generate a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in million $), and Expenses (in million $) | Q1 | 100 | 20 | 80 | | Q2 | 120 | 25 | 95 | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...\n",
            "Question: on the following financial data:: Generate a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in million $), and Expenses (in million $) | Q1 | 100 | 20 | 80 | | | Q2 | 120 | 25 | 95 | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
            "T5 Answer: True\n",
            "BART Answer: Generate a table with financial data for a company including columns for Quarter, Revenue (in million $), Profit (in certain amounts), and Expenses (in millions of dollars) | Q1 | 100 | 20 | 80 | | Q2 | 120 | 25 | 95 | | ... | ... . ... |... | Q3 | 90 | 50 | |  Question: on the following financial data:: Generate a chart with the following columns for a business: Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11, Q12, Q13, Q14, Q15, Q16, Q17, Q18\n",
            "Roberta Answer: <s>\n",
            "\n",
            "Testing on Generated Sales Data:\n",
            "Context:\n",
            "Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). The table should contain at least 4 rows of data in the following format: | A | 100 | 50 | 0.5 | | | B | 150 | 75 | 0.5 | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...\n",
            "Question: Average Price (in thousand $). Generate 3 questions based on the following sales data: Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). The table should contain at least 4 rows in the following format: | A | 100 | 50 | 0.5 | | | | | | B | 150 | 75 | 0.5 | | | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
            "T5 Answer: Revenue (in thousand $), and Average Price (in thousand $). Generate 3 questions based on the following sales data: Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). Generate 3 questions based on the following sales data: Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). Generate 3 questions based on the following sales data: Sales (in units), Revenue (in thousand $), and Average Price (in thousand $). Generate 3 questions\n",
            "BART Answer: Generate 3 questions based on the following sales data: Sales (in units), Revenue (in thousand $), and Average Price (in Thousand $). The table should contain at least 4 rows in the following format: | A | 100 | 50 | 0.5 | | | B | 150 | 75 | 0% | |   |  | | A Question: Average Unit (in thousands $). B Question: Revenue (million $). C Question: Total Revenue (millions $).   Question: Total Unit (million dollars).  Question: Price (millennium dollars).Question: Revenue Question: Sales QuestionQuestion: Average PriceQuestion: PriceQuestion Question Question QuestionQuestion QuestionQuestionQuestionQuestion QuestionAnswer Question\n",
            "Roberta Answer: <s>\n",
            "\n",
            "Testing on Generated Student Data:\n",
            "Context:\n",
            "Student Name, Age, Math Score, Physics Score, and Chemistry Score. The table should contain at least 4 rows of data in the following format: | John Doe | 20 | 85 | 90 | | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
            "Question: Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: Student Name, Age, Math Score, and Physics Score. Generate 3 questions based on the following student data: Student Name, Age, Math Score, Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: Student Name, Age, Math Score, Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: | John Doe | 20 | 85 | 90 | | | | |\n",
            "T5 Answer: Generate 3 questions based on the following student data: Student Name, Age, Math Score, Physics Score, and Chemistry Score. Generate 3 questions based on the following student data: John Doe | 20 | 85 | 90 | | | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
            "BART Answer: Student Name, Age, Math Score, Physics Score, and Chemistry Score. The table should contain at least 4 rows of data in the following format: | John Doe | 20 | 85 | 90 | | |... | ... | ... . ... | | ... ... | . | | . . . |   |  | | Student Question: Student Name, age, and Physics Score. Generate 3 questions based on the following student data: Student name, age.Student Question: Physics score, and chemistry score.Generate 3 Questions Based on the Following Student Data:Student Name.Student Age.Student Number.Student Score.Student Date.Student Year.Student Name: John Doe.Student age: 20.Student year\n",
            "Roberta Answer: <s>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "import re\n",
        "import openai  # Для использования GPT-3\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration  # Для использования BART\n",
        "\n",
        "# Загрузка модели и токенизатора T5\n",
        "model_name = 't5-base'\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name)\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Функция для генерации текста с помощью модели T5\n",
        "def generate_text(prompt):\n",
        "    input_ids = tokenizer_t5.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = model_t5.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Промпты для генерации данных с более структурированным выводом\n",
        "financial_prompt = \"\"\"\n",
        "Создайте таблицу с финансовыми данными компании, включающую столбцы: Квартал, Доход (в миллионах $), Прибыль (в миллионах $) и Расходы (в миллионах $). Таблица должна содержать как минимум 4 строки данных в следующем формате:\n",
        "| Квартал | Доход | Прибыль | Расходы |\n",
        "|---------|-------|---------|---------|\n",
        "| Q1      | 100   | 20      | 80      |\n",
        "| Q2      | 120   | 25      | 95      |\n",
        "| ...     | ...   | ...     | ...     |\n",
        "\"\"\"\n",
        "sales_prompt = \"\"\"\n",
        "Создайте таблицу с данными о продажах продуктов, включающую столбцы: Продукт, Продажи (в штуках), Доход (в тысячах $) и Средняя цена (в тысячах $). Таблица должна содержать как минимум 4 строки данных в следующем формате:\n",
        "| Продукт | Продажи | Доход | Средняя цена |\n",
        "|---------|---------|-------|--------------|\n",
        "| A       | 100     | 50    | 0.5          |\n",
        "| B       | 150     | 75    | 0.5          |\n",
        "| ...     | ...     | ...   | ...          |\n",
        "\"\"\"\n",
        "student_prompt = \"\"\"\n",
        "Создайте таблицу со статистикой студентов, включающую столбцы: Имя студента, Возраст, Оценка по математике, Оценка по физике и Оценка по химии. Таблица должна содержать как минимум 4 строки данных в следующем формате:\n",
        "| Имя студента | Возраст | Оценка по математике | Оценка по физике | Оценка по химии |\n",
        "|--------------|---------|----------------------|------------------|-----------------|\n",
        "| Джон Доэ     | 20      | 85                   | 90               | 88              |\n",
        "| Джейн Смит   | 21      | 88                   | 85               | 90              |\n",
        "| ...          | ...     | ...                  | ...              | ...             |\n",
        "\"\"\"\n",
        "\n",
        "# Генерация данных\n",
        "financial_data_text = generate_text(financial_prompt)\n",
        "sales_data_text = generate_text(sales_prompt)\n",
        "student_data_text = generate_text(student_prompt)\n",
        "\n",
        "print(\"Generated Financial Data:\")\n",
        "print(financial_data_text)\n",
        "print(\"Generated Sales Data:\")\n",
        "print(sales_data_text)\n",
        "print(\"Generated Student Data:\")\n",
        "print(student_data_text)\n",
        "\n",
        "# Функция для преобразования текстовой таблицы в DataFrame\n",
        "def text_to_dataframe(text):\n",
        "    # Удаление пробелов и символов новой строки\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Разделение на строки\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Извлечение заголовков столбцов\n",
        "    headers = []\n",
        "    for line in lines:\n",
        "        if '|' in line and '---' not in line:\n",
        "            headers = [header.strip() for header in line.split('|') if header.strip()]\n",
        "            break\n",
        "\n",
        "    # Извлечение данных строк\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        if '|' in line and '---' not in line and len(headers) > 0:\n",
        "            row = [cell.strip() for cell in line.split('|') if cell.strip()]\n",
        "            if len(row) == len(headers):\n",
        "                data.append(row)\n",
        "\n",
        "    # Создание DataFrame\n",
        "    df = pd.DataFrame(data, columns=headers)\n",
        "    return df\n",
        "\n",
        "# Преобразование сгенерированных данных в DataFrame\n",
        "try:\n",
        "    financial_df = text_to_dataframe(financial_data_text)\n",
        "    sales_df = text_to_dataframe(sales_data_text)\n",
        "    student_df = text_to_dataframe(student_data_text)\n",
        "except Exception as e:\n",
        "    print(f\"Error converting text to DataFrame: {e}\")\n",
        "\n",
        "print(\"Financial Data DataFrame:\")\n",
        "print(financial_df)\n",
        "print(\"Sales Data DataFrame:\")\n",
        "print(sales_df)\n",
        "print(\"Student Data DataFrame:\")\n",
        "print(student_df)\n",
        "\n",
        "# Промпты для генерации вопросов на русском языке\n",
        "financial_questions_prompt = f\"\"\"\n",
        "Создайте 3 вопроса на основе следующих финансовых данных:\n",
        "{financial_data_text}\n",
        "\"\"\"\n",
        "sales_questions_prompt = f\"\"\"\n",
        "Создайте 3 вопроса на основе следующих данных о продажах:\n",
        "{sales_data_text}\n",
        "\"\"\"\n",
        "student_questions_prompt = f\"\"\"\n",
        "Создайте 3 вопроса на основе следующих данных о студентах:\n",
        "{student_data_text}\n",
        "\"\"\"\n",
        "\n",
        "# Генерация вопросов\n",
        "financial_questions_text = generate_text(financial_questions_prompt)\n",
        "sales_questions_text = generate_text(sales_questions_prompt)\n",
        "student_questions_text = generate_text(student_questions_prompt)\n",
        "\n",
        "print(\"Generated Financial Questions:\")\n",
        "print(financial_questions_text)\n",
        "print(\"Generated Sales Questions:\")\n",
        "print(sales_questions_text)\n",
        "print(\"Generated Student Questions:\")\n",
        "print(student_questions_text)\n",
        "\n",
        "# Загрузка модели и токенизатора Roberta для QA\n",
        "qa_model_name = \"deepset/roberta-large-squad2\"\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "model_roberta = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "\n",
        "# Функция для работы с моделью BART\n",
        "def use_bart(input_text):\n",
        "    bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "    bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "    inputs = bart_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = bart_model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Функция для работы с моделью GPT-3 (используем API)\n",
        "def use_gpt3(prompt):\n",
        "    openai.api_key = 'your_openai_api_key'  # Замените на ваш ключ API\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Функция для работы с моделью Roberta для QA\n",
        "def use_roberta(question, context):\n",
        "    inputs = tokenizer_roberta.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model_roberta(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer_roberta.convert_tokens_to_string(tokenizer_roberta.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "# Функция для тестирования моделей на вопросах\n",
        "def test_models(questions, context):\n",
        "    print(\"Context:\")\n",
        "    print(context)\n",
        "    for question in questions.split('\\n'):\n",
        "        if question.strip():\n",
        "            print(f\"Question: {question}\")\n",
        "\n",
        "            t5_input = f\"{context} \\n Question: {question}\"\n",
        "            t5_answer = generate_text(t5_input)\n",
        "            print(f\"T5 Answer: {t5_answer}\")\n",
        "\n",
        "            try:\n",
        "                bart_input = f\"{context} \\n Question: {question}\"\n",
        "                bart_answer = use_bart(bart_input)\n",
        "                print(f\"BART Answer: {bart_answer}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error using BART: {e}\")\n",
        "\n",
        "            try:\n",
        "                roberta_answer = use_roberta(question, context)\n",
        "                print(f\"Roberta Answer: {roberta_answer}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error using Roberta: {e}\")\n",
        "\n",
        "# Тестирование моделей на финансовых данных\n",
        "print(\"Testing on Generated Financial Data:\")\n",
        "test_models(financial_questions_text, financial_data_text)\n",
        "\n",
        "# Тестирование моделей на данных о продажах\n",
        "print(\"Testing on Generated Sales Data:\")\n",
        "test_models(sales_questions_text, sales_data_text)\n",
        "\n",
        "# Тестирование моделей на данных о студентах\n",
        "print(\"Testing on Generated Student Data:\")\n",
        "test_models(student_questions_text, student_data_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRG8bLI_os-o",
        "outputId": "2ec161b5-8ac2-4779-fc14-02f5b3eb8e9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Financial Data:\n",
            "(в миллиона $), рил (в миллиона $) и асод (в миллиона $). алиа долна содерат как минимум 4 строки даннми комании: | вартал | оод | рил\n",
            "Generated Sales Data:\n",
            "родукт, родаи (в тука), оод (в тса $) и редн ена (в тса $). алиа долна содерат как минимум 4 строки даннми в следуем ормате:\n",
            "Generated Student Data:\n",
            "ораст, енка о математике, енка о иике и енка о имии. алиа долна содерат как минимум 4 строки данн в следуем ормате: | м студента\n",
            "Financial Data DataFrame:\n",
            "  (в миллиона $), рил (в миллиона $) и асод (в миллиона $). алиа долна содерат как минимум 4 строки даннми комании:  \\\n",
            "0  (в миллиона $), рил (в миллиона $) и асод (в м...                                                                  \n",
            "\n",
            "   вартал  оод  рил  \n",
            "0  вартал  оод  рил  \n",
            "Sales Data DataFrame:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "Student Data DataFrame:\n",
            "  ораст, енка о математике, енка о иике и енка о имии. алиа долна содерат как минимум 4 строки данн в следуем ормате:  \\\n",
            "0  ораст, енка о математике, енка о иике и енка о...                                                                    \n",
            "\n",
            "   м студента  \n",
            "0  м студента  \n",
            "Generated Financial Questions:\n",
            "3 вороса на основе следуи инансов данн: (в миллиона $), рил (в миллиона $) и асод (в миллиона $). алиа долна содерат как минимум 4 строки даннми комании: | вар\n",
            "Generated Sales Questions:\n",
            "одате 3 вороса на основе следуи данн о родаа: родукт, родаи (в тука), оод (в тса $) и редн ена (в тса $). одате 3 вороса на основе следуи да\n",
            "Generated Student Questions:\n",
            "одате 3 вороса на основе следуи данн о студента: ораст, енка о математике, енка о иике и енка о имии. алиа долна содерат как минимум 4 строки данн\n",
            "Testing on Generated Financial Data:\n",
            "Context:\n",
            "(в миллиона $), рил (в миллиона $) и асод (в миллиона $). алиа долна содерат как минимум 4 строки даннми комании: | вартал | оод | рил\n",
            "Question: 3 вороса на основе следуи инансов данн: (в миллиона $), рил (в миллиона $) и асод (в миллиона $). алиа долна содерат как минимум 4 строки даннми комании: | вар\n",
            "T5 Answer: True\n",
            "BART Answer: (в миллиона $), рил (в следуи инансов данн). алиа долна содерат как вартал | оод | рiл  строки также общественный команииQuestion: 3 вороса на основе сказал\n",
            "Roberta Answer: <s>\n",
            "\n",
            "Testing on Generated Sales Data:\n",
            "Context:\n",
            "родукт, родаи (в тука), оод (в тса $) и редн ена (в тса $). алиа долна содерат как минимум 4 строки даннми в следуем ормате:\n",
            "Question: одате 3 вороса на основе следуи данн о родаа: родукт, родаи (в тука), оод (в тса $) и редн ена (в тса $). одате 3 вороса на основе следуи да\n",
            "T5 Answer: True\n",
            "BART Answer: родолна содерат как минимум 4 строки даннми в следуем ормате:  российские объявляется работаться на основе тука и редн ена (в тса $). так теперьб\n",
            "Roberta Answer: <s>\n",
            "\n",
            "Testing on Generated Student Data:\n",
            "Context:\n",
            "ораст, енка о математике, енка о иике и енка о имии. алиа долна содерат как минимум 4 строки данн в следуем ормате: | м студента\n",
            "Question: одате 3 вороса на основе следуи данн о студента: ораст, енка о математике, енка о иике и енка о имии. алиа долна содерат как минимум 4 строки данн\n",
            "T5 Answer: True\n",
            "BART Answer: Question: одате 3 вороса на основе следуи данн о студента: производительно ораст, енка и иике и только поставления. алиа долна содерат как минимум 4 �строки\n",
            "Roberta Answer: <s>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CgXrOeMrk8y",
        "outputId": "96680c4a-b5c6-4817-e934-aa5f081b3a1e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2020 The HuggingFace Datasets Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"TabFact: A Large-scale Dataset for Table-based Fact Verification\"\"\"\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@inproceedings{2019TabFactA,\n",
        "  title={TabFact : A Large-scale Dataset for Table-based Fact Verification},\n",
        "  author={Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou and William Yang Wang},\n",
        "  booktitle = {International Conference on Learning Representations (ICLR)},\n",
        "  address = {Addis Ababa, Ethiopia},\n",
        "  month = {April},\n",
        "  year = {2020}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "The problem of verifying whether a textual hypothesis holds the truth based on the given evidence, \\\n",
        "also known as fact verification, plays an important role in the study of natural language \\\n",
        "understanding and semantic representation. However, existing studies are restricted to \\\n",
        "dealing with unstructured textual evidence (e.g., sentences and passages, a pool of passages), \\\n",
        "while verification using structured forms of evidence, such as tables, graphs, and databases, remains unexplored. \\\n",
        "TABFACT is large scale dataset with 16k Wikipedia tables as evidence for 118k human annotated statements \\\n",
        "designed for fact verification with semi-structured evidence. \\\n",
        "The statements are labeled as either ENTAILED or REFUTED. \\\n",
        "TABFACT is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning.\n",
        "\"\"\"\n",
        "\n",
        "_HOMEPAGE = \"https://tabfact.github.io/\"\n",
        "\n",
        "_GIT_ARCHIVE_URL = (\n",
        "    \"https://github.com/wenhuchen/Table-Fact-Checking/archive/948b5560e2f7f8c9139bd91c7f093346a2bb56a8.zip\"\n",
        ")\n",
        "\n",
        "\n",
        "class TabFact(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"TabFact: A Large-scale Dataset for Table-based Fact Verification\"\"\"\n",
        "\n",
        "    VERSION = datasets.Version(\"1.0.0\")\n",
        "    BUILDER_CONFIGS = [\n",
        "        datasets.BuilderConfig(\n",
        "            name=\"tab_fact\",\n",
        "            version=datasets.Version(\"1.0.0\"),\n",
        "        ),\n",
        "        datasets.BuilderConfig(\n",
        "            name=\"blind_test\",\n",
        "            version=datasets.Version(\"1.0.0\"),\n",
        "            description=\"Blind test dataset\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    def _info(self):\n",
        "        features = {\n",
        "            \"id\": datasets.Value(\"int32\"),\n",
        "            \"table_id\": datasets.Value(\"string\"),\n",
        "            \"table_text\": datasets.Value(\"string\"),\n",
        "            \"table_caption\": datasets.Value(\"string\"),\n",
        "            \"statement\": datasets.Value(\"string\"),\n",
        "        }\n",
        "        if self.config.name == \"tab_fact\":\n",
        "            features[\"label\"] = datasets.ClassLabel(names=[\"refuted\", \"entailed\"])\n",
        "        else:\n",
        "            features[\"test_id\"] = datasets.Value(\"string\")\n",
        "\n",
        "        return datasets.DatasetInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            features=datasets.Features(features),\n",
        "            supervised_keys=None,\n",
        "            homepage=_HOMEPAGE,\n",
        "            citation=_CITATION,\n",
        "        )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        extracted_path = dl_manager.download_and_extract(_GIT_ARCHIVE_URL)\n",
        "\n",
        "        repo_path = os.path.join(extracted_path, \"Table-Fact-Checking-948b5560e2f7f8c9139bd91c7f093346a2bb56a8\")\n",
        "        all_csv_path = os.path.join(repo_path, \"data\", \"all_csv\")\n",
        "\n",
        "        if self.config.name == \"blind_test\":\n",
        "            test_file_path = os.path.join(repo_path, \"challenge\", \"blind_test.json\")\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TEST,\n",
        "                    gen_kwargs={\"statements_file\": test_file_path, \"all_csv_path\": all_csv_path},\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "        train_statements_file = os.path.join(repo_path, \"tokenized_data\", \"train_examples.json\")\n",
        "        val_statements_file = os.path.join(repo_path, \"tokenized_data\", \"val_examples.json\")\n",
        "        test_statements_file = os.path.join(repo_path, \"tokenized_data\", \"test_examples.json\")\n",
        "\n",
        "        return [\n",
        "            datasets.SplitGenerator(\n",
        "                name=datasets.Split.TRAIN,\n",
        "                gen_kwargs={\"statements_file\": train_statements_file, \"all_csv_path\": all_csv_path},\n",
        "            ),\n",
        "            datasets.SplitGenerator(\n",
        "                name=datasets.Split.VALIDATION,\n",
        "                gen_kwargs={\"statements_file\": val_statements_file, \"all_csv_path\": all_csv_path},\n",
        "            ),\n",
        "            datasets.SplitGenerator(\n",
        "                name=datasets.Split.TEST,\n",
        "                gen_kwargs={\"statements_file\": test_statements_file, \"all_csv_path\": all_csv_path},\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    def _generate_examples(self, statements_file, all_csv_path):\n",
        "        with open(statements_file, encoding=\"utf-8\") as f:\n",
        "            examples = json.load(f)\n",
        "\n",
        "        if self.config.name == \"blind_test\":\n",
        "            test_examples = self._generate_blind_test_examples(examples, all_csv_path)\n",
        "            for idx, example in test_examples:\n",
        "                yield idx, example\n",
        "        else:\n",
        "            for i, (table_id, example) in enumerate(examples.items()):\n",
        "                table_file_path = os.path.join(all_csv_path, table_id)\n",
        "                with open(table_file_path, encoding=\"utf-8\") as f:\n",
        "                    tabel_text = f.read()\n",
        "\n",
        "                statements, labels, caption = example\n",
        "\n",
        "                for statement_idx, (statement, label) in enumerate(zip(statements, labels)):\n",
        "                    yield f\"{i}_{statement_idx}\", {\n",
        "                        \"id\": i,\n",
        "                        \"table_id\": table_id,\n",
        "                        \"table_text\": tabel_text,\n",
        "                        \"table_caption\": caption,\n",
        "                        \"statement\": statement,\n",
        "                        \"label\": label,\n",
        "                    }\n",
        "\n",
        "    def _generate_blind_test_examples(self, examples, all_csv_path):\n",
        "        for i, (test_id, example) in enumerate(examples.items()):\n",
        "            statement, table_id, caption = example\n",
        "            table_file_path = os.path.join(all_csv_path, table_id)\n",
        "            with open(table_file_path, encoding=\"utf-8\") as f:\n",
        "                tabel_text = f.read()\n",
        "\n",
        "            yield i, {\n",
        "                \"id\": i,\n",
        "                \"test_id\": test_id,\n",
        "                \"table_id\": table_id,\n",
        "                \"table_text\": tabel_text,\n",
        "                \"table_caption\": caption,\n",
        "                \"statement\": statement,\n",
        "            }"
      ],
      "metadata": {
        "id": "8wqDxZN8rhYC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PICDK3qNsieb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Загрузка набора данных TabFact\n",
        "dataset = load_dataset('https://github.com/wenhuchen/Table-Fact-Checking/archive/948b5560e2f7f8c9139bd91c7f093346a2bb56a8.zip', 'tab_fact')\n",
        "\n",
        "# Преобразование данных в DataFrame для удобства работы\n",
        "train_data = dataset['train'].to_pandas()\n",
        "validation_data = dataset['validation'].to_pandas()\n",
        "\n",
        "# Загрузка моделей один раз\n",
        "model_name_t5 = 't5-base'\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name_t5)\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(model_name_t5)\n",
        "\n",
        "qa_model_name = \"deepset/roberta-large-squad2\"\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "model_roberta = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "# Функция для генерации текста с помощью модели T5\n",
        "def generate_text(prompt, tokenizer, model):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Функция для работы с моделью BART\n",
        "def use_bart(input_text, tokenizer, model):\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Функция для работы с моделью Roberta для QA\n",
        "def use_roberta(question, context, tokenizer, model):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    output = model(**inputs)\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "# Функция для тестирования моделей на вопросах\n",
        "def test_models(statement, table_text, label=None):\n",
        "    print(\"Context (Table):\")\n",
        "    print(table_text)\n",
        "    print(f\"Statement: {statement}\")\n",
        "\n",
        "    t5_input = f\"Table: {table_text}\\nStatement: {statement}\\nIs the statement true or false based on the table?\"\n",
        "    t5_answer = generate_text(t5_input, tokenizer_t5, model_t5)\n",
        "    print(f\"T5 Answer: {t5_answer}\")\n",
        "\n",
        "    try:\n",
        "        bart_input = f\"Table: {table_text}\\nStatement: {statement}\\nIs the statement true or false based on the table?\"\n",
        "        bart_answer = use_bart(bart_input, bart_tokenizer, bart_model)\n",
        "        print(f\"BART Answer: {bart_answer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error using BART: {e}\")\n",
        "\n",
        "    try:\n",
        "        roberta_answer = use_roberta(statement, table_text, tokenizer_roberta, model_roberta)\n",
        "        print(f\"Roberta Answer: {roberta_answer}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error using Roberta: {e}\")\n",
        "\n",
        "    if label:\n",
        "        print(f\"True Label: {label}\")\n",
        "\n",
        "# Пример использования на данных из набора данных TabFact\n",
        "for index, row in train_data.iterrows():\n",
        "    if index >= 5:  # Ограничение количества примеров для демонстрации\n",
        "        break\n",
        "\n",
        "    table_text = row['table_text']\n",
        "    statement = row['statement']\n",
        "    label = row['label'] if 'label' in row else None\n",
        "\n",
        "    print(f\"Example {index + 1}:\")\n",
        "    test_models(statement, table_text, label)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "1Cv9foBAsoli",
        "outputId": "781574f5-71d8-44d1-d411-7649acd47792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any data file at /content/https:/github.com/wenhuchen/Table-Fact-Checking/archive/948b5560e2f7f8c9139bd91c7f093346a2bb56a8.zip.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b239ad614a58>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Загрузка набора данных TabFact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://github.com/wenhuchen/Table-Fact-Checking/archive/948b5560e2f7f8c9139bd91c7f093346a2bb56a8.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tab_fact'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Преобразование данных в DataFrame для удобства работы\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2130\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         )\n\u001b[1;32m   1736\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any data file at {relative_to_absolute_path(path)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any data file at /content/https:/github.com/wenhuchen/Table-Fact-Checking/archive/948b5560e2f7f8c9139bd91c7f093346a2bb56a8.zip."
          ]
        }
      ]
    }
  ]
}