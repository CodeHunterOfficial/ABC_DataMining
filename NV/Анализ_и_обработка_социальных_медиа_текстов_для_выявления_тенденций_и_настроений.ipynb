{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM22DlUX5kmYqX/l735sekt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/ABC_DataMining/blob/main/NV/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%B8_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%BC%D0%B5%D0%B4%D0%B8%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2_%D0%B4%D0%BB%D1%8F_%D0%B2%D1%8B%D1%8F%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%82%D0%B5%D0%BD%D0%B4%D0%B5%D0%BD%D1%86%D0%B8%D0%B9_%D0%B8_%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "1wH6DFDQDyh1",
        "outputId": "f5281a5c-84d7-4dfa-e35c-bbb2a347e607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b7e0fb787882>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Загрузка данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdata_source_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mairline_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Предобработка текста\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ],
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from transformers import pipeline\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Загрузка данных https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment/data\n",
        "data_source_url = \"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\"\n",
        "airline_tweets = pd.read_csv(data_source_url)\n",
        "\n",
        "# Предобработка текста\n",
        "def preprocess_text(text):\n",
        "    # Удаление ссылок, хэштегов, упоминаний и специальных символов\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Оставляем только буквы\n",
        "    text = text.lower()  # Преобразуем в нижний регистр\n",
        "\n",
        "    # Токенизация, удаление стоп-слов и лемматизация\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Применяем предобработку к столбцу \"text\"\n",
        "airline_tweets['clean_lemmatized_text'] = airline_tweets['text'].apply(preprocess_text)\n",
        "\n",
        "# Векторизация текста с помощью TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=3000)\n",
        "X = vectorizer.fit_transform(airline_tweets['clean_lemmatized_text']).toarray()\n",
        "\n",
        "# Кодирование меток классов\n",
        "y = airline_tweets['airline_sentiment'].map({'positive': 2, 'neutral': 1, 'negative': 0})\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Массив моделей\n",
        "models = [\n",
        "    ('Naive Bayes', MultinomialNB()),\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('SVM', SVC(kernel='linear', random_state=42))\n",
        "]\n",
        "\n",
        "# Функция для оценки модели\n",
        "def evaluate_model(model, name, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n{name} Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Визуализация матрицы ошибок\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix ({name})')\n",
        "    plt.show()\n",
        "\n",
        "# Функция для построения графиков обучения\n",
        "def plot_learning_curve(model, X, y, name):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, label='Training Accuracy', color='blue', marker='o')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')\n",
        "\n",
        "    plt.plot(train_sizes, test_mean, label='Validation Accuracy', color='green', marker='s')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.15, color='green')\n",
        "\n",
        "    plt.title(f'Learning Curve ({name})')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Тематическое моделирование (LDA)\n",
        "def topic_modeling():\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(airline_tweets['clean_lemmatized_text'])\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X_tfidf)\n",
        "\n",
        "    def display_topics(model, feature_names, n_top_words):\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            print(f\"Topic #{topic_idx}:\")\n",
        "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "\n",
        "    display_topics(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n",
        "\n",
        "# Анализ трендов во времени\n",
        "def sentiment_trends_over_time():\n",
        "    airline_tweets['tweet_created'] = pd.to_datetime(airline_tweets['tweet_created'])\n",
        "    sentiment_over_time = airline_tweets.groupby([pd.Grouper(key='tweet_created', freq='D'), 'airline_sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for sentiment in ['positive', 'neutral', 'negative']:\n",
        "        plt.plot(sentiment_over_time.index, sentiment_over_time[sentiment], label=sentiment.capitalize())\n",
        "\n",
        "    plt.title('Sentiment Trends Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Tweets')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Сравнение тональности между авиакомпаниями\n",
        "def sentiment_by_airline():\n",
        "    sentiment_by_airline = airline_tweets.groupby(['airline', 'airline_sentiment']).size().unstack(fill_value=0)\n",
        "    sentiment_by_airline = sentiment_by_airline.div(sentiment_by_airline.sum(axis=1), axis=0)\n",
        "\n",
        "    sentiment_by_airline.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "    plt.title('Sentiment Distribution by Airline')\n",
        "    plt.xlabel('Airline')\n",
        "    plt.ylabel('Proportion of Sentiment')\n",
        "    plt.show()\n",
        "\n",
        "# Интерактивная визуализация\n",
        "def interactive_visualization():\n",
        "    fig = px.bar(sentiment_by_airline.reset_index(), x='airline', y=['positive', 'neutral', 'negative'],\n",
        "                 title='Sentiment Distribution by Airline', labels={'value': 'Proportion'})\n",
        "    fig.show()\n",
        "\n",
        "# Тестирование на новых данных\n",
        "def predict_sentiment(new_texts, vectorizer, model):\n",
        "    new_texts_preprocessed = [preprocess_text(text) for text in new_texts]\n",
        "    new_texts_vectorized = vectorizer.transform(new_texts_preprocessed).toarray()\n",
        "    predictions = model.predict(new_texts_vectorized)\n",
        "    sentiment_map = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
        "    return [sentiment_map[pred] for pred in predictions]\n",
        "\n",
        "# Основной блок кода\n",
        "if __name__ == \"__main__\":\n",
        "    # Оценка каждой модели\n",
        "    for name, model in models:\n",
        "        print(f\"Evaluating {name}...\")\n",
        "        evaluate_model(model, name, X_train, X_test, y_train, y_test)\n",
        "        plot_learning_curve(model, X_train, y_train, name)\n",
        "\n",
        "    # Тематическое моделирование\n",
        "    print(\"\\nTopic Modeling (LDA):\")\n",
        "    topic_modeling()\n",
        "\n",
        "    # Анализ трендов во времени\n",
        "    print(\"\\nSentiment Trends Over Time:\")\n",
        "    sentiment_trends_over_time()\n",
        "\n",
        "    # Сравнение тональности между авиакомпаниями\n",
        "    print(\"\\nSentiment Distribution by Airline:\")\n",
        "    sentiment_by_airline()\n",
        "\n",
        "    # Интерактивная визуализация\n",
        "    print(\"\\nInteractive Visualization:\")\n",
        "    interactive_visualization()\n",
        "\n",
        "    # Тестирование на новых данных\n",
        "    new_tweets = [\n",
        "        \"I love the service provided by Virgin America!\",\n",
        "        \"The flight was delayed and the staff was rude.\",\n",
        "        \"The seats were comfortable but the food was terrible.\"\n",
        "    ]\n",
        "    svm_model = models[3][1]  # SVM модель\n",
        "    predicted_sentiments = predict_sentiment(new_tweets, vectorizer, svm_model)\n",
        "    print(\"\\nPredicted Sentiments for New Tweets:\")\n",
        "    for tweet, sentiment in zip(new_tweets, predicted_sentiments):\n",
        "        print(f\"Tweet: \\\"{tweet}\\\" → Sentiment: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK для русского языка\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Загрузка данных\n",
        "train_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/train.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/test.csv')\n",
        "\n",
        "# Предобработка текста\n",
        "def preprocess_text(text):\n",
        "    # Удаление ссылок, хэштегов, упоминаний и специальных символов\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    text = re.sub(r\"[^а-яА-ЯёЁ]\", \" \", text)  # Оставляем только русские буквы\n",
        "    text = text.lower()  # Преобразуем в нижний регистр\n",
        "\n",
        "    # Токенизация, удаление стоп-слов и стемминг\n",
        "    tokens = word_tokenize(text, language='russian')\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    stemmer = SnowballStemmer('russian')\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Применяем предобработку к столбцу \"text\"\n",
        "train_data['clean_text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['clean_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Векторизация текста с помощью TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=3000)\n",
        "X_train = vectorizer.fit_transform(train_data['clean_text']).toarray()\n",
        "X_test = vectorizer.transform(test_data['clean_text']).toarray()\n",
        "\n",
        "# Кодирование меток классов\n",
        "y_train = train_data['category']\n",
        "y_test = test_data['category']\n",
        "\n",
        "# Массив моделей\n",
        "models = [\n",
        "    ('Naive Bayes', MultinomialNB()),\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('SVM', SVC(kernel='linear', random_state=42))\n",
        "]\n",
        "\n",
        "# Функция для оценки модели\n",
        "def evaluate_model(model, name, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n{name} Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Визуализация матрицы ошибок\n",
        "    labels = sorted(set(y_test))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix ({name})')\n",
        "    plt.show()\n",
        "\n",
        "# Тематическое моделирование (LDA)\n",
        "def topic_modeling():\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X_tfidf)\n",
        "\n",
        "    def display_topics(model, feature_names, n_top_words):\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            print(f\"Topic #{topic_idx}:\")\n",
        "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "\n",
        "    display_topics(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n",
        "\n",
        "# Анализ распределения категорий\n",
        "def category_distribution():\n",
        "    category_counts = train_data['category'].value_counts()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=category_counts.index, y=category_counts.values, palette='viridis')\n",
        "    plt.title('Category Distribution')\n",
        "    plt.xlabel('Category')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Интерактивная визуализация распределения категорий\n",
        "def interactive_category_distribution():\n",
        "    category_counts = train_data['category'].value_counts().reset_index()\n",
        "    category_counts.columns = ['Category', 'Count']\n",
        "    fig = px.bar(category_counts, x='Category', y='Count', title='Category Distribution', color='Category')\n",
        "    fig.show()\n",
        "\n",
        "# Тестирование на новых данных\n",
        "def predict_new_texts(new_texts, vectorizer, model):\n",
        "    new_texts_preprocessed = [preprocess_text(text) for text in new_texts]\n",
        "    new_texts_vectorized = vectorizer.transform(new_texts_preprocessed).toarray()\n",
        "    predictions = model.predict(new_texts_vectorized)\n",
        "    return predictions\n",
        "\n",
        "# Основной блок кода\n",
        "if __name__ == \"__main__\":\n",
        "    # Оценка каждой модели\n",
        "    for name, model in models:\n",
        "        print(f\"Evaluating {name}...\")\n",
        "        evaluate_model(model, name, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Тематическое моделирование\n",
        "    print(\"\\nTopic Modeling (LDA):\")\n",
        "    topic_modeling()\n",
        "\n",
        "    # Анализ распределения категорий\n",
        "    print(\"\\nCategory Distribution:\")\n",
        "    category_distribution()\n",
        "\n",
        "    # Интерактивная визуализация распределения категорий\n",
        "    print(\"\\nInteractive Category Distribution:\")\n",
        "    interactive_category_distribution()\n",
        "\n",
        "    # Тестирование на новых данных\n",
        "    new_texts = [\n",
        "        \"Хоккей — это отличный вид спорта!\",\n",
        "        \"Футболисты показали невероятную игру вчера.\",\n",
        "        \"Гонщики выступили на высшем уровне в этом сезоне.\"\n",
        "    ]\n",
        "    svm_model = models[3][1]  # SVM модель\n",
        "    predicted_categories = predict_new_texts(new_texts, vectorizer, svm_model)\n",
        "    print(\"\\nPredicted Categories for New Texts:\")\n",
        "    for text, category in zip(new_texts, predicted_categories):\n",
        "        print(f\"Text: \\\"{text}\\\" → Category: {category}\")"
      ],
      "metadata": {
        "id": "BsrdnTF1JVzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Возможные улучшения:\n",
        "1. **Балансировка данных**: Если данные несбалансированы, можно применить методы oversampling или undersampling.\n",
        "2. **Гиперпараметрическая настройка**: Использовать GridSearchCV или RandomizedSearchCV для настройки гиперпараметров моделей.\n",
        "3. **Другие методы векторизации**: Попробовать использовать Word2Vec, GloVe или BERT для получения более качественных эмбеддингов.\n",
        "4. **Ансамблирование моделей**: Использовать ансамбли моделей для улучшения качества предсказаний.\n",
        "5. **Кросс-валидация**: Вместо простого разделения на train/test использовать кросс-валидацию для более надежной оценки моделей.\n",
        "6. **Логирование и сохранение моделей**: Добавить логирование процесса обучения и сохранение обученных моделей для последующего использования.\n"
      ],
      "metadata": {
        "id": "Z657mJNrVuse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models import Word2Vec\n",
        "import joblib\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK для русского языка\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TextClassifier:\n",
        "    def __init__(self, vectorization_method='tfidf', balance_data_flag=True, log_file='model_training.log'):\n",
        "        self.vectorization_method = vectorization_method\n",
        "        self.balance_data_flag = balance_data_flag\n",
        "        self.vectorizer = None\n",
        "        self.models = [\n",
        "            ('Naive Bayes', MultinomialNB()),\n",
        "            ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "            ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('SVM', SVC(kernel='linear', random_state=42))\n",
        "        ]\n",
        "        self.best_models = {}\n",
        "        self.ensemble_model = None\n",
        "        self.log_file = log_file\n",
        "        logging.basicConfig(filename=log_file, level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Предобработка текста.\"\"\"\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "        text = re.sub(r\"[^а-яА-ЯёЁ]\", \" \", text)\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text, language='russian')\n",
        "        stop_words = set(stopwords.words('russian'))\n",
        "        stemmer = SnowballStemmer('russian')\n",
        "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def vectorize_text(self, texts, method='tfidf'):\n",
        "        \"\"\"Векторизация текста.\"\"\"\n",
        "        if method == 'tfidf':\n",
        "            if not self.vectorizer:\n",
        "                self.vectorizer = TfidfVectorizer(max_features=3000)\n",
        "                return self.vectorizer.fit_transform(texts).toarray()\n",
        "            else:\n",
        "                return self.vectorizer.transform(texts).toarray()\n",
        "        elif method == 'word2vec':\n",
        "            return self.word2vec_vectorization(texts)\n",
        "        elif method == 'bert':\n",
        "            return self.bert_vectorization(texts)\n",
        "        else:\n",
        "            raise ValueError(\"Неверный метод векторизации. Выберите 'tfidf', 'word2vec' или 'bert'.\")\n",
        "\n",
        "    def word2vec_vectorization(self, texts, vector_size=100, window=5, min_count=1):\n",
        "        \"\"\"Векторизация с использованием Word2Vec.\"\"\"\n",
        "        tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "        word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window, min_count=min_count)\n",
        "        vectors = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "                            or [np.zeros(vector_size)], axis=0) for words in tokenized_texts])\n",
        "        return vectors\n",
        "\n",
        "    def bert_vectorization(self, texts):\n",
        "        \"\"\"Векторизация с использованием BERT.\"\"\"\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "        return embeddings\n",
        "\n",
        "    def balance_data(self, X, y):\n",
        "        \"\"\"Балансировка данных с использованием SMOTE.\"\"\"\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "    def hyperparameter_tuning(self, model, param_grid, X, y):\n",
        "        \"\"\"Гиперпараметрическая настройка с использованием GridSearchCV.\"\"\"\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "        grid_search.fit(X, y)\n",
        "        logging.info(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
        "        logging.info(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "    def train_models(self, X_train, y_train):\n",
        "        \"\"\"Обучение и настройка моделей.\"\"\"\n",
        "        for name, model in self.models:\n",
        "            if name == 'Logistic Regression':\n",
        "                param_grid = {'C': [0.1, 1, 10], 'penalty': ['l2']}\n",
        "                best_model = self.hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
        "            elif name == 'Random Forest':\n",
        "                param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "                best_model = self.hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
        "            else:\n",
        "                best_model = model\n",
        "            self.best_models[name] = best_model\n",
        "\n",
        "    def ensemble_models(self, X_train, y_train):\n",
        "        \"\"\"Ансамблирование моделей.\"\"\"\n",
        "        estimators = [(name, model) for name, model in self.best_models.items()]\n",
        "        self.ensemble_model = VotingClassifier(estimators=estimators, voting='hard')\n",
        "        self.ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    def evaluate_model(self, model, name, X_test, y_test):\n",
        "        \"\"\"Оценка модели.\"\"\"\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        logging.info(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "        logging.info(\"Confusion Matrix:\")\n",
        "        logging.info(confusion_matrix(y_test, y_pred))\n",
        "        logging.info(\"Classification Report:\")\n",
        "        logging.info(classification_report(y_test, y_pred))\n",
        "        return accuracy\n",
        "\n",
        "    def save_model(self, model, filename):\n",
        "        \"\"\"Сохранение модели.\"\"\"\n",
        "        joblib.dump(model, filename)\n",
        "        logging.info(f\"Model saved to {filename}\")\n",
        "\n",
        "    def run(self, train_data, test_data):\n",
        "        \"\"\"Основной метод для запуска классификации.\"\"\"\n",
        "        # Предобработка текста\n",
        "        train_data['clean_text'] = train_data['text'].apply(self.preprocess_text)\n",
        "        test_data['clean_text'] = test_data['text'].apply(self.preprocess_text)\n",
        "\n",
        "        # Векторизация текста\n",
        "        X_train = self.vectorize_text(train_data['clean_text'], method=self.vectorization_method)\n",
        "        X_test = self.vectorize_text(test_data['clean_text'], method=self.vectorization_method)\n",
        "        y_train = train_data['category']\n",
        "        y_test = test_data['category']\n",
        "\n",
        "        # Балансировка данных\n",
        "        if self.balance_data_flag:\n",
        "            X_train, y_train = self.balance_data(X_train, y_train)\n",
        "\n",
        "        # Обучение и настройка моделей\n",
        "        self.train_models(X_train, y_train)\n",
        "\n",
        "        # Ансамблирование моделей\n",
        "        self.ensemble_models(X_train, y_train)\n",
        "\n",
        "        # Оценка ансамбля\n",
        "        ensemble_accuracy = self.evaluate_model(self.ensemble_model, \"Ensemble Model\", X_test, y_test)\n",
        "        logging.info(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "        # Сохранение лучшей модели\n",
        "        self.save_model(self.ensemble_model, 'best_ensemble_model.pkl')\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Загрузка данных\n",
        "    train_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/train.csv')\n",
        "    test_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/test.csv')\n",
        "\n",
        "    # Создание и запуск классификатора\n",
        "    classifier = TextClassifier(vectorization_method='tfidf', balance_data_flag=True)\n",
        "    classifier.run(train_data, test_data)"
      ],
      "metadata": {
        "id": "1rirSaDHYWg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models import Word2Vec\n",
        "import joblib\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK для русского языка\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TextClassifier:\n",
        "    def __init__(self, vectorization_method='tfidf', balance_data_flag=True, log_file='model_training.log'):\n",
        "        self.vectorization_method = vectorization_method\n",
        "        self.balance_data_flag = balance_data_flag\n",
        "        self.vectorizer = None\n",
        "        self.models = [\n",
        "            ('Naive Bayes', MultinomialNB()),\n",
        "            ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "            ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('SVM', SVC(kernel='linear', random_state=42))\n",
        "        ]\n",
        "        self.best_models = {}\n",
        "        self.ensemble_model = None\n",
        "        self.log_file = log_file\n",
        "        logging.basicConfig(filename=log_file, level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Предобработка текста.\"\"\"\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "        text = re.sub(r\"[^а-яА-ЯёЁ]\", \" \", text)\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text, language='russian')\n",
        "        stop_words = set(stopwords.words('russian'))\n",
        "        stemmer = SnowballStemmer('russian')\n",
        "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def vectorize_text(self, texts, method='tfidf'):\n",
        "        \"\"\"Векторизация текста.\"\"\"\n",
        "        if method == 'tfidf':\n",
        "            if not self.vectorizer:\n",
        "                self.vectorizer = TfidfVectorizer(max_features=3000)\n",
        "                return self.vectorizer.fit_transform(texts).toarray()\n",
        "            else:\n",
        "                return self.vectorizer.transform(texts).toarray()\n",
        "        elif method == 'word2vec':\n",
        "            return self.word2vec_vectorization(texts)\n",
        "        elif method == 'bert':\n",
        "            return self.bert_vectorization(texts)\n",
        "        else:\n",
        "            raise ValueError(\"Неверный метод векторизации. Выберите 'tfidf', 'word2vec' или 'bert'.\")\n",
        "\n",
        "    def word2vec_vectorization(self, texts, vector_size=100, window=5, min_count=1):\n",
        "        \"\"\"Векторизация с использованием Word2Vec.\"\"\"\n",
        "        tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "        word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window, min_count=min_count)\n",
        "        vectors = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "                            or [np.zeros(vector_size)], axis=0) for words in tokenized_texts])\n",
        "        return vectors\n",
        "\n",
        "    def bert_vectorization(self, texts):\n",
        "        \"\"\"Векторизация с использованием BERT.\"\"\"\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "        return embeddings\n",
        "\n",
        "    def balance_data(self, X, y):\n",
        "        \"\"\"Балансировка данных с использованием SMOTE.\"\"\"\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "    def hyperparameter_tuning(self, model, param_grid, X, y):\n",
        "        \"\"\"Гиперпараметрическая настройка с использованием GridSearchCV.\"\"\"\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "        grid_search.fit(X, y)\n",
        "        logging.info(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
        "        logging.info(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "    def train_models(self, X_train, y_train):\n",
        "        \"\"\"Обучение и настройка моделей.\"\"\"\n",
        "        for name, model in self.models:\n",
        "            if name == 'Logistic Regression':\n",
        "                param_grid = {'C': [0.1, 1, 10], 'penalty': ['l2']}\n",
        "                best_model = self.hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
        "            elif name == 'Random Forest':\n",
        "                param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "                best_model = self.hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
        "            else:\n",
        "                best_model = model\n",
        "            self.best_models[name] = best_model\n",
        "\n",
        "    def ensemble_models(self, X_train, y_train):\n",
        "        \"\"\"Ансамблирование моделей.\"\"\"\n",
        "        estimators = [(name, model) for name, model in self.best_models.items()]\n",
        "        self.ensemble_model = VotingClassifier(estimators=estimators, voting='hard')\n",
        "        self.ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    def evaluate_model(self, model, name, X_test, y_test):\n",
        "        \"\"\"Оценка модели.\"\"\"\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        logging.info(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "        logging.info(\"Confusion Matrix:\")\n",
        "        logging.info(confusion_matrix(y_test, y_pred))\n",
        "        logging.info(\"Classification Report:\")\n",
        "        logging.info(classification_report(y_test, y_pred))\n",
        "        return accuracy\n",
        "\n",
        "    def save_model(self, model, filename):\n",
        "        \"\"\"Сохранение модели.\"\"\"\n",
        "        joblib.dump(model, filename)\n",
        "        logging.info(f\"Model saved to {filename}\")\n",
        "\n",
        "    def topic_modeling(self, train_data):\n",
        "        \"\"\"Тематическое моделирование (LDA).\"\"\"\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "        X_tfidf = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
        "        lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "        lda.fit(X_tfidf)\n",
        "\n",
        "        def display_topics(model, feature_names, n_top_words):\n",
        "            for topic_idx, topic in enumerate(model.components_):\n",
        "                print(f\"Topic #{topic_idx}:\")\n",
        "                print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "\n",
        "        display_topics(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n",
        "\n",
        "    def category_distribution(self, train_data):\n",
        "        \"\"\"Анализ распределения категорий.\"\"\"\n",
        "        category_counts = train_data['category'].value_counts()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=category_counts.index, y=category_counts.values, palette='viridis')\n",
        "        plt.title('Category Distribution')\n",
        "        plt.xlabel('Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "\n",
        "    def interactive_category_distribution(self, train_data):\n",
        "        \"\"\"Интерактивная визуализация распределения категорий.\"\"\"\n",
        "        category_counts = train_data['category'].value_counts().reset_index()\n",
        "        category_counts.columns = ['Category', 'Count']\n",
        "        fig = px.bar(category_counts, x='Category', y='Count', title='Category Distribution', color='Category')\n",
        "        fig.show()\n",
        "\n",
        "    def predict_new_texts(self, new_texts):\n",
        "        \"\"\"Тестирование на новых данных.\"\"\"\n",
        "        new_texts_preprocessed = [self.preprocess_text(text) for text in new_texts]\n",
        "        new_texts_vectorized = self.vectorizer.transform(new_texts_preprocessed).toarray()\n",
        "        predictions = self.ensemble_model.predict(new_texts_vectorized)\n",
        "        return predictions\n",
        "\n",
        "    def plot_learning_curve(self, model, X, y, name):\n",
        "        \"\"\"Функция для построения графиков обучения.\"\"\"\n",
        "        train_sizes, train_scores, test_scores = learning_curve(\n",
        "            model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "        )\n",
        "        train_mean = np.mean(train_scores, axis=1)\n",
        "        train_std = np.std(train_scores, axis=1)\n",
        "        test_mean = np.mean(test_scores, axis=1)\n",
        "        test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_sizes, train_mean, label='Training Accuracy', color='blue', marker='o')\n",
        "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')\n",
        "        plt.plot(train_sizes, test_mean, label='Validation Accuracy', color='green', marker='s')\n",
        "        plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.15, color='green')\n",
        "        plt.title(f'Learning Curve ({name})')\n",
        "        plt.xlabel('Training Set Size')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_tsne(self, X, y):\n",
        "        \"\"\"Построение облака точек с помощью t-SNE.\"\"\"\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_encoded, cmap='viridis', alpha=0.7)\n",
        "        plt.title('t-SNE Visualization of Data')\n",
        "        plt.xlabel('t-SNE Component 1')\n",
        "        plt.ylabel('t-SNE Component 2')\n",
        "        plt.colorbar(scatter, ticks=range(len(le.classes_)), label='Category')\n",
        "        plt.show()\n",
        "\n",
        "    def run(self, train_data, test_data):\n",
        "        \"\"\"Основной метод для запуска классификации.\"\"\"\n",
        "        # Предобработка текста\n",
        "        train_data['clean_text'] = train_data['text'].apply(self.preprocess_text)\n",
        "        test_data['clean_text'] = test_data['text'].apply(self.preprocess_text)\n",
        "\n",
        "        # Векторизация текста\n",
        "        X_train = self.vectorize_text(train_data['clean_text'], method=self.vectorization_method)\n",
        "        X_test = self.vectorize_text(test_data['clean_text'], method=self.vectorization_method)\n",
        "        y_train = train_data['category']\n",
        "        y_test = test_data['category']\n",
        "\n",
        "        # Балансировка данных\n",
        "        if self.balance_data_flag:\n",
        "            X_train, y_train = self.balance_data(X_train, y_train)\n",
        "\n",
        "        # Обучение и настройка моделей\n",
        "        self.train_models(X_train, y_train)\n",
        "\n",
        "        # Ансамблирование моделей\n",
        "        self.ensemble_models(X_train, y_train)\n",
        "\n",
        "        # Оценка ансамбля\n",
        "        ensemble_accuracy = self.evaluate_model(self.ensemble_model, \"Ensemble Model\", X_test, y_test)\n",
        "        logging.info(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "        # Сохранение лучшей модели\n",
        "        self.save_model(self.ensemble_model, 'best_ensemble_model.pkl')\n",
        "\n",
        "        # Дополнительные анализы\n",
        "        self.topic_modeling(train_data)\n",
        "        self.category_distribution(train_data)\n",
        "        self.interactive_category_distribution(train_data)\n",
        "        self.plot_learning_curve(self.ensemble_model, X_train, y_train, \"Ensemble Model\")\n",
        "        self.plot_tsne(X_train, y_train)\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Загрузка данных\n",
        "    train_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/train.csv')\n",
        "    test_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/test.csv')\n",
        "\n",
        "    # Создание и запуск классификатора\n",
        "    classifier = TextClassifier(vectorization_method='tfidf', balance_data_flag=True)\n",
        "    classifier.run(train_data, test_data)"
      ],
      "metadata": {
        "id": "YyPdcm33ccnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xH9ZUeTpedQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HTMLReportGenerator:\n",
        "    def __init__(self, results):\n",
        "        \"\"\"\n",
        "        Инициализация генератора HTML-отчета.\n",
        "\n",
        "        :param results: Словарь с результатами анализа.\n",
        "        \"\"\"\n",
        "        self.results = results\n",
        "\n",
        "    def create_html_report(self, output_path=\"report.html\"):\n",
        "        \"\"\"\n",
        "        Создание HTML-отчета на основе результатов анализа.\n",
        "\n",
        "        :param output_path: Путь для сохранения HTML-файла.\n",
        "        \"\"\"\n",
        "        # Начало HTML-документа\n",
        "        html = \"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html lang=\"en\">\n",
        "        <head>\n",
        "            <meta charset=\"UTF-8\">\n",
        "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "            <title>Text Classification Report</title>\n",
        "            <style>\n",
        "                body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "                h1, h2, h3 { color: #333; }\n",
        "                table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }\n",
        "                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
        "                th { background-color: #f4f4f4; }\n",
        "                img { max-width: 100%; height: auto; }\n",
        "                .section { margin-bottom: 20px; }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "        \"\"\"\n",
        "\n",
        "        # Заголовок отчета\n",
        "        html += \"<h1>Text Classification Report</h1>\\n\"\n",
        "\n",
        "        # Таблица метрик моделей\n",
        "        html += \"<div class='section'>\\n\"\n",
        "        html += \"  <h2>Model Metrics</h2>\\n\"\n",
        "        html += \"  <table>\\n\"\n",
        "        html += \"    <tr><th>Model Name</th><th>Accuracy</th></tr>\\n\"\n",
        "        for metric in self.results.get('model_metrics', []):\n",
        "            html += f\"    <tr><td>{metric['model_name']}</td><td>{metric['accuracy']}</td></tr>\\n\"\n",
        "        html += \"  </table>\\n\"\n",
        "        html += \"</div>\\n\"\n",
        "\n",
        "        # Графики и оценки для каждой модели\n",
        "        for name, lc in self.results.get('learning_curves', {}).items():\n",
        "            html += f\"<div class='section'>\\n\"\n",
        "            html += f\"  <h3>Learning Curve for {name}</h3>\\n\"\n",
        "            html += f\"  <img src='data:image/png;base64,{lc}' alt='{name} Learning Curve'>\\n\"\n",
        "            html += \"</div>\\n\"\n",
        "\n",
        "        for name, cm in self.results.get('confusion_matrices', {}).items():\n",
        "            html += f\"<div class='section'>\\n\"\n",
        "            html += f\"  <h3>Confusion Matrix for {name}</h3>\\n\"\n",
        "            html += f\"  <img src='data:image/png;base64,{cm}' alt='{name} Confusion Matrix'>\\n\"\n",
        "            html += \"</div>\\n\"\n",
        "\n",
        "        # Наилучшая модель\n",
        "        best_model_metrics = self.results.get('best_model_metrics', {})\n",
        "        if best_model_metrics:\n",
        "            html += \"<div class='section'>\\n\"\n",
        "            html += \"  <h2>Best Model</h2>\\n\"\n",
        "            html += f\"  <p><strong>Model Name:</strong> {best_model_metrics.get('model_name', 'N/A')}</p>\\n\"\n",
        "            html += f\"  <p><strong>Accuracy:</strong> {best_model_metrics.get('accuracy', 'N/A')}</p>\\n\"\n",
        "            html += \"</div>\\n\"\n",
        "\n",
        "        # Кривая обучения для наилучшей модели\n",
        "        best_model_learning_curve = self.results.get('best_model_learning_curve', '')\n",
        "        if best_model_learning_curve:\n",
        "            html += \"<div class='section'>\\n\"\n",
        "            html += \"  <h3>Learning Curve for Best Model</h3>\\n\"\n",
        "            html += f\"  <img src='data:image/png;base64,{best_model_learning_curve}' alt='Best Model Learning Curve'>\\n\"\n",
        "            html += \"</div>\\n\"\n",
        "\n",
        "        # Матрица путаницы для наилучшей модели\n",
        "        best_model_confusion_matrix = self.results.get('best_model_confusion_matrix', '')\n",
        "        if best_model_confusion_matrix:\n",
        "            html += \"<div class='section'>\\n\"\n",
        "            html += \"  <h3>Confusion Matrix for Best Model</h3>\\n\"\n",
        "            html += f\"  <img src='data:image/png;base64,{best_model_confusion_matrix}' alt='Best Model Confusion Matrix'>\\n\"\n",
        "            html += \"</div>\\n\"\n",
        "\n",
        "        # Конец HTML-документа\n",
        "        html += \"</body>\\n</html>\"\n",
        "\n",
        "        # Сохранение HTML-файла\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(html)\n",
        "\n",
        "        print(f\"HTML report generated: {output_path}\")"
      ],
      "metadata": {
        "id": "MEIZph4sjG3-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models import Word2Vec\n",
        "import joblib\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from jinja2 import Environment, FileSystemLoader\n",
        "\n",
        "# Скачиваем необходимые ресурсы NLTK для русского языка\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TextClassifier:\n",
        "    def __init__(self, vectorization_method='tfidf', balance_data_flag=True, log_file='model_training.log'):\n",
        "        self.vectorization_method = vectorization_method\n",
        "        self.balance_data_flag = balance_data_flag\n",
        "        self.vectorizer = None\n",
        "        self.models = [\n",
        "            ('Naive Bayes', MultinomialNB()),\n",
        "            ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "            ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "            ('SVM', SVC(kernel='linear', random_state=42))\n",
        "        ]\n",
        "        self.best_models = {}\n",
        "        self.ensemble_model = None\n",
        "        self.results = {}  # Словарь для хранения всех результатов\n",
        "        self.log_file = log_file\n",
        "        logging.basicConfig(filename=log_file, level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Предобработка текста\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "        text = re.sub(r\"[^а-яА-ЯёЁ]\", \" \", text)\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text, language='russian')\n",
        "        stop_words = set(stopwords.words('russian'))\n",
        "        stemmer = SnowballStemmer('russian')\n",
        "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    # Векторизация текста\n",
        "    def vectorize_text(self, texts, method='tfidf'):\n",
        "        if method == 'tfidf':\n",
        "            if not self.vectorizer:\n",
        "                self.vectorizer = TfidfVectorizer(max_features=3000)\n",
        "                return self.vectorizer.fit_transform(texts).toarray()\n",
        "            else:\n",
        "                return self.vectorizer.transform(texts).toarray()\n",
        "        elif method == 'word2vec':\n",
        "            return self.word2vec_vectorization(texts)\n",
        "        elif method == 'bert':\n",
        "            return self.bert_vectorization(texts)\n",
        "        else:\n",
        "            raise ValueError(\"Неверный метод векторизации. Выберите 'tfidf', 'word2vec' или 'bert'.\")\n",
        "\n",
        "    # Векторизация с помощью Word2Vec\n",
        "    def word2vec_vectorization(self, texts, vector_size=100, window=5, min_count=1):\n",
        "        tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "        word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window, min_count=min_count)\n",
        "        vectors = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "                            or [np.zeros(vector_size)], axis=0) for words in tokenized_texts])\n",
        "        return vectors\n",
        "\n",
        "    # Векторизация с помощью BERT\n",
        "    def bert_vectorization(self, texts):\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "        return embeddings\n",
        "\n",
        "    # Балансировка данных\n",
        "    def balance_data(self, X, y):\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "    # Гиперпараметрическая настройка\n",
        "    def hyperparameter_tuning(self, model, param_grid, X, y):\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "        grid_search.fit(X, y)\n",
        "        logging.info(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
        "        logging.info(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "    # Обучение моделей\n",
        "    def train_models(self, X_train, y_train):\n",
        "        for name, model in self.models:\n",
        "            if name == 'Logistic Regression':\n",
        "                param_grid = {'C': [0.1, 1, 10], 'penalty': ['l2']}\n",
        "                best_model = self.hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
        "            elif name == 'Random Forest':\n",
        "                param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "                best_model = self.hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
        "            else:\n",
        "                best_model = model\n",
        "            self.best_models[name] = best_model\n",
        "\n",
        "    # Ансамблирование моделей\n",
        "    def ensemble_models(self, X_train, y_train):\n",
        "        estimators = [(name, model) for name, model in self.best_models.items()]\n",
        "        self.ensemble_model = VotingClassifier(estimators=estimators, voting='hard')\n",
        "        self.ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    # Оценка модели\n",
        "    def evaluate_model(self, model, name, X_test, y_test):\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        logging.info(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "        logging.info(\"Confusion Matrix:\")\n",
        "        logging.info(confusion_matrix(y_test, y_pred))\n",
        "        logging.info(\"Classification Report:\")\n",
        "        logging.info(classification_report(y_test, y_pred))\n",
        "        return accuracy\n",
        "\n",
        "    # Сохранение модели\n",
        "    def save_model(self, model, filename):\n",
        "        joblib.dump(model, filename)\n",
        "        logging.info(f\"Model saved to {filename}\")\n",
        "\n",
        "    # Тематическое моделирование (LDA)\n",
        "    def topic_modeling(self, train_data):\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "        X_tfidf = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
        "        lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "        lda.fit(X_tfidf)\n",
        "\n",
        "        def display_topics(model, feature_names, n_top_words):\n",
        "            topics = []\n",
        "            for topic_idx, topic in enumerate(model.components_):\n",
        "                top_words = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "                topics.append(f\"Topic #{topic_idx}: {top_words}\")\n",
        "            return topics\n",
        "\n",
        "        topics = display_topics(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n",
        "        return topics\n",
        "\n",
        "    # Анализ распределения категорий\n",
        "    def category_distribution(self, train_data):\n",
        "        category_counts = train_data['category'].value_counts()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=category_counts.index, y=category_counts.values, palette='viridis')\n",
        "        plt.title('Category Distribution')\n",
        "        plt.xlabel('Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        buffer = BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "        buffer.close()\n",
        "        plt.close()\n",
        "        return image_base64\n",
        "\n",
        "    # Интерактивная визуализация распределения категорий\n",
        "    def interactive_category_distribution(self, train_data):\n",
        "        category_counts = train_data['category'].value_counts().reset_index()\n",
        "        category_counts.columns = ['Category', 'Count']\n",
        "        fig = px.bar(category_counts, x='Category', y='Count', title='Category Distribution', color='Category')\n",
        "        return fig.to_html(full_html=False)\n",
        "\n",
        "    # Тестирование на новых данных\n",
        "    def predict_new_texts(self, new_texts):\n",
        "        new_texts_preprocessed = [self.preprocess_text(text) for text in new_texts]\n",
        "        new_texts_vectorized = self.vectorizer.transform(new_texts_preprocessed).toarray()\n",
        "        predictions = self.ensemble_model.predict(new_texts_vectorized)\n",
        "        return predictions\n",
        "\n",
        "    # Графики обучения\n",
        "    def plot_learning_curve(self, model, X, y, name):\n",
        "        train_sizes, train_scores, test_scores = learning_curve(\n",
        "            model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "        )\n",
        "        train_mean = np.mean(train_scores, axis=1)\n",
        "        train_std = np.std(train_scores, axis=1)\n",
        "        test_mean = np.mean(test_scores, axis=1)\n",
        "        test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(train_sizes, train_mean, label='Training Accuracy', color='blue', marker='o')\n",
        "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')\n",
        "        plt.plot(train_sizes, test_mean, label='Validation Accuracy', color='green', marker='s')\n",
        "        plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.15, color='green')\n",
        "        plt.title(f'Learning Curve ({name})')\n",
        "        plt.xlabel('Training Set Size')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid()\n",
        "\n",
        "        buffer = BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "        buffer.close()\n",
        "        plt.close()\n",
        "        return image_base64\n",
        "\n",
        "    # Облако точек (t-SNE)\n",
        "    def plot_tsne(self, X, y):\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_encoded, cmap='viridis', alpha=0.7)\n",
        "        plt.title('t-SNE Visualization of Data')\n",
        "        plt.xlabel('t-SNE Component 1')\n",
        "        plt.ylabel('t-SNE Component 2')\n",
        "        plt.colorbar(scatter, ticks=range(len(le.classes_)), label='Category')\n",
        "\n",
        "        buffer = BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "        buffer.close()\n",
        "        plt.close()\n",
        "        return image_base64\n",
        "\n",
        "    # Кластеризация текстов\n",
        "    def cluster_texts(self, X, method='kmeans', n_clusters=5):\n",
        "        if method == 'kmeans':\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "            clusters = kmeans.fit_predict(X)\n",
        "        elif method == 'dbscan':\n",
        "            dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "            clusters = dbscan.fit_predict(X)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported clustering method. Choose 'kmeans' or 'dbscan'.\")\n",
        "        return clusters\n",
        "\n",
        "    # Временной анализ\n",
        "    def temporal_analysis(self, data, date_col='date', freq='M'):\n",
        "        data[date_col] = pd.to_datetime(data[date_col])\n",
        "        data.set_index(date_col, inplace=True)\n",
        "        trends = data.resample(freq).size()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        trends.plot(title=f\"Temporal Trends ({freq})\", figsize=(10, 6))\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        buffer = BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "        buffer.close()\n",
        "        plt.close()\n",
        "        return image_base64\n",
        "\n",
        "    # Анализ настроений с использованием BERT\n",
        "    def sentiment_analysis_with_bert(self, texts):\n",
        "        tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "        model = BertModel.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        sentiment_scores = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "        sentiments = []\n",
        "        for score in sentiment_scores:\n",
        "            if score.mean() > 0.5:\n",
        "                sentiments.append('positive')\n",
        "            elif score.mean() < -0.5:\n",
        "                sentiments.append('negative')\n",
        "            else:\n",
        "                sentiments.append('neutral')\n",
        "        return sentiments\n",
        "\n",
        "    # Добавление дополнительных признаков для анализа настроений\n",
        "    def add_sentiment_features(self, data):\n",
        "        data['exclamation_count'] = data['text'].apply(lambda x: x.count('!'))\n",
        "        data['question_count'] = data['text'].apply(lambda x: x.count('?'))\n",
        "        data['uppercase_ratio'] = data['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
        "        return data\n",
        "\n",
        "    # Многомерный анализ (PCA)\n",
        "    def plot_pca(self, X, y):\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='viridis', alpha=0.7)\n",
        "        plt.title('PCA Visualization of Data')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.colorbar(scatter, ticks=range(len(le.classes_)), label='Category')\n",
        "\n",
        "        buffer = BytesIO()\n",
        "        plt.savefig(buffer, format='png')\n",
        "        buffer.seek(0)\n",
        "        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "        buffer.close()\n",
        "        plt.close()\n",
        "        return image_base64\n",
        "\n",
        "    # Основной метод для запуска классификации\n",
        "    def run(self, train_data, test_data):\n",
        "        results = {}\n",
        "\n",
        "        # Предобработка текста\n",
        "        train_data['clean_text'] = train_data['text'].apply(self.preprocess_text)\n",
        "        test_data['clean_text'] = test_data['text'].apply(self.preprocess_text)\n",
        "\n",
        "        # Векторизация текста\n",
        "        X_train = self.vectorize_text(train_data['clean_text'], method=self.vectorization_method)\n",
        "        X_test = self.vectorize_text(test_data['clean_text'], method=self.vectorization_method)\n",
        "        y_train = train_data['category']\n",
        "        y_test = test_data['category']\n",
        "\n",
        "        # Балансировка данных\n",
        "        if self.balance_data_flag:\n",
        "            X_train, y_train = self.balance_data(X_train, y_train)\n",
        "\n",
        "        # Обучение и настройка моделей\n",
        "        self.train_models(X_train, y_train)\n",
        "\n",
        "        # Ансамблирование моделей\n",
        "        self.ensemble_models(X_train, y_train)\n",
        "\n",
        "        # Оценка ансамбля\n",
        "        ensemble_accuracy = self.evaluate_model(self.ensemble_model, \"Ensemble Model\", X_test, y_test)\n",
        "        results['ensemble_accuracy'] = ensemble_accuracy\n",
        "\n",
        "        # Сохранение лучшей модели\n",
        "        self.save_model(self.ensemble_model, 'best_ensemble_model.pkl')\n",
        "\n",
        "        # Тематическое моделирование\n",
        "        topics = self.topic_modeling(train_data)\n",
        "        results['topics'] = topics\n",
        "\n",
        "        # Распределение категорий\n",
        "        category_distribution_image = self.category_distribution(train_data)\n",
        "        results['category_distribution_image'] = category_distribution_image\n",
        "\n",
        "        # Интерактивная визуализация распределения категорий\n",
        "        interactive_category_distribution_html = self.interactive_category_distribution(train_data)\n",
        "        results['interactive_category_distribution_html'] = interactive_category_distribution_html\n",
        "\n",
        "        # График обучения\n",
        "        learning_curve_image = self.plot_learning_curve(self.ensemble_model, X_train, y_train, \"Ensemble Model\")\n",
        "        results['learning_curve_image'] = learning_curve_image\n",
        "\n",
        "        # t-SNE визуализация\n",
        "        tsne_image = self.plot_tsne(X_train, y_train)\n",
        "        results['tsne_image'] = tsne_image\n",
        "\n",
        "        # Кластеризация текстов\n",
        "        clusters = self.cluster_texts(X_train, method='kmeans', n_clusters=5)\n",
        "        results['clusters'] = clusters.tolist()\n",
        "\n",
        "        # Временной анализ\n",
        "        if 'date' in train_data.columns:\n",
        "            temporal_analysis_image = self.temporal_analysis(train_data, date_col='date', freq='M')\n",
        "            results['temporal_analysis_image'] = temporal_analysis_image\n",
        "\n",
        "        # Анализ настроений\n",
        "        sentiments = self.sentiment_analysis_with_bert(train_data['clean_text'].tolist())\n",
        "        results['sentiments'] = sentiments\n",
        "\n",
        "        # Многомерный анализ (PCA)\n",
        "        pca_image = self.plot_pca(X_train, y_train)\n",
        "        results['pca_image'] = pca_image\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        self.results = results\n",
        "\n",
        "    def generate_html_report(self, output_path=\"report.html\"):\n",
        "        \"\"\"\n",
        "        Генерация HTML-отчета на основе сохраненных результатов.\n",
        "\n",
        "        :param output_path: Путь для сохранения HTML-файла.\n",
        "        \"\"\"\n",
        "        # Создаем экземпляр генератора отчета\n",
        "        report_generator = HTMLReportGenerator(self.results)\n",
        "        # Генерируем отчет\n",
        "        report_generator.create_html_report(output_path)"
      ],
      "metadata": {
        "id": "eauHTSK7eeOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Загрузка данных\n",
        "    train_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/train.csv')\n",
        "    test_data = pd.read_csv('/kaggle/input/russian-social-media-text-classification/test.csv')\n",
        "\n",
        "    # Создание и запуск классификатора\n",
        "    classifier = TextClassifier(vectorization_method='tfidf', balance_data_flag=True)\n",
        "    classifier.run(train_data, test_data)\n",
        "\n",
        "    # Генерация HTML-отчета\n",
        "    classifier.generate_html_report(output_path=\"report.html\")"
      ],
      "metadata": {
        "id": "COi_zkQhjYYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}